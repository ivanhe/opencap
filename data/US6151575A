US	O
6151575	O
A	O
20001121	O

US	O
08958957	O
19971028	O

eng	O
eng	O

US	O
08029828	O
19961028	O

US	O
08039571	O
19970228	O

20001121	O

20001121	O

7G	O
10L	O
13/00	O
A	O
7	O
G	O
10	O
L	O
13	O
00	O
A	O

G10L	O
15/00	O
20060101C	O
I20051008RMEP	O

20060101	O

C	O
G	O
10	O
L	O
15	O
00	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/06	O
20060101A	O
I20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
06	O
I	O

20051008	O

EP	O

R	O
M	O

US	O

704/260	O
704	O
260	O

704/231	O
704	O
231	O

704/236	O
704	O
236	O

704/239	O
704	O
239	O

704/240	O
704	O
240	O

704/247	O
704	O
247	O

704/254	O
704	O
254	O

704/255	O
704	O
255	O

704/E15.011	O
704	O
E15	O
.	O
011	O

G10L	O
15/06A3	O
G	O
10	O
L	O
15	O
06	O

A	O
3	O

US	O

704/231	O
704	O
231	O

US	O

704/236	O
704	O
236	O

US	O

704/239	O
704	O
239	O

US	O

704/240	O
704	O
240	O

US	O

704/247	O
704	O
247	O

US	O

704/254	O
704	O
254	O

US	O

704/255	O
704	O
255	O

25	O
Rapid	O
adaptation	O
of	O
speech	O
models	O

US	O
4759068	O
A	O
Bahl	O
et_al.	O

19880719	O

19850529	O

US	O
4805218	O
A	O
Bamberg	O
et_al.	O

19890214	O

19870403	O

US	O
4805219	O
A	O
Baker	O
et_al.	O

19890214	O

19870403	O

US	O
4817156	O
A	O
Bahl	O
et_al.	O

19890328	O

19870810	O

US	O
4817158	O
A	O
Picheny	O
19890328	O

19841019	O

US	O
4817161	O
A	O
Kaneko	O
19890328	O

19870319	O

US	O
4819271	O
A	O
Bahl	O
et_al.	O

19890404	O

19871216	O

US	O
4827521	O
A	O
Bahl	O
et_al.	O

19890502	O

19860327	O

US	O
4829576	O
A	O
Porter	O
19890509	O

19861021	O

US	O
4829577	O
A	O
Kuroda	O
et_al.	O

19890509	O

19870312	O

US	O
4831550	O
A	O
Katz	O
19890516	O

19860327	O

US	O
4833712	O
A	O
Bahl	O
et_al.	O

19890523	O

19850529	O

US	O
4837831	O
A	O
Gillick	O
et_al.	O

19890606	O

19861015	O

US	O
4876720	O
A	O
Kaneko	O
et_al.	O

19891024	O

19870312	O

US	O
4882759	O
A	O
Bahl	O
et_al.	O

19891121	O

19860418	O

US	O
4903305	O
A	O
Gillick	O
et_al.	O

19900220	O

19890323	O

US	O
4914703	O
A	O
Gillick	O
19900403	O

19861205	O

US	O
4926488	O
A	O
Nadas	O
et_al.	O

19900515	O

19870709	O

US	O
4931950	O
A	O
Isle	O
et_al.	O

19900605	O

19880725	O

US	O
4972485	O
A	O
Dautrich	O
et_al.	O

19901120	O

19890523	O

US	O
4980918	O
A	O
Bahl	O
et_al.	O

19901225	O

19850509	O

US	O
5027406	O
A	O
Roberts	O
et_al.	O

19910625	O

19881206	O

US	O
5031217	O
A	O
Nishimura	O
19910709	O

19890921	O

US	O
5033087	O
A	O
Bahl	O
et_al.	O

19910716	O

19890314	O

US	O
5036538	O
A	O
Oken	O
et_al.	O

19910730	O

19891122	O

US	O
5046099	O
A	O
Nishimura	O
19910903	O

19900227	O

US	O
5050215	O
A	O
Nishimura	O
19910917	O

19900510	O

US	O
5054074	O
A	O
Bakis	O
19911001	O

19900917	O

US	O
5054085	O
A	O
Meisel	O
et_al.	O

19911001	O

19901119	O

US	O
5072452	O
A	O
Brown	O
et_al.	O

19911210	O

19891102	O

US	O
5127055	O
A	O
Larkey	O
19920630	O

19910211	O

US	O
5129001	O
A	O
Bahl	O
et_al.	O

19920707	O

19900425	O

US	O
5170432	O
A	O
Hackbarth	O
et_al.	O

19921208	O

19900921	O

US	O
5182773	O
A	O
Bahl	O
et_al.	O

19930126	O

19910322	O

US	O
5202952	O
A	O
Gillick	O
et_al.	O

19930413	O

19900622	O

US	O
5276766	O
A	O
Bahl	O
et_al.	O

19940104	O

19910716	O

US	O
5278942	O
A	O
Bahl	O
et_al.	O

19940111	O

19911205	O

US	O
5280562	O
A	O
Bahl	O
et_al.	O

19940118	O

19911003	O

US	O
5280563	O
A	O
Ganong	O
19940118	O

19911220	O

US	O
5293451	O
A	O
Brown	O
et_al.	O

19940308	O

19901023	O

US	O
5428707	O
A	O
Gould	O
et_al.	O

19950627	O

19921113	O

US	O
5440663	O
A	O
Moese	O
et_al.	O

19950808	O

19930604	O

US	O
5467425	O
A	O
Lau	O
et_al.	O

19951114	O

19930226	O

US	O
5497447	O
A	O
Bahl	O
et_al.	O

19960305	O

19930308	O

US	O
5623578	O
A	O
Mikkilineni	O
19970422	O

19931028	O

US	O
5710864	O
A	O
Juang	O
et_al.	O

19980120	O

19941229	O

US	O
5715367	O
A	O
Gillick	O
et_al.	O

19980203	O

19950123	O

US	O
5793891	O
A	O
Takahashi	O
et_al.	O

19980811	O

19950703	O

US	O

704/231	O
704	O
231	O

US	O
5864810	O
A	O
Digalakis	O
et_al.	O

19990126	O

19950120	O

US	O

704/255	O
704	O
255	O

EP	O
238693	O
A1	O
19870930	O

19860327	O

EP	O
562138	O
A1	O
19930929	O

19920325	O

Asadi	B-Citation
,	I-Citation
Ayman	I-Citation
,	I-Citation
Automatic	I-Citation
Modeling	I-Citation
for	I-Citation
Adding	I-Citation
New	I-Citation
Words	I-Citation
to	I-Citation
a	I-Citation
Large	I-Citation
Vocabulary	I-Citation
,	I-Citation
ICASSP	I-Citation
91	I-Citation
,	I-Citation
vol.	I-Citation
1	I-Citation
(	I-Citation
1991	I-Citation
)	I-Citation
,	I-Citation
pp.	I-Citation
305	I-Citation
308	I-Citation
.	O

Bahl	B-Citation
,	I-Citation
Lalit	I-Citation
,	I-Citation
A	I-Citation
Maximum	I-Citation
Likelihood	I-Citation
Approach	I-Citation
to	I-Citation
Continuous	I-Citation
Speech	I-Citation
Recognition	I-Citation
,	I-Citation
IEEE	I-Citation
Transactions	I-Citation
on	I-Citation
Pattern	I-Citation
Analysis	I-Citation
and	I-Citation
Machine	I-Citation
Intelligence	I-Citation
,	I-Citation
vol.	I-Citation
PAMI	I-Citation
5	I-Citation
,	I-Citation
No.	I-Citation
2	I-Citation
(	I-Citation
Mar.	I-Citation
1983	I-Citation
)	I-Citation
,	I-Citation
pp.	I-Citation
179	I-Citation
190	I-Citation
.	O

Bahl	B-Citation
,	I-Citation
L.	I-Citation
R.	I-Citation
,	I-Citation
Adaptation	I-Citation
of	I-Citation
Large	I-Citation
Vocabulary	I-Citation
Recognition	I-Citation
System	I-Citation
,	I-Citation
ICASSP	I-Citation
92	I-Citation
,	I-Citation
vol.	I-Citation
1	I-Citation
(	I-Citation
Mar.	I-Citation
1992	I-Citation
)	I-Citation
,	I-Citation
pp.	I-Citation
1477	I-Citation
1480	I-Citation
.	O

Bahl	B-Citation
,	I-Citation
L.	I-Citation
R.	I-Citation
,	I-Citation
Automatic	I-Citation
Selection	I-Citation
of	I-Citation
Speech	I-Citation
Prototypes	I-Citation
,	I-Citation
IBM	I-Citation
Technical	I-Citation
Disclosure	I-Citation
Bulletin	I-Citation
,	I-Citation
vol.	I-Citation
24	I-Citation
,	I-Citation
No.	I-Citation
4	I-Citation
(	I-Citation
Sep.	I-Citation
1981	I-Citation
)	I-Citation
,	I-Citation
pp.	I-Citation
2042	I-Citation
2043	I-Citation
.	O

Bahl	B-Citation
,	I-Citation
L.	I-Citation
R.	I-Citation
,	I-Citation
Automatic	I-Citation
High	I-Citation
Resolution	I-Citation
Labeling	I-Citation
of	I-Citation
Speech	I-Citation
Waveforms	I-Citation
,	I-Citation
IBM	I-Citation
Technical	I-Citation
Disclosure	I-Citation
Bulletin	I-Citation
,	I-Citation
vol.	I-Citation
23	I-Citation
,	I-Citation
No.	I-Citation
7B	I-Citation
(	I-Citation
Dec.	I-Citation
1980	I-Citation
)	I-Citation
,	I-Citation
pp.	I-Citation
3466	I-Citation
3467	I-Citation
.	O

Bahl	B-Citation
,	I-Citation
L.	I-Citation
R.	I-Citation
et_al.	I-Citation
,	I-Citation
Constructing	I-Citation
Groups	I-Citation
of	I-Citation
Acoustically	I-Citation
Confusable	I-Citation
Words	I-Citation
,	I-Citation
IEEE	I-Citation
(	I-Citation
1990	I-Citation
)	I-Citation
,	I-Citation
pp.	I-Citation
85	I-Citation
88	I-Citation
.	O

Bamberg	B-Citation
,	I-Citation
Paul	I-Citation
G.	I-Citation
et_al.	I-Citation
,	I-Citation
Adaptation	I-Citation
Performance	I-Citation
in	I-Citation
a	I-Citation
Large	I-Citation
Vocabulary	I-Citation
Recognizer	I-Citation
,	I-Citation
Dragon	I-Citation
Systems	I-Citation
,	I-Citation
Inc.	I-Citation
,	I-Citation
Newton	I-Citation
,	I-Citation
MA	I-Citation
,	I-Citation
pp.	I-Citation
1	I-Citation
7	I-Citation
.	O

Fissore	B-Citation
,	I-Citation
Luciano	I-Citation
et_al.	I-Citation
,	I-Citation
Lexical	I-Citation
Access	I-Citation
to	I-Citation
Large	I-Citation
Vocabularies	I-Citation
For	I-Citation
Speech	I-Citation
Recognition	I-Citation
,	I-Citation
IEEE	I-Citation
Transactions	I-Citation
on	I-Citation
Acoustics	I-Citation
,	I-Citation
Speech	I-Citation
,	I-Citation
and	I-Citation
Signal	I-Citation
Processing	I-Citation
,	I-Citation
vol.	I-Citation
37	I-Citation
,	I-Citation
No.	I-Citation
8	I-Citation
(	I-Citation
Aug.	I-Citation
1989	I-Citation
)	I-Citation
,	I-Citation
pp.	I-Citation
1197	I-Citation
1213	I-Citation
.	O

Gillick	B-Citation
,	I-Citation
Larry	I-Citation
et_al.	I-Citation
,	I-Citation
Rapid	I-Citation
Match	I-Citation
Training	I-Citation
For	I-Citation
Large	I-Citation
Vocabularies	I-Citation
,	I-Citation
Dragon	I-Citation
Systems	I-Citation
,	I-Citation
Inc.	I-Citation
,	I-Citation
Newton	I-Citation
,	I-Citation
MA	I-Citation
.	O

Haeb	B-Citation
Unbach	I-Citation
,	I-Citation
R.	I-Citation
,	I-Citation
Automatic	I-Citation
Transcription	I-Citation
of	I-Citation
Unknown	I-Citation
Words	I-Citation
in	I-Citation
a	I-Citation
Speech	I-Citation
Recognition	I-Citation
System	I-Citation
,	I-Citation
The	I-Citation
1995	I-Citation
International	I-Citation
Conference	I-Citation
on	I-Citation
Acoustics	I-Citation
,	I-Citation
Speech	I-Citation
,	I-Citation
and	I-Citation
Signal	I-Citation
Processing	I-Citation
,	I-Citation
vol.	I-Citation
1	I-Citation
(	I-Citation
May	I-Citation
1995	I-Citation
)	I-Citation
,	I-Citation
pp.	I-Citation
840	I-Citation
843	I-Citation
.	O

Imai	B-Citation
,	I-Citation
Toru	I-Citation
.	I-Citation
A	I-Citation
New	I-Citation
Method	I-Citation
for	I-Citation
Automatic	I-Citation
Generation	I-Citation
of	I-Citation
Speaker	I-Citation
Dependent	I-Citation
Phonological	I-Citation
Rules	I-Citation
,	I-Citation
The	I-Citation
1995	I-Citation
International	I-Citation
Conference	I-Citation
on	I-Citation
Acoustics	I-Citation
,	I-Citation
Speech	I-Citation
,	I-Citation
and	I-Citation
Signal	I-Citation
Processing	I-Citation
,	I-Citation
vol.	I-Citation
1	I-Citation
(	I-Citation
May	I-Citation
1995	I-Citation
)	I-Citation
,	I-Citation
pp.	I-Citation
864	I-Citation
867	I-Citation
.	O

Mandel	B-Citation
,	I-Citation
Mark	I-Citation
A.	I-Citation
et_al.	I-Citation
,	I-Citation
A	I-Citation
Commercial	I-Citation
Large	I-Citation
Vocabulary	I-Citation
Discrete	I-Citation
Speech	I-Citation
Recognition	I-Citation
System	I-Citation
:	I-Citation
DragonDictate	I-Citation
,	I-Citation
Language	I-Citation
and	I-Citation
Speech	I-Citation
,	I-Citation
vol.	I-Citation
35	I-Citation
(	I-Citation
1	I-Citation
,	I-Citation
2	I-Citation
)	I-Citation
(	I-Citation
1992	I-Citation
)	I-Citation
,	I-Citation
pp.	I-Citation
237	I-Citation
246	I-Citation
.	O

US	O
7478041	O
B2	O
20090113	O

20030312	O

US	O
7496509	O
B2	O
20090224	O

20040528	O

US	O
7392186	O
B2	O
20080624	O

20040330	O

US	O
7373294	O
B2	O
20080513	O

20030515	O

US	O
7324941	O
B2	O
20080129	O

20040726	O

US	O
7302389	O
B2	O
20071127	O

20030808	O

US	O
6915260	O
B2	O
20050705	O

20010924	O

US	O
6917919	O
B2	O
20050712	O

20010924	O

US	O
6868381	O
B1	O
20050315	O

19991221	O

US	O
6754625	O
B2	O
20040622	O

20001226	O

US	O
6678658	O
B1	O
20040113	O

20000707	O

US	O
6499012	O
B1	O
20021224	O

19991223	O

US	O
6374221	O
B1	O
20020416	O

19990622	O

US	O
7716052	O
B2	O
20100511	O

20050407	O

US	O
7720679	O
B2	O
20100518	O

20080924	O

US	O
7788096	O
B2	O
20100831	O

20020903	O

WO	O
2006128107	O
A2	O
20061130	O

20060530	O

US	O
7865358	O
B2	O
20110104	O

20040901	O

US	O
7912721	O
B2	O
20110322	O

20051229	O

US	O
8000962	O
B2	O
20110816	O

20060519	O

US	O
2982896	O
19961028	O

US	O
3957197	O
19970228	O

Dragon	O
Systems	O
,	O
Inc.	O

Newton	O
MA	O
US	O

Newman	O
Michael	O
Jack	O

Somerville	O
US	O

Gillick	O
Laurence	O
S.	O

Newton	O
US	O

Nagesha	O
Venkatesh	O

Waltham	O
US	O

Fish	O
&	O
Richardson	O
P.	O
C.	O

Zele	O
;	O
Krista	O

Opsasnick	O
;	O
Michael	O
N.	O

US	O
6151575	O
A	O
20001121	O

19971028	O

US	O
6151575	O
A	O
20001121	O

19971028	O

A	O
source-adapted	O
model	O
for	O
use	O
in	O
speech	O
recognition	O
is	O
generated	O
by	O
defining	O
a	O
linear	O
relationship	O
between	O
a	O
first	O
element	O
of	O
an	O
initial	O
model	O
and	O
a	O
first	O
element	O
of	O
the	O
source-adapted	O
model	O
.	O
Thereafter	O
,	O
speech	O
data	O
that	O
corresponds	O
to	O
the	O
first	O
element	O
of	O
the	O
initial	O
model	O
is	O
assembled	O
from	O
a	O
set	O
of	O
speech	O
data	O
for	O
a	O
particular	O
source	O
associated	O
with	O
the	O
source-adapted	O
model	O
.	O
A	O
linear	O
transform	O
that	O
maps	O
between	O
the	O
assembled	O
speech	O
data	O
and	O
the	O
first	O
element	O
of	O
the	O
initial	O
model	O
is	O
then	O
determined	O
.	O
Finally	O
,	O
a	O
first	O
element	O
of	O
the	O
source-adapted	O
model	O
is	O
produced	O
from	O
the	O
first	O
element	O
of	O
the	O
initial	O
model	O
using	O
the	O
linear	O
transform	O
.	O

19980803	O

AS	O
ASSIGNMENT	O
N	O
US	O
6151575A	O
DRAGON	O
SYSTEMS	O
,	O
INC.	O
,	O
MASSACHUSETTS	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNORS:NEWMAN	O
,	O
MICHAEL	O
JACK;GILLICK	O
,	O
LAURENCE	O
S.	O
;	O
NAGESHA	O
,	O
VENKATESH;REEL/FRAME:009370/0814	O

19980528	O

20021008	O

AS	O
ASSIGNMENT	O
N	O
US	O
6151575A	O
L	O
&	O
H	O
HOLDINGS	O
USA	O
,	O
INC.	O
,	O
MASSACHUSETTS	O
MERGER;ASSIGNOR:DRAGON	O
SYSTEMS	O
,	O
INC.;REEL/FRAME:013362/0732	O

20000607	O

20021008	O

AS	O
ASSIGNMENT	O
N	O
US	O
6151575A	O
SCANSOFT	O
,	O
INC.	O
,	O
MASSACHUSETTS	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNOR:L	O
&	O
H	O
HOLDINGS	O
USA	O
,	O
INC.;REEL/FRAME:013362/0739	O

20011212	O

20040521	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
6151575A	O
4	O

20051206	O

AS	O
ASSIGNMENT	O
N	O
US	O
6151575A	O
NUANCE	O
COMMUNICATIONS	O
,	O
INC.	O
,	O
MASSACHUSETTS	O
CHANGE	O
OF	O
NAME;ASSIGNOR:SCANSOFT	O
,	O
INC.;REEL/FRAME:016851/0772	O

20051017	O

20060407	O

AS	O
ASSIGNMENT	O
C	O
US	O
6151575A	O
USB	O
AG	O
,	O
STAMFORD	O
BRANCH	O
,	O
CONNECTICUT	O
SECURITY	O
AGREEMENT;ASSIGNOR:NUANCE	O
COMMUNICATIONS	O
,	O
INC.;REEL/FRAME:017435/0199	O

20060331	O

20060824	O

AS	O
ASSIGNMENT	O
C	O
US	O
6151575A	O
USB	O
AG	O
.	O
STAMFORD	O
BRANCH	O
,	O
CONNECTICUT	O
SECURITY	O
AGREEMENT;ASSIGNOR:NUANCE	O
COMMUNICATIONS	O
,	O
INC.;REEL/FRAME:018160/0909	O

20060331	O

20060824	O

AS	O
ASSIGNMENT	O
N	O
US	O
6151575A	O
USB	O
AG	O
.	O
STAMFORD	O
BRANCH,CONNECTICUT	O
SECURITY	O
AGREEMENT;ASSIGNOR:NUANCE	O
COMMUNICATIONS	O
,	O
INC.;REEL/FRAME:18160/909	O

20060331	O

20080522	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
6151575A	O
8	O

20080522	O

SULP	O
+	O
SURCHARGE	O
FOR	O
LATE	O
PAYMENT	O
N	O
US	O
6151575A	O
7	O

CROSS	O
REFERENCE	O
TO	O
RELATED	O
APPLICATIONS	O
This	O
application	O
claims	O
priority	O
from	O
U.S.	O
Provisional	O
Application	O
No.	O
60/029	O
,	O
828	O
,	O
filed	O
Oct.	O
28	O
,	O
1996	O
,	O
and	O
U.S.	O
Provisional	O
Application	O
No.	O
60/039	O
,	O
571	O
,	O
filed	O
Feb.	O
28	O
,	O
1997	O
.	O

BACKGROUND	O
The	O
invention	O
relates	O
to	O
speech	O
recognition	O
.	O

A	O
speech	O
recognition	O
system	O
analyzes	O
a	O
person	O
's	O
speech	O
to	O
determine	O
what	O
the	O
person	O
said	O
.	O
Most	O
speech	O
recognition	O
systems	O
are	O
frame-based	O
.	O
In	O
a	O
frame-based	O
system	O
,	O
a	O
processor	O
divides	O
a	O
signal	O
descriptive	O
of	O
the	O
speech	O
to	O
be	O
recognized	O
into	O
a	O
series	O
of	O
digital	O
frames	O
,	O
each	O
of	O
which	O
corresponds	O
to	O
a	O
small	O
time	O
increment	O
of	O
the	O
speech	O
.	O
The	O
processor	O
then	O
compares	O
the	O
digital	O
frames	O
to	O
a	O
set	O
of	O
speech	O
models	O
.	O
Each	O
speech	O
model	O
may	O
represent	O
a	O
word	O
from	O
a	O
vocabulary	O
of	O
words	O
,	O
and	O
may	O
represent	O
how	O
that	O
word	O
is	O
spoken	O
by	O
a	O
variety	O
of	O
speakers	O
.	O
A	O
speech	O
model	O
also	O
may	O
represent	O
a	O
sound	O
,	O
or	O
phoneme	O
,	O
that	O
corresponds	O
to	O
a	O
portion	O
of	O
a	O
word	O
.	O
Collectively	O
,	O
the	O
constituent	O
phonemes	O
for	O
a	O
word	O
represent	O
the	O
phonetic	O
spelling	O
of	O
the	O
word	O
.	O

The	O
processor	O
determines	O
what	O
the	O
speaker	O
said	O
by	O
finding	O
the	O
speech	O
models	O
that	O
best	O
match	O
the	O
digital	O
frames	O
that	O
represent	O
the	O
person	O
's	O
speech	O
.	O
The	O
words	O
or	O
phrases	O
corresponding	O
to	O
the	O
best	O
matching	O
speech	O
models	O
are	O
referred	O
to	O
as	O
recognition	O
candidates	O
.	O
The	O
processor	O
may	O
produce	O
a	O
single	O
recognition	O
candidate	O
for	O
each	O
utterance	O
,	O
or	O
may	O
produce	O
a	O
list	O
of	O
recognition	O
candidates	O
.	O
Speech	O
recognition	O
is	O
discussed	O
in	O
U.S.	O
Pat	O
.	O
No.	O
4	O
,	O
805	O
,	O
218	O
,	O
entitled	O
"	O
METHOD	O
FOR	O
SPEECH	O
ANALYSIS	O
AND	O
SPEECH	O
RECOGNITION	O
,	O
"	O
which	O
is	O
incorporated	O
by	O
reference	O
.	O

A	O
speech	O
recognition	O
system	O
may	O
be	O
a	O
"	O
discrete	O
"	O
system--i.e	O
.	O
,	O
one	O
which	O
recognizes	O
discrete	O
words	O
or	O
phrases	O
but	O
which	O
requires	O
the	O
speaker	O
to	O
pause	O
briefly	O
between	O
each	O
discrete	O
word	O
or	O
phrase	O
.	O
Alternatively	O
,	O
a	O
speech	O
recognition	O
system	O
may	O
be	O
"	O
continuous	O
"	O
meaning	O
that	O
the	O
recognition	O
software	O
can	O
recognize	O
spoken	O
words	O
or	O
phrases	O
regardless	O
of	O
whether	O
the	O
speaker	O
pauses	O
between	O
them	O
.	O
Continuous	O
speech	O
recognition	O
systems	O
typically	O
have	O
a	O
higher	O
incidence	O
of	O
recognition	O
errors	O
in	O
comparison	O
to	O
discrete	O
recognition	O
systems	O
due	O
to	O
complexities	O
of	O
recognizing	O
continuous	O
speech	O
.	O
A	O
more	O
detailed	O
description	O
of	O
continuous	O
speech	O
recognition	O
is	O
provided	O
in	O
U.S.	O
Pat	O
.	O
No.	O
5	O
,	O
202	O
,	O
952	O
,	O
entitled	O
"	O
LARGE-VOCABULARY	O
CONTINUOUS	O
SPEECH	O
PREFILTERING	O
AND	O
PROCESSING	O
SYSTEM	O
,	O
"	O
which	O
is	O
incorporated	O
by	O
reference	O
.	O

Speech	O
models	O
that	O
represent	O
the	O
speech	O
of	O
a	O
large	O
group	O
of	O
speakers	O
are	O
referred	O
to	O
as	O
speaker	O
independent	O
models	O
.	O
In	O
general	O
,	O
the	O
performance	O
of	O
a	O
speech	O
recognition	O
system	O
may	O
be	O
improved	O
by	O
adapting	O
the	O
speech	O
models	O
according	O
to	O
the	O
speech	O
of	O
a	O
particular	O
speaker	O
who	O
is	O
using	O
the	O
system	O
.	O
These	O
adapted	O
speech	O
models	O
are	O
referred	O
to	O
as	O
speaker-adapted	O
models	O
.	O
A	O
speaker-adapted	O
model	O
may	O
be	O
produced	O
by	O
adapting	O
a	O
speaker-independent	O
model	O
,	O
also	O
referred	O
to	O
as	O
a	O
speaker-adaptable	O
model	O
,	O
based	O
on	O
speech	O
material	O
,	O
referred	O
to	O
as	O
adaptation	O
data	O
,	O
acquired	O
from	O
the	O
speaker	O
associated	O
with	O
the	O
speaker-adapted	O
model	O
.	O
This	O
process	O
of	O
producing	O
a	O
speaker-adapted	O
model	O
may	O
be	O
referred	O
to	O
as	O
speaker	O
adaptation	O
.	O
The	O
speaker-adaptable	O
model	O
also	O
may	O
be	O
improved	O
by	O
modifying	O
the	O
model	O
using	O
adaptation	O
data	O
for	O
a	O
group	O
of	O
speakers	O
.	O

One	O
method	O
of	O
speaker	O
adaptation	O
is	O
to	O
update	O
the	O
model	O
parameters	O
of	O
a	O
speech	O
unit	O
(	O
e.g.	O
,	O
a	O
word	O
or	O
a	O
phoneme	O
)	O
every	O
time	O
that	O
the	O
speaker	O
utters	O
the	O
speech	O
unit	O
.	O
This	O
method	O
may	O
be	O
referred	O
to	O
as	O
a	O
Bayesian	O
or	O
maximum	O
a	O
posteriori	O
(	O
MAP	O
)	O
method	O
and	O
can	O
be	O
interpreted	O
as	O
a	O
way	O
of	O
combining	O
a	O
priori	O
information	O
(	O
i	O
.	O
e	O
.	O
,	O
a	O
speaker-independent	O
model	O
)	O
with	O
observed	O
data	O
(	O
i	O
.	O
e	O
.	O
,	O
the	O
adaptation	O
data	O
)	O
.	O

In	O
one	O
known	O
approach	O
to	O
generating	O
a	O
speaker-independent	O
model	O
,	O
speech	O
data	O
from	O
multiple	O
speakers	O
are	O
combined	O
to	O
form	O
the	O
speaker-independent	O
model	O
.	O
In	O
general	O
,	O
each	O
unit	O
of	O
the	O
model	O
represents	O
a	O
phoneme	O
in	O
a	O
particular	O
context	O
,	O
different	O
units	O
model	O
a	O
phoneme	O
in	O
different	O
contexts	O
or	O
different	O
phonemes	O
.	O
To	O
ensure	O
that	O
the	O
speaker-independent	O
model	O
is	O
of	O
reasonable	O
size	O
,	O
multiple	O
contexts	O
for	O
one	O
or	O
more	O
phonemes	O
or	O
parts	O
of	O
phoneme	O
are	O
represented	O
by	O
a	O
single	O
model	O
.	O
In	O
particular	O
,	O
each	O
phoneme/context	O
pair	O
is	O
represented	O
by	O
a	O
small	O
number	O
of	O
nodes	O
(	O
e.g.	O
,	O
three	O
)	O
mapping	O
is	O
generated	O
between	O
a	O
set	O
of	O
all	O
nodes	O
for	O
all	O
phoneme/context	O
pairs	O
to	O
a	O
set	O
of	O
node	O
models	O
,	O
where	O
most	O
,	O
if	O
not	O
all	O
,	O
node	O
models	O
represent	O
a	O
large	O
number	O
of	O
phoneme/context	O
pairs	O
.	O
The	O
mapping	O
may	O
be	O
referred	O
to	O
as	O
a	O
decision	O
tree	O
,	O
and	O
a	O
particular	O
node	O
model	O
may	O
be	O
represented	O
as	O
a	O
collection	O
of	O
Gaussian	O
distributions	O
that	O
each	O
include	O
a	O
mean	O
component	O
and	O
a	O
variance	O
component	O
.	O
The	O
speaker-independent	O
model	O
may	O
be	O
generated	O
or	O
refined	O
by	O
modifying	O
the	O
means	O
and	O
variances	O
of	O
the	O
Gaussian	O
distributions	O
to	O
conform	O
to	O
speaker	O
data	O
.	O

SUMMARY	O
In	O
one	O
general	O
aspect	O
,	O
the	O
invention	O
features	O
generating	O
a	O
source-adapted	O
model	O
for	O
use	O
in	O
speech	O
recognition	O
.	O
A	O
collection	O
of	O
elements	O
is	O
generated	O
from	O
an	O
initial	O
model	O
,	O
and	O
source	O
speech	O
data	O
that	O
corresponds	O
to	O
elements	O
in	O
the	O
collection	O
of	O
elements	O
is	O
assembled	O
from	O
a	O
set	O
of	O
source	O
speech	O
data	O
for	O
a	O
particular	O
source	O
associated	O
with	O
the	O
source-adapted	O
model	O
.	O
A	O
linear	O
transform	O
that	O
maps	O
between	O
the	O
assembled	O
source	O
speech	O
data	O
and	O
the	O
collection	O
of	O
elements	O
of	O
the	O
initial	O
model	O
then	O
is	O
determined	O
by	O
determining	O
a	O
relationship	O
between	O
each	O
element	O
of	O
the	O
initial	O
model	O
in	O
the	O
collection	O
and	O
a	O
portion	O
of	O
the	O
assembled	O
source	O
speech	O
data	O
that	O
corresponds	O
to	O
that	O
element	O
.	O
Finally	O
,	O
elements	O
of	O
the	O
source-adapted	O
model	O
are	O
produced	O
from	O
corresponding	O
elements	O
of	O
the	O
initial	O
model	O
by	O
applying	O
the	O
linear	O
transform	O
to	O
the	O
elements	O
of	O
the	O
initial	O
model	O
.	O

Sources	O
of	O
speech	O
data	O
include	O
particular	O
speakers	O
or	O
groups	O
of	O
related	O
speakers	O
.	O
Examples	O
of	O
related	O
groups	O
of	O
speakers	O
include	O
speakers	O
of	O
the	O
same	O
gender	O
,	O
speakers	O
from	O
the	O
same	O
geographic	O
region	O
(	O
e.g.	O
,	O
New	O
England	O
or	O
the	O
Southern	O
United	O
States	O
)	O
,	O
or	O
speakers	O
of	O
different	O
ages	O
.	O
Speaker	O
groupings	O
may	O
be	O
designated	O
by	O
a	O
human	O
operator	O
or	O
by	O
automatic	O
partitioning	O
techniques	O
.	O

By	O
generating	O
a	O
transform	O
using	O
data	O
associated	O
with	O
a	O
collection	O
of	O
elements	O
(	O
e.g.	O
,	O
phoneme	O
nodes	O
,	O
mixture	O
components	O
,	O
or	O
other	O
speech	O
units	O
)	O
and	O
using	O
the	O
transform	O
to	O
adapt	O
a	O
speech	O
model	O
for	O
each	O
speech	O
unit	O
within	O
the	O
collection	O
,	O
the	O
technique	O
leverages	O
adaptation	O
data	O
associated	O
with	O
multiple	O
speech	O
units	O
.	O
This	O
permits	O
effective	O
source	O
adaptation	O
to	O
be	O
achieved	O
even	O
when	O
there	O
is	O
little	O
or	O
no	O
adaptation	O
data	O
for	O
a	O
particular	O
speech	O
unit	O
.	O
Stated	O
another	O
way	O
,	O
effective	O
source	O
adaptation	O
may	O
be	O
achieved	O
using	O
a	O
relatively	O
small	O
amount	O
of	O
adaptation	O
data	O
.	O
By	O
contrast	O
,	O
Bayesian	O
or	O
MAP	O
approaches	O
require	O
relatively	O
large	O
amounts	O
of	O
adaptation	O
data	O
so	O
as	O
to	O
cover	O
the	O
entire	O
collection	O
of	O
speech	O
units	O
and	O
to	O
obtain	O
reasonable	O
estimates	O
of	O
the	O
model	O
parameters	O
for	O
each	O
speech	O
unit	O
.	O

Embodiments	O
may	O
include	O
one	O
or	O
more	O
of	O
the	O
following	O
features	O
.	O
The	O
collection	O
of	O
elements	O
may	O
be	O
associated	O
with	O
similarly-sounding	O
speech	O
units	O
,	O
may	O
have	O
similar	O
values	O
,	O
or	O
both	O
.	O
Elements	O
to	O
be	O
included	O
in	O
the	O
collection	O
of	O
elements	O
may	O
be	O
identified	O
by	O
a	O
human	O
operator	O
or	O
an	O
automatic	O
procedure	O
.	O
For	O
example	O
,	O
the	O
human	O
operator	O
may	O
identify	O
elements	O
as	O
having	O
similarly-sounding	O
speech	O
units	O
.	O
In	O
a	O
hybrid	O
,	O
human-assisted	O
technique	O
for	O
generating	O
collections	O
,	O
a	O
human	O
operator	O
identifies	O
classes	O
of	O
elements	O
having	O
similarly-sounding	O
speech	O
units	O
and	O
the	O
collection	O
is	O
generated	O
using	O
an	O
automatic	O
procedure	O
that	O
employs	O
the	O
identified	O
classes	O
and	O
similarities	O
between	O
elements	O
.	O
For	O
example	O
,	O
the	O
human	O
operator	O
may	O
identify	O
classes	O
of	O
related	O
elements	O
and	O
the	O
collection	O
of	O
elements	O
may	O
be	O
generated	O
using	O
an	O
automatic	O
procedure	O
that	O
favors	O
including	O
elements	O
from	O
a	O
common	O
class	O
in	O
the	O
collection	O
and	O
penalizes	O
including	O
elements	O
from	O
different	O
classes	O
in	O
the	O
collection	O
.	O

Statistics	O
may	O
be	O
generated	O
from	O
the	O
assembled	O
source	O
speech	O
data	O
and	O
used	O
in	O
determining	O
the	O
linear	O
transform	O
based	O
on	O
the	O
generated	O
statistics	O
.	O
For	O
example	O
,	O
an	O
average	O
value	O
of	O
the	O
assembled	O
source	O
speech	O
data	O
for	O
a	O
particular	O
element	O
or	O
a	O
count	O
of	O
speech	O
frames	O
associated	O
with	O
the	O
assembled	O
source	O
speech	O
data	O
for	O
the	O
particular	O
element	O
may	O
be	O
determined	O
.	O

A	O
smoothing	O
factor	O
that	O
accounts	O
for	O
an	O
amount	O
of	O
source	O
speech	O
data	O
available	O
for	O
a	O
particular	O
element	O
may	O
be	O
used	O
in	O
generating	O
the	O
statistics	O
.	O
For	O
example	O
,	O
an	O
average	O
value	O
of	O
the	O
assembled	O
source	O
speech	O
data	O
for	O
a	O
particular	O
element	O
may	O
be	O
modified	O
based	O
on	O
the	O
smoothing	O
factor	O
and	O
a	O
value	O
for	O
the	O
element	O
from	O
the	O
initial	O
model	O
.	O

When	O
each	O
element	O
of	O
the	O
initial	O
model	O
includes	O
a	O
mean	O
portion	O
and	O
a	O
variance	O
portion	O
,	O
an	O
element	O
of	O
the	O
source-adapted	O
model	O
may	O
be	O
produced	O
from	O
a	O
corresponding	O
element	O
of	O
the	O
initial	O
model	O
by	O
applying	O
the	O
linear	O
transform	O
to	O
the	O
mean	O
portion	O
of	O
the	O
element	O
of	O
the	O
initial	O
model	O
and	O
leaving	O
the	O
variance	O
portion	O
unchanged	O
.	O

In	O
another	O
general	O
aspect	O
,	O
the	O
invention	O
features	O
generating	O
a	O
source-adapted	O
model	O
for	O
use	O
in	O
speech	O
recognition	O
.	O
A	O
collection	O
of	O
elements	O
is	O
generated	O
from	O
an	O
initial	O
model	O
,	O
and	O
speech	O
data	O
that	O
corresponds	O
to	O
elements	O
in	O
the	O
collection	O
of	O
elements	O
is	O
assembled	O
from	O
a	O
set	O
of	O
speech	O
data	O
for	O
a	O
particular	O
source	O
associated	O
with	O
the	O
source-adapted	O
model	O
.	O
Statistics	O
then	O
are	O
generated	O
from	O
the	O
assembled	O
source	O
speech	O
data	O
using	O
a	O
smoothing	O
factor	O
that	O
accounts	O
for	O
an	O
amount	O
of	O
source	O
speech	O
data	O
available	O
for	O
a	O
particular	O
element	O
.	O
These	O
statistics	O
are	O
used	O
in	O
determining	O
a	O
linear	O
transform	O
that	O
maps	O
between	O
the	O
assembled	O
speech	O
data	O
and	O
the	O
collection	O
of	O
elements	O
of	O
the	O
initial	O
model	O
.	O
Finally	O
,	O
elements	O
of	O
the	O
source-adapted	O
model	O
are	O
produced	O
from	O
corresponding	O
elements	O
of	O
the	O
initial	O
model	O
by	O
applying	O
the	O
linear	O
transform	O
to	O
the	O
elements	O
of	O
the	O
initial	O
model	O
.	O

In	O
addition	O
to	O
creating	O
source-adapted	O
models	O
,	O
the	O
techniques	O
of	O
the	O
invention	O
may	O
be	O
used	O
in	O
training	O
a	O
speech	O
model	O
such	O
as	O
a	O
source-independent	O
model	O
.	O
Typically	O
,	O
source-independent	O
models	O
are	O
trained	O
using	O
training	O
data	O
from	O
a	O
large	O
number	O
of	O
sources	O
,	O
without	O
regard	O
to	O
the	O
particular	O
source	O
used	O
in	O
the	O
training	O
.	O
The	O
transformation	O
techniques	O
of	O
the	O
invention	O
may	O
be	O
used	O
to	O
reduce	O
the	O
effects	O
of	O
superfluous	O
,	O
source-specific	O
variations	O
in	O
the	O
training	O
data	O
.	O
For	O
example	O
,	O
an	O
inverse	O
of	O
the	O
transformation	O
used	O
to	O
produce	O
the	O
source-adapted	O
model	O
for	O
a	O
particular	O
source	O
may	O
be	O
used	O
to	O
reduce	O
or	O
eliminate	O
source-induced	O
variations	O
in	O
the	O
training	O
data	O
from	O
that	O
source	O
.	O

In	O
this	O
regard	O
,	O
and	O
in	O
another	O
general	O
aspect	O
,	O
the	O
invention	O
features	O
training	O
a	O
source-independent	O
speech	O
model	O
for	O
use	O
in	O
speech	O
recognition	O
.	O
Sets	O
of	O
speech	O
data	O
that	O
corresponds	O
to	O
elements	O
of	O
an	O
initial	O
model	O
are	O
assembled	O
from	O
a	O
set	O
of	O
speech	O
data	O
for	O
one	O
or	O
more	O
sources	O
and	O
representative	O
values	O
for	O
each	O
set	O
of	O
speech	O
data	O
are	O
calculated	O
.	O
Next	O
,	O
relationships	O
between	O
the	O
representative	O
values	O
of	O
the	O
assembled	O
speech	O
data	O
and	O
values	O
of	O
the	O
corresponding	O
elements	O
of	O
the	O
initial	O
model	O
are	O
determined	O
.	O
Different	O
relationships	O
are	O
permitted	O
so	O
that	O
a	O
relationship	O
for	O
a	O
first	O
set	O
of	O
speech	O
data	O
may	O
differ	O
from	O
a	O
relationship	O
for	O
a	O
second	O
set	O
of	O
speech	O
data	O
.	O
These	O
relationships	O
are	O
used	O
to	O
modify	O
each	O
item	O
of	O
the	O
sets	O
of	O
speech	O
data	O
.	O
Finally	O
,	O
elements	O
of	O
the	O
source-independent	O
speech	O
model	O
are	O
generated	O
using	O
the	O
modified	O
sets	O
of	O
speech	O
data	O
.	O

This	O
technique	O
provides	O
tremendous	O
flexibility	O
in	O
that	O
it	O
may	O
be	O
used	O
to	O
generate	O
a	O
source-independent	O
speech	O
model	O
having	O
a	O
structure	O
or	O
size	O
that	O
differs	O
from	O
the	O
initial	O
model	O
.	O
For	O
example	O
,	O
a	O
relatively	O
small	O
source-independent	O
speech	O
model	O
may	O
be	O
generated	O
from	O
a	O
relatively	O
large	O
initial	O
model	O
.	O
Thus	O
,	O
a	O
number	O
of	O
elements	O
of	O
the	O
initial	O
model	O
may	O
be	O
substantially	O
larger	O
than	O
a	O
number	O
of	O
elements	O
of	O
the	O
speech	O
model	O
.	O
Other	O
ways	O
in	O
which	O
the	O
structure	O
of	O
the	O
models	O
may	O
vary	O
is	O
in	O
the	O
number	O
of	O
phonemes	O
included	O
in	O
the	O
models	O
,	O
the	O
number	O
of	O
nodes	O
per	O
phoneme	O
,	O
the	O
number	O
of	O
output	O
PDFs	O
per	O
node	O
,	O
and	O
the	O
number	O
of	O
mixture	O
components	O
.	O

An	O
advantage	O
of	O
this	O
approach	O
is	O
that	O
it	O
permits	O
the	O
creation	O
of	O
a	O
relatively	O
small	O
source-independent	O
model	O
that	O
takes	O
advantage	O
of	O
all	O
available	O
source	O
data	O
.	O
In	O
initial	O
model	O
for	O
which	O
a	O
relatively	O
small	O
number	O
of	O
phoneme/context	O
pairs	O
maps	O
to	O
each	O
node	O
model	O
is	O
used	O
to	O
back	O
transform	O
the	O
source	O
data	O
(	O
i	O
.	O
e	O
.	O
,	O
to	O
remove	O
source	O
idiosyncracies	O
)	O
.	O
The	O
transformed	O
source	O
data	O
then	O
is	O
used	O
in	O
producing	O
a	O
smaller	O
source-independent	O
speech	O
model	O
for	O
which	O
a	O
relatively	O
large	O
number	O
of	O
phoneme/context	O
pairs	O
maps	O
to	O
each	O
node	O
model	O
.	O
Use	O
of	O
larger	O
initial	O
models	O
are	O
needed	O
due	O
to	O
large	O
variability	O
in	O
the	O
training	O
data	O
;	O
the	O
removal	O
of	O
spurious	O
variability	O
by	O
modifying	O
each	O
speaker	O
's	O
training	O
data	O
allows	O
the	O
use	O
of	O
smaller	O
final	O
models	O
.	O
The	O
use	O
of	O
smaller	O
final	O
models	O
reduces	O
recognition	O
time	O
and	O
memory	O
usage	O
,	O
both	O
of	O
which	O
are	O
important	O
in	O
designing	O
effective	O
,	O
practical	O
speech	O
recognition	O
systems	O
.	O

As	O
noted	O
above	O
,	O
relationships	O
may	O
differ	O
for	O
different	O
elements	O
of	O
the	O
model	O
,	O
even	O
for	O
a	O
single	O
source	O
.	O
When	O
sets	O
of	O
speech	O
data	O
that	O
correspond	O
to	O
elements	O
of	O
the	O
initial	O
model	O
are	O
assembled	O
for	O
speech	O
data	O
from	O
different	O
sources	O
,	O
relationships	O
for	O
the	O
same	O
element	O
may	O
differ	O
for	O
different	O
sources	O
.	O
The	O
element	O
of	O
the	O
speech	O
model	O
is	O
generated	O
using	O
modified	O
sets	O
of	O
speech	O
data	O
from	O
different	O
sources	O
.	O

The	O
relationships	O
may	O
be	O
linear	O
transformations	O
.	O
In	O
particular	O
,	O
a	O
relationship	O
may	O
be	O
determined	O
by	O
determining	O
a	O
transform	O
that	O
maps	O
between	O
the	O
set	O
of	O
speech	O
data	O
and	O
the	O
element	O
of	O
the	O
initial	O
model	O
,	O
and	O
generating	O
an	O
inverse	O
of	O
the	O
transform	O
.	O
Each	O
element	O
of	O
the	O
set	O
of	O
speech	O
data	O
is	O
modified	O
by	O
applying	O
the	O
inverse	O
of	O
the	O
transform	O
to	O
each	O
element	O
of	O
the	O
set	O
of	O
speech	O
data	O
.	O

In	O
yet	O
another	O
general	O
aspect	O
,	O
the	O
invention	O
features	O
training	O
a	O
speech	O
model	O
for	O
use	O
in	O
speech	O
recognition	O
by	O
assembling	O
speech	O
data	O
that	O
corresponds	O
to	O
a	O
first	O
element	O
of	O
the	O
speech	O
model	O
from	O
a	O
set	O
of	O
speech	O
data	O
for	O
a	O
first	O
source	O
,	O
determining	O
a	O
transform	O
that	O
maps	O
between	O
the	O
assembled	O
speech	O
data	O
and	O
the	O
first	O
element	O
of	O
the	O
speech	O
model	O
,	O
and	O
generating	O
an	O
inverse	O
of	O
the	O
transform	O
.	O
The	O
first	O
element	O
of	O
the	O
speech	O
model	O
is	O
updated	O
,	O
or	O
retrained	O
,	O
using	O
the	O
assembled	O
speech	O
data	O
and	O
the	O
inverse	O
of	O
the	O
transform	O
.	O

Training	O
of	O
the	O
speech	O
model	O
may	O
further	O
include	O
assembling	O
speech	O
data	O
that	O
corresponds	O
to	O
the	O
first	O
element	O
of	O
the	O
speech	O
model	O
from	O
a	O
set	O
of	O
speech	O
data	O
for	O
a	O
second	O
source	O
,	O
determining	O
a	O
second	O
transform	O
that	O
maps	O
between	O
the	O
assembled	O
speech	O
data	O
for	O
the	O
second	O
source	O
and	O
the	O
first	O
element	O
of	O
the	O
speech	O
model	O
,	O
and	O
generating	O
an	O
inverse	O
of	O
the	O
second	O
transform	O
.	O
The	O
first	O
element	O
of	O
the	O
speech	O
model	O
is	O
updated	O
using	O
the	O
assembled	O
speech	O
data	O
for	O
the	O
first	O
and	O
second	O
sources	O
and	O
the	O
inverses	O
of	O
the	O
first	O
and	O
second	O
transforms	O
.	O

Instead	O
of	O
modifying	O
the	O
speech	O
model	O
,	O
the	O
transform	O
also	O
may	O
be	O
used	O
to	O
modify	O
the	O
training	O
data	O
.	O
In	O
this	O
regard	O
,	O
and	O
in	O
another	O
aspect	O
,	O
generally	O
,	O
the	O
invention	O
features	O
determining	O
a	O
difference	O
for	O
each	O
component	O
and	O
then	O
determining	O
a	O
correction	O
factor	O
for	O
each	O
frame	O
as	O
a	O
weighted	O
average	O
of	O
the	O
above	O
differences	O
,	O
and	O
modifying	O
the	O
speech	O
data	O
using	O
the	O
determined	O
correction	O
factor	O
.	O
The	O
entire	O
acoustic	O
model	O
may	O
be	O
updated	O
using	O
the	O
modified	O
speech	O
data	O
according	O
to	O
desired	O
size	O
and/or	O
structure	O
.	O

The	O
techniques	O
of	O
the	O
invention	O
may	O
be	O
implemented	O
in	O
computer	O
hardware	O
or	O
software	O
,	O
or	O
a	O
combination	O
of	O
the	O
two	O
.	O
However	O
,	O
the	O
techniques	O
are	O
not	O
limited	O
to	O
any	O
particular	O
hardware	O
or	O
software	O
configuration	O
;	O
they	O
may	O
find	O
applicability	O
in	O
any	O
computing	O
or	O
processing	O
environment	O
that	O
may	O
be	O
used	O
for	O
speech	O
recognition	O
.	O
Preferably	O
,	O
the	O
techniques	O
are	O
implemented	O
in	O
computer	O
programs	O
executing	O
on	O
programmable	O
computers	O
that	O
each	O
include	O
a	O
processor	O
,	O
a	O
storage	O
medium	O
readable	O
by	O
the	O
processor	O
(	O
including	O
volatile	O
and	O
non-volatile	O
memory	O
and/or	O
storage	O
elements	O
)	O
,	O
at	O
least	O
one	O
input	O
device	O
,	O
and	O
one	O
or	O
more	O
output	O
devices	O
.	O
Program	O
code	O
is	O
applied	O
to	O
data	O
entered	O
using	O
the	O
input	O
device	O
to	O
perform	O
the	O
functions	O
described	O
and	O
to	O
generate	O
output	O
information	O
.	O
The	O
output	O
information	O
is	O
applied	O
to	O
one	O
or	O
more	O
output	O
devices	O
.	O

Each	O
program	O
is	O
preferably	O
implemented	O
in	O
a	O
high	O
level	O
procedural	O
or	O
object	O
oriented	O
programming	O
language	O
to	O
communicate	O
with	O
a	O
computer	O
system	O
.	O
However	O
,	O
the	O
programs	O
can	O
be	O
implemented	O
in	O
assembly	O
or	O
machine	O
language	O
,	O
if	O
desired	O
.	O
In	O
any	O
case	O
,	O
the	O
language	O
may	O
be	O
a	O
compiled	O
or	O
interpreted	O
language	O
.	O

Each	O
such	O
computer	O
program	O
is	O
preferably	O
stored	O
on	O
a	O
storage	O
medium	O
or	O
device	O
(	O
e.g.	O
,	O
CD-ROM	O
,	O
hard	O
disk	O
or	O
magnetic	O
diskette	O
)	O
that	O
is	O
readable	O
by	O
a	O
general	O
or	O
special	O
purpose	O
programmable	O
computer	O
for	O
configuring	O
and	O
operating	O
the	O
computer	O
when	O
the	O
storage	O
medium	O
or	O
device	O
is	O
read	O
by	O
the	O
computer	O
to	O
perform	O
the	O
procedures	O
described	O
in	O
this	O
document	O
.	O
The	O
system	O
may	O
also	O
be	O
considered	O
to	O
be	O
implemented	O
as	O
a	O
computer-readable	O
storage	O
medium	O
,	O
configured	O
with	O
a	O
computer	O
program	O
,	O
where	O
the	O
storage	O
medium	O
so	O
configured	O
causes	O
a	O
computer	O
to	O
operate	O
in	O
a	O
specific	O
and	O
predefined	O
manner	O
.	O

Other	O
features	O
and	O
advantages	O
will	O
become	O
apparent	O
from	O
the	O
following	O
description	O
,	O
including	O
the	O
drawings	O
,	O
and	O
from	O
the	O
claims	O
.	O

BRIEF	O
DESCRIPTION	O
OF	O
THE	O
DRAWING	O
FIG	O
.	O
1	O
is	O
a	O
block	O
diagram	O
of	O
a	O
speech	O
recognition	O
system	O
.	O

FIG	O
.	O
2	O
is	O
a	O
flow	O
diagram	O
of	O
data	O
flow	O
through	O
the	O
speech	O
recognition	O
system	O
of	O
FIG	O
.	O
1	O
.	O

FIG	O
.	O
3	O
is	O
a	O
flow	O
chart	O
of	O
a	O
procedure	O
performed	O
by	O
the	O
speech	O
recognition	O
system	O
of	O
FIG	O
.	O
1	O
.	O

FIG	O
.	O
4	O
is	O
a	O
block	O
diagram	O
of	O
a	O
data	O
structure	O
of	O
the	O
speech	O
recognition	O
system	O
of	O
FIG	O
.	O
1	O
.	O

FIGS	O
.	O
5	O
-	O
7	O
are	O
flow	O
charts	O
of	O
procedures	O
performed	O
by	O
the	O
speech	O
recognition	O
system	O
of	O
FIG	O
.	O
1	O
.	O

DESCRIPTION	O
Referring	O
to	O
FIG	O
.	O
1	O
,	O
a	O
speech	O
recognition	O
system	O
100	O
includes	O
input/output	O
(	O
I/O	O
)	O
devices	O
(	O
e.g.	O
,	O
microphone	O
105	O
,	O
mouse	O
110	O
,	O
keyboard	O
115	O
,	O
and	O
display	O
120	O
)	O
and	O
a	O
general	O
purpose	O
computer	O
125	O
having	O
a	O
processor	O
130	O
,	O
an	O
I/O	O
unit	O
135	O
and	O
a	O
sound	O
card	O
140	O
.	O
A	O
memory	O
145	O
stores	O
data	O
and	O
software	O
such	O
as	O
an	O
operating	O
system	O
150	O
,	O
application	O
software	O
155	O
(	O
e.g.	O
,	O
a	O
word	O
processing	O
program	O
)	O
,	O
and	O
speech	O
recognition	O
software	O
160	O
.	O

Referring	O
also	O
to	O
FIG	O
.	O
2	O
,	O
the	O
microphone	O
105	O
receives	O
an	O
utterance	O
200	O
from	O
a	O
speaker	O
and	O
conveys	O
the	O
utterance	O
,	O
in	O
the	O
form	O
of	O
an	O
analog	O
signal	O
205	O
,	O
to	O
the	O
sound	O
card	O
140	O
,	O
which	O
in	O
turn	O
passes	O
the	O
signal	O
through	O
an	O
analog-to-digital	O
(	O
A/D	O
)	O
converter	O
210	O
to	O
transform	O
the	O
analog	O
signal	O
205	O
into	O
a	O
set	O
of	O
digital	O
samples	O
215	O
.	O
The	O
processor	O
130	O
analyzes	O
the	O
digital	O
samples	O
215	O
using	O
front	O
end	O
processing	O
software	O
220	O
to	O
produce	O
a	O
sequence	O
of	O
frames	O
225	O
which	O
represent	O
the	O
frequency	O
content	O
of	O
the	O
utterance	O
.	O
Each	O
frame	O
includes	O
several	O
(	O
e.g.	O
,	O
24	O
)	O
parameters	O
and	O
represents	O
a	O
short	O
portion	O
(	O
e.g.	O
,	O
10	O
milliseconds	O
)	O
of	O
the	O
utterance	O
.	O

In	O
one	O
implementation	O
,	O
as	O
shown	O
in	O
FIG	O
.	O
3	O
,	O
the	O
front	O
end	O
processing	O
software	O
220	O
causes	O
the	O
processor	O
130	O
to	O
operate	O
according	O
to	O
a	O
procedure	O
300	O
to	O
produce	O
a	O
frame	O
from	O
digital	O
samples	O
215	O
corresponding	O
to	O
a	O
portion	O
of	O
an	O
utterance	O
to	O
be	O
represented	O
by	O
the	O
frame	O
.	O
First	O
,	O
the	O
processor	O
produces	O
a	O
frequency	O
domain	O
representation	O
X	O
(	O
f	O
)	O
of	O
the	O
portion	O
of	O
the	O
utterance	O
by	O
performing	O
a	O
Fast	O
Fourier	O
Transform	O
(	O
FFT	O
)	O
on	O
the	O
digital	O
samples	O
215	O
(	O
step	O
305	O
)	O
.	O
Next	O
,	O
the	O
processor	O
determines	O
log	O
(	O
X	O
(	O
f	O
)	O
)	O
.	O
sup	O
.	O
2	O
(	O
step	O
310	O
)	O
.	O
The	O
processor	O
then	O
performs	O
frequency	O
warping	O
(	O
step	O
315	O
)	O
and	O
a	O
filter	O
bank	O
analysis	O
(	O
step	O
320	O
)	O
to	O
achieve	O
speaker	O
normalization	O
.	O
From	O
the	O
normalized	O
results	O
,	O
the	O
processor	O
performs	O
cepstral	O
analysis	O
(	O
step	O
325	O
)	O
to	O
produces	O
twelve	O
cepstral	O
parameters	O
.	O
The	O
processor	O
generates	O
the	O
cepstral	O
parameters	O
by	O
performing	O
an	O
inverse	O
cosine	O
transformation	O
on	O
the	O
logarithms	O
of	O
the	O
frequency	O
parameters	O
.	O
Cepstral	O
parameters	O
and	O
cepstral	O
differences	O
have	O
been	O
found	O
to	O
emphasize	O
information	O
important	O
to	O
speech	O
recognition	O
more	O
effectively	O
than	O
do	O
the	O
frequency	O
parameters	O
.	O
Next	O
,	O
the	O
processor	O
performs	O
channel	O
normalization	O
of	O
the	O
cepstral	O
parameters	O
(	O
step	O
330	O
)	O
.	O
The	O
processor	O
then	O
produces	O
twelve	O
cepstral	O
differences	O
(	O
i	O
.	O
e	O
.	O
,	O
the	O
differences	O
between	O
cepstral	O
parameters	O
in	O
successive	O
frames	O
)	O
(	O
step	O
335	O
)	O
and	O
twelve	O
cepstral	O
second	O
differences	O
(	O
i	O
.	O
e	O
.	O
,	O
the	O
differences	O
between	O
cepstral	O
differences	O
in	O
successive	O
frames	O
)	O
(	O
step	O
340	O
)	O
.	O
Finally	O
,	O
the	O
processor	O
performs	O
an	O
IMELDA	O
linear	O
combination	O
transformation	O
(	O
step	O
345	O
)	O
to	O
select	O
the	O
twenty	O
four	O
most	O
useful	O
parameters	O
from	O
the	O
twelve	O
cepstral	O
parameters	O
,	O
the	O
twelve	O
cepstral	O
differences	O
,	O
and	O
the	O
twelve	O
cepstral	O
second	O
differences	O
.	O

Referring	O
again	O
to	O
FIG	O
.	O
2	O
,	O
the	O
processor	O
130	O
compares	O
the	O
sequence	O
of	O
frames	O
225	O
to	O
acoustic	O
models	O
from	O
a	O
recognition	O
vocabulary	O
230	O
to	O
identify	O
the	O
text	O
235	O
that	O
corresponds	O
to	O
the	O
sample	O
.	O
As	O
shown	O
in	O
FIG	O
.	O
4	O
,	O
the	O
recognition	O
vocabulary	O
230	O
uses	O
a	O
pronunciation	O
model	O
in	O
which	O
each	O
word	O
400	O
is	O
represented	O
by	O
a	O
series	O
of	O
phonemes	O
405	O
(	O
e.g.	O
,	O
/ah/	O
,	O
/r/	O
,	O
/d/	O
)	O
that	O
comprise	O
the	O
phonetic	O
spelling	O
of	O
the	O
word	O
.	O
The	O
processor	O
130	O
compares	O
the	O
sequence	O
of	O
frames	O
225	O
to	O
acoustic	O
models	O
for	O
the	O
phonemes	O
using	O
a	O
dynamic	O
programming	O
technique	O
to	O
identify	O
the	O
series	O
of	O
phonemes	O
,	O
and	O
the	O
corresponding	O
series	O
of	O
words	O
,	O
that	O
best	O
correspond	O
to	O
the	O
sequence	O
of	O
frames	O
.	O
In	O
making	O
the	O
comparison	O
,	O
the	O
processor	O
produces	O
a	O
time	O
alignment	O
that	O
relates	O
each	O
frame	O
of	O
the	O
sequence	O
to	O
a	O
particular	O
phoneme	O
.	O

If	O
the	O
speech	O
recognition	O
software	O
160	O
has	O
been	O
adapted	O
for	O
use	O
with	O
the	O
speaker	O
who	O
produced	O
the	O
utterance	O
,	O
then	O
the	O
processor	O
130	O
compares	O
the	O
sequence	O
of	O
frames	O
225	O
to	O
adapted	O
acoustic	O
models	O
from	O
a	O
speaker-adapted	O
model	O
240	O
in	O
the	O
recognition	O
vocabulary	O
230	O
.	O
Otherwise	O
,	O
the	O
processor	O
compares	O
the	O
sequence	O
of	O
frames	O
225	O
to	O
acoustic	O
models	O
from	O
a	O
speaker-independent	O
speech	O
model	O
245	O
(	O
also	O
referred	O
to	O
as	O
a	O
speaker-adaptable	O
speech	O
model	O
)	O
in	O
the	O
recognition	O
vocabulary	O
230	O
.	O

As	O
noted	O
above	O
,	O
and	O
as	O
shown	O
in	O
FIG	O
.	O
4	O
,	O
the	O
recognition	O
vocabulary	O
represents	O
each	O
word	O
400	O
using	O
a	O
series	O
of	O
phonemes	O
405	O
.	O
In	O
one	O
implementation	O
,	O
the	O
speaker-independent	O
speech	O
model	O
245	O
represents	O
each	O
phoneme	O
as	O
a	O
triphone	O
410	O
that	O
includes	O
three	O
nodes	O
415	O
.	O
A	O
triphone	O
is	O
a	O
context-dependent	O
phoneme	O
.	O
For	O
example	O
,	O
the	O
triphone	O
"	O
abc	O
"	O
represents	O
the	O
phoneme	O
"	O
b	O
"	O
in	O
the	O
context	O
of	O
the	O
phonemes	O
"	O
a	O
"	O
and	O
"	O
c	O
"	O
,	O
with	O
the	O
phoneme	O
"	O
b	O
"	O
being	O
preceded	O
by	O
the	O
phoneme	O
"	O
a	O
"	O
and	O
followed	O
by	O
the	O
phoneme	O
"	O
c	O
"	O
.	O

The	O
speaker-independent	O
model	O
245	O
represents	O
each	O
triphone	O
node	O
415	O
as	O
a	O
mixture	O
of	O
Gaussian	O
probability	O
density	O
functions	O
(	O
"	O
PDFs	O
"	O
)	O
.	O
For	O
example	O
,	O
the	O
speaker-independent	O
model	O
245	O
may	O
represent	O
node	O
"	O
i	O
"	O
of	O
a	O
triphone	O
"	O
abc	O
"	O
as	O
ab	O
.	O
sup	O
.	O
i	O
c	O
:	O
#	O
#	O
EQU1	O
#	O
#	O
where	O
each	O
w	O
.	O
sub	O
.	O
k	O
is	O
a	O
mixture	O
weight	O
,	O
#	O
#	O
EQU2	O
#	O
#	O
.	O
mu	O
.	O
.	O
sub	O
.	O
k	O
is	O
a	O
mean	O
vector	O
for	O
the	O
probability	O
density	O
function	O
(	O
"	O
PDF	O
"	O
)	O
N.	O
sub	O
.	O
k	O
,	O
and	O
C.	O
sub	O
.	O
k	O
is	O
the	O
covanance	O
matrix	O
for	O
the	O
PDF	O
N.	O
sub	O
.	O
k	O
.	O
Like	O
the	O
frames	O
in	O
the	O
sequence	O
of	O
frames	O
225	O
,	O
the	O
vectors	O
.	O
mu	O
.	O
.	O
sub	O
.	O
k	O
each	O
include	O
24	O
parameters	O
and	O
the	O
matrices	O
c	O
.	O
sub	O
.	O
k	O
are	O
twenty	O
four	O
by	O
twenty	O
four	O
matrices	O
.	O
Each	O
triphone	O
node	O
may	O
be	O
represented	O
as	O
a	O
mixture	O
of	O
up	O
to	O
,	O
for	O
example	O
,	O
sixteen	O
different	O
PDFs	O
.	O

A	O
particular	O
PDF	O
may	O
be	O
used	O
in	O
the	O
representation	O
of	O
multiple	O
triphone	O
nodes	O
.	O
Accordingly	O
,	O
the	O
speaker-independent	O
model	O
245	O
represents	O
each	O
triphone	O
node	O
415	O
as	O
a	O
collection	O
of	O
mixture	O
weights	O
w	O
.	O
sub	O
.	O
k	O
420	O
associated	O
with	O
up	O
to	O
sixteen	O
different	O
PDFs	O
N.	O
sub	O
.	O
k	O
and	O
separately	O
represents	O
each	O
PDF	O
N.	O
sub	O
.	O
k	O
425	O
using	O
a	O
mean	O
vector	O
.	O
mu	O
.	O
.	O
sub	O
.	O
k	O
430	O
and	O
a	O
covariance	O
matrix	O
c	O
.	O
sub	O
.	O
k	O
435	O
.	O
Use	O
of	O
a	O
particular	O
PDF	O
to	O
represent	O
multiple	O
triphone	O
nodes	O
permits	O
the	O
model	O
245	O
to	O
include	O
a	O
smaller	O
number	O
of	O
PDFs	O
than	O
would	O
be	O
required	O
if	O
each	O
triphone	O
node	O
included	O
entirely	O
separate	O
PDFs	O
.	O
Since	O
the	O
English	O
language	O
may	O
be	O
roughly	O
represented	O
using	O
43	O
different	O
phonemes	O
,	O
there	O
may	O
be	O
up	O
to	O
79	O
,	O
507	O
(	O
43	O
.	O
sup	O
.	O
3	O
)	O
different	O
triphones	O
,	O
which	O
would	O
result	O
in	O
a	O
huge	O
number	O
of	O
PDFs	O
if	O
each	O
triphone	O
node	O
were	O
represented	O
by	O
a	O
separate	O
set	O
of	O
PDFs	O
.	O
Representing	O
multiple	O
nodes	O
with	O
common	O
PDFs	O
also	O
may	O
remedy	O
or	O
reduce	O
a	O
data	O
sparsity	O
problem	O
that	O
results	O
because	O
some	O
triphones	O
(	O
e.g.	O
,	O
"	O
tzp	O
"	O
in	O
the	O
English	O
language	O
)	O
rarely	O
occur	O
.	O
These	O
rare	O
triphones	O
may	O
be	O
represented	O
by	O
having	O
closely-related	O
triphones	O
share	O
the	O
same	O
set	O
of	O
PDFs	O
.	O

In	O
one	O
implementation	O
,	O
the	O
speaker-adapted	O
model	O
240	O
uses	O
the	O
mixture	O
weights	O
w	O
.	O
sub	O
.	O
k	O
420	O
and	O
the	O
covariance	O
matrices	O
c	O
.	O
sub	O
.	O
k	O
435	O
of	O
the	O
speaker-independent	O
model	O
245	O
.	O
However	O
,	O
unlike	O
the	O
speaker-independent	O
model	O
245	O
,	O
the	O
speaker-adapted	O
model	O
240	O
uses	O
adapted	O
mean	O
vectors	O
.mu..sub.kA	O
440	O
that	O
have	O
been	O
adapted	O
for	O
a	O
particular	O
speaker	O
.	O
In	O
other	O
implementations	O
,	O
the	O
speaker-adapted	O
model	O
may	O
use	O
adapted	O
mixture	O
weights	O
and	O
covariance	O
matrices	O
.	O

Referring	O
to	O
FIG	O
.	O
5	O
,	O
the	O
adapted	O
mean	O
vectors	O
.mu..sub.kA	O
440	O
of	O
the	O
speaker-adapted	O
model	O
240	O
for	O
a	O
particular	O
speaker	O
are	O
produced	O
according	O
to	O
a	O
procedure	O
500	O
.	O
Starting	O
with	O
the	O
mean	O
vectors	O
.	O
mu	O
.	O
.	O
sub	O
.	O
k	O
430	O
of	O
the	O
speaker-independent	O
model	O
245	O
(	O
step	O
505	O
)	O
,	O
a	O
transformation-based	O
approach	O
is	O
used	O
to	O
produce	O
the	O
adapted	O
mean	O
vectors	O
.mu..sub.kA	O
440	O
.	O
As	O
noted	O
above	O
,	O
the	O
covariance	O
matrices	O
c	O
.	O
sub	O
.	O
k	O
435	O
are	O
not	O
adapted	O
.	O
The	O
transformation-based	O
approach	O
assumes	O
that	O
an	O
adapted	O
mean	O
vector	O
.mu..sub.kA	O
may	O
be	O
expressed	O
as	O
:	O
.mu..sub.kA	O
=A.mu..sub.k	O
+	O
b	O
,	O
where	O
A	O
and	O
b	O
are	O
transforms	O
.	O
When	O
each	O
mean	O
vector	O
has	O
twenty	O
four	O
entries	O
,	O
A	O
is	O
a	O
twenty	O
four	O
by	O
twenty	O
four	O
matrix	O
and	O
b	O
has	O
twenty	O
four	O
entries	O
.	O

Next	O
,	O
collections	O
of	O
PDFs	O
that	O
are	O
expected	O
to	O
share	O
a	O
common	O
transformation	O
are	O
determined	O
(	O
step	O
510	O
)	O
.	O
A	O
collection	O
C	O
could	O
include	O
the	O
PDFs	O
representing	O
similarly-sounding	O
triphones	O
,	O
such	O
as	O
all	O
PDFs	O
associated	O
with	O
vowels	O
or	O
all	O
PDFs	O
associated	O
with	O
consonants	O
.	O
In	O
general	O
,	O
components	O
of	O
a	O
collection	O
should	O
be	O
of	O
closely-related	O
phonemes	O
,	O
and	O
each	O
collection	O
should	O
have	O
a	O
minimum	O
number	O
of	O
components	O
.	O
For	O
example	O
,	O
in	O
one	O
implementation	O
,	O
when	O
an	O
acoustic	O
model	O
includes	O
24	O
features	O
,	O
each	O
collection	O
is	O
required	O
to	O
include	O
at	O
least	O
96	O
components	O
.	O
It	O
also	O
may	O
be	O
desirable	O
for	O
collections	O
to	O
represent	O
similar	O
amounts	O
of	O
speaker	O
data	O
.	O

Another	O
approach	O
to	O
generating	O
collections	O
is	O
to	O
group	O
PDFs	O
having	O
mean	O
vectors	O
.	O
mu	O
.	O
.	O
sub	O
.	O
k	O
430	O
that	O
have	O
similar	O
values	O
.	O
This	O
approach	O
permits	O
simplified	O
,	O
automatic	O
generation	O
of	O
large	O
numbers	O
of	O
collections	O
.	O
In	O
general	O
,	O
implementations	O
may	O
include	O
from	O
one	O
to	O
several	O
hundred	O
collections	O
of	O
PDFs	O
.	O
Initial	O
experimental	O
results	O
indicate	O
that	O
the	O
first	O
approach	O
provides	O
better	O
results	O
(	O
i	O
.	O
e	O
,	O
results	O
with	O
lower	O
recognition	O
error	O
rates	O
)	O
.	O

As	O
mentioned	O
above	O
,	O
collections	O
may	O
be	O
generated	O
by	O
a	O
human	O
operator	O
or	O
may	O
be	O
generated	O
automatically	O
.	O
A	O
hybrid	O
approach	O
also	O
may	O
be	O
used	O
.	O
With	O
such	O
an	O
approach	O
,	O
a	O
human	O
operator	O
may	O
generally	O
classify	O
different	O
phonemes	O
as	O
being	O
related	O
and	O
an	O
automatic	O
procedure	O
then	O
may	O
be	O
used	O
to	O
generate	O
the	O
collections	O
.	O
The	O
automatic	O
procedure	O
may	O
penalize	O
collections	O
that	O
include	O
components	O
designated	O
by	O
the	O
human	O
operator	O
as	O
being	O
unrelated	O
.	O
In	O
another	O
implementation	O
,	O
the	O
automatic	O
procedure	O
may	O
cluster	O
only	O
PDFs	O
that	O
correspond	O
to	O
phonemes	O
designated	O
by	O
the	O
human	O
operator	O
as	O
being	O
related	O
.	O

Next	O
,	O
for	O
each	O
collection	O
,	O
all	O
speaker	O
data	O
for	O
the	O
PDFs	O
included	O
in	O
the	O
collection	O
are	O
assembled	O
(	O
step	O
515	O
)	O
.	O
The	O
speaker	O
data	O
may	O
be	O
supervised	O
training	O
data	O
or	O
unsupervised	O
training	O
data	O
.	O
Unsupervised	O
training	O
data	O
may	O
be	O
produced	O
as	O
the	O
speaker	O
uses	O
the	O
speech	O
recognition	O
system	O
.	O
Supervised	O
training	O
data	O
may	O
be	O
produced	O
,	O
for	O
example	O
,	O
by	O
prompting	O
the	O
speaker	O
to	O
say	O
certain	O
words	O
,	O
having	O
the	O
speaker	O
read	O
from	O
a	O
script	O
,	O
or	O
using	O
a	O
human	O
transcriber	O
to	O
verify	O
the	O
accuracy	O
of	O
results	O
produced	O
by	O
the	O
speech	O
recognition	O
system	O
.	O

Next	O
,	O
y	O
[	O
j	O
]	O
,	O
the	O
average	O
value	O
of	O
the	O
speaker	O
data	O
vector	O
,	O
is	O
determined	O
for	O
each	O
PDF	O
j	O
(	O
step	O
520	O
)	O
.	O
If	O
f	O
[	O
n	O
]	O
is	O
the	O
vector	O
for	O
the	O
n-th	O
frame	O
of	O
speaker	O
data	O
,	O
p.	O
sub	O
.	O
j	O
(	O
n	O
)	O
is	O
the	O
probability	O
that	O
the	O
n-th	O
frame	O
of	O
speaker	O
data	O
corresponds	O
to	O
a	O
PDF	O
j	O
,	O
and	O
N	O
is	O
the	O
total	O
number	O
of	O
frames	O
of	O
speaker	O
data	O
,	O
then	O
y	O
[	O
j	O
]	O
and	O
the	O
frame	O
count	O
,	O
N	O
[	O
j	O
]	O
,	O
for	O
the	O
frame	O
j	O
may	O
be	O
determined	O
as	O
:	O
This	O
technique	O
for	O
generating	O
y	O
[	O
j	O
]	O
and	O
N	O
[	O
j	O
]	O
usually	O
is	O
referred	O
to	O
as	O
the	O
Baum-Welch	O
or	O
EM	O
algorithm	O
.	O

Next	O
,	O
transforms	O
(	O
A.	O
sub	O
.	O
c	O
and	O
b	O
.	O
sub	O
.	O
c	O
)	O
are	O
produced	O
for	O
each	O
collection	O
C	O
using	O
the	O
relationship	O
between	O
the	O
average	O
values	O
of	O
the	O
speaker	O
data	O
and	O
the	O
PDFs	O
from	O
the	O
#	O
#	O
EQU3	O
#	O
#	O
speaker-independent	O
model	O
245	O
(	O
step	O
525	O
)	O
.	O
This	O
relationship	O
may	O
be	O
expressed	O
generally	O
as	O
:	O
y	O
[	O
j	O
]	O
=	O
A.	O
sub	O
.	O
c	O
x	O
[	O
j	O
]	O
+	O
b	O
.	O
sub	O
.	O
c	O
+	O
e	O
[	O
j	O
]	O
,	O
where	O
x	O
[	O
j	O
]	O
corresponds	O
to	O
the	O
mean	O
vector	O
.	O
mu	O
.	O
.	O
sub	O
.	O
j	O
for	O
a	O
PDF	O
j	O
and	O
e	O
[	O
j	O
]	O
is	O
an	O
error	O
term	O
.	O

In	O
general	O
,	O
the	O
error	O
term	O
includes	O
a	O
term	O
corresponding	O
to	O
sampling	O
error	O
in	O
y	O
[	O
j	O
]	O
and	O
an	O
additional	O
term	O
corresponding	O
to	O
modeling	O
error	O
,	O
which	O
may	O
be	O
significant	O
for	O
large	O
data	O
records	O
.	O
The	O
sampling	O
error	O
term	O
has	O
a	O
variance	O
that	O
is	O
inversely	O
proportional	O
to	O
N	O
[	O
j	O
]	O
.	O
As	O
such	O
,	O
the	O
this	O
portion	O
of	O
the	O
error	O
term	O
may	O
be	O
reduced	O
to	O
zero	O
as	O
N	O
[	O
j	O
]	O
increases	O
.	O
The	O
error	O
term	O
may	O
be	O
modeled	O
as	O
:	O
e	O
[	O
j	O
]	O
.	O
about	O
.	O
N	O
(	O
O	O
,	O
Io	O
.	O
sup	O
.	O
2	O
/N	O
[	O
j	O
]	O
)	O
,	O
where	O
N	O
(	O
m	O
,	O
C	O
)	O
indicates	O
a	O
Gaussian	O
distribution	O
with	O
mean	O
vector	O
m	O
and	O
covariance	O
matrix	O
C	O
,	O
O	O
is	O
a	O
vector	O
of	O
all	O
zeros	O
,	O
I	O
is	O
an	O
identity	O
matrix	O
,	O
and	O
o	O
.	O
sup	O
.	O
2	O
corresponds	O
to	O
error	O
variance	O
.	O

Based	O
on	O
this	O
formulation	O
,	O
maximum	O
likelihood	O
estimates	O
(	O
MLEs	O
)	O
are	O
derived	O
for	O
the	O
transformation	O
parameters	O
(	O
A.	O
sub	O
.	O
c	O
,	O
b	O
.	O
sub	O
.	O
c	O
)	O
under	O
different	O
constraints	O
,	O
such	O
as	O
b	O
.	O
sub	O
.	O
c	O
equals	O
zero	O
,	O
and	O
A.	O
sub	O
.	O
c	O
is	O
block-diagonal	O
,	O
banded	O
,	O
tri-diagonal	O
,	O
or	O
diagonal	O
.	O
The	O
solutions	O
for	O
the	O
MLEs	O
involve	O
solving	O
a	O
set	O
of	O
linear	O
equations	O
.	O
For	O
example	O
,	O
in	O
the	O
unconstrained	O
case	O
,	O
the	O
MLEs	O
are	O
given	O
by	O
:	O
#	O
#	O
EQU4	O
#	O
#	O
where	O
[	O
A.	O
sub	O
.	O
c	O
b	O
.	O
sub	O
.	O
c	O
]	O
.	O
sup	O
.	O
T	O
is	O
the	O
transpose	O
of	O
the	O
matrix	O
formed	O
by	O
inserting	O
b	O
.	O
sub	O
.	O
c	O
as	O
an	O
additional	O
column	O
of	O
A.	O
sub	O
.	O
c	O
,	O
x	O
.	O
sup	O
.	O
+	O
[	O
j	O
]	O
is	O
the	O
column	O
concatenation	O
of	O
x	O
[	O
j	O
]	O
and	O
1	O
,	O
x	O
.	O
sup	O
.	O
+	O
T	O
[	O
j	O
]	O
is	O
the	O
transpose	O
of	O
x	O
.	O
sup	O
.	O
+	O
T	O
[	O
j	O
]	O
,	O
y	O
.	O
sup	O
.	O
T	O
[	O
j	O
]	O
is	O
the	O
transpose	O
of	O
y	O
[	O
j	O
]	O
,	O
and	O
N.	O
sub	O
.	O
C	O
is	O
the	O
number	O
of	O
components	O
in	O
the	O
collection	O
C.	O
The	O
transformation	O
(	O
A.	O
sub	O
.	O
c	O
,	O
b	O
.	O
sub	O
.	O
c	O
)	O
then	O
is	O
determined	O
by	O
transposing	O
[	O
A.	O
sub	O
.	O
c	O
b	O
.	O
sub	O
.	O
c	O
]	O
.	O
sup	O
.	O
T.	O

Using	O
the	O
transformations	O
,	O
adapted	O
mean	O
vectors	O
.mu..sub.jA	O
440	O
are	O
determined	O
for	O
each	O
component	O
in	O
the	O
collection	O
C	O
(	O
step	O
530	O
)	O
.	O
In	O
particular	O
,	O
the	O
adapted	O
mean	O
vectors	O
are	O
determined	O
as	O
:	O
.mu..sub.jA	O
=	O
A.	O
sub	O
.	O
c	O
.	O
mu	O
.	O
.	O
sub	O
.	O
j	O
+	O
b	O
.	O
sub	O
.	O
c	O
.	O

The	O
adapted	O
mean	O
vectors	O
.mu..sub.jA	O
440	O
then	O
are	O
stored	O
along	O
with	O
a	O
speaker-identifier	O
445	O
as	O
a	O
speaker-adapted	O
model	O
240	O
for	O
later	O
use	O
in	O
recognizing	O
speech	O
by	O
the	O
associated	O
speaker	O
(	O
step	O
535	O
)	O
.	O
This	O
process	O
may	O
be	O
iterated	O
several	O
times	O
by	O
using	O
the	O
adapted	O
models	O
to	O
update	O
the	O
frame	O
counts	O
N	O
[	O
j	O
]	O
,	O
the	O
speaker	O
average	O
y	O
[	O
j	O
]	O
,	O
and	O
the	O
transform	O
matrix	O
.	O

In	O
another	O
implementation	O
,	O
fractions	O
of	O
the	O
speaker	O
independent	O
means	O
(	O
x	O
[	O
j	O
]	O
)	O
and	O
the	O
speaker-specific	O
means	O
may	O
be	O
used	O
to	O
compute	O
y	O
[	O
j	O
]	O
and	O
N	O
[	O
j	O
]	O
:	O
#	O
#	O
EQU5	O
#	O
#	O
where	O
r	O
is	O
a	O
smoothing	O
parameter	O
that	O
controls	O
the	O
relative	O
importance	O
of	O
the	O
speaker-independent	O
means	O
and	O
the	O
observed	O
data	O
.	O
The	O
value	O
of	O
r	O
is	O
optimized	O
as	O
a	O
function	O
of	O
the	O
amount	O
of	O
the	O
adaptation	O
data	O
available	O
.	O
In	O
one	O
implementation	O
,	O
the	O
value	O
of	O
r	O
starts	O
at	O
two	O
when	O
no	O
adaptation	O
data	O
is	O
available	O
and	O
is	O
decreased	O
to	O
0	O
.	O
25	O
as	O
the	O
amount	O
of	O
adaptation	O
data	O
increases	O
.	O

As	O
shown	O
in	O
FIG	O
.	O
6	O
,	O
a	O
procedure	O
600	O
may	O
also	O
be	O
used	O
to	O
retrain	O
the	O
speaker-independent	O
model	O
in	O
the	O
same	O
way	O
as	O
the	O
speaker-adapted	O
models	O
are	O
created	O
.	O
The	O
procedure	O
starts	O
with	O
a	O
baseline	O
speaker-independent	O
model	O
M.	O
sup	O
.	O
(	O
j	O
)	O
;	O
i	O
,	O
an	O
iteration	O
count	O
that	O
is	O
initialized	O
to	O
zero	O
;	O
and	O
l	O
,	O
a	O
speaker	O
count	O
that	O
is	O
initialized	O
to	O
one	O
(	O
step	O
605	O
)	O
.	O
A	O
speaker-adapted	O
model	O
M.	O
sup	O
.	O
(	O
i	O
)	O
[	O
l	O
]	O
then	O
is	O
generated	O
for	O
the	O
speaker	O
l	O
by	O
multiplying	O
the	O
mean	O
vectors	O
of	O
the	O
speaker-independent	O
model	O
by	O
the	O
transforms	O
for	O
the	O
speaker	O
l	O
(	O
i	O
.	O
e	O
.	O
,	O
M.	O
sup	O
.	O
(	O
i	O
)	O
[	O
l	O
]	O
equals	O
A.	O
sup	O
.	O
(	O
i	O
)	O
[	O
l	O
]	O
M.	O
sup	O
.	O
(	O
i	O
)	O
+	O
b	O
.	O
sup	O
.	O
(	O
i	O
)	O
[	O
l	O
]	O
)	O
(	O
step	O
610	O
)	O
.	O
For	O
the	O
initial	O
iteration	O
each	O
transform	O
is	O
an	O
identity	O
matrix	O
.	O

Next	O
,	O
the	O
Baum-Welch/EM	O
algorithm	O
is	O
run	O
using	O
the	O
speaker	O
data	O
to	O
determine	O
the	O
mean	O
speaker	O
values	O
(	O
y.sub.j.sup	O
.	O
(	O
i	O
)	O
[	O
l	O
]	O
)	O
and	O
the	O
frame	O
counts	O
(	O
N.sub.j.sup	O
.	O
(	O
i	O
)	O
[	O
l	O
]	O
)	O
(	O
step	O
615	O
)	O
for	O
all	O
mixture	O
components	O
as	O
:	O
#	O
#	O
EQU6	O
#	O
#	O
where	O
f	O
.	O
sub	O
.	O
l	O
[	O
n	O
]	O
is	O
the	O
vector	O
for	O
the	O
n-th	O
frame	O
of	O
speaker	O
data	O
for	O
the	O
speaker	O
l	O
,	O
p.	O
sub	O
.	O
jl	O
(	O
n	O
)	O
is	O
the	O
probability	O
that	O
the	O
n-th	O
frame	O
of	O
speaker	O
data	O
for	O
the	O
speaker	O
l	O
corresponds	O
to	O
a	O
PDF	O
j	O
,	O
and	O
N.	O
sub	O
.	O
l	O
is	O
the	O
number	O
of	O
frames	O
of	O
speaker	O
data	O
for	O
the	O
speaker	O
l	O
.	O
The	O
covariance	O
matrices	O
c	O
.	O
sub	O
.	O
k	O
also	O
are	O
updated	O
based	O
on	O
the	O
speaker	O
data	O
(	O
step	O
620	O
)	O
.	O

Next	O
,	O
the	O
transforms	O
(	O
A.sub.c.sup	O
.	O
(	O
i	O
+	O
l	O
)	O
[	O
l	O
]	O
and	O
b.sub.c.sup	O
.	O
(	O
i	O
+	O
l	O
)	O
[	O
l	O
]	O
)	O
are	O
produced	O
for	O
the	O
speaker	O
l	O
as	O
discussed	O
above	O
with	O
respect	O
to	O
step	O
525	O
of	O
the	O
procedure	O
500	O
(	O
step	O
625	O
)	O
.	O
The	O
transforms	O
are	O
produced	O
using	O
the	O
speaker-independent	O
model	O
M.	O
sup	O
.	O
(	O
i	O
)	O
and	O
the	O
means	O
y.sub.j.sup	O
.	O
(	O
i	O
)	O
[	O
l	O
]	O
.	O

After	O
the	O
transforms	O
are	O
produced	O
,	O
the	O
speaker	O
count	O
is	O
incremented	O
(	O
step	O
630	O
)	O
and	O
a	O
determination	O
is	O
made	O
as	O
to	O
whether	O
the	O
speaker	O
count	O
exceeds	O
N.	O
sub	O
.	O
s	O
,	O
the	O
number	O
of	O
speakers	O
(	O
i	O
.	O
e	O
.	O
,	O
a	O
determination	O
is	O
made	O
as	O
to	O
whether	O
transforms	O
have	O
been	O
generated	O
for	O
all	O
of	O
the	O
speakers	O
)	O
(	O
step	O
635	O
)	O
.	O
If	O
data	O
for	O
additional	O
speakers	O
need	O
to	O
be	O
considered	O
(	O
i	O
.	O
e	O
.	O
,	O
l	O
<	O
=	O
N.	O
sub	O
.	O
s	O
)	O
,	O
then	O
the	O
procedure	O
is	O
repeated	O
starting	O
at	O
step	O
610	O
for	O
the	O
new	O
speaker	O
designated	O
by	O
the	O
incremented	O
speaker	O
count	O
.	O

After	O
transforms	O
are	O
generated	O
for	O
all	O
speakers	O
,	O
an	O
updated	O
speaker-independent	O
model	O
M.	O
sup	O
.	O
(	O
i	O
+	O
l	O
)	O
is	O
produced	O
(	O
step	O
640	O
)	O
.	O
Model	O
means	O
x.sub.j.sup	O
.	O
(	O
i	O
+	O
l	O
)	O
(	O
also	O
referred	O
to	O
as	O
.mu..sub.j.sup	O
.	O
(	O
i	O
+	O
l	O
)	O
)	O
for	O
the	O
updated	O
model	O
may	O
be	O
determined	O
as	O
:	O
#	O
#	O
EQU7	O
#	O
#	O
Model	O
variances	O
and	O
mixture	O
weights	O
then	O
are	O
updated	O
using	O
standard	O
techniques	O
.	O
Such	O
techniques	O
are	O
described	O
,	O
for	O
example	O
,	O
by	O
Robert	B-Citation
Roth	I-Citation
et_al.	I-Citation
,	I-Citation
Dragon	I-Citation
Systems	I-Citation
1994	I-Citation
Large	I-Citation
Vocabulary	I-Citation
Continuous	I-Citation
Speech	I-Citation
Recognizer	I-Citation
,	I-Citation
Proceedings	I-Citation
of	I-Citation
the	I-Citation
Spoken	I-Citation
Language	I-Citation
Systems	I-Citation
Technology	I-Citation
Workshop	I-Citation
,	I-Citation
Jan.	I-Citation
22	I-Citation
-	I-Citation
25	I-Citation
,	I-Citation
1995	I-Citation
,	I-Citation
pages	I-Citation
116	I-Citation
-	I-Citation
120	I-Citation
,	O
which	O
is	O
incorporated	O
by	O
reference	O
.	O
Alternatively	O
,	O
the	O
model	O
means	O
x.sub.j.sup	O
.	O
(	O
i	O
+	O
l	O
)	O
for	O
the	O
updated	O
model	O
may	O
be	O
determined	O
as	O
:	O
#	O
#	O
EQU8	O
#	O
#	O
which	O
corresponds	O
to	O
applying	O
the	O
inverse	O
transformation	O
to	O
each	O
training	O
speaker	O
's	O
mean	O
and	O
then	O
applying	O
the	O
standard	O
re-estimation	O
technique	O
.	O

Next	O
,	O
the	O
iteration	O
count	O
is	O
incremented	O
(	O
step	O
645	O
)	O
and	O
a	O
determination	O
is	O
made	O
as	O
to	O
whether	O
the	O
iteration	O
count	O
equals	O
i	O
.	O
sub	O
.	O
max	O
,	O
which	O
corresponds	O
to	O
the	O
number	O
of	O
iterations	O
to	O
be	O
performed	O
(	O
step	O
650	O
)	O
.	O
In	O
one	O
implementation	O
,	O
i	O
.	O
sub	O
.	O
max	O
equals	O
three	O
so	O
that	O
three	O
iterations	O
are	O
performed	O
.	O
If	O
the	O
iteration	O
count	O
equals	O
i	O
.	O
sub	O
.	O
max	O
,	O
then	O
the	O
procedure	O
is	O
complete	O
.	O
If	O
the	O
iteration	O
count	O
is	O
less	O
than	O
i	O
.	O
sub	O
.	O
max	O
,	O
then	O
the	O
speaker	O
count	O
is	O
reset	O
to	O
one	O
(	O
step	O
655	O
)	O
and	O
the	O
procedure	O
is	O
repeated	O
starting	O
at	O
step	O
610	O
.	O

FIG	O
.	O
7	O
illustrates	O
an	O
alternative	O
procedure	O
700	O
for	O
creating	O
speaker-independent	O
models	O
.	O
According	O
to	O
the	O
procedure	O
700	O
,	O
the	O
training	O
data	O
from	O
the	O
different	O
speakers	O
are	O
modified	O
to	O
account	O
for	O
speaker	O
variations	O
prior	O
to	O
updating	O
the	O
speaker-independent	O
model	O
.	O
The	O
procedure	O
starts	O
with	O
l	O
,	O
the	O
speaker	O
count	O
,	O
initialized	O
to	O
one	O
(	O
step	O
705	O
)	O
.	O
Using	O
the	O
current	O
speaker	O
independent	O
models	O
and	O
frames	O
of	O
training	O
data	O
[	O
f	O
.	O
sub	O
.	O
l	O
[	O
n	O
]	O
]	O
,	O
transformation	O
parameters	O
[	O
A.	O
sub	O
.	O
c	O
,	O
b	O
.	O
sub	O
.	O
c	O
]	O
are	O
produced	O
for	O
each	O
speaker	O
l	O
.	O
Next	O
,	O
for	O
each	O
mixture	O
component	O
j	O
,	O
the	O
difference	O
.DELTA..mu..sub.j	O
[	O
l	O
]	O
between	O
the	O
speaker-adapted	O
mean	O
value	O
.mu..sub.jA	O
l	O
[	O
l	O
]	O
and	O
the	O
mean	O
value	O
.	O
mu	O
.	O
.	O
sub	O
.	O
j	O
(	O
i	O
.	O
e	O
.	O
,	O
x	O
.	O
sub	O
.	O
j	O
)	O
from	O
a	O
baseline	O
speaker-independent	O
is	O
determined	O
as	O
(	O
step	O
710	O
)	O
:	O
.DELTA..mu..sub.j	O
[	O
l	O
]	O
=.mu..sub.jA	O
[	O
l	O
]	O
-	O
.	O
mu	O
.	O
.	O
sub	O
.	O
j	O
=	O
(	O
A.	O
sub	O
.	O
c	O
[	O
l	O
]	O
-	O
I	O
)	O
.	O
mu	O
.	O
.	O
sub	O
.	O
j	O
+	O
b	O
.	O
sub	O
.	O
c	O
.	O

Adjusted	O
speaker	O
data	O
f	O
*	O
.	O
sub	O
.	O
l	O
[	O
n	O
]	O
then	O
is	O
produced	O
by	O
subtracting	O
.DELTA..mu..sub.j	O
[	O
l	O
]	O
from	O
each	O
frame	O
f	O
.	O
sub	O
.	O
l	O
[	O
n	O
]	O
of	O
speaker	O
data	O
in	O
a	O
probabilistic	O
way	O
as	O
(	O
step	O
715	O
)	O
:	O
#	O
#	O
EQU9	O
#	O
#	O
where	O
p.	O
sub	O
.	O
jl	O
[	O
n	O
]	O
is	O
the	O
probability	O
that	O
frame	O
f	O
.	O
sub	O
.	O
l	O
[	O
n	O
]	O
came	O
from	O
mixture	O
component	O
j	O
,	O
and	O
J	O
is	O
the	O
total	O
number	O
of	O
mixture	O
components	O
.	O

Next	O
,	O
the	O
speaker	O
count	O
is	O
incremented	O
(	O
step	O
720	O
)	O
and	O
a	O
determination	O
is	O
made	O
as	O
to	O
whether	O
the	O
speaker	O
count	O
exceeds	O
N.	O
sub	O
.	O
s	O
,	O
the	O
number	O
of	O
speakers	O
(	O
step	O
725	O
)	O
.	O
If	O
data	O
for	O
additional	O
speakers	O
needs	O
to	O
be	O
considered	O
(	O
i	O
.	O
e	O
.	O
,	O
l	O
<	O
=	O
N.	O
sub	O
.	O
s	O
)	O
,	O
then	O
the	O
procedure	O
is	O
repeated	O
starting	O
at	O
step	O
710	O
for	O
the	O
new	O
speaker	O
designated	O
by	O
the	O
incremented	O
speaker	O
count	O
.	O

After	O
the	O
data	O
for	O
all	O
of	O
the	O
training	O
speakers	O
has	O
been	O
adjusted	O
,	O
the	O
speaker	O
independent	O
model	O
is	O
updated	O
using	O
the	O
adjusted	O
training	O
speaker	O
data	O
(	O
f	O
*	O
.	O
sub	O
.	O
I	O
[	O
n	O
]	O
)	O
(	O
step	O
730	O
)	O
.	O
The	O
overall	O
procedure	O
may	O
be	O
iterated	O
by	O
recomputing	O
the	O
speaker	O
transformations	O
and	O
updating	O
the	O
values	O
for	O
.DELTA..mu..sub.j	O
,	O
the	O
adjusted	O
training	O
data	O
,	O
and	O
,	O
finally	O
,	O
the	O
speaker	O
independent	O
models	O
.	O

The	O
same	O
approach	O
may	O
be	O
used	O
to	O
adjust	O
speech	O
data	O
prior	O
to	O
performing	O
recognition	O
using	O
the	O
data	O
.	O
In	O
particular	O
,	O
as	O
described	O
above	O
,	O
frames	O
of	O
speech	O
data	O
f	O
.	O
sub	O
.	O
sl	O
[	O
n	O
]	O
from	O
a	O
speaker	O
l	O
may	O
be	O
adjusted	O
based	O
on	O
variations	O
for	O
that	O
speaker	O
to	O
produce	O
adjusted	O
frames	O
of	O
speech	O
data	O
f	O
*	O
.	O
sub	O
.	O
sl	O
[	O
n	O
]	O
:	O
f	O
*	O
.	O
sub	O
.	O
sl	O
[	O
n	O
]	O
=	O
f	O
.	O
sub	O
.	O
sl	O
[	O
n	O
]	O
-	O
.	O
DELTA	O
.	O
.	O
mu	O
.	O
[	O
l	O
]	O
,	O
where	O
.	O
DELTA	O
.	O
.	O
mu	O
.	O
[	O
l	O
]	O
[	O
n	O
]	O
is	O
a	O
weighted	O
average	O
of	O
.DELTA..mu..sub.j	O
[	O
l	O
]	O
for	O
all	O
mixture	O
components	O
j	O
.	O
The	O
adjusted	O
frames	O
of	O
speech	O
data	O
then	O
may	O
be	O
evaluated	O
against	O
a	O
speaker-independent	O
model	O
to	O
achieve	O
some	O
of	O
the	O
effects	O
of	O
using	O
a	O
speaker-adapted	O
model	O
without	O
actually	O
using	O
a	O
speaker-adapted	O
model	O
.	O

Other	O
embodiments	O
are	O
within	O
the	O
scope	O
of	O
the	O
following	O
claims	O
.	O

1	O
.	O
A	O
method	O
of	O
generating	O
a	O
source-adapted	O
model	O
for	O
use	O
in	O
speech	O
recognition	O
,	O
the	O
method	O
comprising	O
:	O
generating	O
a	O
collection	O
of	O
elements	O
from	O
an	O
initial	O
model	O
;	O
assembling	O
source	O
speech	O
data	O
that	O
corresponds	O
to	O
elements	O
in	O
the	O
collection	O
of	O
elements	O
from	O
a	O
set	O
of	O
source	O
speech	O
data	O
for	O
a	O
particular	O
source	O
associated	O
with	O
the	O
source-adapted	O
model	O
;	O
generating	O
statistics	O
from	O
the	O
assembled	O
source	O
speech	O
data	O
;	O
modifying	O
the	O
statistics	O
using	O
an	O
element	O
of	O
the	O
initial	O
model	O
and	O
a	O
smoothing	O
factor	O
that	O
accounts	O
for	O
the	O
relative	O
importance	O
of	O
the	O
element	O
of	O
the	O
initial	O
model	O
and	O
the	O
assembled	O
source	O
speech	O
data	O
;	O
using	O
the	O
modified	O
statistics	O
in	O
determining	O
a	O
transform	O
that	O
maps	O
between	O
the	O
assembled	O
source	O
speech	O
data	O
and	O
the	O
collection	O
of	O
elements	O
of	O
the	O
initial	O
model	O
;	O
and	O
producing	O
elements	O
of	O
the	O
source-adapted	O
model	O
from	O
corresponding	O
elements	O
of	O
the	O
initial	O
model	O
by	O
applying	O
the	O
transform	O
to	O
the	O
elements	O
of	O
the	O
initial	O
model	O
;	O
wherein	O
determining	O
the	O
transform	O
comprises	O
determining	O
a	O
relationship	O
between	O
each	O
element	O
of	O
the	O
initial	O
model	O
in	O
the	O
collection	O
and	O
a	O
portion	O
of	O
the	O
assembled	O
source	O
speech	O
data	O
that	O
corresponds	O
to	O
that	O
element	O
.	O

2	O
.	O
The	O
method	O
of	O
claim	O
1	O
,	O
wherein	O
generating	O
the	O
collection	O
comprises	O
generating	O
a	O
collection	O
of	O
elements	O
associated	O
with	O
similarly-sounding	O
speech	O
units	O
.	O

3	O
.	O
The	O
method	O
of	O
claim	O
1	O
,	O
wherein	O
generating	O
the	O
collection	O
comprises	O
generating	O
a	O
collection	O
of	O
elements	O
having	O
similar	O
values	O
.	O

4	O
.	O
The	O
method	O
of	O
claim	O
1	O
,	O
wherein	O
generating	O
the	O
collection	O
comprises	O
generating	O
a	O
collection	O
of	O
elements	O
associated	O
with	O
similarly-sounding	O
speech	O
units	O
and	O
having	O
similar	O
values	O
.	O

5	O
.	O
The	O
method	O
of	O
claim	O
4	O
,	O
wherein	O
a	O
human	O
operator	O
identifies	O
classes	O
of	O
elements	O
having	O
similarly-sounding	O
speech	O
units	O
and	O
generating	O
the	O
collection	O
comprises	O
using	O
an	O
automatic	O
procedure	O
that	O
employs	O
the	O
identified	O
classes	O
and	O
similarities	O
between	O
elements	O
.	O

6	O
.	O
The	O
method	O
of	O
claim	O
1	O
,	O
wherein	O
the	O
statistics	O
comprise	O
an	O
average	O
value	O
of	O
the	O
assembled	O
source	O
speech	O
data	O
for	O
a	O
particular	O
element	O
.	O

7	O
.	O
The	O
method	O
of	O
claim	O
6	O
,	O
wherein	O
the	O
statistics	O
comprise	O
a	O
count	O
of	O
speech	O
frames	O
associated	O
with	O
the	O
assembled	O
source	O
speech	O
data	O
for	O
the	O
particular	O
element	O
.	O

8	O
.	O
The	O
method	O
of	O
claim	O
1	O
,	O
wherein	O
the	O
smoothing	O
factor	O
accounts	O
for	O
an	O
amount	O
of	O
source	O
speech	O
data	O
available	O
for	O
a	O
particular	O
element	O
.	O

9	O
.	O
The	O
method	O
of	O
claim	O
8	O
,	O
wherein	O
the	O
statistics	O
comprise	O
an	O
average	O
value	O
of	O
the	O
assembled	O
source	O
speech	O
data	O
for	O
a	O
particular	O
element	O
,	O
the	O
average	O
value	O
being	O
modified	O
based	O
on	O
the	O
smoothing	O
factor	O
and	O
a	O
value	O
for	O
the	O
element	O
from	O
the	O
initial	O
model	O
.	O

10	O
.	O
The	O
method	O
of	O
claim	O
1	O
,	O
wherein	O
:	O
each	O
element	O
of	O
the	O
initial	O
model	O
includes	O
a	O
mean	O
portion	O
and	O
a	O
variance	O
portion	O
,	O
and	O
producing	O
an	O
element	O
of	O
the	O
source-adapted	O
model	O
from	O
a	O
corresponding	O
element	O
of	O
the	O
initial	O
model	O
comprises	O
applying	O
the	O
transform	O
to	O
the	O
mean	O
portion	O
of	O
the	O
element	O
of	O
the	O
initial	O
model	O
and	O
leaving	O
the	O
variance	O
portion	O
unchanged	O
.	O

11	O
.	O
A	O
method	O
of	O
generating	O
a	O
source-adapted	O
model	O
for	O
use	O
in	O
speech	O
recognition	O
,	O
the	O
method	O
comprising	O
:	O
generating	O
a	O
collection	O
of	O
elements	O
from	O
an	O
initial	O
model	O
;	O
assembling	O
speech	O
data	O
that	O
corresponds	O
to	O
elements	O
in	O
the	O
collection	O
of	O
elements	O
from	O
a	O
set	O
of	O
speech	O
data	O
for	O
a	O
particular	O
source	O
associated	O
with	O
the	O
source-adapted	O
model	O
;	O
generating	O
statistics	O
from	O
the	O
assembled	O
source	O
speech	O
data	O
;	O
modifying	O
the	O
statistics	O
using	O
an	O
element	O
of	O
the	O
initial	O
model	O
and	O
a	O
smoothing	O
factor	O
that	O
accounts	O
for	O
an	O
amount	O
of	O
source	O
speech	O
data	O
available	O
for	O
a	O
particular	O
element	O
;	O
determining	O
a	O
transform	O
that	O
maps	O
between	O
the	O
assembled	O
speech	O
data	O
and	O
the	O
collection	O
of	O
elements	O
of	O
the	O
initial	O
model	O
using	O
the	O
modified	O
statistics	O
;	O
and	O
producing	O
elements	O
of	O
the	O
source-adapted	O
model	O
from	O
corresponding	O
elements	O
of	O
the	O
initial	O
model	O
by	O
applying	O
the	O
transform	O
to	O
the	O
elements	O
of	O
the	O
initial	O
model	O
.	O

12	O
.	O
The	O
method	O
of	O
claim	O
11	O
,	O
wherein	O
the	O
statistics	O
comprise	O
an	O
average	O
value	O
of	O
the	O
assembled	O
source	O
speech	O
data	O
for	O
a	O
particular	O
element	O
,	O
the	O
average	O
value	O
being	O
modified	O
based	O
on	O
the	O
smoothing	O
factor	O
and	O
a	O
value	O
for	O
the	O
element	O
from	O
the	O
initial	O
model	O
.	O

13	O
.	O
A	O
method	O
of	O
generating	O
a	O
source-adapted	O
model	O
for	O
use	O
in	O
speech	O
recognition	O
,	O
the	O
method	O
comprising	O
:	O
generating	O
a	O
collection	O
of	O
elements	O
from	O
the	O
initial	O
model	O
;	O
assembling	O
speech	O
data	O
that	O
corresponds	O
to	O
elements	O
in	O
the	O
collection	O
of	O
elements	O
from	O
a	O
set	O
of	O
speech	O
data	O
for	O
a	O
particular	O
source	O
associated	O
with	O
the	O
source-adapted	O
model	O
;	O
generating	O
statistics	O
from	O
the	O
assembled	O
source	O
speech	O
data	O
;	O
modifying	O
the	O
statistics	O
using	O
an	O
element	O
of	O
the	O
initial	O
model	O
and	O
a	O
smoothing	O
factor	O
that	O
controls	O
the	O
relative	O
importance	O
of	O
the	O
element	O
of	O
the	O
initial	O
model	O
and	O
the	O
assembled	O
source	O
speech	O
data	O
;	O
using	O
the	O
modified	O
statistics	O
in	O
determining	O
a	O
transform	O
that	O
maps	O
between	O
the	O
assembled	O
speech	O
data	O
and	O
the	O
collection	O
of	O
elements	O
of	O
the	O
initial	O
model	O
;	O
and	O
producing	O
elements	O
of	O
the	O
source-adapted	O
model	O
from	O
corresponding	O
elements	O
of	O
the	O
initial	O
model	O
by	O
applying	O
the	O
transform	O
to	O
the	O
elements	O
of	O
the	O
initial	O
model	O
;	O
wherein	O
a	O
human	O
operator	O
identifies	O
classes	O
of	O
related	O
elements	O
and	O
the	O
collection	O
of	O
elements	O
is	O
generated	O
using	O
an	O
automatic	O
procedure	O
that	O
favors	O
including	O
elements	O
from	O
a	O
common	O
class	O
in	O
the	O
collection	O
and	O
penalizes	O
including	O
elements	O
from	O
different	O
classes	O
in	O
the	O
collection	O
.	O

14	O
.	O
A	O
method	O
of	O
training	O
a	O
speech	O
model	O
for	O
use	O
in	O
speech	O
recognition	O
,	O
the	O
method	O
comprising	O
:	O
assembling	O
sets	O
of	O
speech	O
data	O
that	O
correspond	O
to	O
elements	O
of	O
an	O
initial	O
model	O
from	O
a	O
set	O
of	O
speech	O
data	O
for	O
one	O
or	O
more	O
sources	O
,	O
the	O
assembled	O
speech	O
data	O
including	O
multiple	O
items	O
;	O
calculating	O
representative	O
values	O
for	O
each	O
set	O
of	O
speech	O
data	O
;	O
modifying	O
a	O
representative	O
value	O
for	O
a	O
set	O
of	O
speech	O
data	O
using	O
a	O
corresponding	O
element	O
of	O
the	O
initial	O
model	O
and	O
a	O
smoothing	O
factor	O
that	O
controls	O
the	O
relative	O
importance	O
of	O
the	O
corresponding	O
element	O
of	O
the	O
initial	O
model	O
and	O
the	O
set	O
of	O
speech	O
data	O
;	O
determining	O
a	O
relationship	O
between	O
the	O
representative	O
values	O
for	O
each	O
set	O
of	O
speech	O
data	O
and	O
values	O
of	O
the	O
corresponding	O
element	O
of	O
the	O
initial	O
model	O
,	O
where	O
a	O
relationship	O
for	O
a	O
first	O
set	O
of	O
speech	O
data	O
differs	O
from	O
a	O
relationship	O
for	O
a	O
second	O
set	O
of	O
speech	O
data	O
;	O
modifying	O
each	O
item	O
of	O
the	O
sets	O
of	O
speech	O
data	O
using	O
the	O
relationship	O
for	O
the	O
set	O
to	O
which	O
the	O
item	O
belongs	O
;	O
and	O
generating	O
elements	O
of	O
the	O
speech	O
model	O
using	O
the	O
modified	O
sets	O
of	O
speech	O
data	O
.	O

15	O
.	O
The	O
method	O
of	O
claim	O
14	O
,	O
wherein	O
the	O
initial	O
model	O
has	O
a	O
structure	O
that	O
differs	O
from	O
a	O
structure	O
of	O
the	O
speech	O
model	O
.	O

16	O
.	O
The	O
method	O
of	O
claim	O
14	O
,	O
wherein	O
the	O
initial	O
model	O
comprises	O
a	O
first	O
number	O
of	O
elements	O
and	O
the	O
speech	O
model	O
comprises	O
a	O
second	O
number	O
of	O
elements	O
,	O
the	O
first	O
number	O
being	O
substantially	O
larger	O
than	O
the	O
second	O
number	O
.	O

17	O
.	O
The	O
method	O
of	O
claim	O
14	O
,	O
wherein	O
the	O
source	O
comprises	O
a	O
particular	O
speaker	O
.	O

18	O
.	O
The	O
method	O
of	O
claim	O
14	O
,	O
wherein	O
:	O
assembling	O
sets	O
of	O
speech	O
data	O
that	O
correspond	O
to	O
elements	O
of	O
an	O
initial	O
model	O
comprises	O
assembling	O
different	O
sets	O
for	O
speech	O
data	O
from	O
different	O
sources	O
;	O
a	O
relationship	O
for	O
a	O
first	O
set	O
of	O
speech	O
data	O
for	O
a	O
first	O
source	O
differs	O
from	O
a	O
relationship	O
for	O
a	O
second	O
set	O
of	O
speech	O
data	O
for	O
the	O
first	O
source	O
and	O
from	O
a	O
relationship	O
for	O
a	O
third	O
set	O
of	O
speech	O
data	O
for	O
a	O
second	O
source	O
;	O
and	O
generating	O
an	O
element	O
of	O
the	O
speech	O
model	O
comprises	O
using	O
modified	O
sets	O
of	O
speech	O
data	O
from	O
different	O
sources	O
.	O

19	O
.	O
The	O
method	O
of	O
claim	O
14	O
,	O
wherein	O
the	O
relationship	O
for	O
a	O
first	O
set	O
of	O
speech	O
data	O
comprises	O
a	O
linear	O
transformation	O
.	O

20	O
.	O
The	O
method	O
of	O
claim	O
14	O
,	O
wherein	O
:	O
determining	O
a	O
relationship	O
comprises	O
determining	O
a	O
transform	O
that	O
maps	O
between	O
the	O
set	O
of	O
speech	O
data	O
and	O
the	O
element	O
of	O
the	O
initial	O
model	O
,	O
and	O
generating	O
an	O
inverse	O
of	O
the	O
transform	O
;	O
and	O
modifying	O
each	O
element	O
of	O
a	O
set	O
of	O
speech	O
data	O
comprises	O
applying	O
the	O
inverse	O
of	O
the	O
transform	O
to	O
each	O
element	O
of	O
the	O
set	O
of	O
speech	O
data	O
.	O

21	O
.	O
A	O
method	O
of	O
training	O
a	O
speech	O
model	O
for	O
use	O
in	O
speech	O
recognition	O
,	O
the	O
method	O
comprising	O
:	O
assembling	O
speech	O
data	O
that	O
corresponds	O
to	O
a	O
first	O
element	O
of	O
the	O
speech	O
model	O
from	O
a	O
set	O
of	O
speech	O
data	O
for	O
a	O
first	O
source	O
;	O
determining	O
a	O
transform	O
that	O
maps	O
between	O
the	O
assembled	O
speech	O
data	O
and	O
the	O
first	O
element	O
of	O
the	O
speech	O
model	O
;	O
generating	O
an	O
inverse	O
of	O
the	O
transform	O
;	O
modifying	O
the	O
assembled	O
speech	O
data	O
using	O
the	O
inverse	O
of	O
the	O
transform	O
;	O
and	O
updating	O
the	O
first	O
element	O
of	O
the	O
speech	O
model	O
using	O
the	O
modified	O
assembled	O
speech	O
data	O
,	O
wherein	O
determining	O
the	O
transform	O
comprises	O
modifying	O
the	O
transform	O
using	O
the	O
first	O
element	O
of	O
the	O
speech	O
model	O
and	O
a	O
smoothing	O
factor	O
that	O
controls	O
the	O
relative	O
importance	O
of	O
the	O
first	O
element	O
of	O
the	O
speech	O
model	O
and	O
the	O
assembled	O
speech	O
data	O
.	O

22	O
.	O
The	O
method	O
of	O
claim	O
21	O
,	O
further	O
comprising	O
:	O
from	O
a	O
set	O
of	O
speech	O
data	O
for	O
a	O
second	O
source	O
,	O
assembling	O
speech	O
data	O
that	O
corresponds	O
to	O
the	O
first	O
element	O
of	O
the	O
speech	O
model	O
;	O
determining	O
a	O
second	O
transform	O
that	O
maps	O
between	O
the	O
assembled	O
speech	O
data	O
for	O
the	O
second	O
source	O
and	O
the	O
first	O
element	O
of	O
the	O
speech	O
model	O
;	O
and	O
generating	O
an	O
inverse	O
of	O
the	O
second	O
transform	O
;	O
wherein	O
the	O
step	O
of	O
updating	O
comprises	O
updating	O
the	O
first	O
element	O
of	O
the	O
speech	O
model	O
using	O
the	O
assembled	O
speech	O
data	O
for	O
the	O
first	O
and	O
second	O
sources	O
and	O
the	O
inverses	O
of	O
the	O
first	O
and	O
second	O
transforms	O
.	O

23	O
.	O
The	O
method	O
of	O
claim	O
1	O
,	O
wherein	O
the	O
transform	O
comprises	O
a	O
linear	O
transform	O
.	O

24	O
.	O
The	O
method	O
of	O
claim	O
11	O
,	O
wherein	O
the	O
transform	O
comprises	O
a	O
linear	O
transform	O
.	O

25	O
.	O
The	O
method	O
of	O
claim	O
13	O
,	O
wherein	O
the	O
transform	O
comprises	O
a	O
linear	O
transform	O
.	O

