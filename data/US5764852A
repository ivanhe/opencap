US	O
5764852	O
A	O
19980609	O

US	O
08291372	O
19940816	O

eng	O
eng	O

19980609	O

19980609	O

6G	O
10L	O
3/02	O
A	O
6	O
G	O
10	O
L	O
3	O
02	O
A	O

G10L	O
15/00	O
20060101C	O
I20051008RMEP	O

20060101	O

C	O
G	O
10	O
L	O
15	O
00	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/20	O
20060101A	O
N20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
20	O
N	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/26	O
20060101A	O
I20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
26	O
I	O

20051008	O

EP	O

R	O
M	O

US	O

704/243	O
704	O
243	O

704/201	O
704	O
201	O

704/226	O
704	O
226	O

704/227	O
704	O
227	O

704/231	O
704	O
231	O

704/233	O
704	O
233	O

704/251	O
704	O
251	O

704/E15.044	O
704	O
E15	O
.	O
044	O

G10L	O
15/26C	O
G	O
10	O
L	O
15	O
26	O

C	O

S10L	O
15	O
:	O
20T3	O
S	O
10	O
L	O
15	O
20	O

T	O
3	O

US	O

395/2	O
395	O
2	O

US	O

395/2	O
.	O
4	O
395	O
2	O
.	O
4	O

US	O

395/2	O
.	O
42	O
395	O
2	O
.	O
42	O

US	O

395/2	O
.	O
35	O
395	O
2	O
.	O
35	O

US	O

395/2	O
.	O
36	O
395	O
2	O
.	O
36	O

US	O

395/2	O
.	O
37	O
395	O
2	O
.	O
37	O

US	O

395/2	O
.	O
52	O
395	O
2	O
.	O
52	O

US	O

395/2	O
.	O
6	O
395	O
2	O
.	O
6	O

US	O

395/2	O
.	O
64	O
395	O
2	O
.	O
64	O

US	O

395/2	O
.	O
79	O
395	O
2	O
.	O
79	O

US	O

395/2	O
.	O
8	O
395	O
2	O
.	O
8	O

US	O

395/2	O
.	O
81	O
395	O
2	O
.	O
81	O

US	O

395/2	O
.	O
84	O
395	O
2	O
.	O
84	O

US	O

395/41	O
395	O
41	O

US	O

395/43	O
395	O
43	O

US	O

395/46	O
395	O
46	O

US	O

395/47	O
395	O
47	O

8	O
Method	O
and	O
apparatus	O
for	O
speech	O
recognition	O
for	O
distinguishing	O
non-speech	O
audio	O
input	O
events	O
from	O
speech	O
audio	O
input	O
events	O

US	O
4383135	O
A	O
Scott	O
et_al.	O

19830510	O

19800123	O

US	O

395/2	O
.	O
52	O
395	O
2	O
.	O
52	O

US	O
4532648	O
A	O
Noso	O
et_al.	O

19850730	O

19820929	O

US	O

395/2	O
.	O
84	O
395	O
2	O
.	O
84	O

US	O
4797924	O
A	O
Schnars	O
et_al.	O

19890110	O

19851025	O

US	O

395/2	O
.	O
84	O
395	O
2	O
.	O
84	O

US	O
4827520	O
A	O
Zeinstra	O
19890502	O

19870116	O

US	O

395/2	O
.	O
84	O
395	O
2	O
.	O
84	O

US	O
4918735	O
A	O
Morito	O
et_al.	O

19900417	O

19890109	O

US	O

395/2	O
.	O
42	O
395	O
2	O
.	O
42	O

US	O
5209695	O
A	O
Rothschild	O
19930511	O

19910513	O

US	O

395/2	O
.	O
84	O
395	O
2	O
.	O
84	O

US	O
5231691	O
A	O
Yasuda	O
19930727	O

19901005	O

US	O

395/2	O
395	O
2	O

US	O
5274739	O
A	O
Woodard	O
19931228	O

19920415	O

US	O

395/2	O
.	O
42	O
395	O
2	O
.	O
42	O

US	O
5369728	O
A	O
Kosaka	O
et_al.	O

19941129	O

19920609	O

US	O

395/2	O
.	O
42	O
395	O
2	O
.	O
42	O

Schmandt	B-Citation
,	I-Citation
Ackerman	I-Citation
,	I-Citation
and	I-Citation
Hindus	I-Citation
,	I-Citation
Augmenting	I-Citation
a	I-Citation
Window	I-Citation
System	I-Citation
with	I-Citation
Speech	I-Citation
Input	I-Citation
,	I-Citation
Computer	I-Citation
,	I-Citation
23	I-Citation
(	I-Citation
8	I-Citation
)	I-Citation
:	I-Citation
50	I-Citation
56	I-Citation
,	I-Citation
Aug.	I-Citation
1990	I-Citation
.	O

US	O
7165134	O
B1	O
20070116	O

20000628	O

US	O
7652593	O
B1	O
20100126	O

20061005	O

US	O
7688225	O
B1	O
20100330	O

20071022	O

US	O
7392183	O
B2	O
20080624	O

20021227	O

US	O
7283953	O
B2	O
20071016	O

19990920	O

US	O
7020292	O
B1	O
20060328	O

20011227	O

US	O
6816085	O
B1	O
20041109	O

20001117	O

US	O
6598017	O
B1	O
20030722	O

19990720	O

US	O
6594632	O
B1	O
20030715	O

19981102	O

US	O
6556968	O
B1	O
20030429	O

19991112	O

US	O
6415258	O
B1	O
20020702	O

19991006	O

US	O
6389561	O
B1	O
20020514	O

19980521	O

US	O
6324499	O
B1	O
20011127	O

19990308	O

US	O
6301559	O
B1	O
20011009	O

19981116	O

International	O
Business	O
Machines	O
Corporation	O

Armonk	O
US	O

Williams	O
Marvin	O
L.	O

Bedford	O
US	O

Duffield	O
Edward	O
H.	O

Dillon	O
Andrew	O
J.	O

MacDonald	O
;	O
Allen	O
R.	O

Collins	O
;	O
Alphonso	O
A.	O

DE	O
69523531	O
D1	O
20011206	O

19950808	O

EP	O
702351	O
A3	O
19971022	O

19950808	O

EP	O
702351	O
A2	O
19960320	O

19950808	O

EP	O
702351	O
B1	O
20011031	O

19950808	O

DE	O
69523531	O
T2	O
20020523	O

19950808	O

US	O
5764852	O
A	O
19980609	O

19940816	O

DE	O
69523531	O
D1	O
20011206	O

19950808	O

EP	O
702351	O
A3	O
19971022	O

19950808	O

EP	O
702351	O
A2	O
19960320	O

19950808	O

EP	O
702351	O
B1	O
20011031	O

19950808	O

DE	O
69523531	O
T2	O
20020523	O

19950808	O

US	O
5764852	O
A	O
19980609	O

19940816	O

A	O
method	O
and	O
apparatus	O
for	O
analyzing	O
audio	O
input	O
events	O
.	O
A	O
template	O
is	O
utilized	O
to	O
analyze	O
audio	O
input	O
events	O
.	O
A	O
speech	O
audio	O
input	O
event	O
is	O
identified	O
.	O
The	O
identified	O
speech	O
audio	O
input	O
event	O
is	O
recorded	O
.	O
The	O
recorded	O
speech	O
audio	O
input	O
event	O
is	O
processed	O
to	O
create	O
a	O
first	O
entry	O
in	O
a	O
template	O
.	O
A	O
selected	O
non-speech	O
audio	O
input	O
event	O
which	O
occurs	O
in	O
a	O
selected	O
environment	O
is	O
identified	O
.	O
The	O
identified	O
non-speech	O
audio	O
input	O
event	O
is	O
recorded	O
.	O
Then	O
the	O
recorded	O
non-speech	O
audio	O
input	O
event	O
is	O
processed	O
to	O
create	O
a	O
second	O
entry	O
in	O
the	O
template	O
.	O
Thereafter	O
,	O
a	O
speech	O
audio	O
input	O
event	O
and	O
a	O
non-speech	O
audio	O
input	O
event	O
is	O
distinguished	O
by	O
comparing	O
an	O
audio	O
input	O
event	O
to	O
the	O
template	O
.	O

19970716	O

AS	O
ASSIGNMENT	O
N	O
US	O
5764852A	O
INTERNATIONAL	O
BUSINESS	O
MACHINES	O
CORPORATION	O
,	O
NEW	O
Y	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNOR:WILLIAMS	O
,	O
MARVIN	O
L.;REEL/FRAME:008598/0014	O

19940715	O

20010920	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
5764852A	O
4	O

20050914	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
5764852A	O
8	O

20091021	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
5764852A	O
12	O

BACKGROUND	O
OF	O
THE	O
INVENTION	O
1	O
.	O
Technical	O
Field	O
The	O
present	O
invention	O
relates	O
in	O
general	O
to	O
the	O
field	O
of	O
speech	O
recognition	O
and	O
in	O
particular	O
to	O
the	O
field	O
of	O
recognition	O
of	O
unknown	O
phrases	O
.	O
Still	O
more	O
particularly	O
,	O
the	O
present	O
invention	O
relates	O
to	O
a	O
method	O
and	O
apparatus	O
for	O
speech	O
recognition	O
,	O
which	O
takes	O
into	O
account	O
background	O
noises	O
.	O

2	O
.	O
Description	O
of	O
the	O
Related	O
Art	O
Speech	O
analysis	O
and	O
speech	O
recognition	O
algorithms	O
,	O
machines	O
,	O
and	O
devices	O
are	O
becoming	O
more	O
and	O
more	O
common	O
.	O
Such	O
systems	O
have	O
become	O
increasingly	O
powerful	O
and	O
less	O
expensive	O
.	O
Within	O
recent	O
years	O
,	O
an	O
explosion	O
in	O
the	O
use	O
of	O
voice	O
recognition	O
systems	O
has	O
occurred	O
.	O
These	O
systems	O
allow	O
a	O
user	O
on	O
a	O
data	O
processing	O
system	O
to	O
employ	O
voice	O
activated	O
commands	O
to	O
direct	O
various	O
programs	O
and	O
applications	O
.	O
One	O
goal	O
of	O
voice	O
recognition	O
systems	O
is	O
to	O
provide	O
a	O
more	O
humanistic	O
interface	O
for	O
operating	O
a	O
data	O
processing	O
system	O
.	O
Voice	O
recognition	O
systems	O
,	O
typically	O
,	O
are	O
used	O
with	O
other	O
input	O
devices	O
,	O
such	O
as	O
a	O
mouse	O
,	O
keyboard	O
,	O
or	O
printer	O
.	O
These	O
input	O
devices	O
often	O
are	O
used	O
to	O
supplement	O
the	O
input/output	O
(	O
"	O
I/O	O
"	O
)	O
processes	O
of	O
voice	O
recognition	O
systems	O
.	O
Various	O
known	O
voice	O
recognition	O
systems	O
,	O
typically	O
,	O
contain	O
a	O
set	O
,	O
i	O
.	O
e	O
.	O
,	O
a	O
template	O
,	O
of	O
recognizable	O
phrases	O
from	O
which	O
the	O
user	O
can	O
speak	O
to	O
use	O
voice	O
activated	O
commands	O
.	O
At	O
any	O
instance	O
in	O
time	O
,	O
the	O
voice	O
recognition	O
system	O
's	O
memory	O
contains	O
a	O
recognizable	O
set	O
.	O
This	O
recognizable	O
set	O
contains	O
a	O
set	O
of	O
digitized	O
audio	O
phrases	O
from	O
which	O
to	O
choose	O
a	O
recognizable	O
phrase	O
.	O
For	O
example	O
,	O
if	O
64	O
trained	O
phrases	O
are	O
within	O
the	O
voice	O
recognition	O
system	O
's	O
memory	O
,	O
the	O
detected	O
sounds	O
,	O
background	O
or	O
intentional	O
,	O
are	O
compared	O
to	O
this	O
recognizable	O
set	O
.	O
Thus	O
,	O
an	O
unintentional	O
background	O
noise	O
may	O
create	O
a	O
confidence	O
factor	O
that	O
may	O
be	O
interpreted	O
as	O
a	O
recognizable	O
phrase	O
within	O
the	O
set	O
.	O

Typically	O
,	O
the	O
monitoring	O
of	O
an	O
audio	O
environment	O
,	O
causes	O
the	O
voice	O
recognition	O
system	O
to	O
detect	O
background	O
noises	O
.	O
These	O
background	O
noises	O
are	O
often	O
interpreted	O
as	O
user	O
recognizable	O
inputs	O
.	O
Such	O
a	O
situation	O
can	O
cause	O
a	O
problem	O
,	O
involving	O
the	O
voice	O
recognition	O
system	O
performing	O
operations	O
or	O
commands	O
because	O
of	O
background	O
noise	O
.	O
Attempts	O
have	O
been	O
made	O
to	O
solve	O
this	O
problem	O
through	O
the	O
use	O
of	O
calibration	O
techniques	O
.	O
Such	O
a	O
method	O
essentially	O
involves	O
using	O
the	O
voice	O
recognition	O
system	O
to	O
initially	O
monitor	O
a	O
background	O
noise	O
sample	O
.	O
The	O
sample	O
functions	O
as	O
an	O
aggregated	O
factor	O
when	O
the	O
voice	O
recognition	O
system	O
is	O
actually	O
listening	O
for	O
recognizable	O
phrases	O
.	O
These	O
calibration	O
techniques	O
are	O
often	O
inefficient	O
and	O
often	O
assume	O
the	O
sample	O
of	O
background	O
noise	O
detected	O
during	O
the	O
calibration	O
phrase	O
is	O
identical	O
or	O
similar	O
to	O
the	O
background	O
noise	O
that	O
will	O
exist	O
during	O
the	O
actual	O
recognition	O
phase	O
.	O

Other	O
approaches	O
have	O
allowed	O
the	O
user	O
to	O
manually	O
disable	O
the	O
recognition	O
mode	O
of	O
the	O
voice	O
recognition	O
system	O
.	O
Such	O
an	O
approach	O
,	O
however	O
,	O
requires	O
manual	O
enabling	O
and	O
disabling	O
of	O
the	O
recognition	O
mode	O
when	O
the	O
user	O
suspects	O
that	O
the	O
background	O
noise	O
will	O
interfere	O
with	O
the	O
operation	O
of	O
the	O
voice	O
recognition	O
system	O
.	O
This	O
technique	O
often	O
requires	O
the	O
user	O
to	O
remember	O
which	O
mode	O
the	O
voice	O
recognition	O
system	O
is	O
operating	O
within	O
.	O
Moreover	O
,	O
it	O
can	O
be	O
extremely	O
cumbersome	O
to	O
enable	O
and	O
disable	O
the	O
voice	O
recognition	O
system	O
.	O
Often	O
the	O
causes	O
of	O
these	O
background	O
noises	O
are	O
induced	O
by	O
the	O
user	O
.	O
Peripheral	O
devices	O
,	O
such	O
as	O
keyboard	O
sounds	O
and	O
printer	O
sounds	O
,	O
are	O
an	O
example	O
of	O
background	O
noise	O
often	O
initiated	O
by	O
the	O
user	O
.	O
These	O
noises	O
can	O
interfere	O
with	O
the	O
operation	O
of	O
the	O
voice	O
recognition	O
system	O
,	O
i	O
.	O
e	O
.	O
,	O
causing	O
the	O
system	O
to	O
recognize	O
background	O
noises	O
as	O
phrases	O
corresponding	O
to	O
a	O
command	O
or	O
function	O
.	O

The	O
problem	O
of	O
inadvertently	O
selecting	O
a	O
background	O
noise	O
as	O
a	O
recognizable	O
phrase	O
is	O
due	O
to	O
the	O
background	O
noise	O
closely	O
mimicking	O
a	O
phrase	O
within	O
the	O
recognizable	O
set	O
that	O
is	O
within	O
the	O
voice	O
recognition	O
system	O
's	O
memory	O
.	O
Therefore	O
,	O
it	O
would	O
be	O
advantageous	O
to	O
have	O
a	O
method	O
and	O
apparatus	O
by	O
which	O
the	O
operation	O
of	O
peripheral	O
devices	O
that	O
produce	O
background	O
noise	O
can	O
be	O
recognized	O
as	O
background	O
noise	O
during	O
the	O
recognition	O
mode	O
of	O
the	O
voice	O
recognition	O
system	O
.	O

SUMMARY	O
OF	O
THE	O
INVENTION	O
It	O
is	O
one	O
object	O
of	O
the	O
present	O
invention	O
to	O
provide	O
an	O
improved	O
method	O
and	O
apparatus	O
for	O
speech	O
recognition	O
.	O

It	O
is	O
another	O
object	O
of	O
the	O
present	O
invention	O
to	O
provide	O
an	O
improved	O
method	O
and	O
apparatus	O
for	O
recognition	O
of	O
unknown	O
phrases	O
.	O

It	O
is	O
yet	O
another	O
object	O
of	O
the	O
present	O
invention	O
to	O
provide	O
an	O
improved	O
method	O
and	O
apparatus	O
for	O
speech	O
recognition	O
,	O
which	O
takes	O
into	O
account	O
background	O
noises	O
.	O

The	O
present	O
invention	O
provides	O
method	O
and	O
apparatus	O
for	O
analyzing	O
audio	O
input	O
events	O
.	O
The	O
present	O
invention	O
utilizes	O
a	O
template	O
to	O
analyze	O
audio	O
input	O
events	O
.	O
A	O
speech	O
audio	O
input	O
event	O
is	O
identified	O
.	O
The	O
identified	O
speech	O
audio	O
input	O
event	O
is	O
recorded	O
.	O
The	O
recorded	O
speech	O
audio	O
input	O
event	O
is	O
processed	O
to	O
create	O
a	O
first	O
entry	O
in	O
a	O
template	O
.	O
A	O
selected	O
non-speech	O
audio	O
input	O
event	O
which	O
occurs	O
in	O
a	O
selected	O
environment	O
is	O
identified	O
.	O
The	O
identified	O
non-speech	O
audio	O
input	O
event	O
is	O
recorded	O
.	O
Then	O
the	O
recorded	O
non-speech	O
audio	O
input	O
event	O
is	O
processed	O
to	O
create	O
a	O
second	O
entry	O
in	O
the	O
template	O
.	O
Thereafter	O
,	O
a	O
speech	O
audio	O
input	O
event	O
and	O
a	O
non-speech	O
audio	O
input	O
event	O
is	O
distinguished	O
from	O
each	O
other	O
by	O
comparing	O
an	O
audio	O
input	O
event	O
to	O
the	O
template	O
,	O
wherein	O
the	O
non-speech	O
audio	O
input	O
event	O
is	O
identified	O
.	O

The	O
above	O
as	O
well	O
as	O
additional	O
objectives	O
,	O
features	O
,	O
and	O
advantages	O
of	O
the	O
present	O
invention	O
will	O
become	O
apparent	O
in	O
the	O
following	O
detailed	O
written	O
description	O
.	O

BRIEF	O
DESCRIPTION	O
OF	O
THE	O
DRAWINGS	O
The	O
novel	O
features	O
believed	O
characteristic	O
of	O
the	O
invention	O
are	O
set	O
forth	O
in	O
the	O
appended	O
claims	O
.	O
The	O
invention	O
itself	O
,	O
however	O
,	O
as	O
well	O
as	O
a	O
preferred	O
mode	O
of	O
use	O
,	O
further	O
objectives	O
and	O
advantages	O
thereof	O
,	O
will	O
best	O
be	O
understood	O
by	O
reference	O
to	O
the	O
following	O
detailed	O
description	O
of	O
an	O
illustrative	O
embodiment	O
when	O
read	O
in	O
conjunction	O
with	O
the	O
accompanying	O
drawings	O
,	O
wherein	O
:	O
FIG	O
.	O
1	O
is	O
a	O
multimedia	O
data	O
processing	O
system	O
in	O
accordance	O
with	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
;	O
FIG	O
.	O
2	O
depicts	O
a	O
block	O
diagram	O
representation	O
of	O
the	O
principal	O
hardware	O
components	O
utilized	O
to	O
execute	O
applications	O
,	O
such	O
as	O
a	O
voice	O
recognition	O
system	O
in	O
accordance	O
with	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
;	O
FIG	O
.	O
3	O
is	O
a	O
high	O
level	O
flow	O
chart	O
of	O
a	O
process	O
employed	O
by	O
a	O
user	O
to	O
train	O
an	O
application	O
to	O
recognize	O
voice	O
recognition	O
commands	O
in	O
accordance	O
with	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
;	O
FIG	O
.	O
4	O
depicts	O
a	O
template	O
illustrated	O
in	O
accordance	O
with	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
;	O
FIG	O
.	O
5	O
is	O
a	O
flow	O
chart	O
of	O
a	O
process	O
for	O
registering	O
peripheral	O
devices	O
that	O
can	O
create	O
noise	O
or	O
background	O
sounds	O
in	O
accordance	O
with	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
;	O
FIG	O
.	O
6	O
depicts	O
a	O
flow	O
chart	O
of	O
a	O
process	O
for	O
training	O
and	O
updating	O
a	O
voice	O
recognition	O
system	O
to	O
detect	O
and	O
differentiate	O
between	O
sounds	O
that	O
are	O
speech	O
audio	O
input	O
events	O
or	O
non-speech	O
audio	O
input	O
event	O
;	O
and	O
FIG	O
.	O
7	O
is	O
a	O
flowchart	O
of	O
a	O
process	O
for	O
analyzing	O
audio	O
input	O
events	O
in	O
a	O
data	O
processing	O
system	O
in	O
accordance	O
with	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
.	O

DETAILED	O
DESCRIPTION	O
OF	O
PREFERRED	O
EMBODIMENT	O
With	O
reference	O
now	O
to	O
the	O
figures	O
and	O
in	O
particular	O
with	O
reference	O
to	O
FIG	O
.	O
1	O
,	O
there	O
is	O
depicted	O
multimedia	O
data	O
processing	O
system	O
11	O
which	O
includes	O
a	O
plurality	O
of	O
multimedia	O
end	O
devices	O
13	O
which	O
are	O
electrically	O
connected	O
to	O
computer	O
15	O
.	O
Those	O
skilled	O
in	O
the	O
art	O
,	O
will	O
,	O
upon	O
reference	O
to	O
the	O
specification	O
,	O
appreciate	O
that	O
computer	O
15	O
may	O
comprise	O
any	O
personal	O
computer	O
system	O
well	O
known	O
in	O
the	O
prior	O
art	O
,	O
such	O
as	O
the	O
PS2	O
IBM	O
Computer	O
manufactured	O
by	O
International	O
Business	O
Machines	O
Corporation	O
of	O
Armonk	O
,	O
N.	O
Y.	O
The	O
plurality	O
of	O
multimedia	O
end	O
devices	O
13	O
include	O
all	O
types	O
of	O
multimedia	O
end	O
devices	O
which	O
either	O
produce	O
or	O
consume	O
real-time	O
and/or	O
asynchronous	O
streamed	O
data	O
,	O
and	O
include	O
without	O
limitation	O
such	O
end	O
and	O
video	O
monitor	O
25	O
.	O
Each	O
of	O
these	O
multimedia	O
end	O
devices	O
13	O
may	O
be	O
called	O
by	O
multimedia	O
application	O
software	O
to	O
produce	O
or	O
consume	O
the	O
streamed	O
data	O
.	O

For	O
example	O
,	O
the	O
operation	O
of	O
CD-ROM	O
player	O
17	O
may	O
be	O
controlled	O
by	O
multimedia	O
application	O
software	O
which	O
is	O
resident	O
in	O
,	O
and	O
executed	O
by	O
,	O
computer	O
15	O
.	O
The	O
real-time	O
digital	O
data	O
stream	O
generated	O
as	O
an	O
output	O
of	O
CD-ROM	O
player	O
17	O
may	O
be	O
received	O
and	O
processed	O
by	O
computer	O
15	O
in	O
accordance	O
with	O
instructions	O
of	O
the	O
multimedia	O
application	O
resident	O
therein	O
.	O
For	O
example	O
,	O
the	O
real-time	O
digital	O
data	O
stream	O
may	O
be	O
compressed	O
for	O
storage	O
on	O
a	O
conventional	O
computer	O
floppy	O
disk	O
or	O
for	O
transmission	O
via	O
modem	O
over	O
ordinary	O
telephone	O
lines	O
for	O
receipt	O
by	O
a	O
remotely	O
located	O
computer	O
system	O
which	O
may	O
decompress	O
and	O
play	O
the	O
digital	O
streamed	O
data	O
on	O
analog	O
audio	O
equipment	O
.	O
Alternatively	O
,	O
the	O
real-time	O
data	O
stream	O
output	O
from	O
CD-ROM	O
player	O
17	O
may	O
be	O
received	O
by	O
computer	O
15	O
,	O
and	O
subjected	O
to	O
digital	O
or	O
analog	O
filtering	O
,	O
amplification	O
,	O
and	O
sound	O
balancing	O
before	O
being	O
directed	O
,	O
in	O
analog	O
signal	O
form	O
,	O
to	O
analog	O
stereo	O
amplifier	O
29	O
for	O
output	O
on	O
audio	O
speakers	O
31	O
and	O
33	O
.	O

Microphone	O
19	O
may	O
be	O
used	O
to	O
receive	O
analog	O
input	O
signals	O
corresponding	O
to	O
ambient	O
sounds	O
.	O
The	O
real-time	O
analog	O
data	O
stream	O
may	O
be	O
directed	O
to	O
computer	O
15	O
,	O
converted	O
into	O
digital	O
form	O
,	O
and	O
subject	O
to	O
manipulation	O
by	O
the	O
multimedia	O
application	O
software	O
,	O
such	O
as	O
a	O
voice	O
recognition	O
program	O
.	O
The	O
digital	O
data	O
may	O
be	O
stored	O
,	O
compressed	O
,	O
encrypted	O
,	O
filtered	O
,	O
subjected	O
to	O
transforms	O
,	O
outputted	O
in	O
analog	O
form	O
to	O
analog	O
stereo	O
amplifier	O
29	O
,	O
directed	O
as	O
an	O
output	O
in	O
analog	O
form	O
to	O
telephone	O
23	O
,	O
presented	O
in	O
digitized	O
analog	O
form	O
as	O
an	O
output	O
of	O
a	O
modem	O
for	O
transmission	O
on	O
telephone	O
lines	O
,	O
transformed	O
into	O
visual	O
images	O
for	O
display	O
on	O
video	O
monitor	O
25	O
,	O
or	O
subjected	O
to	O
a	O
variety	O
of	O
other	O
different	O
and	O
conventional	O
multimedia	O
digital	O
signal	O
processing	O
operations	O
.	O

In	O
a	O
similar	O
fashion	O
,	O
the	O
analog	O
and	O
digital	O
inputs	O
and	O
outputs	O
of	O
keyboard	O
21	O
,	O
telephone	O
23	O
,	O
and	O
video	O
monitor	O
25	O
may	O
be	O
subjected	O
to	O
conventional	O
multimedia	O
operations	O
in	O
computer	O
15	O
.	O
In	O
particular	O
,	O
computer	O
15	O
may	O
be	O
used	O
as	O
a	O
voice	O
recognition	O
system	O
to	O
direct	O
commands	O
and	O
functions	O
for	O
other	O
applications	O
executing	O
on	O
computer	O
15	O
.	O
Microphone	O
19	O
may	O
be	O
used	O
to	O
receive	O
speech	O
audio	O
input	O
events	O
,	O
i	O
.	O
e	O
.	O
,	O
human	O
speech	O
,	O
the	O
audio	O
input	O
events	O
may	O
be	O
processed	O
using	O
a	O
multimedia	O
application	O
that	O
is	O
directed	O
towards	O
recognizing	O
speech	O
from	O
analyzing	O
inputs	O
from	O
microphone	O
19	O
.	O

FIG	O
.	O
2	O
is	O
a	O
block	O
diagram	O
representation	O
of	O
the	O
principal	O
hardware	O
components	O
which	O
are	O
utilized	O
in	O
the	O
present	O
invention	O
to	O
execute	O
multimedia	O
applications	O
which	O
control	O
the	O
operation	O
of	O
multimedia	O
end	O
devices	O
13	O
.	O
As	O
is	O
conventional	O
in	O
multimedia	O
data	O
processing	O
operations	O
,	O
a	O
central	O
processing	O
unit	O
(	O
CPU	O
)	O
33	O
is	O
provided	O
in	O
computer	O
15	O
.	O
Typically	O
,	O
the	O
multimedia	O
application	O
software	O
,	O
such	O
as	O
a	O
voice	O
recognition	O
application	O
,	O
is	O
resident	O
in	O
RAM	O
computer	O
memory	O
35	O
.	O
CPU	O
33	O
executes	O
the	O
instructions	O
which	O
comprise	O
the	O
multimedia	O
application	O
.	O
Also	O
,	O
as	O
is	O
typical	O
in	O
multimedia	O
data	O
processing	O
operations	O
,	O
digital	O
signal	O
processor	O
37	O
is	O
provided	O
as	O
an	O
auxiliary	O
processor	O
,	O
which	O
is	O
dedicated	O
to	O
performing	O
operations	O
on	O
the	O
real-time	O
and/or	O
asynchronous	O
streamed	O
data	O
.	O
As	O
is	O
well	O
known	O
to	O
those	O
skilled	O
in	O
the	O
art	O
,	O
digital	O
signal	O
processors	O
are	O
microprocessor	O
devices	O
which	O
are	O
dedicated	O
to	O
performing	O
operations	O
based	O
upon	O
,	O
or	O
which	O
include	O
,	O
real-time	O
data	O
and	O
are	O
thus	O
designed	O
to	O
be	O
very	O
fast	O
and	O
respond	O
quickly	O
to	O
allow	O
the	O
real-time	O
operational	O
nature	O
of	O
the	O
multimedia	O
end	O
devices	O
.	O
Typically	O
,	O
in	O
order	O
to	O
speed-up	O
the	O
operation	O
of	O
the	O
digital	O
signal	O
processor	O
37	O
,	O
a	O
conventional	O
direct	O
memory	O
access	O
(	O
DMA	O
)	O
39	O
is	O
provided	O
to	O
allow	O
for	O
the	O
rapid	O
fetching	O
and	O
storing	O
of	O
data	O
.	O
In	O
the	O
present	O
invention	O
,	O
separate	O
instruction	O
memory	O
(	O
IM	O
)	O
41	O
and	O
data	O
memory	O
(	O
DM	O
)	O
43	O
are	O
provided	O
to	O
further	O
speed	O
up	O
the	O
operation	O
of	O
digital	O
signal	O
processor	O
37	O
.	O
Bus	O
45	O
is	O
provided	O
to	O
communicate	O
data	O
between	O
digital	O
signal	O
processor	O
37	O
and	O
hardware	O
interface	O
47	O
,	O
which	O
includes	O
digital-to-analog	O
and	O
analog-to-digital	O
converters	O
.	O
Inputs	O
and	O
outputs	O
for	O
the	O
various	O
multimedia	O
end	O
devices	O
13	O
are	O
connected	O
through	O
the	O
digital-to-analog	O
(	O
D/A	O
)	O
and	O
analog-to-digital	O
(	O
A/D	O
)	O
converter	O
47	O
.	O
In	O
FIG	O
.	O
2	O
,	O
a	O
telephone	O
input/output	O
49	O
,	O
a	O
microphone	O
input	O
53	O
,	O
and	O
stereo	O
outputs	O
55	O
,	O
57	O
are	O
depicted	O
,	O
in	O
an	O
exemplary	O
manner	O
,	O
and	O
are	O
connected	O
through	O
the	O
A/D	O
and	O
D/A	O
converters	O
in	O
hardware	O
interface	O
47	O
.	O
MIDI	O
input/output	O
also	O
is	O
connected	O
to	O
hardware	O
interface	O
47	O
to	O
digital	O
signal	O
processor	O
37	O
but	O
is	O
not	O
connected	O
to	O
A/D	O
or	O
D/A	O
converters	O
.	O

Referring	O
next	O
to	O
FIG	O
.	O
3	O
,	O
a	O
high	O
level	O
flow	O
chart	O
of	O
a	O
process	O
employed	O
by	O
a	O
user	O
to	O
train	O
an	O
application	O
to	O
recognize	O
voice	O
recognition	O
commands	O
is	O
depicted	O
in	O
accordance	O
with	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
.	O
The	O
user	O
must	O
"	O
train	O
"	O
a	O
set	O
,	O
i	O
.	O
e	O
.	O
,	O
a	O
template	O
of	O
phrases	O
for	O
recognition	O
,	O
as	O
illustrated	O
in	O
block	O
300	O
.	O
The	O
user	O
also	O
defines	O
a	O
set	O
of	O
actions	O
in	O
the	O
form	O
of	O
macros	O
setting	O
forth	O
predefined	O
actions	O
,	O
as	O
depicted	O
in	O
block	O
302	O
in	O
accordance	O
with	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
.	O
The	O
user	O
then	O
correlates	O
or	O
associates	O
particular	O
phrases	O
to	O
the	O
actions	O
,	O
as	O
illustrated	O
in	O
block	O
304	O
.	O
In	O
other	O
words	O
,	O
the	O
user	O
associates	O
a	O
voice	O
phrase	O
or	O
ideal	O
input	O
event	O
to	O
a	O
macro	O
.	O
The	O
user	O
then	O
loads	O
a	O
template	O
for	O
a	O
particular	O
application	O
,	O
as	O
depicted	O
in	O
block	O
306	O
.	O
Typically	O
during	O
this	O
phase	O
of	O
the	O
process	O
,	O
the	O
voice	O
recognition	O
system	O
may	O
encounter	O
background	O
noise	O
.	O
This	O
noise	O
may	O
match	O
an	O
entry	O
within	O
the	O
template	O
,	O
i	O
.	O
e	O
.	O
,	O
meet	O
a	O
confidence	O
factor	O
of	O
an	O
entry	O
within	O
the	O
set	O
of	O
phrases	O
.	O

Also	O
,	O
the	O
recognizable	O
set	O
within	O
the	O
template	O
may	O
not	O
perform	O
all	O
commands	O
desired	O
by	O
the	O
user	O
for	O
a	O
particular	O
application	O
.	O
For	O
example	O
,	O
the	O
desired	O
voice	O
recognition	O
template	O
may	O
not	O
be	O
currently	O
within	O
the	O
voice	O
recognition	O
system	O
's	O
memory	O
.	O
In	O
such	O
cases	O
,	O
the	O
user	O
may	O
issue	O
a	O
voice	O
command	O
to	O
swap	O
templates	O
from	O
the	O
memory	O
.	O
This	O
occurs	O
when	O
the	O
user	O
requires	O
an	O
additional	O
template	O
,	O
as	O
illustrated	O
in	O
block	O
308	O
.	O
In	O
another	O
situation	O
,	O
the	O
user	O
may	O
require	O
a	O
new	O
set	O
of	O
templates	O
to	O
be	O
loaded	O
when	O
the	O
user	O
loads	O
a	O
new	O
application	O
,	O
as	O
depicted	O
in	O
block	O
310	O
.	O

The	O
present	O
invention	O
employs	O
a	O
method	O
and	O
apparatus	O
that	O
allows	O
a	O
voice	O
recognition	O
system	O
to	O
automatically	O
register	O
background	O
noises	O
produced	O
by	O
peripheral	O
devices	O
.	O
The	O
present	O
invention	O
also	O
may	O
automatically	O
enable	O
and	O
disable	O
the	O
voice	O
recognition	O
mode	O
based	O
on	O
interrupts	O
from	O
the	O
peripheral	O
devices	O
.	O
The	O
present	O
invention	O
involves	O
a	O
method	O
and	O
apparatus	O
by	O
which	O
the	O
background	O
interrupt	O
noise	O
is	O
not	O
disregarded	O
,	O
but	O
dynamically	O
added	O
to	O
the	O
set	O
of	O
recognizable	O
phrases	O
.	O
The	O
registration	O
of	O
a	O
null	O
command	O
accompanies	O
the	O
background	O
noise	O
phrase	O
in	O
accordance	O
with	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
.	O

Alternatively	O
,	O
a	O
command	O
may	O
be	O
associated	O
with	O
the	O
background	O
noise	O
phrase	O
.	O
For	O
example	O
,	O
the	O
command	O
may	O
disable	O
the	O
voice	O
recognition	O
system	O
until	O
some	O
other	O
event	O
occurs	O
.	O
This	O
approach	O
allows	O
for	O
a	O
dynamic	O
training	O
of	O
the	O
voice	O
recognition	O
system	O
for	O
background	O
noise	O
.	O
The	O
system	O
may	O
be	O
trained	O
to	O
recognize	O
different	O
background	O
noises	O
,	O
which	O
decreases	O
the	O
probability	O
that	O
a	O
background	O
noise	O
will	O
be	O
mistaken	O
for	O
a	O
recognizable	O
phrase	O
within	O
the	O
set	O
,	O
i	O
.	O
e	O
.	O
,	O
the	O
recognizable	O
set	O
now	O
includes	O
a	O
confidence	O
factor	O
for	O
the	O
background	O
noise	O
.	O

Referring	O
now	O
to	O
FIG	O
.	O
4	O
,	O
a	O
template	O
400	O
is	O
illustrated	O
in	O
accordance	O
with	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
.	O
The	O
PHRASE	O
column	O
identifies	O
the	O
textural	O
representation	O
of	O
recognizable	O
phrases	O
.	O
The	O
COMMAND	O
column	O
identifies	O
the	O
command	O
,	O
e.g.	O
,	O
keyboard	O
macro	O
that	O
will	O
be	O
executed	O
upon	O
the	O
voice	O
recognition	O
system	O
recognizing	O
the	O
audio	O
phrase	O
.	O
For	O
example	O
,	O
upon	O
the	O
voice	O
recognition	O
system	O
,	O
recognizing	O
the	O
phrase	O
"	O
PRINT	O
DOCUMENT	O
"	O
,	O
the	O
function	O
key	O
F7	O
will	O
be	O
sent	O
to	O
the	O
keyboard	O
buffer	O
,	O
followed	O
by	O
the	O
word	O
DOC	O
,	O
followed	O
by	O
the	O
ENTER	O
key	O
(	O
E	O
)	O
entering	O
the	O
keyboard	O
buffer	O
.	O
As	O
a	O
result	O
,	O
the	O
application	O
will	O
receive	O
the	O
F7	O
DOC	O
ENTER	O
keystrokes	O
upon	O
the	O
voice	O
recognition	O
system	O
recognizing	O
the	O
phrase	O
"	O
PRINT	O
DOCUMENT	O
"	O
.	O
These	O
would	O
be	O
the	O
commands	O
necessary	O
for	O
an	O
application	O
to	O
print	O
a	O
document	O
.	O
The	O
DIGITIZED	O
FORM	O
column	O
shows	O
a	O
graphical	O
representation	O
of	O
an	O
audio	O
sample	O
for	O
each	O
phrase	O
within	O
the	O
template	O
.	O
The	O
representations	O
are	O
for	O
purposes	O
of	O
illustration	O
only	O
and	O
represent	O
an	O
average	O
of	O
how	O
the	O
user	O
speaks	O
the	O
particular	O
phrase	O
,	O
i	O
.	O
e	O
.	O
,	O
trained	O
sample	O
phrases	O
.	O

A	O
comparison	O
of	O
the	O
digitized	O
sound	O
form	O
to	O
the	O
digitized	O
forms	O
trained	O
by	O
the	O
user	O
within	O
the	O
template	O
is	O
performed	O
by	O
the	O
voice	O
recognition	O
system	O
detecting	O
a	O
sound	O
or	O
audio	O
input	O
event	O
.	O
Upon	O
detecting	O
background	O
noises	O
as	O
defined	O
by	O
the	O
interrupt	O
criteria	O
dynamically	O
introduces	O
sound	O
phrases	O
into	O
the	O
template	O
.	O
The	O
symbol	O
[	O
]	O
designate	O
entries	O
for	O
phrases	O
produced	O
by	O
the	O
invention	O
as	O
can	O
be	O
seen	O
in	O
the	O
PHRASE	O
column	O
in	O
FIG	O
.	O
4	O
.	O

In	O
accordance	O
with	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
,	O
an	O
association	O
of	O
a	O
null	O
command	O
to	O
the	O
entry	O
for	O
a	O
created	O
phrase	O
may	O
be	O
made	O
.	O
The	O
voice	O
recognition	O
system	O
,	O
upon	O
detecting	O
an	O
audio	O
input	O
event	O
,	O
background	O
or	O
human	O
voice	O
,	O
compares	O
the	O
sound	O
to	O
all	O
entries	O
within	O
the	O
template	O
.	O
A	O
background	O
sound	O
also	O
is	O
referred	O
to	O
a	O
"	O
non-speech	O
audio	O
input	O
event	O
"	O
and	O
a	O
human	O
voice	O
sound	O
also	O
is	O
referred	O
to	O
as	O
a	O
"	O
speech	O
audio	O
input	O
event	O
"	O
.	O
A	O
higher	O
confidence	O
factor	O
exists	O
for	O
a	O
comparison	O
of	O
a	O
background	O
noise	O
to	O
a	O
background	O
noise	O
sample	O
because	O
the	O
voice	O
recognition	O
system	O
compares	O
audio	O
input	O
events	O
to	O
a	O
recognizable	O
set	O
of	O
entries	O
within	O
the	O
template	O
.	O

Referring	O
now	O
to	O
FIG	O
.	O
5	O
,	O
a	O
flow	O
chart	O
of	O
a	O
process	O
for	O
registering	O
peripheral	O
devices	O
that	O
can	O
create	O
noise	O
or	O
background	O
sounds	O
(	O
non-speech	O
audio	O
input	O
events	O
)	O
is	O
depicted	O
in	O
accordance	O
with	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
.	O
The	O
process	O
begins	O
by	O
receiving	O
user	O
input	O
in	O
the	O
form	O
of	O
names	O
for	O
the	O
peripheral	O
devices	O
,	O
as	O
illustrated	O
in	O
block	O
502	O
.	O
The	O
process	O
then	O
receives	O
user	O
input	O
identifying	O
interrupts	O
for	O
each	O
of	O
the	O
peripheral	O
devices	O
,	O
as	O
depicted	O
in	O
block	O
504	O
.	O
Thereafter	O
,	O
user	O
input	O
is	O
received	O
by	O
the	O
process	O
,	O
designating	O
associated	O
communications	O
ports	O
for	O
the	O
peripheral	O
devices	O
,	O
as	O
illustrated	O
in	O
block	O
506	O
.	O
The	O
process	O
then	O
receives	O
user	O
input	O
as	O
to	O
the	O
elapsed	O
time	O
for	O
the	O
recognition	O
of	O
devices	O
,	O
as	O
depicted	O
in	O
block	O
508	O
.	O

Next	O
,	O
user	O
input	O
identifying	O
any	O
optional	O
commands	O
to	O
be	O
executed	O
upon	O
recognition	O
are	O
received	O
,	O
as	O
illustrated	O
in	O
block	O
510	O
.	O
The	O
process	O
then	O
receives	O
user	O
input	O
as	O
to	O
the	O
notification	O
preferences	O
,	O
as	O
depicted	O
in	O
block	O
512	O
.	O
A	O
user	O
may	O
choose	O
to	O
be	O
notified	O
when	O
an	O
appropriate	O
recognition	O
is	O
made	O
.	O
The	O
process	O
then	O
determines	O
whether	O
the	O
user	O
desires	O
to	O
be	O
notified	O
during	O
detection	O
of	O
noise	O
from	O
peripheral	O
devices	O
,	O
as	O
illustrated	O
in	O
block	O
514	O
.	O
If	O
the	O
user	O
desires	O
notification	O
,	O
the	O
process	O
then	O
receives	O
user	O
input	O
specifying	O
the	O
output	O
device	O
for	O
notification	O
,	O
as	O
depicted	O
in	O
block	O
516	O
.	O
The	O
user	O
may	O
be	O
notified	O
via	O
various	O
output	O
devices	O
,	O
such	O
as	O
a	O
speaker	O
for	O
audio	O
notification	O
or	O
a	O
video	O
monitor	O
for	O
video	O
notification	O
.	O
The	O
process	O
then	O
enables	O
the	O
notification	O
flag	O
,	O
as	O
illustrated	O
in	O
block	O
518	O
.	O

Thereafter	O
,	O
the	O
process	O
terminates	O
after	O
storing	O
the	O
information	O
entered	O
by	O
the	O
user	O
in	O
a	O
device	O
recognition	O
table	O
,	O
as	O
depicted	O
in	O
block	O
520	O
.	O
Referring	O
back	O
to	O
block	O
514	O
,	O
if	O
the	O
user	O
does	O
not	O
desire	O
notification	O
,	O
the	O
process	O
also	O
proceeds	O
to	O
block	O
520	O
.	O
A	O
device	O
recognition	O
table	O
may	O
take	O
various	O
forms	O
,	O
such	O
as	O
a	O
file	O
continuing	O
field	O
or	O
a	O
relational	O
data	O
base	O
.	O

Referring	O
now	O
to	O
FIG	O
.	O
6	O
,	O
a	O
flow	O
chart	O
of	O
a	O
process	O
for	O
training	O
and	O
updating	O
a	O
voice	O
recognition	O
system	O
to	O
detect	O
and	O
differentiate	O
between	O
sounds	O
(	O
also	O
called	O
"	O
audio	O
input	O
events	O
"	O
)	O
that	O
are	O
speech	O
audio	O
input	O
events	O
or	O
non-speech	O
audio	O
input	O
events	O
.	O
The	O
process	O
begins	O
by	O
loading	O
a	O
device	O
recognition	O
table	O
into	O
active	O
memory	O
,	O
as	O
illustrated	O
in	O
block	O
600	O
.	O
The	O
device	O
recognition	O
table	O
is	O
the	O
data	O
entered	O
by	O
the	O
user	O
and	O
stored	O
as	O
illustrated	O
in	O
FIG	O
.	O
5	O
.	O
The	O
process	O
then	O
sets	O
the	O
interrupt	O
vectors	O
to	O
intercept	O
interrupts	O
from	O
the	O
peripheral	O
devices	O
designated	O
in	O
the	O
device	O
recognition	O
table	O
before	O
they	O
reach	O
the	O
target	O
application	O
,	O
as	O
illustrated	O
in	O
block	O
602	O
.	O
The	O
process	O
then	O
activates	O
a	O
monitoring	O
service	O
,	O
as	O
depicted	O
in	O
block	O
604	O
.	O
A	O
monitoring	O
service	O
used	O
to	O
monitor	O
for	O
interrupts	O
is	O
well	O
known	O
to	O
those	O
of	O
ordinary	O
skill	O
in	O
the	O
art	O
and	O
various	O
methods	O
may	O
be	O
employed	O
in	O
accordance	O
with	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
.	O

The	O
process	O
then	O
awaits	O
an	O
interrupt	O
from	O
a	O
peripheral	O
device	O
,	O
as	O
illustrated	O
in	O
block	O
606	O
.	O
The	O
process	O
then	O
receives	O
an	O
interrupt	O
from	O
the	O
peripheral	O
device	O
,	O
as	O
depicted	O
in	O
block	O
608	O
.	O
Next	O
,	O
the	O
process	O
passes	O
the	O
interrupt	O
to	O
an	O
existing	O
application	O
address	O
to	O
finally	O
deliver	O
to	O
the	O
interrupt	O
to	O
the	O
target	O
application	O
,	O
as	O
illustrated	O
in	O
block	O
610	O
.	O
The	O
process	O
next	O
marks	O
the	O
time	O
of	O
the	O
reception	O
of	O
the	O
interrupt	O
,	O
as	O
depicted	O
in	O
block	O
612	O
.	O
Next	O
,	O
the	O
process	O
starts	O
an	O
expiry	O
clock	O
,	O
as	O
illustrated	O
in	O
block	O
614	O
.	O
An	O
expiry	O
clock	O
is	O
basically	O
a	O
timer	O
that	O
is	O
employed	O
in	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
to	O
determine	O
how	O
much	O
time	O
has	O
passed	O
since	O
the	O
detection	O
of	O
an	O
interrupt	O
.	O

The	O
process	O
then	O
awaits	O
an	O
audio	O
recognition	O
,	O
as	O
depicted	O
in	O
block	O
616	O
.	O
In	O
other	O
words	O
,	O
the	O
process	O
waits	O
to	O
see	O
if	O
a	O
recognizable	O
pattern	O
,	O
a	O
pattern	O
that	O
meets	O
a	O
confidence	O
threshold	O
for	O
an	O
entry	O
in	O
the	O
template	O
,	O
is	O
detected	O
.	O
Upon	O
the	O
recognition	O
of	O
audio	O
,	O
the	O
process	O
then	O
determines	O
whether	O
an	O
audio	O
interrupt	O
has	O
been	O
received	O
,	O
as	O
illustrated	O
in	O
block	O
618	O
.	O
An	O
audio	O
interrupt	O
occurs	O
when	O
an	O
input	O
device	O
,	O
such	O
as	O
a	O
microphone	O
,	O
detects	O
an	O
audio	O
input	O
event	O
.	O
If	O
an	O
audio	O
interrupt	O
has	O
not	O
been	O
received	O
,	O
the	O
process	O
then	O
determines	O
whether	O
the	O
time	O
has	O
expired	O
for	O
recognition	O
,	O
as	O
depicted	O
in	O
block	O
620	O
,	O
if	O
time	O
has	O
expired	O
for	O
recognition	O
,	O
the	O
process	O
then	O
clears	O
the	O
mark	O
for	O
the	O
time	O
that	O
the	O
interrupt	O
is	O
received	O
,	O
as	O
illustrated	O
in	O
block	O
622	O
,	O
with	O
the	O
process	O
then	O
returning	O
to	O
block	O
606	O
to	O
await	O
an	O
interrupt	O
from	O
a	O
peripheral	O
device	O
.	O
Referring	O
again	O
to	O
block	O
620	O
,	O
if	O
time	O
has	O
not	O
expired	O
for	O
recognition	O
,	O
the	O
process	O
then	O
returns	O
to	O
block	O
616	O
to	O
await	O
an	O
audio	O
recognition	O
.	O

Referring	O
again	O
to	O
block	O
618	O
,	O
if	O
an	O
audio	O
interrupt	O
is	O
received	O
,	O
the	O
process	O
then	O
proceeds	O
to	O
receive	O
the	O
audio	O
pattern	O
(	O
the	O
audio	O
input	O
event	O
)	O
,	O
as	O
depicted	O
in	O
block	O
624	O
.	O
The	O
process	O
marks	O
the	O
time	O
of	O
reception	O
of	O
the	O
audio	O
pattern	O
,	O
as	O
illustrated	O
in	O
block	O
626	O
.	O
Thereafter	O
,	O
the	O
process	O
determines	O
whether	O
the	O
audio	O
pattern	O
is	O
recognizable	O
,	O
as	O
depicted	O
in	O
block	O
628	O
.	O
If	O
the	O
audio	O
pattern	O
is	O
not	O
recognizable	O
,	O
the	O
process	O
then	O
proceeds	O
to	O
clear	O
the	O
mark	O
for	O
the	O
time	O
of	O
reception	O
of	O
the	O
audio	O
pattern	O
as	O
illustrated	O
in	O
block	O
630	O
.	O
Thereafter	O
,	O
the	O
process	O
proceeds	O
to	O
block	O
622	O
as	O
described	O
above	O
.	O

Referring	O
again	O
to	O
block	O
628	O
,	O
if	O
the	O
audio	O
pattern	O
is	O
recognizable	O
,	O
the	O
process	O
then	O
subtracts	O
the	O
interrupt	O
time	O
for	O
the	O
peripheral	O
device	O
from	O
the	O
audio	O
interrupt	O
time	O
to	O
determine	O
an	O
elapsed	O
period	O
of	O
time	O
,	O
as	O
depicted	O
in	O
block	O
632	O
.	O
The	O
process	O
next	O
determines	O
whether	O
the	O
time	O
period	O
calculated	O
is	O
within	O
an	O
acceptable	O
range	O
,	O
as	O
depicted	O
in	O
block	O
634	O
.	O
If	O
the	O
time	O
period	O
is	O
not	O
within	O
an	O
acceptable	O
range	O
,	O
the	O
process	O
proceeds	O
to	O
block	O
630	O
as	O
described	O
previously	O
.	O
On	O
the	O
other	O
hand	O
,	O
if	O
the	O
period	O
of	O
time	O
is	O
within	O
the	O
acceptable	O
range	O
,	O
the	O
process	O
then	O
determines	O
whether	O
the	O
user	O
is	O
to	O
be	O
notified	O
of	O
the	O
recognition	O
of	O
the	O
non-speech	O
audio	O
input	O
event	O
,	O
as	O
depicted	O
in	O
block	O
636	O
.	O
If	O
the	O
user	O
is	O
to	O
be	O
notified	O
,	O
the	O
process	O
then	O
determines	O
whether	O
commands	O
are	O
to	O
be	O
executed	O
,	O
as	O
illustrated	O
in	O
block	O
638	O
.	O

Referring	O
back	O
to	O
block	O
636	O
,	O
if	O
the	O
user	O
is	O
to	O
be	O
notified	O
,	O
the	O
process	O
then	O
proceeds	O
to	O
notify	O
the	O
user	O
according	O
the	O
recognition	O
table	O
definitions	O
,	O
as	O
illustrated	O
in	O
block	O
640	O
.	O
Thereafter	O
,	O
the	O
process	O
also	O
determines	O
whether	O
commands	O
are	O
to	O
be	O
executed	O
,	O
as	O
depicted	O
in	O
block	O
638	O
.	O
If	O
commands	O
are	O
to	O
be	O
executed	O
,	O
the	O
process	O
then	O
executes	O
the	O
commands	O
according	O
to	O
the	O
recognition	O
table	O
,	O
as	O
depicted	O
in	O
block	O
642	O
.	O
Thereafter	O
,	O
the	O
noise	O
(	O
non-speech	O
audio	O
input	O
event	O
)	O
is	O
stored	O
as	O
a	O
recognizable	O
template	O
pattern	O
,	O
as	O
illustrated	O
in	O
block	O
644	O
.	O
In	O
other	O
words	O
,	O
the	O
non-speech	O
audio	O
input	O
event	O
is	O
stored	O
as	O
an	O
entry	O
in	O
the	O
template	O
.	O
Thereafter	O
,	O
the	O
process	O
proceeds	O
to	O
block	O
630	O
as	O
previously	O
described	O
.	O
Referring	O
again	O
to	O
block	O
638	O
,	O
if	O
commands	O
are	O
not	O
to	O
be	O
executed	O
,	O
the	O
process	O
proceeds	O
directly	O
to	O
block	O
644	O
.	O
With	O
reference	O
now	O
to	O
FIG	O
.	O
7	O
,	O
a	O
flowchart	O
of	O
a	O
process	O
for	O
analyzing	O
audio	O
input	O
events	O
in	O
a	O
data	O
processing	O
system	O
is	O
depicted	O
in	O
accordance	O
with	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
.	O
The	O
process	O
begins	O
by	O
identifying	O
and	O
recording	O
a	O
speech	O
audio	O
input	O
event	O
,	O
as	O
depicted	O
in	O
block	O
700	O
.	O
Next	O
,	O
the	O
recorded	O
speech	O
audio	O
input	O
event	O
is	O
processed	O
to	O
create	O
a	O
first	O
entry	O
in	O
a	O
template	O
,	O
as	O
illustrated	O
in	O
block	O
702	O
.	O
The	O
process	O
then	O
identifies	O
and	O
records	O
a	O
non-speech	O
audio	O
input	O
event	O
,	O
as	O
depicted	O
in	O
block	O
704	O
.	O
The	O
recorded	O
non-speech	O
audio	O
input	O
event	O
is	O
then	O
processed	O
to	O
create	O
a	O
second	O
entry	O
in	O
the	O
template	O
,	O
as	O
illustrated	O
in	O
block	O
706	O
.	O
Next	O
,	O
the	O
process	O
detects	O
an	O
interrupt	O
,	O
as	O
depicted	O
in	O
block	O
708	O
.	O
Then	O
,	O
an	O
audio	O
input	O
event	O
is	O
detected	O
after	O
the	O
detection	O
of	O
an	O
interrupt	O
,	O
as	O
illustrated	O
in	O
block	O
710	O
.	O
The	O
process	O
then	O
identifies	O
a	O
non-speech	O
audio	O
input	O
event	O
by	O
comparing	O
the	O
detected	O
audio	O
input	O
event	O
with	O
the	O
template	O
,	O
as	O
depicted	O
in	O
block	O
712	O
.	O

After	O
the	O
expiration	O
of	O
a	O
period	O
of	O
time	O
after	O
the	O
interrupt	O
,	O
the	O
identified	O
non-speech	O
audio	O
input	O
event	O
is	O
processed	O
to	O
replace	O
the	O
second	O
entry	O
in	O
the	O
template	O
for	O
the	O
processed	O
non-speech	O
audio	O
input	O
event	O
,	O
as	O
illustrated	O
in	O
block	O
714	O
.	O
Additionally	O
,	O
in	O
response	O
to	O
identifying	O
a	O
non-speech	O
audio	O
input	O
event	O
,	O
a	O
determination	O
is	O
made	O
after	O
the	O
expiration	O
of	O
a	O
period	O
of	O
time	O
after	O
the	O
interrupt	O
occurs	O
as	O
to	O
whether	O
a	O
command	O
is	O
associated	O
with	O
the	O
non-speech	O
audio	O
input	O
event	O
,	O
as	O
depicted	O
in	O
block	O
716	O
.	O
If	O
a	O
command	O
is	O
associated	O
with	O
the	O
non-speech	O
audio	O
input	O
event	O
,	O
the	O
command	O
is	O
executed	O
,	O
as	O
illustrated	O
in	O
block	O
718	O
with	O
the	O
process	O
terminating	O
thereafter	O
.	O
With	O
reference	O
again	O
to	O
block	O
716	O
,	O
if	O
a	O
command	O
is	O
not	O
associated	O
with	O
the	O
non-speech	O
audio	O
input	O
event	O
,	O
the	O
process	O
also	O
terminates	O
.	O
Blocks	O
714	O
and	O
710	O
both	O
occur	O
in	O
response	O
to	O
an	O
identification	O
of	O
a	O
non-speech	O
audio	O
input	O
event	O
.	O

In	O
accordance	O
with	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
,	O
the	O
process	O
depicted	O
in	O
FIG	O
.	O
6	O
is	O
implemented	O
as	O
a	O
terminate	O
and	O
stay	O
resident	O
(	O
"	O
TSR	O
"	O
)	O
service	O
that	O
intercepts	O
the	O
interrupts	O
registered	O
by	O
the	O
user	O
or	O
some	O
other	O
entity	O
,	O
such	O
as	O
the	O
administrator	O
of	O
the	O
application	O
.	O
Interrupts	O
from	O
the	O
peripheral	O
devices	O
are	O
immediately	O
sent	O
to	O
their	O
associated	O
interrupt	O
vector	O
table	O
addresses	O
,	O
i	O
.	O
e	O
.	O
,	O
the	O
interrupt	O
transferred	O
to	O
the	O
appropriate	O
device	O
.	O
This	O
ensures	O
that	O
the	O
keyboard	O
interrupt	O
service	O
receives	O
the	O
keyboard	O
interrupt	O
and	O
the	O
printer	O
services	O
receive	O
their	O
output	O
.	O
The	O
detectable	O
voice	O
recognition	O
phrases	O
which	O
meet	O
no	O
confidence	O
factor	O
within	O
the	O
template	O
,	O
but	O
are	O
received	O
within	O
the	O
designated	O
time	O
of	O
an	O
interrupt	O
are	O
candidates	O
of	O
null	O
associates	O
in	O
accordance	O
with	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
.	O
Each	O
peripheral	O
device	O
has	O
an	O
associated	O
interrupt	O
defined	O
for	O
it	O
.	O
For	O
example	O
,	O
a	O
personal	O
computer	O
may	O
use	O
interrupt	O
14H	O
for	O
a	O
keyboard	O
interrupt	O
.	O

The	O
present	O
invention	O
may	O
be	O
directed	O
to	O
intercept	O
hardware	O
interrupts	O
or	O
interrupts	O
of	O
the	O
operating	O
system	O
.	O
The	O
registration	O
service	O
illustrated	O
in	O
FIG	O
.	O
5	O
allows	O
a	O
user	O
to	O
specify	O
the	O
interrupts	O
upon	O
which	O
recording	O
should	O
be	O
activated	O
for	O
an	O
audio	O
input	O
event	O
.	O
The	O
user	O
may	O
adjust	O
the	O
sensitivity	O
at	O
which	O
an	O
interrupt	O
should	O
be	O
interpreted	O
as	O
background	O
noise	O
or	O
a	O
non-speech	O
audio	O
input	O
event	O
.	O
Predefined	O
defaults	O
may	O
be	O
set	O
for	O
existing	O
devices	O
,	O
e.g.	O
,	O
printers	O
normally	O
operate	O
on	O
interrupt	O
5H	O
.	O

In	O
accordance	O
with	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
,	O
a	O
pre-process	O
may	O
be	O
employed	O
to	O
evaluate	O
if	O
the	O
audio	O
input	O
event	O
detected	O
should	O
be	O
compared	O
to	O
an	O
existing	O
null	O
phrase	O
or	O
to	O
create	O
a	O
new	O
phrase	O
.	O
Such	O
a	O
process	O
may	O
involve	O
the	O
continuous	O
employment	O
of	O
background	O
noise	O
to	O
train	O
the	O
system	O
for	O
a	O
particular	O
noise	O
phrase	O
.	O
In	O
addition	O
,	O
null	O
commands	O
may	O
be	O
substituted	O
for	O
user	O
supplied	O
or	O
system	O
default	O
commands	O
.	O
For	O
example	O
,	O
the	O
commands	O
could	O
issue	O
a	O
SAVE	O
command	O
for	O
a	O
word	O
processor	O
.	O
With	O
an	O
increase	O
in	O
noise	O
activity	O
,	O
such	O
as	O
interference	O
,	O
the	O
user	O
may	O
desire	O
to	O
save	O
the	O
work	O
presently	O
completed	O
.	O
In	O
such	O
a	O
situation	O
,	O
one	O
of	O
the	O
background	O
sounds	O
matches	O
an	O
executable	O
phrase	O
.	O

The	O
user	O
also	O
may	O
be	O
graphically	O
notified	O
when	O
null	O
associations	O
are	O
created	O
.	O
Such	O
a	O
notification	O
also	O
can	O
be	O
made	O
through	O
audio	O
means	O
.	O
Moreover	O
,	O
the	O
user	O
may	O
be	O
allowed	O
to	O
modify	O
null	O
commands	O
upon	O
notification	O
of	O
creation	O
.	O

The	O
present	O
invention	O
also	O
may	O
allow	O
for	O
entire	O
template	O
switching	O
based	O
upon	O
the	O
type	O
of	O
interrupt	O
received	O
,	O
rather	O
than	O
a	O
null	O
association	O
.	O
Such	O
an	O
option	O
would	O
signify	O
that	O
the	O
interrupt	O
detected	O
may	O
be	O
a	O
preeminent	O
signal	O
to	O
a	O
new	O
application	O
that	O
employs	O
a	O
different	O
set	O
of	O
voice	O
recognition	O
phrases	O
,	O
requiring	O
a	O
new	O
set	O
of	O
templates	O
.	O

In	O
accordance	O
with	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
,	O
the	O
fundamental	O
problem	O
of	O
voice	O
recognition	O
systems	O
involving	O
differentiation	O
of	O
non-speech	O
audio	O
input	O
events	O
from	O
speech	O
audio	O
input	O
events	O
is	O
addressed	O
.	O
The	O
present	O
invention	O
recognizes	O
that	O
peripheral	O
devices	O
may	O
produce	O
background	O
noise	O
and	O
that	O
a	O
system	O
may	O
be	O
allowed	O
to	O
essentially	O
execute	O
commands	O
that	O
are	O
irrelevant	O
to	O
the	O
applications	O
in	O
response	O
to	O
this	O
background	O
noise	O
.	O
The	O
present	O
invention	O
provides	O
a	O
method	O
and	O
apparatus	O
for	O
allowing	O
peripheral	O
devices	O
to	O
affix	O
digitized	O
sound	O
phrases	O
within	O
voice	O
recognition	O
sets	O
,	O
such	O
as	O
templates	O
.	O
The	O
present	O
invention	O
provides	O
further	O
advantage	O
over	O
prior	O
art	O
methods	O
in	O
that	O
the	O
present	O
invention	O
does	O
not	O
need	O
to	O
always	O
be	O
activated	O
.	O
Once	O
,	O
a	O
background	O
noise	O
is	O
"	O
trained	O
"	O
and	O
registered	O
into	O
the	O
template	O
,	O
the	O
invention	O
may	O
be	O
disabled	O
or	O
removed	O
.	O
This	O
provides	O
an	O
advantage	O
of	O
freeing	O
computer	O
resources	O
for	O
other	O
applications	O
.	O

While	O
the	O
invention	O
has	O
been	O
particularly	O
shown	O
and	O
described	O
with	O
reference	O
to	O
a	O
preferred	O
embodiment	O
,	O
it	O
will	O
be	O
understood	O
by	O
those	O
skilled	O
in	O
the	O
art	O
that	O
various	O
changes	O
in	O
form	O
and	O
detail	O
may	O
be	O
made	O
therein	O
without	O
departing	O
from	O
the	O
spirit	O
and	O
scope	O
of	O
the	O
invention	O
.	O

1	O
.	O
A	O
method	O
for	O
analyzing	O
audio	O
input	O
events	O
in	O
a	O
data	O
processing	O
system	O
,	O
wherein	O
said	O
data	O
processing	O
system	O
utilizes	O
a	O
template	O
to	O
analyze	O
audio	O
input	O
events	O
,	O
wherein	O
said	O
data	O
processing	O
system	O
includes	O
a	O
peripheral	O
device	O
that	O
generates	O
said	O
audio	O
input	O
event	O
and	O
an	O
interrupt	O
,	O
said	O
method	O
comprising	O
the	O
steps	O
of	O
:	O
identifying	O
a	O
speech	O
audio	O
input	O
event	O
;	O
recording	O
said	O
identified	O
speech	O
audio	O
input	O
event	O
;	O
processing	O
said	O
recorded	O
speech	O
audio	O
input	O
event	O
to	O
create	O
a	O
first	O
entry	O
in	O
a	O
template	O
;	O
identifying	O
a	O
selected	O
non-speech	O
audio	O
input	O
event	O
which	O
occurs	O
in	O
a	O
selected	O
environment	O
;	O
recording	O
said	O
identified	O
non-speech	O
audio	O
input	O
event	O
;	O
processing	O
said	O
recorded	O
non-speech	O
audio	O
input	O
event	O
to	O
create	O
a	O
second	O
entry	O
in	O
said	O
template	O
;	O
and	O
thereafter	O
,	O
distinguishing	O
between	O
a	O
speech	O
audio	O
input	O
event	O
and	O
a	O
non-speech	O
audio	O
input	O
event	O
by	O
comparing	O
said	O
audio	O
input	O
event	O
to	O
said	O
template	O
in	O
response	O
to	O
detecting	O
said	O
interrupt	O
and	O
detecting	O
said	O
audio	O
input	O
event	O
within	O
a	O
preselected	O
amount	O
of	O
time	O
wherein	O
said	O
non-speech	O
audio	O
input	O
event	O
is	O
identified	O
.	O

2	O
.	O
The	O
method	O
of	O
claim	O
1	O
,	O
further	O
comprising	O
:	O
determining	O
whether	O
a	O
command	O
is	O
associated	O
with	O
said	O
non-speech	O
audio	O
input	O
event	O
in	O
response	O
to	O
identification	O
said	O
non-speech	O
audio	O
input	O
event	O
;	O
and	O
responsive	O
to	O
said	O
command	O
being	O
associated	O
with	O
said	O
non-speech	O
audio	O
input	O
event	O
,	O
executing	O
said	O
command	O
.	O

3	O
.	O
A	O
method	O
for	O
analyzing	O
audio	O
input	O
events	O
in	O
a	O
data	O
processing	O
system	O
,	O
wherein	O
said	O
data	O
processing	O
system	O
utilizes	O
a	O
template	O
to	O
analyze	O
audio	O
input	O
events	O
and	O
wherein	O
said	O
data	O
processing	O
system	O
includes	O
a	O
peripheral	O
device	O
that	O
generates	O
an	O
audio	O
input	O
event	O
and	O
an	O
interrupt	O
,	O
said	O
method	O
comprising	O
the	O
steps	O
of	O
:	O
identifying	O
a	O
speech	O
audio	O
input	O
event	O
;	O
recording	O
said	O
identified	O
speech	O
audio	O
input	O
event	O
;	O
processing	O
said	O
recorded	O
speech	O
audio	O
input	O
event	O
to	O
create	O
a	O
first	O
entry	O
in	O
a	O
template	O
;	O
identifying	O
a	O
selected	O
non-speech	O
audio	O
input	O
event	O
which	O
occurs	O
in	O
a	O
selected	O
environment	O
;	O
recording	O
said	O
identified	O
non-speech	O
audio	O
input	O
event	O
;	O
processing	O
said	O
recorded	O
non-speech	O
audio	O
input	O
event	O
to	O
create	O
a	O
second	O
entry	O
in	O
said	O
template	O
for	O
said	O
processed	O
non-speech	O
audio	O
input	O
event	O
;	O
detecting	O
an	O
interrupt	O
;	O
detecting	O
said	O
audio	O
input	O
event	O
,	O
wherein	O
said	O
audio	O
input	O
event	O
occurs	O
after	O
said	O
interrupt	O
;	O
identifying	O
a	O
non-speech	O
audio	O
input	O
event	O
by	O
comparing	O
an	O
audio	O
input	O
event	O
to	O
said	O
template	O
;	O
responsive	O
to	O
identifying	O
a	O
non-speech	O
audio	O
input	O
event	O
occurring	O
a	O
preselected	O
amount	O
of	O
time	O
after	O
said	O
interrupt	O
occurs	O
,	O
determining	O
whether	O
a	O
command	O
is	O
associated	O
with	O
said	O
non-speech	O
audio	O
input	O
event	O
;	O
and	O
executing	O
said	O
command	O
in	O
response	O
to	O
said	O
command	O
being	O
associated	O
with	O
said	O
non-speech	O
audio	O
input	O
event	O
.	O

4	O
.	O
The	O
method	O
of	O
claim	O
3	O
further	O
comprising	O
processing	O
said	O
identified	O
non-speech	O
audio	O
input	O
event	O
occurring	O
said	O
preselected	O
amount	O
of	O
time	O
after	O
said	O
interrupt	O
occurs	O
to	O
replace	O
said	O
second	O
entry	O
in	O
said	O
template	O
for	O
said	O
processed	O
non-speech	O
audio	O
input	O
event	O
.	O

5	O
.	O
An	O
apparatus	O
for	O
analyzing	O
audio	O
input	O
events	O
,	O
wherein	O
said	O
utilizes	O
a	O
template	O
to	O
analyze	O
audio	O
input	O
events	O
,	O
wherein	O
apparatus	O
includes	O
a	O
peripheral	O
device	O
that	O
generates	O
an	O
audio	O
input	O
event	O
and	O
an	O
interrupt	O
,	O
said	O
apparatus	O
comprising	O
:	O
first	O
identification	O
means	O
for	O
identifying	O
a	O
speech	O
audio	O
input	O
event	O
;	O
first	O
recording	O
means	O
for	O
recording	O
said	O
identified	O
speech	O
audio	O
input	O
event	O
;	O
first	O
processing	O
means	O
for	O
processing	O
said	O
recorded	O
speech	O
audio	O
input	O
event	O
to	O
create	O
a	O
first	O
entry	O
in	O
a	O
template	O
;	O
second	O
identification	O
means	O
for	O
identifying	O
a	O
selected	O
non-speech	O
audio	O
input	O
event	O
which	O
occurs	O
in	O
a	O
selected	O
environment	O
;	O
second	O
recording	O
means	O
for	O
recording	O
said	O
identified	O
non-speech	O
audio	O
input	O
event	O
;	O
second	O
processing	O
means	O
for	O
processing	O
said	O
recorded	O
non-speech	O
audio	O
input	O
event	O
to	O
create	O
a	O
second	O
entry	O
in	O
said	O
template	O
for	O
said	O
processed	O
non-speech	O
audio	O
input	O
event	O
;	O
and	O
comparison	O
means	O
for	O
distinguishing	O
between	O
a	O
speech	O
audio	O
input	O
event	O
and	O
a	O
non-speech	O
audio	O
input	O
event	O
by	O
comparing	O
said	O
audio	O
input	O
event	O
to	O
said	O
template	O
in	O
response	O
to	O
detecting	O
said	O
interrupt	O
and	O
detecting	O
said	O
audio	O
input	O
event	O
within	O
a	O
preselected	O
amount	O
of	O
time	O
,	O
wherein	O
said	O
non-speech	O
audio	O
input	O
events	O
may	O
be	O
efficiently	O
distinguished	O
from	O
speech	O
audio	O
input	O
events	O
.	O

6	O
.	O
The	O
apparatus	O
of	O
claim	O
5	O
,	O
further	O
comprising	O
:	O
means	O
for	O
determining	O
whether	O
a	O
command	O
is	O
associated	O
with	O
said	O
non-speech	O
audio	O
input	O
event	O
in	O
response	O
to	O
identification	O
of	O
said	O
non-speech	O
audio	O
input	O
event	O
;	O
and	O
responsive	O
to	O
said	O
command	O
being	O
associated	O
with	O
said	O
non-speech	O
audio	O
input	O
event	O
,	O
means	O
for	O
executing	O
said	O
command	O
.	O

7	O
.	O
An	O
apparatus	O
method	O
for	O
analyzing	O
audio	O
input	O
events	O
,	O
where	O
said	O
apparatus	O
utilizes	O
a	O
template	O
to	O
analyze	O
audio	O
input	O
events	O
and	O
wherein	O
said	O
data	O
processing	O
system	O
includes	O
a	O
peripheral	O
device	O
that	O
generates	O
an	O
audio	O
input	O
event	O
and	O
an	O
interrupt	O
,	O
said	O
apparatus	O
comprising	O
:	O
first	O
identification	O
means	O
for	O
identifying	O
a	O
speech	O
audio	O
input	O
event	O
;	O
first	O
recording	O
means	O
for	O
recording	O
said	O
identified	O
speech	O
audio	O
input	O
event	O
;	O
first	O
processing	O
means	O
for	O
processing	O
said	O
recorded	O
speech	O
audio	O
input	O
event	O
to	O
create	O
a	O
first	O
entry	O
in	O
a	O
template	O
;	O
second	O
identification	O
means	O
for	O
identifying	O
a	O
selected	O
non-speech	O
audio	O
input	O
event	O
which	O
occurs	O
in	O
a	O
selected	O
environment	O
;	O
second	O
recording	O
means	O
for	O
recording	O
said	O
identified	O
non-speech	O
audio	O
input	O
event	O
;	O
second	O
processing	O
means	O
for	O
processing	O
said	O
recorded	O
non-speech	O
audio	O
input	O
event	O
to	O
create	O
a	O
second	O
entry	O
in	O
said	O
template	O
for	O
said	O
processed	O
non-speech	O
audio	O
input	O
event	O
;	O
first	O
detection	O
means	O
for	O
detecting	O
an	O
interrupt	O
;	O
second	O
detection	O
means	O
for	O
detecting	O
said	O
audio	O
input	O
event	O
,	O
wherein	O
said	O
audio	O
input	O
event	O
occurs	O
after	O
said	O
interrupt	O
;	O
third	O
identification	O
means	O
for	O
identifying	O
a	O
non-speech	O
audio	O
input	O
event	O
by	O
comparing	O
an	O
audio	O
input	O
event	O
to	O
said	O
template	O
;	O
determination	O
means	O
,	O
responsive	O
to	O
identifying	O
a	O
non-speech	O
audio	O
input	O
event	O
occurring	O
a	O
preselected	O
amount	O
of	O
time	O
after	O
said	O
interrupt	O
occurs	O
,	O
for	O
determining	O
whether	O
a	O
command	O
is	O
associated	O
with	O
said	O
non-speech	O
audio	O
input	O
event	O
;	O
and	O
execution	O
means	O
for	O
executing	O
said	O
command	O
in	O
response	O
to	O
said	O
command	O
being	O
associated	O
with	O
said	O
non-speech	O
audio	O
input	O
event	O
.	O

8	O
.	O
The	O
apparatus	O
of	O
claim	O
7	O
further	O
comprising	O
means	O
for	O
processing	O
said	O
identified	O
non-speech	O
audio	O
input	O
event	O
occurring	O
said	O
preselected	O
amount	O
of	O
time	O
after	O
said	O
interrupt	O
occurs	O
to	O
replace	O
said	O
second	O
entry	O
in	O
said	O
template	O
for	O
said	O
processed	O
non-speech	O
audio	O
input	O
event	O
.	O

