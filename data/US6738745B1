US	O
6738745	O
B1	O
20040518	O

US	O
09544678	O
20000407	O

09	O
eng	O
eng	O

US	O
09544678	O
20000407	O

20040518	O

20040518	O

7G	O
10L	O
15/08	O
A	O
7	O
G	O
10	O
L	O
15	O
08	O
A	O

G10L	O
15/00	O
20060101C	O
I20051008RMEP	O

20060101	O

C	O
G	O
10	O
L	O
15	O
00	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/20	O
20060101A	O
I20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
20	O
I	O

20051008	O

EP	O

R	O
M	O

US	O

704/277	O
704	O
277	O

704/8	O
704	O
8	O

704/240	O
704	O
240	O

704/E15.039	O
704	O
E15	O
.	O
039	O

G10L	O
15/20	O
G	O
10	O
L	O
15	O
20	O

S10L	O
15	O
:	O
20T	O
S	O
10	O
L	O
15	O
20	O

T	O

US	O

704/240	O
704	O
240	O

US	O

704/277	O
704	O
277	O

US	O

704/231	O
704	O
231	O

US	O

704/257	O
704	O
257	O

US	O

704/8	O
704	O
8	O

US	O

704/3	O
704	O
3	O

17	O
Methods	O
and	O
apparatus	O
for	O
identifying	O
a	O
non-target	O
language	O
in	O
a	O
speech	O
recognition	O
system	O

US	O
5724526	O
A	O
Kunita	O
19980303	O

19951215	O

US	O
5913185	O
A	O
Martino	O
et_al.	O

19990615	O

19961220	O

US	O
6047251	O
A	O
Pon	O
et_al.	O

20000404	O

19970915	O

US	O
6061646	O
A	O
Martino	O
et_al.	O

20000509	O

19971218	O

US	O
6085160	O
A	O
D	O
'	O
hoore	O
et_al.	O

20000704	O

19980710	O

CA	O
2160184	O
A1	O
19960630	O

19951010	O

Chen	B-Citation
et_al.	I-Citation
,	I-Citation
“	I-Citation
Clustering	I-Citation
via	I-Citation
the	I-Citation
Bayesian	I-Citation
Information	I-Citation
Criterion	I-Citation
with	I-Citation
Applications	I-Citation
in	I-Citation
Speech	I-Citation
Recognition	I-Citation
,	I-Citation
”	I-Citation
IBM	I-Citation
,	I-Citation
T.	I-Citation
J.	I-Citation
Watson	I-Citation
Research	I-Citation
Center	I-Citation
.	O

Beigi	B-Citation
et_al.	I-Citation
,	I-Citation
“	I-Citation
A	I-Citation
Distance	I-Citation
Measure	I-Citation
Between	I-Citation
Collections	I-Citation
of	I-Citation
Distributions	I-Citation
and	I-Citation
its	I-Citation
Application	I-Citation
to	I-Citation
Speaker	I-Citation
Recognition	I-Citation
,	I-Citation
”	I-Citation
IBM	I-Citation
,	I-Citation
T.	I-Citation
J.	I-Citation
Watson	I-Citation
Research	I-Citation
Center	I-Citation
.	O

Chen	B-Citation
et_al.	I-Citation
,	I-Citation
“	I-Citation
Speaker	I-Citation
,	I-Citation
Environment	I-Citation
and	I-Citation
Channel	I-Citation
Change	I-Citation
Detection	I-Citation
and	I-Citation
Clustering	I-Citation
via	I-Citation
the	I-Citation
Bayesian	I-Citation
Information	I-Citation
Criterion	I-Citation
,	I-Citation
”	I-Citation
IBM	I-Citation
,	I-Citation
T.	I-Citation
J.	I-Citation
Watson	I-Citation
Research	I-Citation
Center	I-Citation
.	O

Dharanipragada	B-Citation
et_al.	I-Citation
,	I-Citation
“	I-Citation
Experimental	I-Citation
Results	I-Citation
in	I-Citation
Audio	I-Citation
Indexing	I-Citation
”	I-Citation
,	I-Citation
IBM	I-Citation
,	I-Citation
T.	I-Citation
J.	I-Citation
Watson	I-Citation
Research	I-Citation
Center	I-Citation
.	O

Neti	B-Citation
et_al.	I-Citation
,	I-Citation
“	I-Citation
Audio-Visual	I-Citation
Speaker	I-Citation
Recognition	I-Citation
for	I-Citation
Video	I-Citation
Broadcast	I-Citation
News	I-Citation
,	I-Citation
”	I-Citation
IBM	I-Citation
,	I-Citation
T.	I-Citation
J.	I-Citation
Watson	I-Citation
Research	I-Citation
Center	I-Citation
.	O

Chen	B-Citation
et_al.	I-Citation
,	I-Citation
“	I-Citation
IBM	I-Citation
's	I-Citation
LVCSR	I-Citation
System	I-Citation
for	I-Citation
Transcription	I-Citation
of	I-Citation
Broadcast	I-Citation
News	I-Citation
Used	I-Citation
in	I-Citation
the	I-Citation
1997	I-Citation
HUB4	I-Citation
English	I-Citation
Evaluation	I-Citation
,	I-Citation
”	I-Citation
IBM	I-Citation
,	I-Citation
T.	I-Citation
J.	I-Citation
Watson	I-Citation
Research	I-Citation
Center	I-Citation
.	O

Navratil	B-Citation
et_al.	I-Citation
,	I-Citation
“	I-Citation
Phonetic-Context	I-Citation
Mapping	I-Citation
in	I-Citation
Language	I-Citation
Identification	I-Citation
,	I-Citation
”	I-Citation
Proc.	I-Citation
Of	I-Citation
the	I-Citation
EUROSPEECH-97	I-Citation
,	I-Citation
vol.	I-Citation
1	I-Citation
,	I-Citation
71	I-Citation
-	I-Citation
74	I-Citation
(	I-Citation
1997	I-Citation
)	I-Citation
.	O

Navratil	B-Citation
et_al.	I-Citation
,	I-Citation
An	I-Citation
Efficient	I-Citation
Phonotactic-Acoustic	I-Citation
System	I-Citation
for	I-Citation
Language	I-Citation
Identification	I-Citation
,	I-Citation
Proc.	I-Citation
of	I-Citation
the	I-Citation
Int'l	I-Citation
Conf.	I-Citation
On	I-Citation
Acoustics	I-Citation
,	I-Citation
Speech	I-Citation
and	I-Citation
Signal	I-Citation
Processing	I-Citation
(	I-Citation
ICASSP	I-Citation
)	I-Citation
,	I-Citation
vol.	I-Citation
2	I-Citation
,	I-Citation
781	I-Citation
-	I-Citation
84	I-Citation
,	I-Citation
Seattle	I-Citation
,	I-Citation
WA	I-Citation
,	I-Citation
IEEE	I-Citation
(	I-Citation
May	I-Citation
,	I-Citation
1998	I-Citation
)	I-Citation
.	O

Ramabhadran	B-Citation
et_al.	I-Citation
,	I-Citation
Acoustics	I-Citation
Only	I-Citation
Based	I-Citation
Automatic	I-Citation
Phonetic	I-Citation
Baseform	I-Citation
Generation	I-Citation
Proc.	I-Citation
Of	I-Citation
the	I-Citation
Int'l	I-Citation
Conf.	I-Citation
On	I-Citation
Acoustics	I-Citation
,	I-Citation
Speech	I-Citation
and	I-Citation
Signal	I-Citation
Processing	I-Citation
(	I-Citation
ICASSP	I-Citation
)	I-Citation
,	I-Citation
Seattle	I-Citation
,	I-Citation
WA	I-Citation
,	I-Citation
IEEE	I-Citation
(	I-Citation
May	I-Citation
,	I-Citation
1998	I-Citation
)	I-Citation
.	O

US	O
7472061	O
B1	O
20081230	O

20080331	O

US	O
7437289	O
B2	O
20081014	O

20010816	O

US	O
7043429	O
B2	O
20060509	O

20020328	O

US	O
7069216	O
B2	O
20060627	O

20011001	O

US	O
7725318	O
B2	O
20100525	O

20050801	O

US	O
7801910	O
B2	O
20100921	O

20060601	O

US	O
7957969	O
B2	O
20110607	O

20081001	O

US	O
7979266	O
B2	O
20110712	O

20070131	O

International	O
Business	O
Machines	O
Corporation	O
02	O

Armonk	O
NY	O

Navratil	O
,	O
Jiri	O

White	O
Plains	O
NY	O

Viswanathan	O
,	O
Mahesh	O

Yorktown	O
Heights	O
NY	O

Ryan	O
,	O
Mason	O
&	O
Lewis	O
,	O
LLP	O

Dang	O
,	O
Esq	O
.	O
,	O
Thu	O
Ann	O

Dorvil	O
Richemond	O
2654	O

Storm	O
Donald	O
L.	O

DE	O
10111056	O
A1	O
20011018	O

20010308	O

DE	O
10111056	O
B4	O
20051110	O

20010308	O

US	O
6738745	O
B1	O
20040518	O

20000407	O

CN	O
1211779	O
C	O
20050720	O

20010406	O

CN	O
1317783	O
A	O
20011017	O

20010406	O

US	O
6738745	O
B1	O
20040518	O

20000407	O

CN	O
1317783	O
A	O
20011017	O

20010406	O

DE	O
10111056	O
A1	O
20011018	O

20010308	O

DE	O
10111056	O
B4	O
20051110	O

20010308	O

CN	O
1211779	O
C	O
20050720	O

20010406	O

Methods	O
and	O
apparatus	O
are	O
disclosed	O
for	O
detecting	O
non-target	O
language	O
references	O
in	O
an	O
audio	O
transcription	O
or	O
speech	O
recognition	O
system	O
using	O
a	O
confidence	O
score	O
.	O
The	O
confidence	O
score	O
may	O
be	O
based	O
on	O
(	O
i	O
)	O
a	O
probabilistic	O
engine	O
score	O
provided	O
by	O
a	O
speech	O
recognition	O
system	O
,	O
(	O
ii	O
)	O
additional	O
scores	O
based	O
on	O
background	O
models	O
,	O
or	O
(	O
iii	O
)	O
a	O
combination	O
of	O
the	O
foregoing	O
.	O
The	O
engine	O
score	O
provided	O
by	O
the	O
speech	O
recognition	O
system	O
for	O
a	O
given	O
input	O
speech	O
utterance	O
reflects	O
the	O
degree	O
of	O
acoustic	O
and	O
linguistic	O
match	O
of	O
the	O
utterance	O
with	O
the	O
trained	O
target	O
language	O
.	O
The	O
background	O
models	O
are	O
created	O
or	O
trained	O
based	O
on	O
speech	O
data	O
in	O
other	O
languages	O
,	O
which	O
may	O
or	O
may	O
not	O
include	O
the	O
target	O
language	O
itself	O
.	O
A	O
number	O
of	O
types	O
of	O
background	O
language	O
models	O
may	O
be	O
employed	O
for	O
each	O
modeled	O
language	O
,	O
including	O
one	O
or	O
more	O
of	O
(	O
i	O
)	O
prosodic	O
models	O
;	O
(	O
ii	O
)	O
acoustic	O
models	O
;	O
(	O
iii	O
)	O
phonotactic	O
models	O
;	O
and	O
(	O
iv	O
)	O
keyword	O
spotting	O
models	O
.	O
The	O
engine	O
score	O
can	O
be	O
combined	O
with	O
the	O
background	O
model	O
scores	O
to	O
normalize	O
the	O
engine	O
score	O
for	O
non-target	O
languages	O
.	O
The	O
present	O
invention	O
identifies	O
a	O
non-target	O
language	O
utterance	O
within	O
an	O
audio	O
stream	O
when	O
the	O
confidence	O
score	O
falls	O
below	O
a	O
predefined	O
criteria	O
.	O
A	O
language	O
rejection	O
mechanism	O
can	O
interrupt	O
or	O
modify	O
the	O
transcription	O
process	O
when	O
speech	O
in	O
the	O
non-target	O
language	O
is	O
detected	O
.	O

20000712	O

AS	O
ASSIGNMENT	O
N	O
US	O
6738745B1	O
INTERNATIONAL	O
BUSINESS	O
MACHINES	O
CORPORATION	O
,	O
NEW	O
Y	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNORS:NAVRATIL	O
,	O
JIRI;VISWANATHAN	O
,	O
MAHESH;REEL/FRAME:010916/0001;SIGNING	O
DATES	O
FROM	O
20000620	O
TO	O
20000621	O

20070919	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
6738745B1	O
4	O

20090306	O

AS	O
ASSIGNMENT	O
N	O
US	O
6738745B1	O
NUANCE	O
COMMUNICATIONS	O
,	O
INC.	O
,	O
MASSACHUSETTS	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNOR:INTERNATIONAL	O
BUSINESS	O
MACHINES	O
CORPORATION;REEL/FRAME:022354/0566	O

20081231	O

FIELD	O
OF	O
THE	O
INVENTION	O
The	O
present	O
invention	O
relates	O
to	O
speech	O
recognition	O
systems	O
and	O
,	O
more	O
particularly	O
,	O
to	O
methods	O
and	O
apparatus	O
for	O
detecting	O
non-target	O
languages	O
in	O
a	O
monolingual	O
speech	O
recognition	O
system	O
.	O

BACKGROUND	O
OF	O
THE	O
INVENTION	O
Speech	O
recognition	O
and	O
audio	O
indexing	O
systems	O
are	O
generally	O
developed	O
for	O
a	O
specific	O
target	O
language	O
.	O
The	O
lexica	O
,	O
grammar	O
and	O
acoustic	O
models	O
of	O
such	O
monolingual	O
systems	O
reflect	O
the	O
typical	O
properties	O
of	O
the	O
target	O
language	O
.	O
In	O
practice	O
,	O
however	O
,	O
these	O
monolingual	O
systems	O
may	O
be	O
exposed	O
to	O
other	O
non-target	O
languages	O
,	O
leading	O
to	O
poor	O
performance	O
,	O
including	O
improper	O
transcription	O
or	O
indexing	O
,	O
potential	O
misinterpretations	O
or	O
false	O
system	O
reaction	O
.	O

For	O
example	O
,	O
many	O
organizations	O
,	O
such	O
as	O
broadcast	O
news	O
organizations	O
and	O
information	O
retrieval	O
services	O
,	O
must	O
process	O
large	O
amounts	O
of	O
audio	O
information	O
,	O
for	O
storage	O
and	O
retrieval	O
purposes	O
.	O
Frequently	O
,	O
the	O
audio	O
information	O
must	O
be	O
classified	O
by	O
subject	O
or	O
speaker	O
name	O
,	O
or	O
both	O
.	O
In	O
order	O
to	O
classify	O
audio	O
information	O
by	O
subject	O
,	O
a	O
speech	O
recognition	O
system	O
initially	O
transcribes	O
the	O
audio	O
information	O
into	O
text	O
for	O
automated	O
classification	O
or	O
indexing	O
.	O
Thereafter	O
,	O
the	O
index	O
can	O
be	O
used	O
to	O
perform	O
query-document	O
matching	O
to	O
return	O
relevant	O
documents	O
to	O
the	O
user	O
.	O

If	O
the	O
source	O
audio	O
information	O
includes	O
non-target	O
language	O
references	O
,	O
however	O
,	O
the	O
speech	O
recognition	O
system	O
may	O
improperly	O
transcribe	O
the	O
non-target	O
language	O
references	O
,	O
potentially	O
leading	O
to	O
improper	O
classification	O
or	O
indexing	O
of	O
the	O
source	O
information	O
.	O
A	O
need	O
therefore	O
exists	O
for	O
a	O
method	O
and	O
apparatus	O
for	O
detecting	O
non-target	O
language	O
references	O
in	O
an	O
audio	O
transcription	O
or	O
speech	O
recognition	O
system	O
.	O

With	O
the	O
trend	O
in	O
globalizing	O
communication	O
technologies	O
and	O
providing	O
services	O
to	O
a	O
wide	O
,	O
multilingual	O
public	O
,	O
the	O
ability	O
to	O
distinguish	O
between	O
languages	O
has	O
become	O
increasingly	O
important	O
.	O
The	O
language-rejection	O
problem	O
is	O
closely	O
related	O
to	O
this	O
ability	O
and	O
thus	O
to	O
the	O
problem	O
of	O
automatic	O
language	O
identification	O
(	O
ALI	O
)	O
.	O
For	O
a	O
detailed	O
discussion	O
of	O
automatic	O
language	O
identification	O
techniques	O
,	O
see	O
,	O
for	O
example	O
,	O
Y.	B-Citation
K.	I-Citation
Muthusamy	I-Citation
et_al.	I-Citation
,	I-Citation
“	I-Citation
Reviewing	I-Citation
Automatic	I-Citation
Language	I-Citation
Identification	I-Citation
,	I-Citation
”	I-Citation
IEEE	I-Citation
Signal	I-Citation
Processing	I-Citation
Magazine	I-Citation
,	I-Citation
11	I-Citation
(	I-Citation
4	I-Citation
)	I-Citation
:	I-Citation
33	I-Citation
-	I-Citation
41	I-Citation
(	I-Citation
October	I-Citation
1994	I-Citation
)	I-Citation
;	O
J.	B-Citation
Navrátil	I-Citation
and	I-Citation
W.	I-Citation
Zühlke	I-Citation
,	I-Citation
“	I-Citation
Phonetic-Context	I-Citation
Mapping	I-Citation
in	I-Citation
Language	I-Citation
Identification	I-Citation
,	I-Citation
”	I-Citation
Proc.	I-Citation
of	I-Citation
the	I-Citation
EUROSPEECH-97	I-Citation
,	I-Citation
Vol.	I-Citation
1	I-Citation
,	I-Citation
71	I-Citation
-	I-Citation
74	I-Citation
(	I-Citation
1997	I-Citation
)	I-Citation
;	O
and	O
J.	B-Citation
Navrátil	I-Citation
and	I-Citation
W.	I-Citation
Zühilke	I-Citation
,	I-Citation
“	I-Citation
An	I-Citation
Efficient	I-Citation
Phonotactic-Acoustic	I-Citation
System	I-Citation
for	I-Citation
Language	I-Citation
Identification	I-Citation
,	I-Citation
”	I-Citation
Proc.	I-Citation
of	I-Citation
the	I-Citation
Int'l	I-Citation
Conf.	I-Citation
on	I-Citation
Acoustics	I-Citation
,	I-Citation
Speech	I-Citation
and	I-Citation
Signal	I-Citation
Processing	I-Citation
(	I-Citation
ICASSP	I-Citation
)	I-Citation
,	I-Citation
Vol.	I-Citation
2	I-Citation
,	I-Citation
781	I-Citation
-	I-Citation
84	I-Citation
,	I-Citation
Seattle	I-Citation
,	I-Citation
Wash	I-Citation
.	I-Citation
,	I-Citation
IEEE	I-Citation
(	I-Citation
May	I-Citation
,	I-Citation
1998	I-Citation
)	I-Citation
,	O
each	O
incorporated	O
by	O
reference	O
herein	O
.	O

A	O
number	O
of	O
automatic	O
language	O
identification	O
techniques	O
have	O
been	O
proposed	O
or	O
suggested	O
for	O
distinguishing	O
languages	O
based	O
on	O
various	O
features	O
contained	O
in	O
the	O
speech	O
signal	O
.	O
Several	O
sources	O
of	O
language-discriminative	O
information	O
have	O
been	O
identified	O
as	O
relevant	O
for	O
the	O
task	O
of	O
language	O
identification	O
including	O
,	O
for	O
example	O
,	O
the	O
prosody	O
,	O
the	O
acoustics	O
,	O
and	O
the	O
grammatical	O
and	O
lexical	O
structure	O
.	O
Automatic	O
language	O
identification	O
techniques	O
based	O
on	O
the	O
prosody	O
or	O
acoustics	O
of	O
speech	O
attempt	O
to	O
identify	O
a	O
given	O
language	O
based	O
on	O
typical	O
melodic	O
and	O
pronunciation	O
patterns	O
,	O
respectively	O
.	O

Due	O
to	O
the	O
complexity	O
of	O
automatic	O
language	O
identification	O
techniques	O
based	O
on	O
the	O
grammatical	O
and	O
lexical	O
structure	O
,	O
however	O
,	O
most	O
proposals	O
have	O
advanced	O
techniques	O
based	O
on	O
acoustic-prosodic	O
information	O
or	O
derived	O
lexical	O
features	O
in	O
order	O
to	O
represent	O
the	O
phonetic	O
structure	O
in	O
a	O
less	O
complex	O
manner	O
.	O
ALI	O
techniques	O
have	O
been	O
developed	O
that	O
model	O
statistical	O
dependencies	O
inherent	O
in	O
phonetic	O
chains	O
,	O
referred	O
to	O
as	O
the	O
phonotactics	O
.	O
In	O
the	O
statistical	O
sense	O
,	O
phonotactics	O
can	O
be	O
viewed	O
as	O
a	O
subset	O
of	O
grammatical	O
and	O
lexical	O
rules	O
of	O
a	O
language	O
.	O
Since	O
these	O
rules	O
differ	O
among	O
languages	O
,	O
the	O
ability	O
to	O
discriminate	O
among	O
languages	O
is	O
naturally	O
reflected	O
in	O
the	O
phonotactic	O
properties	O
.	O

SUMMARY	O
OF	O
THE	O
INVENTION	O
Generally	O
,	O
methods	O
and	O
apparatus	O
are	O
disclosed	O
for	O
detecting	O
non-target	O
language	O
references	O
in	O
an	O
audio	O
transcription	O
or	O
speech	O
recognition	O
system	O
using	O
confidence	O
scores	O
.	O
The	O
confidence	O
score	O
may	O
be	O
based	O
on	O
(	O
i	O
)	O
a	O
probabilistic	O
engine	O
score	O
provided	O
by	O
a	O
speech	O
recognition	O
system	O
,	O
(	O
ii	O
)	O
additional	O
scores	O
based	O
on	O
background	O
models	O
,	O
or	O
(	O
iii	O
)	O
a	O
combination	O
of	O
the	O
foregoing	O
.	O
The	O
engine	O
score	O
provided	O
by	O
the	O
speech	O
recognition	O
system	O
for	O
a	O
given	O
input	O
speech	O
utterance	O
reflects	O
the	O
degree	O
of	O
acoustic	O
and	O
linguistic	O
match	O
of	O
the	O
utterance	O
with	O
the	O
trained	O
target	O
language	O
.	O
In	O
one	O
illustrative	O
implementation	O
,	O
the	O
probabilistic	O
engine	O
score	O
provided	O
by	O
the	O
speech	O
recognition	O
system	O
is	O
combined	O
with	O
the	O
background	O
model	O
scores	O
to	O
normalize	O
the	O
engine	O
score	O
as	O
well	O
as	O
to	O
account	O
for	O
the	O
potential	O
presence	O
of	O
a	O
non-target	O
language	O
.	O
The	O
normalization	O
narrows	O
the	O
variability	O
range	O
of	O
the	O
scores	O
across	O
speakers	O
and	O
channels	O
.	O

The	O
present	O
invention	O
identifies	O
a	O
non-target	O
language	O
utterance	O
within	O
an	O
audio	O
stream	O
when	O
the	O
confidence	O
score	O
falls	O
below	O
a	O
predefined	O
criteria	O
.	O
According	O
to	O
one	O
aspect	O
of	O
the	O
invention	O
,	O
a	O
language	O
rejection	O
mechanism	O
interrupts	O
or	O
modifies	O
the	O
transcription	O
process	O
when	O
speech	O
in	O
the	O
non-target	O
language	O
is	O
detected	O
.	O
In	O
this	O
manner	O
,	O
the	O
present	O
invention	O
prevents	O
improper	O
transcription	O
and	O
indexing	O
and	O
false	O
interpretations	O
of	O
the	O
speech	O
recognition	O
output	O
.	O

In	O
the	O
presence	O
of	O
non-target	O
language	O
utterances	O
,	O
the	O
transcription	O
system	O
is	O
not	O
able	O
to	O
find	O
a	O
good	O
match	O
based	O
on	O
its	O
native	O
vocabulary	O
,	O
language	O
models	O
and	O
acoustic	O
models	O
.	O
The	O
resulting	O
recognized	O
text	O
will	O
have	O
associated	O
lower	O
engine	O
score	O
values	O
.	O
Thus	O
,	O
the	O
engine	O
score	O
alone	O
may	O
be	O
used	O
to	O
identify	O
a	O
non-target	O
language	O
when	O
the	O
engine	O
score	O
is	O
below	O
a	O
predefined	O
threshold	O
.	O

The	O
background	O
models	O
are	O
created	O
or	O
trained	O
based	O
on	O
speech	O
data	O
in	O
several	O
languages	O
,	O
which	O
may	O
or	O
may	O
not	O
include	O
the	O
target	O
language	O
itself	O
.	O
A	O
number	O
of	O
types	O
of	O
background	O
language	O
models	O
may	O
be	O
employed	O
for	O
each	O
modeled	O
language	O
,	O
including	O
one	O
or	O
more	O
of	O
(	O
i	O
)	O
prosodic	O
models	O
;	O
(	O
ii	O
)	O
acoustic	O
models	O
;	O
(	O
iii	O
)	O
phonotactic	O
models	O
;	O
and	O
(	O
iv	O
)	O
keyword	O
spotting	O
models	O
.	O

A	O
more	O
complete	O
understanding	O
of	O
the	O
present	O
invention	O
,	O
as	O
well	O
as	O
further	O
features	O
and	O
advantages	O
of	O
the	O
present	O
invention	O
,	O
will	O
be	O
obtained	O
by	O
reference	O
to	O
the	O
following	O
detailed	O
description	O
and	O
drawings	O
.	O

BRIEF	O
DESCRIPTION	O
OF	O
THE	O
DRAWINGS	O

FIG	O
.	O
1illustrates	O
a	O
non-target	O
language	O
identification	O
system	O
in	O
accordance	O
with	O
the	O
present	O
invention	O
;	O

FIG	O
.	O
2is	O
a	O
schematic	O
block	O
diagram	O
showing	O
the	O
architecture	O
of	O
an	O
illustrative	O
background	O
language	O
modeling	O
module	O
ofFIG	O
.	O
1	O
;	O
and	O

FIG	O
.	O
3is	O
a	O
flow	O
chart	O
describing	O
an	O
exemplary	O
background	O
model	O
score	O
calculation	O
process	O
employed	O
by	O
the	O
background	O
language	O
modeling	O
module	O
of	O
FIG	O
.	O
2	O
.	O

DETAILED	O
DESCRIPTION	O
OF	O
PREFERRED	O
EMBODIMENTS	O

FIG	O
.	O
1illustrates	O
a	O
non-target	O
language	O
identification	O
system100in	O
accordance	O
with	O
the	O
present	O
invention	O
.	O
According	O
to	O
one	O
feature	O
of	O
the	O
present	O
invention	O
,	O
a	O
language	O
rejection	O
mechanism	O
interrupts	O
or	O
modifies	O
an	O
otherwise	O
conventional	O
speech	O
recognition	O
process	O
when	O
speech	O
in	O
the	O
non-target	O
language	O
is	O
detected	O
.	O
In	O
this	O
manner	O
,	O
the	O
present	O
invention	O
prevents	O
improper	O
transcription	O
and	O
indexing	O
and	O
false	O
interpretations	O
of	O
the	O
speech	O
recognition	O
output	O
.	O
The	O
present	O
invention	O
employs	O
probabilistic	O
engine	O
scores	O
provided	O
by	O
a	O
speech	O
recognition	O
system	O
combined	O
with	O
additional	O
scores	O
based	O
on	O
background	O
models	O
to	O
normalize	O
the	O
engine	O
score	O
for	O
non-target	O
languages	O
.	O

As	O
shown	O
inFIG	O
.	O
1	O
,	O
the	O
non-target	O
language	O
identification	O
system100includes	O
a	O
transcription	O
system110	O
,	O
a	O
background	O
language	O
modeling	O
module200	O
,	O
discussed	O
further	O
below	O
in	O
conjunction	O
withFIG	O
.	O
2	O
,	O
a	O
normalization	O
module150and	O
a	O
threshold	O
decision	O
module160	O
.	O
As	O
discussed	O
further	O
below	O
,	O
the	O
transcription	O
system110transcribes	O
a	O
speech	O
signal	O
and	O
provides	O
an	O
engine	O
score	O
indicating	O
the	O
degree	O
of	O
confidence	O
in	O
a	O
given	O
transcription	O
.	O
In	O
addition	O
,	O
the	O
background	O
language	O
modeling	O
module200generates	O
a	O
background	O
(	O
BG	O
)	O
model	O
score	O
indicating	O
the	O
probabilities	O
for	O
the	O
hypotheses	O
that	O
the	O
given	O
transcription	O
is	O
associated	O
with	O
(	O
i	O
)	O
the	O
target	O
and	O
(	O
ii	O
)	O
with	O
a	O
non-target	O
language	O
.	O
As	O
discussed	O
further	O
below	O
,	O
the	O
normalization	O
module150integrates	O
one	O
or	O
both	O
of	O
the	O
engine	O
and	O
BG	O
model	O
scores	O
and	O
the	O
threshold	O
decision	O
module160compares	O
the	O
integrated	O
score	O
to	O
predefined	O
criteria	O
to	O
determine	O
if	O
a	O
given	O
transcription	O
is	O
likely	O
associated	O
with	O
a	O
non-target	O
language	O
utterance	O
.	O

ENGINE	O
SCORE	O
The	O
transcription	O
system110may	O
be	O
embodied	O
as	O
any	O
speech	O
recognition	O
or	O
transcription	O
system	O
that	O
provides	O
a	O
confidence	O
score	O
,	O
such	O
as	O
the	O
ViaVoice	O
™	O
speech	O
recognition	O
system	O
,	O
commercially	O
available	O
from	O
IBM	O
Corporation	O
of	O
Armonk	O
,	O
N.	O
Y.	O
The	O
transcription	O
system110typically	O
calculates	O
a	O
probabilistic	O
engine	O
score	O
for	O
the	O
decoded	O
audio	O
stream	O
given	O
some	O
set	O
of	O
acoustic	O
model	O
(	O
s	O
)	O
,	O
pronunciation	O
vocabulary	O
and	O
language	O
model	O
(	O
s	O
)	O
.	O
In	O
the	O
monolingual	O
environment	O
of	O
the	O
present	O
invention	O
,	O
these	O
models	O
are	O
trained	O
on	O
one	O
specific	O
target	O
language	O
.	O

During	O
speech	O
recognition	O
based	O
on	O
speech	O
in	O
the	O
target	O
language	O
,	O
the	O
value	O
of	O
the	O
engine	O
score	O
depends	O
on	O
the	O
type	O
of	O
speech	O
and	O
the	O
channel	O
quality	O
.	O
Nonetheless	O
,	O
there	O
is	O
a	O
strong	O
correspondence	O
between	O
the	O
recognized	O
text	O
and	O
the	O
acoustic	O
evidence	O
.	O
In	O
the	O
presence	O
of	O
non-target	O
language	O
utterances	O
,	O
however	O
,	O
the	O
transcription	O
system110is	O
not	O
able	O
to	O
find	O
a	O
good	O
match	O
based	O
on	O
its	O
native	O
vocabulary	O
,	O
language	O
models	O
and	O
acoustic	O
models	O
.	O
Thus	O
,	O
the	O
resulting	O
recognized	O
text	O
will	O
have	O
associated	O
lower	O
engine	O
score	O
values	O
.	O
In	O
this	O
manner	O
,	O
the	O
engine	O
score	O
alone	O
may	O
be	O
used	O
to	O
identify	O
a	O
non-target	O
language	O
when	O
the	O
engine	O
score	O
is	O
below	O
a	O
predefined	O
threshold	O
.	O

BACKGROUND	O
MODELS	O
SCORES	O
As	O
previously	O
indicated	O
,	O
the	O
present	O
invention	O
supplements	O
the	O
engine	O
scores	O
provided	O
by	O
the	O
transcription	O
system110with	O
additional	O
scores	O
based	O
on	O
background	O
models	O
.	O
In	O
this	O
manner	O
,	O
the	O
present	O
invention	O
improves	O
the	O
accuracy	O
of	O
identifying	O
target	O
and	O
non-target	O
language	O
utterances	O
using	O
background	O
models	O
.	O
The	O
background	O
models	O
are	O
created	O
or	O
trained	O
based	O
on	O
speech	O
data	O
in	O
other	O
languages	O
,	O
which	O
may	O
or	O
may	O
not	O
include	O
the	O
target	O
language	O
itself	O
.	O
For	O
identification	O
purposes	O
,	O
scores	O
based	O
on	O
all	O
of	O
these	O
background	O
models	O
are	O
calculated	O
and	O
are	O
then	O
used	O
to	O
normalize	O
the	O
engine	O
score	O
.	O
As	O
discussed	O
further	O
below	O
in	O
a	O
section	O
entitled	O
“	O
NORMALIZATION	O
,	O
”	O
the	O
normalization	O
helps	O
to	O
narrow	O
the	O
variability	O
range	O
of	O
the	O
scores	O
across	O
speakers	O
and	O
channels	O
.	O

Generally	O
,	O
the	O
present	O
invention	O
utilizes	O
a	O
number	O
of	O
types	O
of	O
background	O
language	O
models	O
for	O
each	O
non-target	O
language	O
to	O
be	O
modeled	O
.	O
The	O
type	O
of	O
the	O
background	O
models	O
should	O
be	O
diverse	O
and	O
should	O
capture	O
the	O
properties	O
of	O
languages	O
on	O
the	O
acoustic	O
and	O
linguistic	O
level	O
.	O
The	O
features	O
used	O
for	O
training	O
may	O
range	O
from	O
amplitude	O
and	O
fundamental	O
frequency	O
measurements	O
(	O
prosodic	O
models	O
)	O
to	O
higher	O
phonetic	O
features	O
,	O
such	O
as	O
phone-level	O
statistics	O
(	O
phonotactic	O
models	O
)	O
,	O
partial	O
or	O
whole	O
word	O
keywords	O
(	O
keyword	O
spotting	O
models	O
)	O
up	O
to	O
full-fledged	O
large-vocabulary	O
recognizers	O
.	O

Thus	O
,	O
the	O
background	O
language	O
models	O
may	O
include	O
one	O
or	O
more	O
of	O
(	O
i	O
)	O
prosodic	O
models	O
;	O
(	O
ii	O
)	O
acoustic	O
models	O
;	O
(	O
iii	O
)	O
phonotactic	O
models	O
;	O
and	O
(	O
iv	O
)	O
keyword	O
spotting	O
models	O
.	O
For	O
a	O
more	O
detailed	O
discussion	O
of	O
various	O
types	O
of	O
models	O
,	O
see	O
,	O
for	O
example	O
,	O
Y.	B-Citation
K.	I-Citation
Muthusamy	I-Citation
et_al.	I-Citation
,	I-Citation
“	I-Citation
Reviewing	I-Citation
Automatic	I-Citation
Language	I-Citation
Identification	I-Citation
,	I-Citation
”	I-Citation
IEEE	I-Citation
Signal	I-Citation
Processing	I-Citation
Magazine	I-Citation
,	I-Citation
11	I-Citation
(	I-Citation
4	I-Citation
)	I-Citation
:	I-Citation
33	I-Citation
-	I-Citation
41	I-Citation
(	I-Citation
October	I-Citation
1994	I-Citation
)	I-Citation
;	O
J.	B-Citation
Navrátil	I-Citation
and	I-Citation
W.	I-Citation
Zühlke	I-Citation
,	I-Citation
“	I-Citation
Phonetic-Context	I-Citation
Mapping	I-Citation
in	I-Citation
Language	I-Citation
Identification	I-Citation
,	I-Citation
”	I-Citation
Proc.	I-Citation
of	I-Citation
the	I-Citation
EUROSPEECH-97	I-Citation
,	I-Citation
Vol.	I-Citation
1	I-Citation
,	I-Citation
71	I-Citation
-	I-Citation
74	I-Citation
(	I-Citation
1997	I-Citation
)	I-Citation
;	O
and	O
J.	B-Citation
Navrátil	I-Citation
and	I-Citation
W.	I-Citation
Zühlke	I-Citation
,	I-Citation
“	I-Citation
An	I-Citation
Efficient	I-Citation
Phonotactic-Acoustic	I-Citation
System	I-Citation
for	I-Citation
Language	I-Citation
Identification	I-Citation
,	I-Citation
”	I-Citation
Proc.	I-Citation
of	I-Citation
the	I-Citation
Int'l	I-Citation
Conf.	I-Citation
on	I-Citation
Acoustics	I-Citation
,	I-Citation
Speech	I-Citation
and	I-Citation
Signal	I-Citation
Processing	I-Citation
(	I-Citation
ICASSP	I-Citation
)	I-Citation
,	I-Citation
Vol.	I-Citation
2	I-Citation
,	I-Citation
781	I-Citation
-	I-Citation
84	I-Citation
,	I-Citation
Seattle	I-Citation
,	I-Citation
Wash	I-Citation
.	I-Citation
,	I-Citation
IEEE	I-Citation
(	I-Citation
May	I-Citation
,	I-Citation
1998	I-Citation
)	I-Citation
,	O
each	O
incorporated	O
by	O
reference	O
herein	O
.	O

FIG	O
.	O
2is	O
a	O
schematic	O
block	O
diagram	O
showing	O
the	O
architecture	O
of	O
an	O
illustrative	O
background	O
language	O
modeling	O
module200in	O
accordance	O
with	O
the	O
present	O
invention	O
.	O
The	O
background	O
language	O
modeling	O
module200may	O
be	O
embodied	O
as	O
a	O
general	O
purpose	O
computing	O
system	O
,	O
such	O
as	O
the	O
general	O
purpose	O
computing	O
system	O
shown	O
in	O
FIG	O
.	O
2	O
.	O
The	O
background	O
language	O
modeling	O
module200includes	O
a	O
processor210and	O
related	O
memory	O
,	O
such	O
as	O
a	O
data	O
storage	O
device220	O
,	O
which	O
may	O
be	O
distributed	O
or	O
local	O
.	O
The	O
processor210may	O
be	O
embodied	O
as	O
a	O
single	O
processor	O
,	O
or	O
a	O
number	O
of	O
local	O
or	O
distributed	O
processors	O
operating	O
in	O
parallel	O
.	O
The	O
data	O
storage	O
device220and/or	O
a	O
read	O
only	O
memory	O
(	O
ROM	O
)	O
are	O
operable	O
to	O
store	O
one	O
or	O
more	O
instructions	O
,	O
which	O
the	O
processor210is	O
operable	O
to	O
retrieve	O
,	O
interpret	O
and	O
execute	O
.	O
It	O
is	O
noted	O
that	O
the	O
background	O
language	O
modeling	O
module200may	O
be	O
integrated	O
with	O
the	O
transcription	O
system110shown	O
inFIG	O
.	O
1	O
,	O
or	O
the	O
background	O
language	O
modeling	O
module200may	O
be	O
a	O
stand-alone	O
device	O
,	O
as	O
shown	O
inFIG	O
.	O
2	O
,	O
as	O
would	O
be	O
apparent	O
to	O
a	O
person	O
of	O
ordinary	O
skill	O
in	O
the	O
art	O
.	O

The	O
data	O
storage	O
device220preferably	O
includes	O
a	O
set	O
of	O
background	O
models	O
250	O
-	O
1	O
for	O
the	O
target	O
language	O
and	O
a	O
set	O
of	O
background	O
models	O
250	O
-	O
2	O
through	O
250-K	O
for	O
each	O
modeled	O
non-target	O
language	O
.	O
As	O
previously	O
indicated	O
,	O
each	O
set	O
of	O
background	O
language	O
models	O
250-K	O
can	O
include	O
one	O
or	O
more	O
of	O
a	O
(	O
i	O
)	O
prosodic	O
model	O
;	O
(	O
ii	O
)	O
acoustic	O
model	O
;	O
(	O
iii	O
)	O
phonotactic	O
model	O
;	O
and	O
(	O
iv	O
)	O
keyword	O
spotting	O
model	O
.	O
In	O
one	O
preferred	O
embodiment	O
,	O
shown	O
inFIG	O
.	O
2	O
,	O
each	O
set	O
of	O
background	O
models	O
includes	O
acoustic	O
and	O
phonotactic	O
models	O
due	O
to	O
a	O
favorable	O
performance	O
and	O
cost	O
ratio	O
.	O
In	O
addition	O
,	O
as	O
discussed	O
further	O
below	O
in	O
conjunction	O
withFIG	O
.	O
3	O
,	O
the	O
data	O
storage	O
device220includes	O
a	O
background	O
model	O
score	O
calculation	O
process300	O
.	O
The	O
exemplary	O
background	O
model	O
score	O
calculation	O
process300calculates	O
a	O
phonotactic-acoustic	O
score	O
for	O
each	O
background	O
model	O
.	O

It	O
is	O
noted	O
that	O
while	O
the	O
background	O
models	O
are	O
trained	O
on	O
a	O
certain	O
set	O
of	O
languages	O
,	O
the	O
normalization	O
method	O
of	O
the	O
present	O
invention	O
may	O
contribute	O
to	O
improving	O
the	O
identification	O
of	O
a	O
non-target	O
language	O
even	O
for	O
non-target	O
languages	O
that	O
were	O
not	O
previously	O
seen	O
in	O
the	O
training	O
data	O
set	O
.	O

CALCULATION	O
OF	O
BACKGROUND	O
MODEL	O
SCORE	O
As	O
previously	O
indicated	O
,	O
the	O
background	O
language	O
modeling	O
module200executes	O
a	O
background	O
model	O
score	O
calculation	O
process300to	O
calculate	O
a	O
score	O
for	O
each	O
background	O
model	O
.	O
FIG	O
.	O
3is	O
a	O
flow	O
chart	O
describing	O
an	O
exemplary	O
background	O
model	O
score	O
calculation	O
process300	O
.	O
It	O
is	O
again	O
noted	O
that	O
the	O
exemplary	O
background	O
model	O
score	O
calculation	O
process300generates	O
background	O
model	O
scores	O
based	O
on	O
phonotactics	O
(	O
phone	O
statistics	O
)	O
and	O
acoustics	O
.	O

As	O
shown	O
inFIG	O
.	O
3	O
,	O
the	O
background	O
model	O
score	O
calculation	O
process300initially	O
retrieves	O
the	O
utterance	O
(	O
speech	O
sample	O
)	O
during	O
step310	O
.	O
ST-BGdenotes	O
the	O
background	O
score	O
for	O
the	O
target	O
language	O
and	O
SN-BG	O
(	O
i	O
)	O
denotes	O
the	O
background	O
score	O
for	O
the	O
i-th	O
non-target	O
model	O
.	O
Thereafter	O
,	O
the	O
phonotactic-acoustic	O
score	O
is	O
calculated	O
for	O
each	O
background	O
model	O
,	O
i	O
,	O
during	O
step320as	O
follows	O
:	O

S	O
N-BG	O
(	O
i	O
)	O
=	O
log	O
Πt	O
=	O
1TP	O
(	O
vt	O
|	O
at	O
,	O
i	O
)	O
·	O
P	O
(	O
at	O
|	O
at	O
−	O
1	O
,	O
.	O
.	O
.	O
,	O
at	O
−	O
k	O
,	O
i	O
)	O
(	O
1	O
)	O
where	O
a1	O
,	O
.	O
.	O
.	O
aT	O
,	O
denote	O
a	O
phone	O
sequence	O
obtained	O
from	O
a	O
phone	O
recognizer	O
,	O
such	O
as	O
a	O
ballistic	O
labeler	O
,	O
described	O
for	O
example	O
,	O
in	O
U.S.	O
patent	O
application	O
Ser	O
.	O
No.	O
09/015	O
,	O
150	O
,	O
or	O
Ramabhadan	B-Citation
et_al.	I-Citation
,	I-Citation
“	I-Citation
Acoustics	I-Citation
Only	I-Citation
Based	I-Citation
Automatic	I-Citation
Phonetic	I-Citation
Baseform	I-Citation
Generation	I-Citation
,	I-Citation
”	I-Citation
Proc.	I-Citation
of	I-Citation
the	I-Citation
Int'l	I-Citation
Conf.	I-Citation
on	I-Citation
Acoustics	I-Citation
,	I-Citation
Speech	I-Citation
and	I-Citation
Signal	I-Citation
Processing	I-Citation
(	I-Citation
ICASSP	I-Citation
)	I-Citation
,	I-Citation
Seattle	I-Citation
,	I-Citation
Wash	I-Citation
.	I-Citation
IEEE	I-Citation
(	I-Citation
May	I-Citation
,	I-Citation
1998	I-Citation
)	I-Citation
,	O
each	O
incorporated	O
by	O
reference	O
herein	O
.	O
In	O
addition	O
,	O
vtstands	O
for	O
the	O
acoustic	O
evidence	O
(	O
observation	O
)	O
within	O
the	O
speech	O
segment	O
of	O
the	O
phone	O
atand	O
P	O
(	O
at	O
|	O
at	O
−	O
1	O
,	O
.	O
.	O
.	O
,	O
at	O
−	O
k	O
,	O
i	O
)	O
for	O
a	O
phonotactic	O
model	O
of	O
the	O
k-th	O
order	O
modeling	O
(	O
k	O
+	O
1	O
)	O
-	O
tuples	O
of	O
phones	O
in	O
a	O
sequence	O
.	O
It	O
is	O
noted	O
that	O
Equation	O
(	O
1	O
)	O
is	O
one	O
of	O
many	O
possible	O
ways	O
to	O
obtain	O
the	O
phonotactic	O
score	O
,	O
as	O
would	O
be	O
apparent	O
to	O
a	O
person	O
of	O
ordinary	O
skill	O
in	O
the	O
art	O
.	O
Alternative	O
language	O
modeling	O
and	O
language	O
identification	O
techniques	O
may	O
consist	O
,	O
for	O
example	O
,	O
of	O
calculating	O
the	O
phonotactic	O
and	O
acoustic	O
scores	O
separately	O
and	O
combining	O
them	O
in	O
the	O
log	O
domain	O
in	O
a	O
weighted	O
manner	O
.	O
Furthermore	O
,	O
the	O
phone	O
duration	O
information	O
may	O
be	O
included	O
,	O
for	O
example	O
,	O
using	O
a	O
Hidden	O
Markov	O
Model	O
(	O
HMM	O
)	O
.	O
The	O
background	O
score	O
for	O
the	O
target	O
language	O
,	O
ST-BG	O
,	O
is	O
also	O
obtained	O
using	O
equation	O
(	O
1	O
)	O
.	O

NORMALIZATION	O
Mathematically	O
,	O
the	O
normalization	O
performed	O
by	O
the	O
normalization	O
module150	O
(	O
FIG	O
.	O
1	O
)	O
can	O
be	O
formulated	O
in	O
several	O
ways	O
.	O
For	O
example	O
,	O
if	O
the	O
engine	O
and	O
background	O
scores	O
are	O
probabilistic	O
,	O
the	O
normalization	O
can	O
be	O
expressed	O
as	O
a	O
ratio	O
of	O
the	O
probability	O
values	O
of	O
the	O
target	O
and	O
non-target	O
scores	O
.	O
Likewise	O
,	O
if	O
the	O
engine	O
and	O
background	O
scores	O
are	O
expressed	O
as	O
log	O
likelihoods	O
,	O
the	O
normalization	O
can	O
be	O
expressed	O
as	O
a	O
difference	O
between	O
logarithmic	O
scores	O
of	O
the	O
target	O
and	O
non-target	O
scores	O
.	O

In	O
the	O
illustrative	O
embodiment	O
,	O
ST-Edenotes	O
the	O
engine	O
target	O
score	O
,	O
ST-BGdenotes	O
the	O
background	O
score	O
for	O
the	O
target	O
language	O
and	O
SN-BG	O
(	O
i	O
)	O
denotes	O
the	O
background	O
score	O
for	O
the	O
i-th	O
non-target	O
model	O
.	O
Thus	O
,	O
the	O
normalized	O
score	O
,	O
S	O
,	O
may	O
be	O
obtained	O
as	O
follows:S=a1	O
[	O
!	O
it	O
!	O
]	O
ST-E	O
+	O
a2	O
[	O
!	O
it	O
!	O
]	O
ST-BG-	O
[	O
!	O
Sum	O
!	O
]	O
i	O
=	O
1N	O
[	O
!	O
it	O
!	O
]	O
bi	O
[	O
!	O
CenterDot	O
!	O
]	O
SN-BG	O
[	O
!	O
af	O
!	O
]	O
(	O
i	O
)	O
(	O
2	O
)	O
where	O
N	O
is	O
the	O
number	O
of	O
background	O
models	O
,	O
and	O
ai	O
,	O
biare	O
weights	O
for	O
the	O
target	O
and	O
non-target	O
scores	O
,	O
respectively	O
.	O
It	O
is	O
noted	O
that	O
the	O
robustness	O
of	O
the	O
model	O
of	O
the	O
background	O
languages	O
increases	O
in	O
proportion	O
to	O
the	O
number	O
,	O
N	O
,	O
of	O
background	O
models	O
.	O
Thus	O
,	O
the	O
language	O
repertoire	O
should	O
be	O
chosen	O
as	O
large	O
and	O
wide-covering	O
as	O
possible	O
.	O
It	O
is	O
again	O
noted	O
that	O
while	O
the	O
background	O
models	O
are	O
trained	O
on	O
a	O
certain	O
set	O
of	O
languages	O
,	O
the	O
normalization	O
method	O
of	O
the	O
present	O
invention	O
may	O
contribute	O
to	O
improving	O
the	O
identification	O
of	O
a	O
non-target	O
language	O
even	O
for	O
non-target	O
languages	O
that	O
were	O
not	O
previously	O
seen	O
in	O
the	O
training	O
data	O
set	O
.	O

REJECTION	O
MECHANISM	O
As	O
previously	O
indicated	O
,	O
a	O
non-target	O
language	O
utterance	O
is	O
identified	O
based	O
on	O
the	O
total	O
normalized	O
score	O
,	O
calculated	O
in	O
accordance	O
with	O
equation	O
(	O
2	O
)	O
and	O
applying	O
a	O
threshold	O
,	O
T	O
,	O
as	O
follows	O
:	O
S	O
−	O
T	O
≧	O
0Accept	O
(	O
Target	O
Language	O
)	O
(	O
3	O
)	O
S	O
−	O
T	O
<	O
0Reject	O
(	O
Non-target	O
Language	O
)	O
Equation	O
(	O
3	O
)	O
leads	O
to	O
a	O
positive	O
or	O
negative	O
left	O
side	O
of	O
the	O
equation	O
,	O
resulting	O
in	O
acceptance	O
or	O
rejection	O
of	O
the	O
utterance	O
,	O
respectively	O
.	O
The	O
threshold	O
value	O
,	O
T	O
,	O
may	O
be	O
obtained	O
from	O
a	O
training	O
stage	O
and/or	O
be	O
derived	O
in	O
an	O
adaptive	O
manner	O
from	O
the	O
current	O
audio	O
stream	O
,	O
as	O
would	O
be	O
apparent	O
to	O
a	O
person	O
of	O
ordinary	O
skill	O
in	O
the	O
art	O
.	O

The	O
normalized	O
score	O
measure	O
,	O
S	O
,	O
at	O
a	O
certain	O
time	O
during	O
transcription	O
may	O
be	O
calculated	O
within	O
a	O
window	O
taking	O
into	O
account	O
a	O
history	O
of	O
the	O
likelihood	O
values	O
from	O
a	O
predetermined	O
time	O
period	O
.	O
For	O
example	O
,	O
a	O
mean	O
value	O
of	O
the	O
word-based	O
likelihoods	O
within	O
a	O
predefined	O
period	O
of	O
time	O
may	O
be	O
utilized	O
.	O

In	O
one	O
application	O
,	O
the	O
present	O
invention	O
may	O
be	O
employed	O
as	O
a	O
language	O
rejection	O
mechanism	O
to	O
interrupt	O
or	O
modify	O
the	O
transcription	O
system110when	O
speech	O
in	O
the	O
non-target	O
language	O
is	O
detected	O
.	O
In	O
other	O
words	O
,	O
if	O
the	O
non-target	O
language	O
is	O
detected	O
in	O
real-time	O
using	O
the	O
present	O
invention	O
,	O
then	O
the	O
speech	O
recognition	O
process	O
can	O
be	O
suspended	O
until	O
the	O
audio	O
stream	O
switches	O
back	O
to	O
the	O
target	O
language	O
.	O
Performance	O
of	O
speech-based	O
text	O
retrieval	O
systems	O
depend	O
heavily	O
on	O
the	O
accuracy	O
of	O
the	O
transcription	O
.	O
Generally	O
,	O
the	O
higher	O
the	O
speech	O
recognition	O
accuracy	O
,	O
the	O
better	O
the	O
performance	O
of	O
the	O
information	O
retrieval	O
.	O
In	O
this	O
manner	O
,	O
the	O
present	O
invention	O
prevents	O
improper	O
transcription	O
and	O
indexing	O
and	O
false	O
interpretations	O
of	O
the	O
speech	O
recognition	O
output	O
.	O

In	O
a	O
further	O
variation	O
,	O
the	O
non-target	O
language	O
identification	O
system100may	O
use	O
a	O
different	O
threshold	O
value	O
,	O
TRESUME	O
,	O
for	O
switching	O
back	O
to	O
the	O
target-language	O
transcription	O
after	O
a	O
previous	O
rejection	O
.	O
The	O
threshold	O
value	O
,	O
TRESUME	O
,	O
may	O
be	O
adaptive	O
or	O
.	O
predetermined	O
,	O
as	O
discussed	O
above	O
for	O
the	O
primary	O
threshold	O
value	O
,	O
T.	O
In	O
yet	O
another	O
variation	O
,	O
the	O
present	O
invention	O
uses	O
precomputed	O
likelihoods	O
(	O
e.g.	O
,	O
by-products	O
)	O
of	O
the	O
recognition	O
process	O
and	O
low-computation	O
background	O
models	O
.	O

It	O
is	O
to	O
be	O
understood	O
that	O
the	O
embodiments	O
and	O
variations	O
shown	O
and	O
described	O
herein	O
are	O
merely	O
illustrative	O
of	O
the	O
principles	O
of	O
this	O
invention	O
and	O
that	O
various	O
modifications	O
may	O
be	O
implemented	O
by	O
those	O
skilled	O
in	O
the	O
art	O
without	O
departing	O
from	O
the	O
scope	O
and	O
spirit	O
of	O
the	O
invention	O
.	O

1	O
.	O
A	O
method	O
for	O
identifying	O
a	O
non-target	O
language	O
utterance	O
in	O
an	O
audio	O
stream	O
,	O
comprising	O
the	O
steps	O
of:transcribing	O
each	O
utterance	O
in	O
said	O
audio	O
stream	O
using	O
a	O
transcription	O
system	O
trained	O
on	O
a	O
target	O
language;generating	O
a	O
confidence	O
score	O
associated	O
with	O
each	O
of	O
said	O
transcribed	O
utterances	O
;	O
andidentifying	O
a	O
transcribed	O
utterance	O
as	O
being	O
in	O
a	O
non-target	O
language	O
if	O
said	O
confidence	O
score	O
generated	O
by	O
said	O
transcription	O
system	O
trained	O
on	O
a	O
target	O
language	O
fails	O
to	O
meet	O
predefined	O
criteria	O
.	O

2	O
.	O
The	O
method	O
ofclaim	O
1	O
,	O
wherein	O
said	O
confidence	O
score	O
is	O
an	O
engine	O
score	O
generated	O
by	O
said	O
transcription	O
system	O
.	O

3	O
.	O
The	O
method	O
ofclaim	O
1	O
,	O
further	O
comprising	O
the	O
step	O
of	O
interrupting	O
said	O
transcription	O
system	O
when	O
said	O
non-target	O
language	O
is	O
detected	O
.	O

4	O
.	O
The	O
method	O
ofclaim	O
1	O
,	O
further	O
comprising	O
the	O
step	O
of	O
modifying	O
said	O
transcription	O
system	O
when	O
said	O
non-target	O
language	O
is	O
detected	O
.	O

5	O
.	O
The	O
method	O
ofclaim	O
1	O
,	O
wherein	O
said	O
confidence	O
score	O
is	O
based	O
on	O
one	O
or	O
more	O
background	O
models	O
trained	O
on	O
at	O
least	O
one	O
non-target	O
language	O
.	O

6	O
.	O
The	O
method	O
ofclaim	O
5	O
,	O
wherein	O
said	O
background	O
models	O
include	O
one	O
or	O
more	O
of	O
(	O
i	O
)	O
prosodic	O
models	O
;	O
(	O
ii	O
)	O
acoustic	O
models	O
;	O
(	O
iii	O
)	O
phonotactic	O
models	O
;	O
and	O
(	O
iv	O
)	O
keyword	O
spotting	O
models	O
for	O
each	O
modeled	O
language	O
.	O

7	O
.	O
The	O
method	O
ofclaim	O
1	O
,	O
wherein	O
said	O
confidence	O
score	O
is	O
based	O
on	O
an	O
engine	O
score	O
provided	O
by	O
said	O
transcription	O
system	O
combined	O
with	O
a	O
background	O
model	O
score	O
to	O
normalize	O
said	O
engine	O
score	O
for	O
said	O
non-target	O
language	O
.	O

8	O
.	O
A	O
method	O
for	O
identifying	O
a	O
non-target	O
language	O
utterance	O
in	O
an	O
audio	O
stream	O
,	O
comprising	O
the	O
steps	O
of:transcribing	O
each	O
utterance	O
in	O
said	O
audio	O
stream	O
using	O
a	O
transcription	O
system	O
trained	O
on	O
a	O
target	O
language;generating	O
a	O
confidence	O
score	O
associated	O
with	O
each	O
of	O
said	O
transcribed	O
utterances	O
based	O
on	O
an	O
engine	O
score	O
provided	O
by	O
said	O
transcription	O
system	O
trained	O
on	O
a	O
target	O
language	O
and	O
at	O
least	O
one	O
background	O
model	O
score	O
;	O
andidentifying	O
a	O
transcribed	O
utterance	O
as	O
being	O
in	O
a	O
non-target	O
language	O
if	O
said	O
confidence	O
score	O
generated	O
by	O
said	O
transcription	O
system	O
trained	O
on	O
a	O
target	O
language	O
fails	O
to	O
meet	O
predefined	O
criteria	O
.	O

9	O
.	O
The	O
method	O
ofclaim	O
8	O
,	O
further	O
comprising	O
the	O
step	O
of	O
interrupting	O
said	O
transcription	O
system	O
when	O
said	O
non-target	O
language	O
is	O
detected	O
.	O

10	O
.	O
The	O
method	O
ofclaim	O
8	O
,	O
further	O
comprising	O
the	O
step	O
of	O
modifying	O
said	O
transcription	O
system	O
when	O
said	O
non-target	O
language	O
is	O
detected	O
.	O

11	O
.	O
The	O
method	O
ofclaim	O
8	O
,	O
wherein	O
said	O
at	O
least	O
one	O
background	O
model	O
is	O
trained	O
on	O
at	O
least	O
one	O
non-target	O
language	O
.	O

12	O
.	O
The	O
method	O
ofclaim	O
11	O
,	O
wherein	O
said	O
at	O
least	O
one	O
background	O
model	O
includes	O
one	O
or	O
more	O
of	O
(	O
i	O
)	O
prosodic	O
models	O
;	O
(	O
ii	O
)	O
acoustic	O
models	O
;	O
(	O
iii	O
)	O
phonotactic	O
models	O
;	O
and	O
(	O
iv	O
)	O
keyword	O
spotting	O
models	O
for	O
each	O
modeled	O
language	O
.	O

13	O
.	O
The	O
method	O
ofclaim	O
8	O
,	O
wherein	O
said	O
confidence	O
score	O
normalizes	O
said	O
engine	O
score	O
for	O
said	O
non-target	O
language	O
.	O

14	O
.	O
A	O
system	O
for	O
identifying	O
a	O
non-target	O
language	O
utterance	O
in	O
an	O
audio	O
stream	O
,	O
comprising:a	O
memory	O
that	O
stores	O
computer-readable	O
code	O
;	O
anda	O
processor	O
operatively	O
coupled	O
to	O
said	O
memory	O
,	O
said	O
processor	O
configured	O
to	O
implement	O
said	O
computer-readable	O
code	O
,	O
said	O
computer-readable	O
code	O
configured	O
to:transcribe	O
each	O
utterance	O
in	O
said	O
audio	O
stream	O
using	O
a	O
transcription	O
system	O
trained	O
on	O
a	O
target	O
language;generate	O
a	O
confidence	O
score	O
associated	O
with	O
each	O
of	O
said	O
transcribed	O
utterances	O
;	O
andidentify	O
a	O
transcribed	O
utterance	O
as	O
being	O
in	O
a	O
non-target	O
language	O
if	O
said	O
confidence	O
score	O
generated	O
by	O
said	O
transcription	O
system	O
trained	O
on	O
a	O
target	O
language	O
fails	O
to	O
meet	O
predefined	O
criteria	O
.	O

15	O
.	O
A	O
system	O
for	O
identifying	O
a	O
non-target	O
language	O
utterance	O
in	O
an	O
audio	O
stream	O
,	O
comprising:a	O
memory	O
that	O
stores	O
computer-readable	O
code	O
;	O
anda	O
processor	O
operatively	O
coupled	O
to	O
said	O
memory	O
,	O
said	O
processor	O
configured	O
to	O
implement	O
said	O
computer-readable	O
code	O
,	O
said	O
computer-readable	O
code	O
configured	O
to:transcribe	O
each	O
utterance	O
in	O
said	O
audio	O
stream	O
using	O
a	O
transcription	O
system	O
trained	O
on	O
a	O
target	O
language;generate	O
a	O
confidence	O
score	O
associated	O
with	O
each	O
of	O
said	O
transcribed	O
utterances	O
based	O
on	O
an	O
engine	O
score	O
provided	O
by	O
said	O
transcription	O
system	O
trained	O
on	O
a	O
target	O
language	O
and	O
at	O
least	O
one	O
background	O
model	O
score	O
;	O
andidentify	O
a	O
transcribed	O
utterance	O
as	O
being	O
in	O
a	O
non-target	O
language	O
if	O
said	O
confidence	O
score	O
generated	O
by	O
said	O
transcription	O
system	O
trained	O
on	O
a	O
target	O
language	O
fails	O
to	O
meet	O
predefined	O
criteria	O
.	O

16	O
.	O
An	O
article	O
of	O
manufacture	O
for	O
identifying	O
a	O
non-target	O
language	O
utterance	O
in	O
an	O
audio	O
stream	O
,	O
comprising:a	O
computer	O
readable	O
medium	O
having	O
computer	O
readable	O
code	O
means	O
embodied	O
thereon	O
,	O
said	O
computer	O
readable	O
program	O
code	O
means	O
comprising:a	O
step	O
to	O
transcribe	O
each	O
utterance	O
in	O
said	O
audio	O
stream	O
using	O
a	O
transcription	O
system	O
trained	O
on	O
a	O
target	O
language	O
;	O
a	O
step	O
to	O
generate	O
a	O
confidence	O
score	O
associated	O
with	O
each	O
of	O
said	O
transcribed	O
utterances	O
;	O
anda	O
step	O
to	O
identify	O
a	O
transcribed	O
utterance	O
as	O
being	O
in	O
a	O
non-target	O
language	O
if	O
said	O
confidence	O
score	O
generated	O
by	O
said	O
transcription	O
system	O
trained	O
on	O
a	O
target	O
language	O
fails	O
to	O
meet	O
predefined	O
criteria	O
.	O

17	O
.	O
An	O
article	O
of	O
manufacture	O
for	O
identifying	O
a	O
non-target	O
language	O
utterance	O
in	O
an	O
audio	O
stream	O
,	O
comprising:a	O
computer	O
readable	O
medium	O
having	O
computer	O
readable	O
code	O
means	O
embodied	O
thereon	O
,	O
said	O
computer	O
readable	O
program	O
code	O
means	O
comprising:a	O
step	O
to	O
transcribe	O
each	O
utterance	O
in	O
said	O
audio	O
stream	O
using	O
a	O
transcription	O
system	O
trained	O
on	O
a	O
target	O
language	O
;	O
a	O
step	O
to	O
generate	O
a	O
confidence	O
score	O
associated	O
with	O
each	O
of	O
said	O
transcribed	O
utterances	O
based	O
on	O
an	O
engine	O
score	O
provided	O
by	O
said	O
transcription	O
system	O
trained	O
on	O
a	O
target	O
language	O
and	O
at	O
least	O
one	O
background	O
model	O
score	O
;	O
anda	O
step	O
to	O
identify	O
a	O
transcribed	O
utterance	O
as	O
being	O
in	O
a	O
non-target	O
language	O
if	O
said	O
confidence	O
score	O
generated	O
by	O
said	O
transcription	O
system	O
trained	O
on	O
a	O
target	O
language	O
fails	O
to	O
meet	O
predefined	O
criteria	O
.	O

