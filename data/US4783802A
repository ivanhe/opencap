US	O
4783802	O
A	O
19881108	O

US	O
06775398	O
19850912	O

eng	O
eng	O

JP	O
59206656	O
A	O
19841002	O

59	O
-	O
206656	O

JP19840206656	O

19881108	O

19881108	O

3G	O
10L	O
1/00	O
A	O
3	O
G	O
10	O
L	O
1	O
00	O
A	O

G10L	O
11/00	O
20060101CFI20051220RMJP	O

20060101	O

C	O
G	O
10	O
L	O
11	O
00	O
F	O
I	O

20051220	O

JP	O

R	O
M	O

G10L	O
11/00	O
20060101AFI20051220RMJP	O

20060101	O

A	O
G	O
10	O
L	O
11	O
00	O
F	O
I	O

20051220	O

JP	O

R	O
M	O

G10L	O
15/00	O
20060101C	O
I20051008RMEP	O

20060101	O

C	O
G	O
10	O
L	O
15	O
00	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/06	O
20060101A	O
I20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
06	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/10	O
20060101ALI20051220RMJP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
10	O
L	O
I	O

20051220	O

JP	O

R	O
M	O

G10L	O
15/20	O
20060101A	O
I20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
20	O
I	O

20051008	O

EP	O

R	O
M	O

US	O

704/243	O
704	O
243	O

704/233	O
704	O
233	O

704/E15.008	O
704	O
E15	O
.	O
008	O

704/E15.039	O
704	O
E15	O
.	O
039	O

G10L	O
15/06T	O
G	O
10	O
L	O
15	O
06	O

T	O

G10L	O
15/20	O
G	O
10	O
L	O
15	O
20	O

US	O

381/36	O
381	O
36	O

US	O

381/41	O
381	O
41	O

US	O

381/42	O
381	O
42	O

US	O

381/43	O
381	O
43	O

US	O

381/44	O
381	O
44	O

US	O

381/45	O
381	O
45	O

US	O

381/46	O
381	O
46	O

US	O

381/51	O
381	O
51	O

US	O

381/57	O
381	O
57	O

US	O

364/513	O
.	O
5	O
364	O
513	O
.	O
5	O

US	O

382/15	O
382	O
15	O

US	O

382/18	O
382	O
18	O

US	O

382/30	O
382	O
30	O

US	O

382/14	O
382	O
14	O

US	O

382/37	O
382	O
37	O

US	O

367/198	O
367	O
198	O

US	O

340/365	O
.	O
R	O
340	O
365	O
.	O
R	O

US	O

379/37	O
379	O
37	O

US	O

379/88	O
379	O
88	O

US	O

379/199	O
379	O
199	O

US	O

434/389	O
434	O
389	O

US	O

434/314	O
434	O
314	O

US	O

434/321	O
434	O
321	O

US	O

434/323	O
434	O
323	O

US	O

434/335	O
434	O
335	O

34	O
Learning	O
system	O
of	O
dictionary	O
for	O
speech	O
recognition	O

US	O
4059725	O
A	O
Sakoe	O
19771122	O

19761221	O

US	O

381/43	O
381	O
43	O

US	O
4394538	O
A	O
Warren	O
et_al.	O

19830719	O

19810304	O

US	O

381/43	O
381	O
43	O

US	O
4581755	O
A	O
Sakoe	O
19860408	O

19821027	O

US	O

381/42	O
381	O
42	O

US	O
4593403	O
A	O
Kishi	O
et_al.	O

19860603	O

19820929	O

US	O

364/513	O
.	O
5	O
364	O
513	O
.	O
5	O

US	O
4618983	O
A	O
Nishioka	O
et_al.	O

19861021	O

19821222	O

US	O

381/43	O
381	O
43	O

US	O
4618984	O
A	O
Das	O
et_al.	O

19861021	O

19830608	O

US	O

381/43	O
381	O
43	O

US	O
4624011	O
A	O
Watanabe	O
et_al.	O

19861118	O

19830128	O

US	O

381/43	O
381	O
43	O

US	O
4633499	O
A	O
Nishioka	O
et_al.	O

19861230	O

19821008	O

US	O

381/43	O
381	O
43	O

US	O
4667341	O
A	O
Watari	O
19870519	O

19830201	O

US	O

381/43	O
381	O
43	O

DE	O
3048107	O
A1	O
19810910	O

19801219	O

C.	B-Citation
C.	I-Citation
Tappert	I-Citation
et_al.	I-Citation
,	I-Citation
Fast	I-Citation
Training	I-Citation
Method	I-Citation
for	I-Citation
Speech	I-Citation
Recognition	I-Citation
Systems	I-Citation
,	I-Citation
IBM	I-Citation
Technical	I-Citation
Disclosure	I-Citation
Bulletin	I-Citation
,	I-Citation
vol.	I-Citation
21	I-Citation
,	I-Citation
No.	I-Citation
8	I-Citation
,	I-Citation
Jan.	I-Citation
1979	I-Citation
,	I-Citation
pp.	I-Citation
3413	I-Citation
,	I-Citation
3414	I-Citation
.	O

E.	B-Citation
Reuhkala	I-Citation
,	I-Citation
Pattern	I-Citation
Recognition	I-Citation
for	I-Citation
Strings	I-Citation
of	I-Citation
Discrete	I-Citation
Symbols	I-Citation
,	I-Citation
Proceedings	I-Citation
of	I-Citation
the	I-Citation
6th	I-Citation
Int	I-Citation
l	I-Citation
Conference	I-Citation
on	I-Citation
Pattern	I-Citation
Recognition	I-Citation
,	I-Citation
Oct.	I-Citation
19	I-Citation
,	I-Citation
20	I-Citation
,	I-Citation
1982	I-Citation
,	I-Citation
pp.	I-Citation
969	I-Citation
972	I-Citation
.	O

N.	B-Citation
Ishii	I-Citation
et_al.	I-Citation
Speaker	I-Citation
Independent	I-Citation
Speech	I-Citation
Recognition	I-Citation
Unit	I-Citation
Development	I-Citation
for	I-Citation
Telephone	I-Citation
Line	I-Citation
Use	I-Citation
,	I-Citation
Japan	I-Citation
Telecommunications	I-Citation
Review	I-Citation
,	I-Citation
No.	I-Citation
3	I-Citation
,	I-Citation
vol.	I-Citation
24	I-Citation
,	I-Citation
Jul.	I-Citation
,	I-Citation
1982	I-Citation
,	I-Citation
pp.	I-Citation
267	I-Citation
274	I-Citation
.	O

C.	B-Citation
Lee	I-Citation
,	I-Citation
et_al.	I-Citation
,	I-Citation
Speech	I-Citation
Recognition	I-Citation
Under	I-Citation
Additive	I-Citation
Noise	I-Citation
,	I-Citation
IEEE	I-Citation
International	I-Citation
Conference	I-Citation
on	I-Citation
Acoustics	I-Citation
,	I-Citation
Speech	I-Citation
and	I-Citation
Signal	I-Citation
Processing	I-Citation
84	I-Citation
,	I-Citation
vol.	I-Citation
3	I-Citation
,	I-Citation
19	I-Citation
21	I-Citation
,	I-Citation
Mar.	I-Citation
,	I-Citation
1984	I-Citation
,	I-Citation
pp.	I-Citation
35	I-Citation
.	I-Citation
7	I-Citation
.	I-Citation
1	I-Citation
35	I-Citation
.	I-Citation
7	I-Citation
.	I-Citation
4	I-Citation
.	O

GB	O
2331827	O
A	O
19990602	O

19971013	O

GB	O
2331827	O
B	O
20001206	O

19971013	O

EP	O
938727	O
B1	O
20020626	O

19971013	O

US	O
5754681	O
A	O
19980519	O

19950622	O

US	O
5761639	O
A	O
19980602	O

19940824	O

US	O
5794194	O
A	O
19980811	O

19970203	O

US	O
5416887	O
A	O
19950516	O

19940224	O

US	O
4916743	O
A	O
19900410	O

19880427	O

US	O
5325445	O
A	O
19940628	O

19920529	O

US	O
5845246	O
A	O
19981201	O

19950228	O

US	O
5347612	O
A	O
19940913	O

19900716	O

US	O
5692097	O
A	O
19971125	O

19941123	O

US	O
4972485	O
A	O
19901120	O

19890523	O

US	O
5295190	O
A	O
19940315	O

19910906	O

US	O
4827522	O
A	O
19890502	O

19860925	O

US	O
5617855	O
A	O
19970408	O

19940901	O

US	O
5182773	O
A	O
19930126	O

19910322	O

US	O
5255342	O
A	O
19931019	O

19921217	O

US	O
5257314	O
A	O
19931026	O

19901205	O

US	O
5129002	O
A	O
19920707	O

19911105	O

US	O
5129000	O
A	O
19920707	O

19901212	O

US	O
5651094	O
A	O
19970722	O

19950605	O

US	O
5953699	O
A	O
19990914	O

19971028	O

US	O
5956676	O
A	O
19990921	O

19960827	O

WO	O
9822937	O
A1	O
19980528	O

19971013	O

WO	O
03036621	O
A1	O
20030501	O

20021022	O

US	O
7676362	O
B2	O
20100309	O

20041231	O

US	O
7502736	O
B2	O
20090310	O

20040921	O

US	O
7373294	O
B2	O
20080513	O

20030515	O

US	O
7302389	O
B2	O
20071127	O

20030808	O

US	O
7054388	O
B2	O
20060530	O

20010425	O

US	O
6292775	O
B1	O
20010918	O

19990218	O

US	O
6178399	O
B1	O
20010123	O

19950424	O

EP	O
1161098	O
A2	O
20011205	O

20010424	O

US	O
7962336	O
B2	O
20110614	O

20070921	O

Kabushiki	O
Kaisha	O
Toshiba	O

Kawasaki	O
JP	O

Takebayashi	O
Yoichi	O

Chigasaki	O
JP	O

Shinoda	O
Hidenori	O

Yokohama	O
JP	O

Foley	O
&	O
Lardner	O
,	O
Schwartz	O
,	O
Jeffery	O
,	O
Schwaab	O
,	O
Mack	O
,	O
Blumenthal	O
&	O
Evans	O

Shoop	O
Jr	O
.	O
;	O
William	O
M.	O

Ip	O
;	O
Paul	O

JP	O
2057443	O
C	O
19960523	O

19841002	O

DE	O
3579662	O
D1	O
19901018	O

19850925	O

EP	O
178509	O
B1	O
19900912	O

19850925	O

JP	O
61084694	O
A	O
19860430	O

19841002	O

JP	O
07092673	O
B	O
19951009	O

19841002	O

JP	O
07092673	O
B2	O
19951009	O

19841002	O

EP	O
178509	O
A1	O
19860423	O

19850925	O

US	O
4783802	O
A	O
19881108	O

19850912	O

JP	O
07092673	O
B	O
19951009	O

19841002	O

JP	O
61084694	O
A	O
19860430	O

19841002	O

EP	O
178509	O
B1	O
19900912	O

19850925	O

EP	O
178509	O
A1	O
19860423	O

19850925	O

JP	O
2057443	O
C	O
19960523	O

19841002	O

US	O
4783802	O
A	O
19881108	O

19850912	O

DE	O
3579662	O
D1	O
19901018	O

19850925	O

JP	O
07092673	O
B2	O
19951009	O

19841002	O

The	O
learning	O
method	O
of	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
in	O
accordance	O
with	O
the	O
present	O
invention	O
,	O
a	O
plurality	O
of	O
speech	O
feature	O
vectors	O
are	O
generated	O
from	O
the	O
time	O
series	O
of	O
speech	O
feature	O
parameter	O
for	O
the	O
input	O
speech	O
pattern	O
,	O
by	O
taking	O
into	O
account	O
knowledge	O
concerning	O
the	O
variation	O
tendencies	O
of	O
the	O
speech	O
patterns	O
,	O
and	O
the	O
learning	O
(	O
preparation	O
)	O
of	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
is	O
carried	O
out	O
by	O
the	O
use	O
of	O
these	O
speech	O
feature	O
vectors	O
thus	O
generated	O
.	O
Therefore	O
,	O
it	O
becomes	O
possible	O
to	O
prepare	O
highly	O
reliable	O
reference	O
pattern	O
vectors	O
in	O
an	O
easy	O
manner	O
from	O
a	O
small	O
number	O
of	O
speech	O
patterns	O
,	O
which	O
makes	O
it	O
possible	O
to	O
achieve	O
an	O
improvement	O
in	O
the	O
speech	O
recognition	O
factor	O
.	O
In	O
particular	O
,	O
it	O
becomes	O
possible	O
to	O
plan	O
an	O
easy	O
improvement	O
of	O
the	O
reference	O
pattern	O
vectors	O
by	O
an	O
effective	O
use	O
of	O
a	O
relatively	O
small	O
number	O
of	O
input	O
speech	O
patterns	O
.	O

19850912	O

AS	O
ASSIGNMENT	O
N	O
US	O
4783802A	O
KABUSHIKI	O
KAISHA	O
TOSHIBA	O
72	O
HORIKAWA-CHO	O
,	O
SAIWAI-	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST.;ASSIGNORS:TAKEBAYASHI	O
,	O
YOICHI;SHINODA	O
,	O
HIDENORI;REEL/FRAME:004459/0913	O

19850813	O

19920427	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
4783802A	O
4	O

19960423	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
4783802A	O
8	O

20000530	O

REMI	O
MAINTENANCE	O
FEE	O
REMINDER	O
MAILED	O
N	O
US	O
4783802A	O

20001105	O

LAPS	O
-	O
LAPSE	O
FOR	O
FAILURE	O
TO	O
PAY	O
MAINTENANCE	O
FEES	O
N	O
US	O
4783802A	O

20010109	O

FP	O
-	O
EXPIRED	O
DUE	O
TO	O
FAILURE	O
TO	O
PAY	O
MAINTENANCE	O
FEE	O
C	O
US	O
4783802A	O

20001108	O

BACKGROUND	O
OF	O
THE	O
INVENTION	O
1	O
.	O
Field	O
of	O
the	O
Invention	O
The	O
present	O
invention	O
relates	O
to	O
a	O
method	O
of	O
and	O
a	O
device	O
for	O
learning	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
for	O
an	O
effective	O
improvement	O
of	O
the	O
recognition	O
performance	O
for	O
a	O
speech	O
recognition	O
device	O
.	O

2	O
.	O
Description	O
of	O
the	O
Prior	O
Art	O
The	O
recent	O
development	O
in	O
the	O
techniques	O
for	O
pattern	O
recognition	O
such	O
as	O
character	O
recognition	O
and	O
speech	O
recognition	O
has	O
been	O
remarkable	O
.	O
In	O
the	O
field	O
related	O
to	O
speech	O
,	O
too	O
,	O
devices	O
for	O
recognizing	O
spoken	O
word	O
and	O
the	O
like	O
are	O
being	O
put	O
into	O
practical	O
use	O
.	O
However	O
,	O
majority	O
of	O
the	O
speech	O
recognition	O
devices	O
are	O
constructed	O
in	O
such	O
a	O
way	O
as	O
to	O
warp	O
the	O
input	O
speech	O
pattern	O
along	O
the	O
time	O
axis	O
by	O
means	O
of	O
the	O
dynamic	O
programming	O
method	O
(	O
DP	O
matching	O
method	O
)	O
.	O
These	O
devices	O
recognize	O
the	O
input	O
speech	O
pattern	O
,	O
by	O
matching	O
the	O
input	O
speech	O
pattern	O
that	O
is	O
normalized	O
through	O
the	O
warping	O
in	O
the	O
time	O
axis	O
with	O
the	O
reference	O
patterns	O
(	O
standard	O
pattern	O
)	O
that	O
has	O
been	O
prepared	O
in	O
advance	O
.	O

However	O
,	O
the	O
prior	O
art	O
speech	O
recognition	O
device	O
with	O
the	O
above	O
construction	O
has	O
a	O
weakness	O
that	O
its	O
recognition	O
capability	O
,	O
that	O
is	O
,	O
the	O
recognition	O
rate	O
is	O
reduced	O
by	O
undergoing	O
various	O
kinds	O
of	O
deformation	O
in	O
the	O
speech	O
pattern	O
under	O
the	O
influence	O
of	O
the	O
level	O
shift	O
in	O
the	O
input	O
speech	O
pattern	O
,	O
variations	O
in	O
the	O
utterance	O
speed	O
,	O
variations	O
due	O
to	O
the	O
speaker	O
,	O
variations	O
introduced	O
by	O
the	O
public	O
telephone	O
line	O
,	O
variations	O
due	O
to	O
the	O
pitch	O
of	O
the	O
speech	O
,	O
variations	O
due	O
to	O
the	O
background	O
noise	O
,	O
and	O
the	O
like	O
.	O
In	O
particular	O
the	O
decreasing	O
tendency	O
in	O
the	O
recognition	O
performance	O
accuracy	O
,	O
such	O
as	O
the	O
decreasing	O
tendency	O
in	O
recognition	O
performance	O
accuracy	O
as	O
mentioned	O
in	O
the	O
above	O
will	O
reveal	O
itself	O
more	O
conspicuously	O
in	O
the	O
telephone	O
word	O
speech	O
recognition	O
device	O
which	O
is	O
aimed	O
at	O
an	O
unspecified	O
majority	O
of	O
speakers	O
or	O
in	O
the	O
word	O
speech	O
recognition	O
device	O
with	O
numerous	O
categories	O
of	O
recognition	O
objects	O
,	O
and	O
further	O
,	O
in	O
the	O
recognition	O
device	O
for	O
phonemes	O
and	O
syllables	O
,	O
remaining	O
as	O
problem	O
to	O
be	O
solved	O
in	O
the	O
speech	O
recognition	O
techniques	O
.	O

In	O
the	O
meantime	O
,	O
it	O
has	O
been	O
well	O
known	O
from	O
the	O
standpoint	O
of	O
the	O
statistical	O
pattern	O
recognition	O
theory	O
that	O
there	O
is	O
a	O
method	O
for	O
improving	O
the	O
recognition	O
capability	O
(	O
recognition	O
performance	O
accuracy	O
)	O
by	O
carrying	O
out	O
the	O
learning	O
of	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
based	O
on	O
the	O
use	O
of	O
a	O
large	O
number	O
of	O
speech	O
patterns	O
that	O
have	O
been	O
collected	O
beforehand	O
.	O
In	O
the	O
above	O
learning	O
method	O
,	O
the	O
larger	O
the	O
number	O
of	O
collected	O
speech	O
patterns	O
is	O
,	O
the	O
higher	O
the	O
recognition	O
score	O
is	O
,	O
due	O
to	O
the	O
corresponding	O
improvement	O
in	O
the	O
capabilities	O
of	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
.	O
However	O
,	O
for	O
a	O
speech	O
recognition	O
device	O
with	O
large	O
number	O
of	O
categories	O
of	O
objects	O
to	O
be	O
recognized	O
or	O
for	O
a	O
word	O
speech	O
recognition	O
device	O
in	O
which	O
practically	O
there	O
are	O
required	O
frequent	O
changes	O
of	O
vocabulary	O
it	O
becomes	O
necessary	O
,	O
in	O
order	O
to	O
improve	O
the	O
recognition	O
performance	O
accuracy	O
,	O
to	O
collect	O
a	O
very	O
large	O
number	O
of	O
speech	O
patterns	O
,	O
which	O
has	O
been	O
difficult	O
to	O
accomplish	O
in	O
practice	O
.	O
In	O
particular	O
,	O
in	O
the	O
case	O
of	O
a	O
speech	O
recognition	O
device	O
for	O
an	O
unspecified	O
speaker	O
,	O
there	O
has	O
been	O
a	O
problem	O
that	O
a	O
reference	O
pattern	O
vectors	O
may	O
not	O
be	O
sufficiently	O
designed	O
based	O
only	O
on	O
a	O
small	O
number	O
of	O
speech	O
patterns	O
.	O
Moreover	O
,	O
in	O
the	O
case	O
of	O
a	O
speech	O
recognition	O
device	O
for	O
a	O
specified	O
speaker	O
or	O
a	O
speech	O
recognition	O
device	O
of	O
the	O
speaker-adapted	O
type	O
,	O
the	O
inputting	O
of	O
the	O
speech	O
pattern	O
is	O
made	O
by	O
uttering	O
of	O
the	O
same	O
category	O
by	O
an	O
identical	O
speaker	O
for	O
a	O
large	O
number	O
of	O
times	O
in	O
order	O
to	O
allow	O
for	O
the	O
variations	O
due	O
to	O
the	O
speaker	O
.	O
This	O
bears	O
burden	O
to	O
the	O
user	O
but	O
also	O
results	O
a	O
significant	O
loss	O
in	O
time	O
.	O

SUMMARY	O
OF	O
THE	O
INVENTION	O
An	O
object	O
of	O
the	O
present	O
invention	O
is	O
to	O
provide	O
a	O
speech	O
recognition	O
device	O
which	O
is	O
capable	O
of	O
improving	O
the	O
recognition	O
accuracy	O
of	O
the	O
speech	O
recognition	O
device	O
.	O

Another	O
object	O
of	O
the	O
present	O
invention	O
is	O
to	O
provide	O
a	O
method	O
of	O
and	O
a	O
device	O
for	O
learning	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
which	O
is	O
capable	O
of	O
learning	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
by	O
an	O
effective	O
use	O
of	O
a	O
small	O
number	O
of	O
speech	O
patterns	O
.	O

Another	O
object	O
of	O
the	O
present	O
invention	O
is	O
to	O
provide	O
a	O
method	O
of	O
and	O
a	O
device	O
for	O
learning	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
by	O
taking	O
into	O
account	O
the	O
various	O
variation	O
tendencies	O
of	O
the	O
speech	O
feature	O
vectors	O
.	O

Another	O
object	O
of	O
the	O
present	O
invention	O
is	O
to	O
provide	O
a	O
method	O
of	O
and	O
a	O
device	O
for	O
learning	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
which	O
make	O
it	O
possible	O
to	O
easily	O
prepare	O
a	O
highly	O
reliable	O
reference	O
pattern	O
vectors	O
by	O
use	O
of	O
the	O
recognition	O
feature	O
vectors	O
modified	O
through	O
the	O
use	O
of	O
a	O
prior	O
knowledge	O
concerning	O
linguistics	O
,	O
phonetics	O
,	O
acoustics	O
or	O
speaker	O
and	O
noise	O
.	O

One	O
of	O
the	O
features	O
of	O
the	O
present	O
invention	O
is	O
that	O
,	O
in	O
extracting	O
the	O
speech	O
feature	O
vectors	O
needed	O
in	O
building	O
a	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
out	O
of	O
a	O
portion	O
of	O
the	O
time	O
series	O
of	O
a	O
speech	O
feature	O
parameter	O
obtained	O
by	O
analyzing	O
the	O
input	O
speech	O
,	O
it	O
is	O
arranged	O
to	O
extract	O
a	O
plurality	O
of	O
speech	O
feature	O
vectors	O
out	O
of	O
a	O
time	O
series	O
of	O
speech	O
feature	O
parameter	O
,	O
by	O
taking	O
into	O
account	O
various	O
variation	O
tendencies	O
in	O
the	O
speech	O
feature	O
vectors	O
caused	O
by	O
the	O
variations	O
in	O
the	O
level	O
of	O
input	O
speech	O
,	O
variations	O
in	O
the	O
utterance	O
speed	O
,	O
variations	O
due	O
to	O
speaker	O
,	O
variations	O
due	O
to	O
telephone	O
line	O
,	O
variations	O
due	O
to	O
fundamental	O
frequency	O
of	O
the	O
speech	O
,	O
variations	O
due	O
to	O
background	O
noise	O
,	O
and	O
so	O
forth	O
,	O
in	O
learning	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
by	O
the	O
use	O
of	O
the	O
speech	O
feature	O
vectors	O
thus	O
extracted	O
.	O

That	O
is	O
,	O
by	O
utilizing	O
the	O
knowledge	O
concerning	O
the	O
factors	O
for	O
the	O
pattern	O
variations	O
in	O
the	O
input	O
speech	O
pattern	O
,	O
the	O
speech	O
feature	O
vectors	O
other	O
than	O
those	O
speech	O
feature	O
vectors	O
used	O
in	O
actual	O
recognition	O
processing	O
are	O
extracted	O
also	O
out	O
of	O
the	O
time	O
series	O
of	O
the	O
feature	O
vectors	O
for	O
the	O
input	O
speech	O
to	O
carry	O
out	O
the	O
learning	O
of	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
by	O
the	O
use	O
of	O
these	O
speech	O
feature	O
vectors	O
.	O

These	O
and	O
other	O
objects	O
,	O
features	O
and	O
advantages	O
of	O
the	O
present	O
invention	O
will	O
be	O
more	O
apparent	O
from	O
the	O
following	O
description	O
of	O
preferred	O
embodiments	O
,	O
taken	O
in	O
conjunction	O
with	O
the	O
accompanying	O
drawings	O
.	O

BRIEF	O
DESCRIPTION	O
OF	O
THE	O
DRAWINGS	O
FIG	O
.	O
1	O
is	O
a	O
simplified	O
block	O
diagram	O
of	O
a	O
word	O
speech	O
recognition	O
device	O
embodying	O
the	O
present	O
invention	O
;	O
FIG	O
.	O
2	O
is	O
a	O
block	O
diagram	O
of	O
the	O
extracting	O
section	O
of	O
feature	O
vectors	O
for	O
learning	O
in	O
the	O
device	O
shown	O
in	O
FIG	O
.	O
1	O
;	O
FIG	O
.	O
3	O
is	O
a	O
diagram	O
for	O
illustrating	O
the	O
extracting	O
concept	O
of	O
the	O
recognition	O
feature	O
vectors	O
;	O
FIG	O
.	O
4	O
is	O
a	O
diagram	O
for	O
illustrating	O
the	O
extracting	O
concept	O
of	O
the	O
feature	O
vectors	O
for	O
learning	O
in	O
word	O
recognition	O
that	O
take	O
into	O
account	O
of	O
the	O
power	O
level	O
variations	O
in	O
the	O
input	O
speech	O
;	O
FIG	O
.	O
5	O
is	O
a	O
schematic	O
block	O
diagram	O
of	O
a	O
modification	O
to	O
the	O
recognition	O
device	O
shown	O
in	O
FIG	O
.	O
1	O
in	O
which	O
the	O
influence	O
of	O
noise	O
on	O
the	O
pattern	O
variation	O
is	O
taken	O
into	O
account	O
;	O
FIG	O
.	O
6	O
is	O
a	O
schematic	O
block	O
diagram	O
of	O
a	O
modification	O
to	O
the	O
extracting	O
section	O
of	O
the	O
feature	O
vectors	O
for	O
learning	O
in	O
the	O
device	O
shown	O
in	O
FIG	O
.	O
1	O
in	O
which	O
the	O
influence	O
of	O
noise	O
on	O
the	O
pattern	O
variation	O
is	O
taken	O
into	O
account	O
;	O
and	O
FIG	O
.	O
7	O
is	O
a	O
schematic	O
block	O
diagram	O
of	O
a	O
second	O
embodiment	O
of	O
the	O
word	O
speech	O
recognition	O
device	O
in	O
accordance	O
with	O
the	O
present	O
invention	O
in	O
which	O
the	O
influence	O
of	O
noise	O
on	O
the	O
pattern	O
variation	O
is	O
taken	O
into	O
account	O
.	O

DESCRIPTION	O
OF	O
THE	O
PREFERRED	O
EMBODIMENTS	O
Referring	O
to	O
FIG	O
.	O
1	O
,	O
a	O
speech	O
recognition	O
device	O
embodying	O
the	O
present	O
invention	O
is	O
shown	O
with	O
a	O
reference	O
number	O
10	O
.	O

The	O
speech	O
recognition	O
device	O
10	O
includes	O
an	O
acoustic	O
analysis	O
section	O
12	O
which	O
analyzes	O
an	O
input	O
speech	O
for	O
each	O
of	O
a	O
fixed	O
period	O
of	O
frame	O
for	O
analysis	O
in	O
order	O
to	O
output	O
the	O
input	O
speech	O
as	O
a	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
for	O
input	O
speech	O
through	O
a	O
speech	O
input	O
section	O
such	O
as	O
a	O
microphone	O
,	O
recognition	O
feature	O
vector	O
extracting	O
section	O
14	O
which	O
extracts	O
a	O
portion	O
of	O
the	O
time	O
series	O
of	O
the	O
speech	O
feature	O
parameter	O
from	O
the	O
acoustic	O
analysis	O
section	O
12	O
as	O
input	O
speech	O
feature	O
vector	O
,	O
a	O
reference	O
pattern	O
vector	O
memory	O
for	O
16	O
which	O
memorizes	O
in	O
advance	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
,	O
and	O
a	O
pattern	O
matching	O
section	O
18	O
which	O
compares	O
the	O
patterns	O
between	O
the	O
feature	O
vectors	O
extracted	O
from	O
the	O
input	O
speech	O
from	O
the	O
recognition	O
feature	O
vector	O
extracting	O
section	O
14	O
and	O
the	O
reference	O
pattern	O
vectors	O
at	O
the	O
speech	O
reference	O
pattern	O
vector	O
memory	O
16	O
.	O
It	O
further	O
includes	O
learning	O
feature	O
vector	O
extracting	O
section	O
20	O
which	O
extracts	O
feature	O
vectors	O
used	O
for	O
the	O
processing	O
,	O
out	O
of	O
the	O
time	O
series	O
of	O
the	O
speech	O
feature	O
parameter	O
from	O
the	O
acoustic	O
analysis	O
section	O
12	O
,	O
as	O
well	O
as	O
other	O
feature	O
vectors	O
that	O
take	O
into	O
account	O
of	O
the	O
variation	O
tendencies	O
of	O
the	O
above	O
feature	O
vectors	O
,	O
and	O
a	O
learning	O
section	O
22	O
which	O
generates	O
a	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
using	O
the	O
feature	O
vectors	O
for	O
learning	O
that	O
are	O
supplied	O
by	O
the	O
learning	O
feature	O
vector	O
extracting	O
section	O
20	O
,	O
in	O
order	O
to	O
supplement	O
or	O
update	O
the	O
reference	O
pattern	O
vectors	O
in	O
the	O
reference	O
pattern	O
vector	O
memory	O
16	O
.	O
In	O
adding	O
,	O
a	O
reference	O
pattern	O
vector	O
learning	O
device	O
24	O
is	O
composed	O
of	O
the	O
learning	O
feature	O
vector	O
extracting	O
section	O
20	O
and	O
the	O
learning	O
section	O
22	O
.	O

Next	O
,	O
referring	O
to	O
FIGS	O
.	O
1	O
and	O
3	O
,	O
the	O
operation	O
of	O
the	O
speech	O
recognition	O
device	O
10	O
will	O
be	O
described	O
.	O

The	O
input	O
speech	O
which	O
is	O
introduced	O
through	O
a	O
microphone	O
or	O
the	O
like	O
is	O
led	O
,	O
after	O
transformed	O
to	O
a	O
digital	O
signal	O
by	O
an	O
A/D	O
converter	O
with	O
sampling	O
frequency	O
of	O
8	O
kHz	O
,	O
say	O
,	O
to	O
the	O
acoustic	O
analysis	O
section	O
12	O
consisting	O
,	O
for	O
example	O
,	O
of	O
an	O
eight-channel	O
filter	O
bank	O
,	O
and	O
is	O
analyzed	O
at	O
a	O
fixed	O
period	O
of	O
frame	O
analysis	O
,	O
for	O
example	O
,	O
at	O
every	O
10	O
ms	O
,	O
to	O
be	O
output	O
as	O
a	O
time	O
series	O
of	O
its	O
speech	O
feature	O
parameter	O
.	O
Here	O
,	O
the	O
speech	O
feature	O
parameter	O
may	O
be	O
a	O
parameter	O
which	O
is	O
obtained	O
either	O
by	O
a	O
digital	O
signal	O
processing	O
,	O
namely	O
,	O
the	O
frequency	O
spectrum	O
derivable	O
by	O
square-law	O
detecting	O
each	O
of	O
the	O
output	O
of	O
the	O
eight-channel	O
filter	O
bank	O
,	O
the	O
cepstrum	O
coefficient	O
,	O
the	O
LPC	O
coefficient	O
,	O
or	O
the	O
discrete	O
Fourier	O
transform	O
,	O
or	O
by	O
an	O
analog	O
analysis	O
method	O
such	O
as	O
the	O
switched	O
capacitor	O
filtering	O
or	O
the	O
analog	O
filtering	O
.	O
The	O
processing	O
for	O
speech	O
recognition	O
and	O
the	O
preparation	O
(	O
learning	O
)	O
of	O
the	O
reference	O
pattern	O
vector	O
are	O
ordinarily	O
carried	O
out	O
by	O
the	O
use	O
of	O
some	O
of	O
these	O
feature	O
parameters	O
.	O
It	O
is	O
to	O
be	O
noted	O
that	O
since	O
the	O
processing	O
for	O
the	O
acoustic	O
analysis	O
has	O
no	O
direct	O
bearing	O
on	O
the	O
main	O
object	O
of	O
the	O
present	O
invention	O
,	O
the	O
ensuing	O
description	O
will	O
be	O
given	O
on	O
the	O
assumption	O
that	O
the	O
acoustic	O
analysis	O
section	O
12	O
is	O
composed	O
of	O
an	O
eight-channel	O
filter	O
bank	O
.	O

Now	O
,	O
the	O
recognition	O
processing	O
for	O
the	O
input	O
speech	O
is	O
carried	O
out	O
by	O
extracting	O
a	O
portion	O
of	O
the	O
time	O
series	O
of	O
the	O
short-term	O
frequency	O
spectrum	O
which	O
represents	O
the	O
output	O
of	O
the	O
eight-channel	O
filter	O
bank	O
in	O
the	O
analysis	O
section	O
12	O
,	O
as	O
the	O
feature	O
vectors	O
for	O
the	O
input	O
speech	O
.	O
In	O
this	O
case	O
,	O
the	O
method	O
for	O
extracting	O
the	O
feature	O
vectors	O
will	O
vary	O
depending	O
upon	O
the	O
unit	O
,	O
such	O
as	O
word	O
,	O
syllable	O
,	O
or	O
phoneme	O
,	O
of	O
the	O
speech	O
which	O
constitutes	O
the	O
object	O
of	O
the	O
recognition	O
.	O

Taking	O
the	O
case	O
of	O
the	O
word	O
speech	O
recognition	O
,	O
in	O
which	O
the	O
variations	O
in	O
the	O
speech	O
level	O
is	O
taken	O
into	O
account	O
as	O
an	O
example	O
,	O
the	O
extracting	O
section	O
of	O
speech	O
feature	O
vector	O
for	O
recognition	O
14	O
determines	O
first	O
,	O
for	O
example	O
,	O
the	O
starting	O
point	O
S	O
and	O
the	O
ending	O
point	O
E	O
of	O
the	O
input	O
speech	O
by	O
means	O
of	O
the	O
power	O
level	O
change	O
,	O
the	O
duration	O
,	O
or	O
others	O
,	O
of	O
the	O
speech	O
.	O
The	O
time	O
segment	O
between	O
the	O
starting	O
point	O
S	O
and	O
the	O
ending	O
point	O
E	O
of	O
the	O
input	O
speech	O
thus	O
determined	O
is	O
divided	O
equally	O
,	O
for	O
example	O
,	O
into	O
seven	O
segments	O
as	O
shown	O
in	O
FIG	O
.	O
3	O
.	O
By	O
selectively	O
extracting	O
the	O
outputs	O
from	O
the	O
filter	O
bank	O
at	O
each	O
of	O
the	O
eight	O
time	O
points	O
chosen	O
,	O
including	O
the	O
starting	O
and	O
the	O
ending	O
points	O
,	O
a	O
64	O
(	O
=	O
8	O
.	O
times	O
.	O
8	O
)	O
dimensional	O
vector	O
,	O
corresponding	O
to	O
the	O
eight	O
points	O
in	O
the	O
direction	O
of	O
the	O
time	O
axis	O
and	O
the	O
eight	O
channels	O
in	O
the	O
direction	O
of	O
the	O
frequency	O
axis	O
,	O
is	O
extracted	O
as	O
a	O
speech	O
feature	O
vector	O
for	O
the	O
input	O
speech	O
.	O
It	O
is	O
needless	O
to	O
say	O
that	O
the	O
feature	O
vector	O
is	O
extracted	O
by	O
the	O
use	O
of	O
other	O
techniques	O
when	O
the	O
object	O
to	O
be	O
recognized	O
is	O
other	O
than	O
a	O
word	O
.	O

The	O
pattern	O
matching	O
section	O
carries	O
out	O
the	O
matching	O
of	O
patterns	O
between	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
stored	O
in	O
advance	O
in	O
the	O
speech	O
reference	O
pattern	O
vector	O
memory	O
16	O
and	O
the	O
speech	O
feature	O
vector	O
extracted	O
in	O
the	O
above	O
manner	O
by	O
the	O
use	O
of	O
,	O
for	O
example	O
,	O
the	O
method	O
of	O
compound	O
degree	O
of	O
similarity	O
multiple	O
similarity	O
method	O
or	O
subspace	O
method	O
.	O
Namely	O
,	O
at	O
the	O
pattern	O
matching	O
section	O
18	O
,	O
the	O
pattern	O
comparison	O
between	O
the	O
reference	O
pattern	O
vectors	O
which	O
has	O
been	O
prepared	O
in	O
advance	O
for	O
each	O
of	O
the	O
object	O
word	O
to	O
be	O
recognized	O
and	O
the	O
speech	O
feature	O
vector	O
is	O
carried	O
out	O
by	O
computing	O
the	O
value	O
of	O
the	O
compound	O
degree	O
of	O
similarity	O
according	O
to	O
the	O
following	O
equation	O
.	O
#	O
#	O
EQU1	O
#	O
#	O
In	O
the	O
above	O
equation	O
,	O
the	O
reference	O
pattern	O
vectors	O
a	O
for	O
speech	O
recognition	O
for	O
a	O
word	O
l	O
is	O
given	O
by	O
a	O
fixed	O
vector	O
.	O
sub	O
.	O
m	O
.	O
sup	O
.	O
(	O
l	O
)	O
,	O
with	O
a.sub.m.sup	O
.	O
(	O
l	O
)	O
as	O
a	O
constant	O
.	O
The	O
pattern	O
matching	O
section	O
18	O
computes	O
the	O
value	O
of	O
the	O
degree	O
of	O
similarity	O
S.	O
sup	O
.	O
(	O
l	O
)	O
between	O
the	O
feature	O
vector	O
for	O
the	O
input	O
speech	O
and	O
the	O
reference	O
pattern	O
vectors	O
of	O
words	O
registered	O
in	O
the	O
reference	O
pattern	O
vectors	O
memory	O
16	O
as	O
recognition	O
object	O
,	O
for	O
each	O
of	O
the	O
word	O
,	O
to	O
output	O
the	O
word	O
l	O
that	O
gives	O
the	O
maximum	O
degree	O
of	O
similarity	O
S.	O
sup	O
.	O
(	O
l	O
)	O
as	O
the	O
result	O
of	O
the	O
recognition	O
.	O
The	O
word	O
recognition	O
of	O
the	O
input	O
speech	O
is	O
accomplished	O
by	O
the	O
process	O
as	O
described	O
in	O
the	O
above	O
.	O

The	O
method	O
according	O
to	O
the	O
present	O
invention	O
is	O
one	O
which	O
makes	O
it	O
possible	O
to	O
efficiently	O
learn	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
that	O
is	O
utilized	O
for	O
the	O
pattern	O
matching	O
as	O
in	O
the	O
above	O
and	O
improves	O
and	O
enhances	O
the	O
capabilities	O
of	O
the	O
reference	O
pattern	O
vectors	O
.	O
The	O
description	O
about	O
the	O
method	O
will	O
be	O
given	O
in	O
what	O
follows	O
.	O

Now	O
,	O
while	O
the	O
learning	O
of	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
is	O
in	O
progress	O
,	O
the	O
extracting	O
section	O
of	O
feature	O
vector	O
for	O
learning	O
20	O
extracts	O
feature	O
vectors	O
to	O
be	O
used	O
for	O
recognition	O
,	O
based	O
on	O
the	O
output	O
from	O
the	O
eight-channel	O
filter	O
bank	O
in	O
the	O
acoustic	O
analysis	O
section	O
12	O
,	O
as	O
well	O
as	O
extracts	O
other	O
feature	O
vectors	O
for	O
learning	O
processing	O
that	O
take	O
into	O
account	O
of	O
the	O
variation	O
tendencies	O
in	O
the	O
first-mentioned	O
feature	O
vectors	O
.	O
In	O
other	O
words	O
,	O
the	O
extracting	O
section	O
of	O
feature	O
vector	O
20	O
is	O
extracting	O
a	O
plurality	O
of	O
speech	O
feature	O
vectors	O
for	O
learning	O
,	O
including	O
those	O
feature	O
vectors	O
that	O
are	O
served	O
for	O
the	O
recognition	O
processing	O
,	O
out	O
of	O
a	O
time	O
series	O
of	O
one	O
feature	O
parameter	O
that	O
is	O
obtained	O
at	O
the	O
acoustic	O
analysis	O
section	O
12	O
by	O
analyzing	O
the	O
input	O
speech	O
.	O

Referring	O
to	O
FIG	O
.	O
2	O
,	O
the	O
construction	O
of	O
the	O
learning	O
feature	O
vector	O
extracting	O
section	O
20	O
will	O
be	O
described	O
in	O
the	O
case	O
of	O
aiming	O
at	O
the	O
word	O
speech	O
recognition	O
as	O
was	O
mentioned	O
earlier	O
.	O

The	O
extracting	O
section	O
of	O
feature	O
vector	O
for	O
learning	O
20	O
comprises	O
a	O
speech	O
level	O
conversion	O
section	O
24	O
,	O
a	O
knowledge	O
source	O
26	O
concerning	O
the	O
level	O
variations	O
,	O
a	O
speech	O
segment	O
detecting	O
section	O
28	O
,	O
and	O
an	O
extracting	O
section	O
of	O
word	O
feature	O
vector	O
29	O
.	O
First	O
,	O
the	O
speech	O
level	O
conversion	O
section	O
24	O
,	O
according	O
to	O
the	O
knowledge	O
concerning	O
the	O
level	O
variations	O
which	O
comes	O
from	O
the	O
knowledge	O
source	O
26	O
,	O
intentionally	O
increases	O
or	O
decreases	O
the	O
output	O
level	O
of	O
the	O
time	O
series	O
for	O
the	O
feature	O
parameter	O
that	O
comes	O
from	O
the	O
acoustic	O
analysis	O
section	O
12	O
,	O
by	O
considering	O
the	O
variations	O
in	O
the	O
power	O
level	O
of	O
the	O
input	O
speech	O
.	O
In	O
other	O
words	O
,	O
it	O
changes	O
the	O
value	O
of	O
the	O
output	O
from	O
the	O
eight-channel	O
filter	O
bank	O
into	O
several	O
steps	O
,	O
for	O
example	O
,	O
three	O
steps	O
within	O
the	O
range	O
of	O
.	O
+	O
-	O
.	O
10	O
dB	O
,	O
to	O
obtain	O
a	O
plurality	O
of	O
kinds	O
of	O
time	O
series	O
for	O
a	O
feature	O
parmameter	O
,	O
for	O
instance	O
,	O
I	O
,	O
I	O
'	O
,	O
and	O
I	O
"	O
as	O
shown	O
in	O
FIG	O
.	O
4	O
.	O
Concerning	O
these	O
time	O
series	O
I	O
,	O
I	O
'	O
,	O
and	O
I	O
"	O
,	O
the	O
starting	O
and	O
ending	O
points	O
S	O
and	O
E	O
of	O
the	O
speech	O
are	O
detected	O
respectively	O
by	O
the	O
speech	O
segment	O
detecting	O
section	O
28	O
,	O
and	O
their	O
respective	O
feature	O
vectors	O
,	O
'	O
,	O
and	O
"	O
are	O
detected	O
by	O
the	O
extracting	O
section	O
of	O
feature	O
vector	O
29	O
to	O
be	O
given	O
to	O
the	O
learning	O
section	O
22	O
.	O
At	O
the	O
learning	O
section	O
22	O
,	O
the	O
covariance	O
matrix	O
is	O
obtained	O
for	O
the	O
plurality	O
of	O
feature	O
vectors	O
for	O
learning	O
thus	O
found	O
for	O
the	O
input	O
speech	O
,	O
and	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
is	O
computed	O
through	O
such	O
operation	O
as	O
the	O
KL	O
expansion	O
of	O
the	O
covariance	O
matrix	O
.	O
A	O
reference	O
pattern	O
vectors	O
obtained	O
in	O
this	O
manner	O
is	O
given	O
to	O
the	O
reference	O
pattern	O
vectors	O
memory	O
for	O
speech	O
recognition	O
16	O
to	O
supplement	O
or	O
update	O
the	O
reference	O
pattern	O
vectors	O
memory	O
16	O
.	O

The	O
above	O
description	O
was	O
made	O
in	O
conjunction	O
with	O
the	O
learning	O
processing	O
of	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
in	O
which	O
use	O
was	O
made	O
of	O
the	O
method	O
of	O
compound	O
degree	O
of	O
similarity	O
.	O
Needless	O
to	O
say	O
,	O
the	O
processing	O
of	O
the	O
reference	O
pattern	O
vectors	O
learning	O
will	O
vary	O
depending	O
upon	O
the	O
method	O
adopted	O
from	O
among	O
other	O
various	O
methods	O
for	O
discrimination	O
,	O
such	O
as	O
the	O
subspace	O
method	O
,	O
the	O
Maharanobis	O
'	O
distance	O
,	O
and	O
the	O
maximum	O
likelihood	O
method	O
.	O
In	O
other	O
words	O
,	O
regardless	O
of	O
the	O
discrimination	O
method	O
adopted	O
for	O
the	O
recognition	O
processing	O
,	O
it	O
is	O
only	O
necessary	O
to	O
carry	O
out	O
the	O
learning	O
of	O
the	O
reference	O
pattern	O
vector	O
by	O
extracting	O
a	O
plurality	O
of	O
speech	O
feature	O
vectors	O
for	O
learning	O
,	O
in	O
adding	O
to	O
the	O
speech	O
feature	O
vectors	O
to	O
be	O
used	O
for	O
recognition	O
,	O
by	O
considering	O
,	O
for	O
example	O
,	O
the	O
variation	O
tendencies	O
of	O
the	O
speech	O
patterns	O
,	O
out	O
of	O
the	O
time	O
series	O
of	O
one	O
feature	O
parameter	O
that	O
is	O
obtained	O
by	O
analyzing	O
the	O
input	O
speech	O
.	O

In	O
this	O
way	O
,	O
according	O
to	O
the	O
present	O
system	O
,	O
a	O
plurality	O
of	O
feature	O
vectors	O
can	O
be	O
extracted	O
from	O
an	O
input	O
speech	O
by	O
making	O
an	O
advantageous	O
use	O
of	O
an	O
a	O
priori	O
knowledge	O
concerning	O
the	O
input	O
speech	O
.	O

Therefore	O
,	O
it	O
becomes	O
possible	O
to	O
obtain	O
a	O
large	O
amount	O
of	O
learning	O
information	O
out	O
of	O
a	O
small	O
amount	O
of	O
information	O
on	O
the	O
input	O
speech	O
,	O
and	O
to	O
carry	O
out	O
an	O
efficient	O
learning	O
of	O
reference	O
pattern	O
vector	O
by	O
the	O
use	O
of	O
the	O
information	O
for	O
learning	O
obtained	O
in	O
this	O
manner	O
.	O
For	O
example	O
,	O
as	O
a	O
prior	O
knowledge	O
on	O
the	O
level	O
of	O
an	O
input	O
speech	O
,	O
it	O
is	O
generally	O
known	O
that	O
there	O
are	O
created	O
variations	O
in	O
the	O
level	O
of	O
the	O
input	O
speech	O
that	O
are	O
caused	O
by	O
the	O
loudness	O
of	O
utterance	O
,	O
the	O
difference	O
in	O
the	O
distance	O
between	O
the	O
speaker	O
and	O
the	O
microphone	O
,	O
the	O
variations	O
in	O
the	O
amplification	O
factor	O
,	O
and	O
the	O
like	O
.	O
Accordingly	O
,	O
in	O
this	O
case	O
,	O
in	O
adding	O
to	O
the	O
feature	O
vector	O
for	O
a	O
word	O
speech	O
that	O
is	O
obtained	O
by	O
detecting	O
the	O
starting	O
and	O
ending	O
points	O
S	O
and	O
E	O
of	O
the	O
word	O
speech	O
from	O
the	O
original	O
speech	O
pattern	O
,	O
and	O
dividing	O
the	O
length	O
of	O
the	O
word	O
speech	O
into	O
seven	O
equal	O
sections	O
,	O
there	O
will	O
be	O
obtained	O
word	O
feature	O
vectors	O
'	O
and	O
"	O
that	O
are	O
different	O
from	O
the	O
feature	O
vector	O
,	O
based	O
on	O
the	O
pairs	O
of	O
the	O
starting	O
and	O
ending	O
points	O
S	O
'	O
,	O
E	O
'	O
and	O
S	O
"	O
,	O
E	O
"	O
for	O
the	O
speech	O
pattern	O
,	O
by	O
varying	O
the	O
input	O
speech	O
level	O
,	O
for	O
example	O
,	O
by	O
.	O
+	O
-	O
.	O
10	O
dB	O
.	O

In	O
this	O
way	O
,	O
it	O
becomes	O
possible	O
to	O
carry	O
out	O
the	O
learning	O
of	O
the	O
reference	O
pattern	O
vector	O
by	O
increasing	O
the	O
data	O
amount	O
for	O
the	O
feature	O
vectors	O
to	O
be	O
utilized	O
for	O
the	O
learning	O
.	O
Moreover	O
,	O
if	O
the	O
a	O
priori	O
knowledge	O
concerning	O
the	O
changes	O
and	O
scaling	O
of	O
utterance	O
,	O
in	O
adding	O
to	O
the	O
level	O
variations	O
described	O
in	O
the	O
above	O
,	O
is	O
utilized	O
,	O
it	O
becomes	O
possible	O
,	O
through	O
changes	O
in	O
the	O
resampling	O
points	O
of	O
the	O
speech	O
feature	O
vectors	O
by	O
considering	O
the	O
temporal	O
tendencies	O
in	O
the	O
variations	O
of	O
utterance	O
,	O
to	O
aim	O
at	O
an	O
increase	O
in	O
the	O
data	O
amount	O
for	O
the	O
feature	O
vectors	O
to	O
be	O
used	O
for	O
the	O
learning	O
of	O
the	O
reference	O
pattern	O
vector	O
by	O
the	O
preparation	O
of	O
a	O
plurality	O
of	O
word	O
feature	O
vectors	O
for	O
learning	O
.	O

In	O
adding	O
,	O
it	O
is	O
also	O
possible	O
to	O
make	O
an	O
advantageous	O
use	O
,	O
besides	O
the	O
variations	O
in	O
the	O
time	O
axis	O
direction	O
of	O
the	O
input	O
speech	O
patterns	O
,	O
of	O
the	O
knowledge	O
concerning	O
the	O
variations	O
in	O
the	O
frequency	O
axis	O
direction	O
,	O
such	O
as	O
the	O
variations	O
due	O
to	O
speaker	O
that	O
will	O
occur	O
when	O
unspecified	O
speakers	O
are	O
to	O
be	O
considered	O
or	O
the	O
variations	O
due	O
to	O
the	O
frequency	O
features	O
of	O
the	O
telephone	O
line	O
.	O
In	O
concrete	O
terms	O
,	O
extracting	O
of	O
the	O
feature	O
vectors	O
for	O
learning	O
will	O
become	O
more	O
effective	O
if	O
an	O
active	O
use	O
is	O
made	O
,	O
for	O
example	O
,	O
of	O
the	O
output	O
of	O
the	O
eight-channel	O
filter	O
,	O
that	O
is	O
,	O
the	O
slope	O
of	O
the	O
frequency	O
spectrum	O
,	O
the	O
variations	O
in	O
the	O
formant	O
frequencies	O
,	O
or	O
the	O
knowledge	O
concerning	O
the	O
influence	O
of	O
the	O
pitch	O
frequency	O
.	O

Moreover	O
,	O
the	O
present	O
invention	O
is	O
not	O
limited	O
to	O
the	O
embodiment	O
that	O
has	O
been	O
described	O
in	O
detail	O
.	O
Thus	O
,	O
for	O
example	O
,	O
it	O
may	O
as	O
well	O
be	O
applied	O
,	O
besides	O
the	O
word	O
speech	O
recognition	O
described	O
in	O
the	O
foregoing	O
,	O
to	O
the	O
phoneme	O
recognition	O
,	O
syllable	O
recognition	O
,	O
connected	O
words	O
recognition	O
,	O
vowel	O
recognition	O
,	O
or	O
the	O
like	O
.	O
Also	O
,	O
in	O
the	O
case	O
of	O
vowel	O
recognition	O
by	O
the	O
use	O
of	O
the	O
speech	O
parameter	O
corresponding	O
to	O
one	O
frame	O
,	O
the	O
learning	O
of	O
the	O
reference	O
pattern	O
vector	O
can	O
be	O
made	O
more	O
effectively	O
if	O
a	O
plurality	O
of	O
feature	O
vectors	O
for	O
learning	O
are	O
obtained	O
by	O
utilizing	O
the	O
previously	O
mentioned	O
level	O
variations	O
or	O
by	O
modifying	O
the	O
slope	O
of	O
the	O
variation	O
spectrum	O
for	O
the	O
formant	O
.	O

Furthermore	O
,	O
in	O
the	O
connected	O
word	O
recognition	O
,	O
too	O
,	O
if	O
the	O
extracting	O
of	O
the	O
word	O
feature	O
vectors	O
is	O
carried	O
out	O
,	O
for	O
example	O
,	O
by	O
changing	O
the	O
word	O
boundaries	O
as	O
plural	O
,	O
a	O
highly	O
efficient	O
reference	O
pattern	O
vectors	O
learning	O
will	O
become	O
possible	O
by	O
the	O
use	O
of	O
a	O
small	O
number	O
of	O
patterns	O
.	O
Moreover	O
,	O
in	O
the	O
single	O
syllable	O
recognition	O
or	O
the	O
consonant	O
recognition	O
,	O
as	O
well	O
,	O
it	O
may	O
be	O
arranged	O
to	O
extract	O
a	O
plurality	O
of	O
time-frequency	O
spectra	O
by	O
shifting	O
the	O
time	O
axis	O
,	O
on	O
the	O
recognition	O
of	O
the	O
shift	O
of	O
the	O
feature	O
vectors	O
in	O
the	O
direction	O
of	O
the	O
time	O
.	O

Referring	O
to	O
FIG	O
.	O
5	O
,	O
a	O
modification	O
to	O
the	O
learning	O
feature	O
vector	O
extracting	O
section	O
for	O
the	O
speech	O
recognition	O
device	O
shown	O
in	O
FIG	O
.	O
1	O
is	O
illustrated	O
with	O
reference	O
numeral	O
30	O
.	O

In	O
order	O
to	O
take	O
into	O
account	O
the	O
influence	O
of	O
the	O
background	O
noise	O
,	O
it	O
is	O
arranged	O
in	O
the	O
modification	O
to	O
obtain	O
the	O
feature	O
vectors	O
for	O
learning	O
by	O
adding	O
,	O
several	O
noises	O
that	O
are	O
prepared	O
beforehand	O
with	O
appropriately	O
assigned	O
levels	O
,	O
at	O
the	O
learning	O
feature	O
vector	O
extracting	O
section	O
.	O
The	O
learning	O
feature	O
vector	O
extracting	O
section	O
30	O
of	O
the	O
modification	O
comprises	O
a	O
knowledge	O
source	O
32	O
which	O
comprises	O
a	O
noise	O
generator	O
for	O
supplying	O
knowledge	O
concerning	O
the	O
noise	O
,	O
a	O
noise	O
adding	O
section	O
34	O
which	O
adds	O
the	O
noise	O
from	O
the	O
noise	O
generator	O
32	O
to	O
the	O
time	O
series	O
of	O
the	O
speech	O
feature	O
parameter	O
from	O
the	O
acoustic	O
analysis	O
section	O
12	O
,	O
a	O
speech	O
segment	O
detecting	O
section	O
36	O
which	O
detects	O
the	O
starting	O
and	O
ending	O
points	O
S	O
and	O
E	O
of	O
the	O
time	O
series	O
to	O
which	O
is	O
added	O
the	O
noises	O
,	O
and	O
an	O
extracting	O
section	O
of	O
word	O
feature	O
vector	O
38	O
which	O
extracts	O
the	O
feature	O
vectors	O
from	O
the	O
time	O
series	O
.	O

A	O
device	O
which	O
is	O
arranged	O
in	O
this	O
manner	O
permits	O
an	O
easy	O
preparation	O
of	O
a	O
reference	O
pattern	O
vectors	O
that	O
takes	O
the	O
noise	O
influence	O
into	O
account	O
,	O
bringing	O
about	O
an	O
enormous	O
practical	O
advantage	O
.	O
It	O
should	O
be	O
noted	O
in	O
the	O
example	O
shown	O
in	O
FIG	O
.	O
5	O
that	O
the	O
speech	O
signal	O
and	O
the	O
noise	O
signal	O
are	O
added	O
on	O
the	O
level	O
of	O
time	O
signal	O
.	O
That	O
is	O
,	O
analog	O
or	O
digital	O
time	O
series	O
signals	O
are	O
added	O
at	O
the	O
noise	O
adding	O
section	O
34	O
.	O

Referring	O
to	O
FIG	O
.	O
6	O
,	O
another	O
modification	O
to	O
the	O
learning	O
feature	O
vector	O
extracting	O
section	O
of	O
the	O
speech	O
recognition	O
device	O
as	O
shown	O
in	O
FIG	O
.	O
1	O
is	O
illustrated	O
with	O
reference	O
numeral	O
40	O
.	O
The	O
learning	O
feature	O
vector	O
extracting	O
section	O
40	O
comprises	O
a	O
speech	O
segment	O
detecting	O
section	O
42	O
which	O
detects	O
the	O
starting	O
and	O
ending	O
points	O
S	O
and	O
E	O
of	O
the	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
which	O
comes	O
from	O
the	O
acoustic	O
analysis	O
section	O
12	O
,	O
a	O
feature	O
vector	O
extracting	O
section	O
44	O
which	O
extracts	O
the	O
feature	O
vectors	O
of	O
the	O
time	O
series	O
for	O
which	O
the	O
starting	O
and	O
ending	O
points	O
are	O
specified	O
,	O
a	O
knowledge	O
source	O
46	O
which	O
comprises	O
a	O
noise	O
generator	O
for	O
supplying	O
knowledge	O
concerning	O
the	O
noises	O
,	O
and	O
a	O
noise	O
adding	O
section	O
48	O
which	O
adds	O
the	O
noises	O
from	O
the	O
noise	O
generator	O
46	O
to	O
the	O
feature	O
vector	O
from	O
the	O
feature	O
vector	O
extracting	O
section	O
44	O
.	O
Analogous	O
to	O
the	O
previous	O
embodiment	O
,	O
the	O
present	O
embodiment	O
possesses	O
an	O
advantage	O
that	O
it	O
permits	O
an	O
easy	O
preparation	O
of	O
a	O
reference	O
pattern	O
vectors	O
that	O
takes	O
into	O
account	O
the	O
influence	O
of	O
the	O
noises	O
.	O
In	O
the	O
example	O
shown	O
in	O
FIG	O
.	O
6	O
,	O
both	O
of	O
the	O
noise	O
generator	O
46	O
and	O
the	O
noise	O
adding	O
section	O
48	O
operate	O
based	O
on	O
digital	O
processing	O
.	O
For	O
example	O
,	O
when	O
the	O
logarithmic	O
power	O
spectrum	O
is	O
output	O
from	O
the	O
extracting	O
section	O
of	O
feature	O
vectors	O
44	O
,	O
it	O
is	O
necessary	O
to	O
convert	O
the	O
logarith	O
to	O
a	O
linear	O
power	O
spectrum	O
once	O
to	O
be	O
added	O
to	O
the	O
noise	O
,	O
and	O
then	O
to	O
reconvert	O
back	O
to	O
logarithm	O
again	O
.	O

Referring	O
to	O
FIG	O
.	O
7	O
,	O
a	O
second	O
embodiment	O
of	O
the	O
speech	O
recognition	O
device	O
in	O
accordance	O
with	O
the	O
present	O
invention	O
is	O
shown	O
with	O
reference	O
numeral	O
50	O
.	O

In	O
the	O
embodiment	O
,	O
it	O
is	O
arranged	O
that	O
the	O
noises	O
from	O
a	O
noise	O
generator	O
52	O
are	O
added	O
at	O
a	O
noise	O
adding	O
section	O
54	O
to	O
the	O
input	O
speech	O
which	O
is	O
sent	O
from	O
the	O
speech	O
inputting	O
section	O
,	O
to	O
be	O
supplied	O
to	O
the	O
acoustic	O
analysis	O
section	O
12	O
.	O

An	O
learning	O
feature	O
vector	O
extracting	O
section	O
56	O
comprises	O
a	O
speech	O
segment	O
detecting	O
section	O
58	O
which	O
detects	O
the	O
starting	O
and	O
ending	O
points	O
S	O
and	O
E	O
of	O
the	O
time	O
series	O
for	O
the	O
feature	O
parameter	O
which	O
comes	O
from	O
the	O
acoustic	O
analysis	O
section	O
12	O
and	O
a	O
feature	O
vector	O
extracting	O
section	O
60	O
which	O
extracts	O
feature	O
vectors	O
of	O
the	O
time	O
series	O
for	O
which	O
the	O
starting	O
and	O
ending	O
points	O
are	O
specified	O
.	O

With	O
a	O
construction	O
as	O
in	O
the	O
second	O
embodiment	O
,	O
it	O
becomes	O
possible	O
to	O
prepare	O
easily	O
a	O
reference	O
pattern	O
vectors	O
that	O
takes	O
the	O
noise	O
influence	O
into	O
account	O
.	O
In	O
the	O
case	O
shown	O
in	O
FIG	O
.	O
7	O
,	O
the	O
noise	O
generator	O
52	O
and	O
the	O
noise	O
adding	O
section	O
54	O
can	O
be	O
constructed	O
to	O
operate	O
in	O
either	O
digital	O
or	O
analog	O
mode	O
.	O

The	O
preparation	O
system	O
of	O
a	O
speech	O
reference	O
pattern	O
vector	O
by	O
taking	O
into	O
account	O
the	O
effect	O
of	O
the	O
noise	O
,	O
as	O
described	O
in	O
conjunction	O
with	O
FIGS	O
.	O
5	O
to	O
7	O
,	O
is	O
an	O
epoch-making	O
system	O
whose	O
standpoint	O
is	O
diametrically	O
opposite	O
to	O
that	O
of	O
the	O
prior	O
art	O
system	O
in	O
which	O
the	O
noise	O
is	O
treated	O
to	O
be	O
removed	O
.	O
It	O
is	O
a	O
powerful	O
method	O
for	O
settling	O
the	O
problem	O
concerning	O
the	O
noise	O
in	O
speech	O
recognition	O
.	O

It	O
is	O
to	O
be	O
mentioned	O
that	O
the	O
reference	O
pattern	O
vectors	O
learning	O
system	O
in	O
accordance	O
with	O
the	O
present	O
invention	O
is	O
also	O
applicable	O
to	O
the	O
recognition	O
of	O
patterns	O
in	O
the	O
acoustic	O
signals	O
and	O
the	O
vibration	O
signals	O
that	O
are	O
different	O
from	O
the	O
speech	O
.	O
Thus	O
,	O
for	O
example	O
,	O
it	O
is	O
applicable	O
to	O
the	O
case	O
of	O
diagnosing	O
the	O
anomalies	O
in	O
a	O
ball	O
bearing	O
by	O
means	O
of	O
the	O
acoustic	O
vibration	O
signals	O
,	O
and	O
detecting	O
of	O
engine	O
faults	O
,	O
or	O
further	O
,	O
to	O
the	O
diagnoisis	O
of	O
anomalies	O
in	O
the	O
vocal	O
chord	O
,	O
speaker	O
recognition	O
,	O
and	O
speaker	O
identification	O
.	O
Since	O
a	O
plurality	O
of	O
feature	O
vectors	O
for	O
learning	O
are	O
obtained	O
in	O
this	O
way	O
for	O
the	O
present	O
invention	O
,	O
by	O
modifying	O
the	O
recognition	O
feature	O
vectors	O
with	O
the	O
use	O
of	O
an	O
a	O
priori	O
knowledge	O
,	O
the	O
tendency	O
for	O
modification	O
need	O
be	O
set	O
for	O
each	O
field	O
of	O
application	O
according	O
to	O
the	O
properties	O
of	O
the	O
speech	O
or	O
acoustic	O
signals	O
that	O
are	O
chosen	O
as	O
the	O
objects	O
of	O
recognition	O
.	O
In	O
short	O
,	O
the	O
present	O
invention	O
may	O
be	O
modified	O
in	O
various	O
ways	O
within	O
a	O
scope	O
which	O
does	O
not	O
depart	O
from	O
its	O
principal	O
point	O
.	O

The	O
present	O
invention	O
is	O
adapted	O
for	O
preparing	O
a	O
reference	O
pattern	O
vectors	O
in	O
a	O
highly	O
capable	O
and	O
efficient	O
manner	O
by	O
an	O
effective	O
use	O
of	O
a	O
small	O
number	O
of	O
input	O
patterns	O
so	O
that	O
the	O
economic	O
propagation	O
effects	O
in	O
various	O
fields	O
to	O
be	O
brought	O
about	O
by	O
the	O
invention	O
will	O
be	O
considerable	O
.	O

In	O
summary	O
,	O
according	O
to	O
the	O
present	O
invention	O
,	O
a	O
plurality	O
of	O
feature	O
vectors	O
are	O
generated	O
from	O
the	O
time	O
series	O
of	O
a	O
speech	O
feature	O
parameter	O
of	O
the	O
input	O
speech	O
pattern	O
by	O
considering	O
the	O
knowledge	O
concerning	O
the	O
variation	O
tendencies	O
of	O
the	O
speech	O
patterns	O
,	O
to	O
learn	O
(	O
prepare	O
)	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
by	O
means	O
of	O
these	O
feature	O
speech	O
vectors	O
.	O
Therefore	O
,	O
it	O
becomes	O
easy	O
to	O
prepare	O
a	O
highly	O
reliable	O
reference	O
pattern	O
vectors	O
based	O
on	O
a	O
small	O
number	O
of	O
speech	O
patterns	O
,	O
making	O
it	O
possible	O
to	O
aim	O
at	O
an	O
improvement	O
in	O
the	O
speech	O
recognition	O
performance	O
accuracy	O
.	O
In	O
other	O
words	O
,	O
it	O
becomes	O
possible	O
to	O
plan	O
an	O
easy	O
enrichment	O
of	O
the	O
reference	O
pattern	O
vectors	O
through	O
an	O
effective	O
use	O
of	O
a	O
relatively	O
small	O
number	O
of	O
input	O
speech	O
patterns	O
.	O

In	O
particular	O
,	O
the	O
effectiveness	O
of	O
the	O
present	O
invention	O
will	O
be	O
enhanced	O
more	O
if	O
it	O
is	O
combined	O
with	O
a	O
statistical	O
pattern	O
recognition	O
method	O
which	O
is	O
capable	O
of	O
collecting	O
wider	O
variations	O
in	O
the	O
speech	O
patterns	O
.	O
For	O
instance	O
,	O
when	O
the	O
method	O
of	O
compound	O
degree	O
of	O
similarity	O
(	O
multiple	O
similarity	O
)	O
,	O
which	O
was	O
proposed	O
in	O
the	O
field	O
of	O
character	O
recognition	O
and	O
has	O
been	O
confirmed	O
as	O
to	O
its	O
effectiveness	O
in	O
the	O
field	O
of	O
printed	O
word	O
recognition	O
,	O
is	O
applied	O
as	O
it	O
is	O
to	O
the	O
word	O
and	O
speech	O
recognition	O
,	O
a	O
large	O
number	O
of	O
speech	O
patterns	O
will	O
be	O
required	O
in	O
designing	O
a	O
reference	O
pattern	O
vectors	O
for	O
word	O
recognition	O
.	O
However	O
,	O
an	O
application	O
of	O
the	O
present	O
invention	O
to	O
this	O
problem	O
will	O
bring	O
a	O
substantial	O
effect	O
to	O
make	O
it	O
possible	O
to	O
obtain	O
a	O
satisfactory	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
in	O
a	O
very	O
efficient	O
manner	O
.	O

Various	O
modifications	O
will	O
become	O
possible	O
for	O
those	O
skilled	O
in	O
the	O
art	O
after	O
receiving	O
the	O
teachings	O
of	O
the	O
present	O
disclosure	O
without	O
departing	O
from	O
the	O
scope	O
thereof	O
.	O

1	O
.	O
In	O
a	O
speech	O
recognition	O
and	O
speech	O
reference	O
pattern	O
vector	O
learning	O
device	O
which	O
obtains	O
a	O
first	O
time	O
series	O
of	O
a	O
feature	O
parameter	O
of	O
an	O
input	O
speech	O
by	O
analyzing	O
the	O
input	O
speech	O
,	O
and	O
extracts	O
speech	O
recognition	O
feature	O
vectors	O
from	O
a	O
portion	O
of	O
the	O
first	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
to	O
recognize	O
said	O
input	O
speech	O
by	O
collating	O
patterns	O
between	O
a	O
speech	O
reference	O
pattern	O
vector	O
registered	O
in	O
advance	O
and	O
the	O
feature	O
vectors	O
for	O
the	O
input	O
speech	O
obtained	O
in	O
the	O
above	O
,	O
a	O
learning	O
device	O
of	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
comprising	O
:	O
a	O
learning	O
feature	O
vector	O
extracting	O
section	O
for	O
extracting	O
recognition	O
feature	O
vectors	O
for	O
the	O
purpose	O
of	O
learning	O
processing	O
from	O
the	O
first	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
of	O
the	O
input	O
speech	O
,	O
as	O
well	O
as	O
for	O
extracting	O
also	O
other	O
feature	O
vectors	O
from	O
a	O
plurality	O
of	O
kinds	O
of	O
time	O
series	O
of	O
feature	O
parameters	O
obtained	O
by	O
varying	O
the	O
first	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
;	O
and	O
a	O
learning	O
section	O
for	O
causing	O
the	O
learning	O
of	O
a	O
speech	O
reference	O
pattern	O
vector	O
which	O
is	O
stored	O
in	O
a	O
reference	O
pattern	O
vectors	O
memory	O
for	O
recognition	O
of	O
said	O
speech	O
by	O
means	O
of	O
the	O
learning	O
feature	O
vectors	O
supplied	O
by	O
said	O
learning	O
feature	O
vector	O
extracting	O
section	O
,	O
said	O
learning	O
feature	O
vector	O
extracting	O
section	O
comprises	O
a	O
knowledge	O
source	O
concerning	O
the	O
variations	O
in	O
the	O
input	O
speech	O
patterns	O
,	O
a	O
conversion	O
section	O
for	O
obtaining	O
a	O
plurality	O
of	O
kinds	O
of	O
time	O
series	O
of	O
feature	O
parameters	O
by	O
varying	O
the	O
first	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
of	O
the	O
input	O
speech	O
in	O
accordance	O
with	O
the	O
knowledge	O
concerning	O
the	O
variations	O
from	O
the	O
knowledge	O
source	O
,	O
and	O
a	O
feature	O
vector	O
extracting	O
section	O
for	O
extracting	O
respective	O
feature	O
vectors	O
from	O
the	O
plurality	O
of	O
kinds	O
of	O
time	O
series	O
and	O
the	O
first	O
time	O
series	O
.	O

2	O
.	O
A	O
speech	O
recognition	O
and	O
speech	O
reference	O
pattern	O
vector	O
learning	O
device	O
as	O
claimed	O
in	O
claim	O
1	O
,	O
in	O
which	O
said	O
learning	O
feature	O
vector	O
extracting	O
section	O
further	O
comprises	O
a	O
speech	O
segment	O
detecting	O
section	O
for	O
detecting	O
each	O
of	O
the	O
starting	O
point	O
and	O
the	O
ending	O
point	O
concerning	O
the	O
time	O
series	O
of	O
the	O
plurality	O
of	O
kinds	O
of	O
feature	O
parameters	O
.	O

3	O
.	O
A	O
speech	O
reference	O
pattern	O
vector	O
learning	O
device	O
as	O
claimed	O
in	O
claim	O
1	O
,	O
in	O
which	O
the	O
input	O
speech	O
is	O
a	O
word	O
speech	O
,	O
and	O
said	O
learning	O
feature	O
vector	O
extracting	O
section	O
obtains	O
the	O
other	O
speech	O
feature	O
vectors	O
by	O
increasing	O
and	O
decreasing	O
the	O
power	O
level	O
of	O
the	O
feature	O
parameter	O
of	O
the	O
analyzed	O
input	O
speech	O
,	O
by	O
considering	O
the	O
power	O
level	O
variations	O
in	O
the	O
input	O
speech	O
.	O

4	O
.	O
A	O
speech	O
recognition	O
and	O
speech	O
reference	O
pattern	O
vector	O
learning	O
device	O
as	O
claimed	O
in	O
claim	O
1	O
,	O
in	O
which	O
the	O
input	O
speech	O
is	O
a	O
word	O
speech	O
,	O
the	O
knowledge	O
source	O
is	O
constructed	O
for	O
supplying	O
knowledge	O
concerning	O
the	O
level	O
variations	O
in	O
the	O
input	O
speech	O
,	O
and	O
the	O
conversion	O
section	O
comprises	O
a	O
speech	O
level	O
conversion	O
section	O
for	O
increasing	O
and	O
decreasing	O
the	O
level	O
of	O
the	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
for	O
the	O
input	O
speech	O
,	O
in	O
accordance	O
with	O
the	O
knowlege	O
concerning	O
the	O
level	O
variations	O
.	O

5	O
.	O
A	O
speech	O
recognition	O
and	O
speech	O
reference	O
pattern	O
vector	O
learning	O
device	O
as	O
claimed	O
in	O
claim	O
1	O
,	O
in	O
which	O
the	O
knowledge	O
source	O
comprises	O
a	O
noise	O
generator	O
for	O
supplying	O
knowledge	O
concerning	O
the	O
noise	O
,	O
and	O
the	O
conversion	O
section	O
comprises	O
a	O
noise	O
adding	O
section	O
for	O
adding	O
a	O
noise	O
from	O
the	O
noise	O
generator	O
to	O
the	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
for	O
the	O
input	O
speech	O
.	O

6	O
.	O
In	O
a	O
speech	O
recognition	O
and	O
speech	O
reference	O
pattern	O
vector	O
learning	O
device	O
which	O
obtains	O
a	O
first	O
time	O
series	O
of	O
a	O
feature	O
parameter	O
of	O
an	O
input	O
speech	O
by	O
analyzing	O
the	O
input	O
speech	O
,	O
and	O
extracts	O
speech	O
recognition	O
feature	O
vectors	O
from	O
a	O
portion	O
of	O
the	O
first	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
to	O
recognize	O
said	O
input	O
speech	O
by	O
collating	O
patterns	O
between	O
a	O
speech	O
reference	O
pattern	O
vector	O
registered	O
in	O
advance	O
and	O
the	O
feature	O
vectors	O
for	O
the	O
input	O
specch	O
obtained	O
in	O
the	O
above	O
,	O
a	O
learning	O
device	O
of	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
comprising	O
:	O
a	O
learning	O
feature	O
vector	O
extracting	O
section	O
for	O
extracting	O
recognition	O
feature	O
vectors	O
for	O
the	O
purpose	O
of	O
learning	O
processing	O
from	O
the	O
first	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
of	O
the	O
input	O
speech	O
,	O
as	O
well	O
as	O
for	O
extracting	O
also	O
other	O
feature	O
vectors	O
from	O
a	O
plurality	O
of	O
kinds	O
of	O
time	O
series	O
of	O
features	O
parameters	O
obtained	O
by	O
varying	O
the	O
first	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
;	O
and	O
a	O
learning	O
section	O
for	O
causing	O
the	O
learning	O
of	O
a	O
speech	O
reference	O
pattern	O
vector	O
which	O
is	O
stored	O
in	O
a	O
reference	O
pattern	O
vectors	O
memory	O
for	O
recognition	O
of	O
said	O
speech	O
by	O
means	O
of	O
the	O
learning	O
feature	O
vectors	O
supplied	O
by	O
said	O
learning	O
feature	O
vector	O
extracting	O
section	O
,	O
said	O
extracting	O
section	O
of	O
feature	O
vector	O
for	O
learning	O
comprises	O
a	O
feature	O
vector	O
extracting	O
section	O
for	O
extracting	O
feature	O
vectors	O
from	O
the	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
for	O
the	O
input	O
speech	O
,	O
a	O
knowledge	O
source	O
concerning	O
the	O
pattern	O
variations	O
in	O
the	O
input	O
speech	O
,	O
and	O
a	O
conversion	O
section	O
for	O
varying	O
the	O
feature	O
vectors	O
from	O
the	O
feature	O
vector	O
extracting	O
section	O
in	O
accordance	O
with	O
the	O
knowledge	O
concerning	O
the	O
variations	O
coming	O
from	O
the	O
knowledge	O
source	O
.	O

7	O
.	O
A	O
speech	O
recognition	O
and	O
speech	O
reference	O
pattern	O
vector	O
learning	O
device	O
as	O
claimed	O
in	O
claim	O
6	O
in	O
which	O
the	O
knowledge	O
source	O
comprises	O
a	O
noise	O
generator	O
for	O
supplying	O
knowledge	O
concerning	O
the	O
noise	O
,	O
and	O
the	O
conversion	O
section	O
comprises	O
a	O
noise	O
adding	O
section	O
for	O
adding	O
a	O
noise	O
from	O
the	O
noise	O
generator	O
to	O
the	O
feature	O
vector	O
.	O

8	O
.	O
A	O
speech	O
recognition	O
and	O
speech	O
reference	O
pattern	O
vector	O
learning	O
device	O
as	O
claimed	O
in	O
claim	O
1	O
,	O
in	O
which	O
the	O
extracting	O
section	O
of	O
feature	O
vectors	O
for	O
learning	O
comprises	O
a	O
noise	O
generator	O
for	O
generating	O
noises	O
,	O
a	O
noise	O
adding	O
section	O
for	O
adding	O
a	O
noise	O
to	O
the	O
input	O
speech	O
,	O
a	O
speech	O
segment	O
detecting	O
section	O
for	O
detecting	O
the	O
starting	O
and	O
ending	O
points	O
of	O
the	O
speech	O
concerning	O
the	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
obtained	O
from	O
the	O
input	O
speech	O
to	O
which	O
the	O
noise	O
is	O
added	O
,	O
and	O
a	O
feature	O
vector	O
extracting	O
section	O
for	O
extracting	O
feature	O
vectors	O
from	O
the	O
time	O
series	O
.	O

9	O
.	O
A	O
speech	O
recognition	O
and	O
speech	O
reference	O
pattern	O
vector	O
learning	O
device	O
as	O
claimed	O
in	O
claim	O
1	O
,	O
in	O
which	O
the	O
pattern	O
collection	O
between	O
the	O
speech	O
feature	O
vector	O
and	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
is	O
carried	O
out	O
by	O
a	O
statistical	O
method	O
.	O

10	O
.	O
A	O
speech	O
recognition	O
and	O
speech	O
reference	O
pattern	O
vector	O
learning	O
device	O
as	O
claimed	O
in	O
claim	O
9	O
in	O
which	O
the	O
statistical	O
method	O
is	O
the	O
method	O
of	O
compound	O
degree	O
of	O
similarity	O
.	O

11	O
.	O
A	O
speech	O
recognition	O
and	O
speech	O
reference	O
pattern	O
vector	O
learning	O
device	O
as	O
claimed	O
in	O
claim	O
1	O
,	O
in	O
which	O
the	O
learning	O
of	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
is	O
carried	O
out	O
by	O
determining	O
covariance	O
matrix	O
for	O
the	O
learning	O
feature	O
speech	O
vectors	O
extracted	O
from	O
the	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
that	O
is	O
obtained	O
by	O
analyzing	O
the	O
input	O
speech	O
,	O
and	O
by	O
supplementing	O
or	O
updating	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
with	O
the	O
reference	O
pattern	O
vectors	O
computed	O
by	O
expanding	O
the	O
covariance	O
matrix	O
.	O

12	O
.	O
A	O
speech	O
recognition	O
and	O
speech	O
reference	O
pattern	O
vector	O
learning	O
device	O
as	O
claimed	O
in	O
claim	O
4	O
in	O
which	O
the	O
conversion	O
section	O
modifies	O
the	O
power	O
level	O
of	O
the	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
for	O
the	O
analyzed	O
input	O
speech	O
into	O
three	O
steps	O
within	O
the	O
range	O
of	O
+	O
10	O
dB	O
.	O

13	O
.	O
A	O
speech	O
recognition	O
and	O
speech	O
reference	O
pattern	O
vector	O
learning	O
device	O
as	O
claimed	O
in	O
claim	O
1	O
,	O
in	O
which	O
the	O
input	O
speech	O
is	O
a	O
syllable	O
speech	O
.	O

14	O
.	O
A	O
speech	O
recognition	O
and	O
speech	O
reference	O
pattern	O
vector	O
learning	O
device	O
as	O
claimed	O
in	O
claim	O
1	O
,	O
in	O
which	O
the	O
input	O
speech	O
is	O
a	O
phoneme	O
speech	O
.	O

15	O
.	O
A	O
speech	O
recognition	O
device	O
which	O
recognizes	O
the	O
input	O
speech	O
by	O
the	O
pattern	O
matching	O
,	O
comprising	O
:	O
an	O
acoustic	O
analysis	O
section	O
for	O
analyzing	O
the	O
input	O
speech	O
to	O
output	O
it	O
as	O
a	O
time	O
series	O
of	O
a	O
speech	O
feature	O
parameter	O
;	O
a	O
recognition	O
feature	O
vector	O
extracting	O
section	O
for	O
extracting	O
an	O
input	O
speech	O
feature	O
vector	O
for	O
recognition	O
out	O
of	O
a	O
portion	O
of	O
the	O
time	O
series	O
of	O
the	O
speech	O
feature	O
parameter	O
that	O
is	O
sent	O
from	O
said	O
acoustic	O
analysis	O
section	O
;	O
a	O
speech	O
reference	O
pattern	O
vector	O
memory	O
which	O
is	O
memorizing	O
in	O
advance	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
;	O
a	O
pattern	O
matching	O
section	O
for	O
carrying	O
out	O
pattern	O
matching	O
between	O
the	O
input	O
speech	O
feature	O
vector	O
from	O
said	O
recognition	O
feature	O
vector	O
extracting	O
section	O
and	O
the	O
reference	O
pattern	O
vectors	O
in	O
said	O
speech	O
reference	O
pattern	O
vector	O
memory	O
;	O
a	O
learning	O
feature	O
vector	O
extracting	O
section	O
for	O
extracting	O
feature	O
vectors	O
for	O
recognition	O
,	O
for	O
the	O
purpose	O
of	O
learning	O
processing	O
,	O
out	O
of	O
the	O
time	O
series	O
of	O
the	O
speech	O
feature	O
parameter	O
from	O
said	O
acoustic	O
analysis	O
section	O
,	O
as	O
well	O
as	O
for	O
extracting	O
also	O
other	O
feature	O
vectors	O
by	O
talking	O
into	O
account	O
the	O
variation	O
tendencies	O
of	O
the	O
learning	O
feature	O
vectors	O
;	O
and	O
a	O
learning	O
section	O
for	O
learning	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
in	O
said	O
speech	O
reference	O
pattern	O
vector	O
memory	O
by	O
the	O
use	O
of	O
the	O
feature	O
vectors	O
for	O
learning	O
from	O
said	O
learning	O
feature	O
vector	O
extracting	O
section	O
,	O
said	O
learning	O
feature	O
vector	O
extracting	O
section	O
comprises	O
a	O
knowledge	O
source	O
concerning	O
the	O
pattern	O
variations	O
in	O
the	O
input	O
speech	O
,	O
a	O
conversion	O
section	O
for	O
obtaining	O
time	O
series	O
of	O
a	O
plurality	O
of	O
kinds	O
of	O
feature	O
parameters	O
by	O
modifying	O
the	O
time	O
series	O
of	O
a	O
feature	O
for	O
the	O
input	O
speech	O
in	O
accordance	O
with	O
the	O
knowlege	O
from	O
the	O
knowledge	O
source	O
concerning	O
the	O
variations	O
,	O
and	O
a	O
feature	O
vector	O
extracting	O
section	O
for	O
extracting	O
respective	O
feature	O
vectors	O
from	O
the	O
plurality	O
of	O
kinds	O
of	O
time	O
series	O
.	O

16	O
.	O
A	O
speech	O
recognition	O
device	O
as	O
claimed	O
in	O
claim	O
15	O
,	O
in	O
which	O
said	O
learning	O
feature	O
vector	O
extracting	O
section	O
further	O
comprises	O
a	O
speech	O
segment	O
detecting	O
section	O
for	O
detecting	O
each	O
of	O
the	O
starting	O
and	O
ending	O
points	O
of	O
the	O
speech	O
concerning	O
the	O
time	O
series	O
of	O
plurality	O
of	O
kinds	O
of	O
the	O
feature	O
parameters	O
.	O

17	O
.	O
A	O
speech	O
recognition	O
device	O
as	O
claimed	O
in	O
claim	O
15	O
,	O
in	O
which	O
the	O
input	O
speech	O
is	O
a	O
word	O
speech	O
,	O
the	O
knowledge	O
source	O
is	O
constructed	O
for	O
supplying	O
knowledge	O
concerning	O
the	O
variations	O
in	O
the	O
level	O
of	O
the	O
input	O
speech	O
,	O
and	O
the	O
conversion	O
section	O
comprises	O
a	O
speech	O
level	O
conversion	O
section	O
for	O
increasing	O
and	O
decreasing	O
the	O
level	O
of	O
the	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
for	O
the	O
input	O
speech	O
,	O
in	O
accordance	O
with	O
the	O
knowledge	O
concerning	O
the	O
level	O
variations	O
.	O

18	O
.	O
A	O
speech	O
recognition	O
device	O
as	O
claimed	O
in	O
claim	O
15	O
,	O
in	O
which	O
the	O
knowledge	O
source	O
comprises	O
a	O
noise	O
generator	O
for	O
supplying	O
knowledge	O
concerning	O
the	O
noise	O
,	O
and	O
the	O
conversion	O
section	O
comprises	O
a	O
noise	O
adding	O
section	O
for	O
adding	O
a	O
noise	O
from	O
the	O
noise	O
generator	O
to	O
the	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
for	O
the	O
input	O
speech	O
.	O

19	O
.	O
A	O
speech	O
recognition	O
device	O
which	O
recognize	O
the	O
input	O
speech	O
by	O
the	O
pattern	O
matching	O
,	O
comprising	O
:	O
an	O
acoustic	O
analysis	O
section	O
for	O
analyzing	O
the	O
input	O
speech	O
to	O
output	O
it	O
as	O
a	O
time	O
series	O
of	O
a	O
speech	O
feature	O
parameter	O
;	O
a	O
recognition	O
feature	O
vector	O
extracting	O
section	O
for	O
extracting	O
an	O
input	O
speech	O
feature	O
vector	O
for	O
recognition	O
out	O
of	O
a	O
portion	O
of	O
the	O
time	O
series	O
of	O
the	O
speech	O
feature	O
parameter	O
that	O
is	O
sent	O
from	O
said	O
acoustic	O
analysis	O
section	O
;	O
a	O
speech	O
reference	O
pattern	O
vector	O
memory	O
which	O
is	O
memorizing	O
in	O
advance	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
;	O
a	O
pattern	O
matching	O
section	O
for	O
carrying	O
out	O
pattern	O
matching	O
between	O
the	O
input	O
speech	O
feature	O
vector	O
from	O
said	O
recognition	O
feature	O
vector	O
extracting	O
section	O
and	O
the	O
reference	O
pattern	O
vectors	O
in	O
said	O
speech	O
reference	O
pattern	O
vector	O
memory	O
;	O
a	O
learning	O
feature	O
vector	O
extracting	O
section	O
for	O
extracting	O
feature	O
vectors	O
for	O
recognition	O
,	O
for	O
the	O
purpose	O
of	O
learning	O
processing	O
,	O
out	O
of	O
the	O
time	O
series	O
of	O
the	O
speech	O
feature	O
parameter	O
from	O
said	O
acoustic	O
analysis	O
section	O
,	O
as	O
well	O
as	O
for	O
extracting	O
also	O
other	O
feature	O
vectors	O
by	O
taking	O
into	O
account	O
the	O
variation	O
tendencies	O
of	O
the	O
learning	O
feature	O
vectors	O
;	O
and	O
a	O
learning	O
section	O
for	O
learning	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
in	O
said	O
speech	O
reference	O
pattern	O
vector	O
memory	O
by	O
the	O
use	O
of	O
the	O
feature	O
vectors	O
for	O
learning	O
from	O
said	O
learning	O
feature	O
vector	O
extracting	O
section	O
,	O
said	O
extracting	O
section	O
of	O
feature	O
vector	O
for	O
learning	O
comprises	O
a	O
feature	O
vector	O
extracting	O
section	O
for	O
extracting	O
feature	O
vectors	O
from	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
for	O
the	O
input	O
speech	O
,	O
a	O
knowledge	O
source	O
concerning	O
the	O
pattern	O
variations	O
in	O
the	O
input	O
speech	O
,	O
and	O
a	O
conversion	O
section	O
for	O
modifying	O
the	O
learning	O
feature	O
vector	O
from	O
the	O
learning	O
feature	O
vector	O
extracting	O
section	O
in	O
accordance	O
with	O
the	O
knowledge	O
concerning	O
the	O
variations	O
coming	O
from	O
the	O
knowledge	O
source	O
.	O

20	O
.	O
A	O
speech	O
recognition	O
device	O
as	O
claimed	O
in	O
claim	O
15	O
,	O
in	O
which	O
the	O
pattern	O
matching	O
between	O
the	O
speech	O
feature	O
vectors	O
and	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
is	O
carried	O
out	O
by	O
a	O
statiscal	O
method	O
.	O

21	O
.	O
A	O
speech	O
recognition	O
device	O
as	O
claimed	O
in	O
claim	O
20	O
,	O
in	O
which	O
the	O
statistical	O
method	O
is	O
the	O
method	O
of	O
multiple	O
similarity	O

22	O
.	O
A	O
speech	O
recognition	O
device	O
as	O
claimed	O
in	O
claim	O
15	O
,	O
in	O
which	O
the	O
learning	O
of	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
is	O
carried	O
out	O
by	O
determining	O
covariance	O
matrix	O
for	O
the	O
learning	O
feature	O
speech	O
vectors	O
extracted	O
from	O
the	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
that	O
is	O
obtained	O
by	O
analyzing	O
the	O
input	O
speech	O
,	O
and	O
by	O
supplementing	O
or	O
updating	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
with	O
the	O
reference	O
pattern	O
vectors	O
computed	O
by	O
expanding	O
the	O
covariance	O
matrix	O
.	O

23	O
.	O
A	O
speech	O
recognition	O
device	O
as	O
claimed	O
in	O
claim	O
17	O
,	O
in	O
which	O
the	O
conversion	O
section	O
modifies	O
the	O
power	O
level	O
of	O
the	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
for	O
the	O
analyzed	O
input	O
speech	O
into	O
three	O
steps	O
within	O
the	O
range	O
of	O
.	O
+	O
-	O
.	O
10	O
dB	O
.	O

24	O
.	O
A	O
speech	O
recognition	O
device	O
as	O
claimed	O
in	O
claim	O
15	O
,	O
further	O
comprising	O
:	O
a	O
noise	O
generator	O
for	O
generating	O
noise	O
;	O
and	O
a	O
noise	O
adding	O
section	O
for	O
adding	O
the	O
noise	O
to	O
the	O
input	O
speech	O
.	O

25	O
.	O
In	O
a	O
speech	O
recognition	O
device	O
having	O
a	O
learning	O
method	O
of	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
which	O
obtains	O
the	O
time	O
series	O
of	O
a	O
feature	O
parameter	O
of	O
an	O
input	O
speech	O
by	O
analyzing	O
the	O
input	O
speech	O
,	O
extracts	O
speech	O
recognition	O
feature	O
vectors	O
from	O
a	O
portion	O
of	O
the	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
,	O
and	O
recognizes	O
said	O
input	O
speech	O
by	O
collating	O
the	O
patterns	O
between	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
that	O
has	O
been	O
registered	O
in	O
advance	O
and	O
the	O
feature	O
vectors	O
for	O
the	O
input	O
speech	O
obtained	O
in	O
the	O
above	O
,	O
said	O
learning	O
method	O
of	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
comprising	O
the	O
steps	O
of	O
:	O
(	O
a	O
)	O
extracting	O
out	O
from	O
the	O
time	O
series	O
of	O
the	O
features	O
parameter	O
obtained	O
by	O
analyzing	O
the	O
input	O
,	O
speech	O
,	O
a	O
speech	O
feature	O
vector	O
for	O
the	O
pattern	O
matching	O
and	O
speech	O
feature	O
vectors	O
for	O
learning	O
consisting	O
of	O
other	O
speech	O
feature	O
vectors	O
that	O
correspond	O
to	O
the	O
variation	O
tendencies	O
of	O
the	O
speech	O
feature	O
vector	O
mentioned	O
;	O
and	O
(	O
b	O
)	O
learning	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
by	O
the	O
use	O
of	O
the	O
speech	O
feature	O
vector	O
and	O
the	O
other	O
feature	O
vectors	O
that	O
correspond	O
to	O
the	O
variation	O
tendencies	O
of	O
the	O
feature	O
vector	O
.	O

26	O
.	O
A	O
learning	O
method	O
of	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
as	O
claimed	O
in	O
claim	O
25	O
,	O
including	O
the	O
step	O
of	O
:	O
obtaining	O
the	O
other	O
speech	O
feature	O
vectors	O
by	O
increasing	O
and	O
decreasing	O
the	O
power	O
level	O
of	O
the	O
feature	O
parameter	O
for	O
the	O
analyzed	O
input	O
speech	O
by	O
taking	O
into	O
account	O
the	O
variations	O
in	O
the	O
power	O
level	O
of	O
the	O
input	O
speech	O
.	O

27	O
.	O
A	O
learning	O
method	O
of	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
as	O
claimed	O
in	O
claim	O
25	O
,	O
including	O
the	O
step	O
of	O
:	O
matching	O
the	O
patterns	O
between	O
the	O
speech	O
feature	O
vectors	O
and	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
by	O
a	O
statistical	O
method	O
.	O

28	O
.	O
A	O
learning	O
method	O
of	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
as	O
claimed	O
in	O
claim	O
27	O
,	O
in	O
which	O
the	O
statistical	O
method	O
is	O
the	O
method	O
of	O
compound	O
degree	O
of	O
similarity	O
.	O

29	O
.	O
A	O
learning	O
method	O
of	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
as	O
claimed	O
in	O
claim	O
25	O
,	O
wherein	O
the	O
step	O
of	O
learning	O
of	O
the	O
reference	O
pattern	O
vectors	O
further	O
includes	O
the	O
steps	O
of	O
:	O
determining	O
a	O
covariance	O
matrix	O
for	O
the	O
speech	O
feature	O
vectors	O
for	O
learning	O
that	O
are	O
extracted	O
from	O
the	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
obtained	O
by	O
analyzing	O
the	O
input	O
speech	O
,	O
and	O
supplementing	O
or	O
updating	O
the	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
with	O
a	O
reference	O
pattern	O
vectors	O
computed	O
by	O
expanding	O
the	O
covariance	O
matrix	O
.	O

30	O
.	O
A	O
learning	O
method	O
of	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
as	O
claimed	O
in	O
claim	O
26	O
,	O
including	O
the	O
step	O
of	O
:	O
obtaining	O
the	O
other	O
speech	O
feature	O
vectors	O
from	O
the	O
power	O
level	O
of	O
the	O
feature	O
parameter	O
for	O
the	O
analyzed	O
input	O
speech	O
by	O
modifying	O
the	O
power	O
level	O
into	O
three	O
steps	O
within	O
the	O
range	O
of	O
.	O
+	O
-	O
.	O
10	O
dB	O
.	O

31	O
.	O
A	O
learning	O
method	O
of	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
as	O
claimed	O
in	O
claim	O
25	O
,	O
in	O
which	O
the	O
input	O
speech	O
is	O
a	O
syllable	O
speech	O
.	O

32	O
.	O
A	O
learning	O
method	O
of	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
as	O
claimed	O
in	O
claim	O
25	O
,	O
in	O
which	O
the	O
input	O
speech	O
is	O
a	O
phoneme	O
speech	O
.	O

33	O
.	O
A	O
learning	O
method	O
of	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
as	O
claimed	O
in	O
claim	O
25	O
,	O
in	O
which	O
the	O
speech	O
feature	O
vector	O
for	O
learning	O
further	O
comprise	O
speech	O
feature	O
vectors	O
to	O
be	O
used	O
for	O
pattern	O
matching	O
and	O
speech	O
feature	O
vectors	O
obtained	O
by	O
adding	O
the	O
influence	O
of	O
noises	O
to	O
other	O
speech	O
feature	O
vectors	O
.	O

34	O
.	O
A	O
learning	O
method	O
of	O
reference	O
pattern	O
vectors	O
for	O
speech	O
recognition	O
as	O
claimed	O
in	O
claim	O
33	O
,	O
including	O
the	O
step	O
of	O
:	O
adding	O
the	O
noises	O
with	O
predetermined	O
levels	O
to	O
the	O
input	O
speech	O
,	O
the	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
obtained	O
by	O
analyzing	O
the	O
input	O
speech	O
,	O
and	O
the	O
speech	O
feature	O
vectors	O
that	O
are	O
extracted	O
from	O
the	O
time	O
series	O
of	O
the	O
feature	O
parameter	O
for	O
the	O
addition	O
of	O
the	O
influence	O
due	O
to	O
noises	O
.	O

