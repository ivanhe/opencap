US	O
5956683	O
A	O
19990921	O

US	O
08627333	O
19960404	O

eng	O
eng	O

US	O
08534080	O
19950921	O

US	O
08173247	O
19931222	O

19990921	O

19990921	O

6G	O
10L	O
3/00	O
A	O
6	O
G	O
10	O
L	O
3	O
00	O
A	O

6G	O
06F	O
15/16	O
B	O
6	O
G	O
06	O
F	O
15	O
16	O
B	O

6G	O
10L	O
5/00	O
B	O
6	O
G	O
10	O
L	O
5	O
00	O
B	O

6H	O
04M	O
1/27	O
B	O
6	O
H	O
04	O
M	O
1	O
27	O
B	O

G10L	O
15/00	O
20060101C	O
I20051008RMEP	O

20060101	O

C	O
G	O
10	O
L	O
15	O
00	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/02	O
20060101A	O
I20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
02	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/28	O
20060101A	O
I20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
28	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
19/00	O
20060101C	O
I20051008RMEP	O

20060101	O

C	O
G	O
10	O
L	O
19	O
00	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
19/00	O
20060101A	O
I20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
19	O
00	O
I	O

20051008	O

EP	O

R	O
M	O

US	O

704/270	O
.	O
1	O
704	O
270	O
.	O
1	O

704/201	O
704	O
201	O

704/251	O
704	O
251	O

704/270	O
704	O
270	O

704/E15.004	O
704	O
E15	O
.	O
004	O

704/E15.047	O
704	O
E15	O
.	O
047	O

704/E19.008	O
704	O
E19	O
.	O
008	O

G10L	O
15/02	O
G	O
10	O
L	O
15	O
02	O

G10L	O
15/28D	O
G	O
10	O
L	O
15	O
28	O

D	O

G10L	O
19/00U	O
G	O
10	O
L	O
19	O
00	O

U	O

30	O
Distributed	O
voice	O
recognition	O
system	O

US	O
4567606	O
A	O
Vensko	O
et_al.	O

19860128	O

19821103	O

US	O

381/43	O
381	O
43	O

US	O
4961229	O
A	O
Takahashi	O
19901002	O

19890721	O

US	O

381/42	O
381	O
42	O

US	O
4991217	O
A	O
Garrett	O
et_al.	O

19910205	O

19841130	O

US	O

381/43	O
381	O
43	O

US	O
5012518	O
A	O
Liu	O
et_al.	O

19910430	O

19900816	O

US	O

381/42	O
381	O
42	O

US	O
5040212	O
A	O
Bethards	O
19910813	O

19900319	O

US	O

395/2	O
.	O
1	O
395	O
2	O
.	O
1	O

US	O
5045082	O
A	O
Smith	O
et_al.	O

19910903	O

19900110	O

US	O

395/2	O
.	O
84	O
395	O
2	O
.	O
84	O

US	O
5109509	O
A	O
Katayama	O
et_al.	O

19920428	O

19881128	O

US	O

395/600	O
395	O
600	O

US	O
5146538	O
A	O
Sobti	O
et_al.	O

19920908	O

19890831	O

US	O

395/2	O
.	O
09	O
395	O
2	O
.	O
09	O

US	O
5231670	O
A	O
Goldhor	O
et_al.	O

19930727	O

19920319	O

US	O

381/43	O
381	O
43	O

US	O
5280585	O
A	O
Kochis	O
et_al.	O

19940118	O

19900928	O

US	O

395/275	O
395	O
275	O

US	O
5321840	O
A	O
Ahlin	O
et_al.	O

19940614	O

19930812	O

US	O

395/700	O
395	O
700	O

US	O
5325524	O
A	O
Black	O
et_al.	O

19940628	O

19920406	O

US	O

395/600	O
395	O
600	O

US	O
5371901	O
A	O
Reed	O
et_al.	O

19941206	O

19940304	O

US	O

395/2	O
.	O
84	O
395	O
2	O
.	O
84	O

EP	O
108354	O
A2	O
19840516	O

19831028	O

EP	O
177405	O
A1	O
19860409	O

19850924	O

EP	O
534410	O
A2	O
19930331	O

19920923	O

International	O
Search	O
Report	O
dated	O
Apr.	O
7	O
,	O
1995	O
.	O

Farvardin	B-Citation
,	I-Citation
et_al.	I-Citation
Efficient	I-Citation
Encoding	I-Citation
of	I-Citation
Speech	I-Citation
LSP	I-Citation
Parameters	I-Citation
Using	I-Citation
The	I-Citation
Discrete	I-Citation
Cosine	I-Citation
Transformation	I-Citation
,	I-Citation
ICASSP	I-Citation
89	I-Citation
,	I-Citation
pp.	I-Citation
168	I-Citation
171	I-Citation
.	O

WO	O
0074034	O
A1	O
20001207	O

20000316	O

WO	O
02059874	O
A3	O
20020801	O

20020102	O

WO	O
02061727	O
A3	O
20020808	O

20020129	O

WO	O
02080500	O
A1	O
20021010	O

20020327	O

WO	O
03046885	O
A3	O
20030605	O

20021120	O

US	O
7197331	O
B2	O
20070327	O

20021230	O

US	O
7203643	O
B2	O
20070410	O

20020528	O

US	O
7203646	O
B2	O
20070410	O

20060522	O

EP	O
1362343	O
B1	O
20070829	O

20020212	O

EP	O
1290678	O
B1	O
20070328	O

20010502	O

EP	O
1435086	O
B1	O
20090128	O

20021120	O

WO	O
2008001991	O
A1	O
20080103	O

20061228	O

EP	O
1215659	O
A1	O
20020619	O

20001214	O

FR	O
2820872	O
A1	O
20020816	O

20010213	O

WO	O
0156020	O
A1	O
20010802	O

20010109	O

WO	O
0195312	O
A1	O
20011213	O

20010502	O

WO	O
02065454	O
A1	O
20020822	O

20020212	O

WO	O
02103679	O
A1	O
20021227	O

20020613	O

WO	O
02059874	O
A2	O
20020801	O

20020102	O

WO	O
02061727	O
A2	O
20020808	O

20020129	O

WO	O
03046885	O
A2	O
20030605	O

20021120	O

WO	O
2008064137	O
A2	O
20080529	O

20071116	O

US	O
7555431	O
B2	O
20090630	O

20040302	O

US	O
7599861	O
B2	O
20091006	O

20060302	O

US	O
7624007	O
B2	O
20091124	O

20041203	O

RU	O
2291499	O
C2	O
20070110	O

20020517	O

US	O
7634064	O
B2	O
20091215	O

20041222	O

US	O
7647225	O
B2	O
20100112	O

20061120	O

US	O
7657424	O
B2	O
20100202	O

20041203	O

US	O
7672841	O
B2	O
20100302	O

20080519	O

US	O
7519536	O
B2	O
20090414	O

20051216	O

US	O
7505921	O
B1	O
20090317	O

20000303	O

US	O
7437287	O
B2	O
20081014	O

20041101	O

US	O
7689416	O
B1	O
20100330	O

20040123	O

US	O
7392185	O
B2	O
20080624	O

20030625	O

US	O
7406421	O
B2	O
20080729	O

20020214	O

US	O
7376556	O
B2	O
20080520	O

20040302	O

US	O
7330786	O
B2	O
20080212	O

20060623	O

US	O
7302391	O
B2	O
20071127	O

20050525	O

US	O
7277854	O
B2	O
20071002	O

20050107	O

US	O
7286993	O
B2	O
20071023	O

20040723	O

US	O
7225125	O
B2	O
20070529	O

20050107	O

US	O
7089178	O
B2	O
20060808	O

20020430	O

US	O
7092816	O
B2	O
20060815	O

20031021	O

US	O
7099825	O
B1	O
20060829	O

20020315	O

US	O
7139704	O
B2	O
20061121	O

20011130	O

US	O
7139714	O
B2	O
20061121	O

20050107	O

US	O
7024359	O
B2	O
20060404	O

20010131	O

US	O
7050977	O
B1	O
20060523	O

19991112	O

US	O
7050974	O
B1	O
20060523	O

20000913	O

US	O
7058580	O
B2	O
20060606	O

20041004	O

US	O
7076428	O
B2	O
20060711	O

20021230	O

US	O
7003463	O
B1	O
20060221	O

20010625	O

US	O
7012996	O
B2	O
20060314	O

20030521	O

US	O
7013275	O
B2	O
20060314	O

20011228	O

US	O
6898567	O
B2	O
20050524	O

20011229	O

US	O
6912496	O
B1	O
20050628	O

20001026	O

US	O
6915262	O
B2	O
20050705	O

20001130	O

US	O
6917917	O
B1	O
20050712	O

20000830	O

US	O
6885735	O
B2	O
20050426	O

20020129	O

US	O
6834265	O
B2	O
20041221	O

20021213	O

US	O
6785653	O
B1	O
20040831	O

20000501	O

US	O
6757655	O
B1	O
20040629	O

19990827	O

US	O
6760699	O
B1	O
20040706	O

20000424	O

US	O
6725190	O
B1	O
20040420	O

19991102	O

US	O
6665640	O
B1	O
20031216	O

19991112	O

US	O
6633846	O
B1	O
20031014	O

19991112	O

US	O
6615172	O
B1	O
20030902	O

19991112	O

US	O
6584179	O
B1	O
20030624	O

19971024	O

US	O
6594628	O
B1	O
20030715	O

19970402	O

US	O
7698131	O
B2	O
20100413	O

20070409	O

US	O
7702508	O
B2	O
20100420	O

20041203	O

US	O
6502070	O
B1	O
20021231	O

20000428	O

US	O
6490621	O
B1	O
20021203	O

19991116	O

US	O
6336090	O
B1	O
20020101	O

19981130	O

US	O
6393403	O
B1	O
20020521	O

19980622	O

US	O
6408272	O
B1	O
20020618	O

19990412	O

US	O
6424945	O
B1	O
20020723	O

19991215	O

US	O
6411926	O
B1	O
20020625	O

19990208	O

US	O
6389389	O
B1	O
20020514	O

19991013	O

US	O
6363349	O
B1	O
20020326	O

19990528	O

US	O
6292781	O
B1	O
20010918	O

19990528	O

US	O
6260010	O
B1	O
20010710	O

19980918	O

US	O
6185535	O
B1	O
20010206	O

19981016	O

US	O
7725307	O
B2	O
20100525	O

20030829	O

US	O
7725320	O
B2	O
20100525	O

20070409	O

US	O
7725321	O
B2	O
20100525	O

20080623	O

US	O
7729904	O
B2	O
20100601	O

20041203	O

US	O
7769591	O
B2	O
20100803	O

20060831	O

US	O
7769143	O
B2	O
20100803	O

20071030	O

US	O
7778831	O
B2	O
20100817	O

20060221	O

US	O
7801731	O
B2	O
20100921	O

20071030	O

US	O
7809663	O
B1	O
20101005	O

20070522	O

WO	O
0135391	O
A1	O
20010517	O

20001110	O

WO	O
0135391	O
A1	O
20010517	O

20001110	O

US	O
7831426	O
B2	O
20101109	O

20060623	O

WO	O
02093555	O
A1	O
20021121	O

20020517	O

US	O
7873519	O
B2	O
20110118	O

20071031	O

US	O
7877088	O
B2	O
20110125	O

20070521	O

US	O
7904298	O
B2	O
20110308	O

20071116	O

US	O
7912702	O
B2	O
20110322	O

20071031	O

US	O
7933777	O
B2	O
20110426	O

20090830	O

US	O
7941313	O
B2	O
20110510	O

20011214	O

US	O
7970613	O
B2	O
20110628	O

20051112	O

US	O
7983911	O
B2	O
20110719	O

20040109	O

US	O
8010358	O
B2	O
20110830	O

20060221	O

US	O
8024194	O
B2	O
20110920	O

20041208	O

US	O
534080	O
19950921	O

PENDING	O

US	O
08627333	O
19960404	O

US	O
173247	O
19931222	O

PENDING	O

US	O
534080	O
19950921	O

QUALCOMM	O
Incorporated	O

San	O
Diego	O
CA	O
US	O

Jacobs	O
Paul	O
E.	O

San	O
Diego	O
US	O

Chang	O
Chienchung	O

San	O
Diego	O
US	O

Miller	O
Russell	O
B.	O

Golden	O
Linli	O
L.	O

English	O
Sean	O

Hudspeth	O
;	O
David	O
R.	O

Opsasnick	O
;	O
Michael	O
N.	O

US	O
5956683	O
A	O
19990921	O

19960404	O

US	O
6594628	O
B1	O
20030715	O

19970402	O

JP	O
09507105	O
A	O
19970715	O

19941220	O

DE	O
69433593	O
D1	O
20040408	O

19941220	O

CN	O
1119794	O
C	O
20030827	O

19941220	O

FI	O
962572	O
D0	O
19960620	O

19941220	O

FI	O
962572	O
A	O
19960820	O

19941220	O

JP	O
3661874	O
B2	O
20050622	O

19941220	O

US	O
5956683	O
A	O
19990921	O

19960404	O

BR	O
PI9408413	O
A	O
19970805	O

19941220	O

CA	O
2179759	O
A1	O
19950629	O

19941220	O

IL	O
112057	O
D0	O
19950315	O

19941219	O

ZA	O
9408426	O
A	O
19950630	O

19941026	O

FI	O
118909	O
B1	O
20080430	O

19941220	O

EP	O
1942487	O
A1	O
20080709	O

19941220	O

EP	O
1381029	O
A1	O
20040114	O

19941220	O

EP	O
736211	O
B1	O
20040303	O

19941220	O

EP	O
736211	O
A1	O
19961009	O

19941220	O

WO	O
9517746	O
A1	O
19950629	O

19941220	O

JP	O
9507105	O
T	O
19970715	O

19941220	O

DE	O
69433593	O
T2	O
20050203	O

19941220	O

AT	O
261172	O
T	O
20040315	O

19941220	O

AU	O
692820	O
B2	O
19980618	O

19941220	O

AU	O
1375395	O
A	O
19950710	O

19941220	O

CA	O
2179759	O
C	O
20051115	O

19941220	O

CN	O
1138386	O
A	O
19961217	O

19941220	O

HK	O
1011109	O
A1	O
20050114	O

19941220	O

MY	O
116482	O
A	O

19941209	O

FI	O
20070933	O
A	O
20071203	O

19941220	O

JP	O
09507105	O
T	O
19970715	O

19941220	O

JP	O
3661874	O
B9	O
20050401	O

19941220	O

A	O
voice	O
recognition	O
system	O
having	O
a	O
feature	O
extraction	O
apparatus	O
is	O
located	O
in	O
a	O
remote	O
station	O
.	O
The	O
feature	O
extraction	O
apparatus	O
extracts	O
features	O
from	O
an	O
input	O
speech	O
frame	O
and	O
then	O
provides	O
the	O
extracted	O
features	O
to	O
a	O
central	O
processing	O
station	O
.	O
In	O
the	O
central	O
processing	O
station	O
,	O
the	O
features	O
are	O
provided	O
to	O
a	O
word	O
decoder	O
which	O
determines	O
the	O
syntax	O
of	O
the	O
input	O
speech	O
frame	O
.	O

20030320	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
5956683A	O
4	O

20030409	O

REMI	O
MAINTENANCE	O
FEE	O
REMINDER	O
MAILED	O
N	O
US	O
5956683A	O

20070220	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
5956683A	O
8	O

20110218	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
5956683A	O
12	O

This	O
application	O
is	O
a	O
continuation	O
of	O
application	O
Ser	O
.	O
No.	O
08/534	O
,	O
080	O
,	O
filed	O
Sep.	O
21	O
,	O
1995	O
which	O
is	O
a	O
continuation	O
of	O
Ser	O
.	O
No.	O
08/173	O
,	O
247	O
,	O
filed	O
Dec.	O
22	O
,	O
1993	O
.	O

FIELD	O
OF	O
THE	O
INVENTION	O
The	O
present	O
invention	O
relates	O
to	O
speech	O
signal	O
processing	O
.	O
More	O
particularly	O
,	O
the	O
present	O
invention	O
relates	O
to	O
a	O
novel	O
method	O
and	O
apparatus	O
for	O
realizing	O
a	O
distributed	O
implementation	O
of	O
a	O
standard	O
voice	O
recognition	O
system	O
.	O

DESCRIPTION	O
OF	O
THE	O
RELATED	O
ART	O
Voice	O
recognition	O
represents	O
one	O
of	O
the	O
most	O
important	O
techniques	O
to	O
endow	O
a	O
machine	O
with	O
simulated	O
intelligence	O
to	O
recognize	O
user	O
voiced	O
commands	O
and	O
to	O
facilitate	O
human	O
interface	O
with	O
the	O
machine	O
.	O
It	O
also	O
represents	O
a	O
key	O
technique	O
for	O
human	O
speech	O
understanding	O
.	O
Systems	O
that	O
employ	O
techniques	O
to	O
recover	O
a	O
linguistic	O
message	O
from	O
an	O
acoustic	O
speech	O
signal	O
are	O
called	O
voice	O
recognizers	O
(	O
VR	O
)	O
.	O
A	O
voice	O
recognizer	O
is	O
composed	O
of	O
an	O
acoustic	O
processor	O
,	O
which	O
extracts	O
a	O
sequence	O
of	O
information-bearing	O
features	O
(	O
vectors	O
)	O
necessary	O
for	O
VR	O
from	O
the	O
incoming	O
raw	O
speech	O
,	O
and	O
a	O
word	O
decoder	O
,	O
which	O
decodes	O
this	O
sequence	O
of	O
features	O
(	O
vectors	O
)	O
to	O
yield	O
the	O
meaningful	O
and	O
desired	O
format	O
of	O
output	O
,	O
such	O
as	O
a	O
sequence	O
of	O
linguistic	O
words	O
corresponding	O
to	O
the	O
input	O
utterance	O
.	O
To	O
increase	O
the	O
performance	O
of	O
a	O
given	O
system	O
,	O
training	O
is	O
required	O
to	O
equip	O
the	O
system	O
with	O
valid	O
parameters	O
.	O
In	O
other	O
words	O
,	O
the	O
system	O
needs	O
to	O
learn	O
before	O
it	O
can	O
function	O
optimally	O
.	O

The	O
acoustic	O
processor	O
represents	O
a	O
front	O
end	O
speech	O
analysis	O
subsystem	O
in	O
a	O
voice	O
recognizer	O
.	O
In	O
response	O
to	O
an	O
input	O
speech	O
signal	O
,	O
it	O
provides	O
an	O
appropriate	O
representation	O
to	O
characterize	O
the	O
time-varying	O
speech	O
signal	O
.	O
It	O
should	O
discard	O
irrelevant	O
information	O
such	O
as	O
background	O
noise	O
,	O
channel	O
distortion	O
,	O
speaker	O
characteristics	O
and	O
manner	O
of	O
speaking	O
.	O
Efficient	O
acoustic	O
feature	O
extraction	O
systems	O
will	O
furnish	O
voice	O
recognizers	O
with	O
higher	O
acoustic	O
discrimination	O
power	O
.	O
The	O
most	O
useful	O
characteristic	O
is	O
the	O
short	O
time	O
spectral	O
envelope	O
.	O
In	O
characterizing	O
the	O
short	O
time	O
spectral	O
envelope	O
,	O
the	O
two	O
most	O
commonly	O
used	O
spectral	O
analysis	O
techniques	O
are	O
linear	O
predictive	O
coding	O
(	O
LPC	O
)	O
and	O
filter-bank	O
based	O
spectral	O
analysis	O
models	O
.	O
However	O
,	O
it	O
is	O
readily	O
shown	O
(	O
as	O
discussed	O
in	O
Rabiner	B-Citation
,	I-Citation
L.	I-Citation
R	I-Citation
and	I-Citation
Schafer	I-Citation
,	I-Citation
R.	I-Citation
W.	I-Citation
,	I-Citation
Digital	I-Citation
Processing	I-Citation
of	I-Citation
Speech	I-Citation
Signals	I-Citation
,	I-Citation
Prentice	I-Citation
Hall	I-Citation
,	I-Citation
1978	I-Citation
)	O
that	O
LPC	O
not	O
only	O
provides	O
a	O
good	O
approximation	O
to	O
the	O
vocal	O
tract	O
spectral	O
envelope	O
,	O
but	O
is	O
considerably	O
less	O
expensive	O
in	O
computation	O
than	O
the	O
filter-bank	O
model	O
in	O
all	O
digital	O
implementations	O
.	O
Experience	O
has	O
also	O
demonstrated	O
that	O
the	O
performance	O
of	O
LPC	O
based	O
voice	O
recognizers	O
is	O
comparable	O
to	O
or	O
better	O
than	O
that	O
of	O
filter-bank	O
based	O
recognizers	O
(	O
Rabiner	B-Citation
,	I-Citation
L.	I-Citation
R.	I-Citation
and	I-Citation
Juang	I-Citation
,	I-Citation
B.	I-Citation
H.	I-Citation
,	I-Citation
Fundamentals	I-Citation
of	I-Citation
Speech	I-Citation
Recognition	I-Citation
,	I-Citation
Prentice	I-Citation
Hall	I-Citation
,	I-Citation
1993	I-Citation
)	O
.	O

Referring	O
to	O
FIG	O
.	O
1	O
,	O
in	O
an	O
LPC	O
based	O
acoustic	O
processor	O
,	O
the	O
input	O
speech	O
is	O
provided	O
to	O
a	O
microphone	O
(	O
not	O
shown	O
)	O
and	O
converted	O
to	O
an	O
analog	O
electrical	O
signal	O
.	O
This	O
electrical	O
signal	O
is	O
then	O
digitized	O
by	O
an	O
A/D	O
converter	O
(	O
not	O
shown	O
)	O
.	O
The	O
digitized	O
speech	O
signals	O
are	O
passed	O
through	O
preemphasis	O
filter	O
2	O
in	O
order	O
to	O
spectrally	O
flatten	O
the	O
signal	O
and	O
to	O
make	O
it	O
less	O
susceptible	O
to	O
finite	O
precision	O
effects	O
in	O
subsequent	O
signal	O
processing	O
.	O
The	O
preemphasis	O
filtered	O
speech	O
is	O
then	O
provided	O
to	O
segmentation	O
element	O
4	O
where	O
it	O
is	O
segmented	O
or	O
blocked	O
into	O
either	O
temporally	O
overlapped	O
or	O
nonoverlapped	O
blocks	O
.	O
The	O
frames	O
of	O
speech	O
data	O
are	O
then	O
provided	O
to	O
windowing	O
element	O
6	O
where	O
framed	O
DC	O
components	O
are	O
removed	O
and	O
a	O
digital	O
windowing	O
operation	O
is	O
performed	O
on	O
each	O
frame	O
to	O
lessen	O
the	O
blocking	O
effects	O
due	O
to	O
the	O
discontinuity	O
at	O
frame	O
boundaries	O
.	O
A	O
most	O
commonly	O
used	O
window	O
function	O
in	O
LPC	O
analysis	O
is	O
the	O
Hamming	O
window	O
,	O
w	O
(	O
n	O
)	O
defined	O
as	O
:	O
#	O
#	O
EQU1	O
#	O
#	O
The	O
windowed	O
speech	O
is	O
provided	O
to	O
LPC	O
analysis	O
element	O
8	O
.	O
In	O
LPC	O
analysis	O
element	O
8	O
autocorrelation	O
functions	O
are	O
calculated	O
based	O
on	O
the	O
windowed	O
samples	O
and	O
corresponding	O
LPC	O
parameters	O
are	O
obtained	O
directly	O
from	O
autocorrelation	O
functions	O
.	O

Generally	O
speaking	O
,	O
the	O
word	O
decoder	O
translates	O
the	O
acoustic	O
feature	O
sequence	O
produced	O
by	O
the	O
acoustic	O
processor	O
into	O
an	O
estimate	O
of	O
the	O
speaker	O
's	O
original	O
word	O
string	O
.	O
This	O
is	O
accomplished	O
in	O
two	O
steps	O
:	O
acoustic	O
pattern	O
matching	O
and	O
language	O
modeling	O
.	O
Language	O
modeling	O
can	O
be	O
avoided	O
in	O
the	O
applications	O
of	O
isolated	O
word	O
recognition	O
.	O
The	O
LPC	O
parameters	O
from	O
LPC	O
analysis	O
element	O
8	O
are	O
provided	O
to	O
acoustic	O
pattern	O
matching	O
element	O
10	O
to	O
detect	O
and	O
classify	O
possible	O
acoustic	O
patterns	O
,	O
such	O
as	O
phonemes	O
,	O
syllables	O
,	O
words	O
,	O
etc	O
.	O
The	O
candidate	O
patterns	O
are	O
provided	O
to	O
language	O
modeling	O
element	O
12	O
,	O
which	O
models	O
the	O
rules	O
of	O
syntactic	O
constraints	O
that	O
determine	O
what	O
sequences	O
of	O
words	O
are	O
grammatically	O
well	O
formed	O
and	O
meaningful	O
.	O
Syntactic	O
information	O
can	O
be	O
a	O
valuable	O
guide	O
to	O
voice	O
recognition	O
when	O
acoustic	O
information	O
alone	O
is	O
ambiguous	O
.	O
Based	O
on	O
language	O
modeling	O
,	O
the	O
VR	O
sequentially	O
interprets	O
the	O
acoustic	O
feature	O
matching	O
results	O
and	O
provides	O
the	O
estimated	O
word	O
string	O
.	O

Both	O
the	O
acoustic	O
pattern	O
matching	O
and	O
language	O
modeling	O
in	O
the	O
word	O
decoder	O
requires	O
a	O
mathematical	O
model	O
,	O
either	O
deterministic	O
or	O
stochastic	O
,	O
to	O
describe	O
the	O
speaker	O
's	O
phonological	O
and	O
acoustic-phonetic	O
variations	O
.	O
The	O
performance	O
of	O
a	O
speech	O
recognition	O
system	O
is	O
directly	O
related	O
to	O
the	O
quality	O
of	O
these	O
two	O
modelings	O
.	O
Among	O
the	O
various	O
classes	O
of	O
models	O
for	O
acoustic	O
pattern	O
matching	O
,	O
template-based	O
dynamic	O
time	O
warping	O
(	O
DTW	O
)	O
and	O
stochastic	O
hidden	O
Markov	O
modeling	O
(	O
HMM	O
)	O
are	O
the	O
two	O
most	O
commonly	O
used	O
.	O
However	O
,	O
it	O
has	O
been	O
shown	O
that	O
DTW	O
based	O
approach	O
can	O
be	O
viewed	O
as	O
a	O
special	O
case	O
of	O
an	O
HMM	O
based	O
one	O
,	O
which	O
is	O
a	O
parametric	O
,	O
doubly	O
stochastic	O
model	O
.	O
HMM	O
systems	O
are	O
currently	O
the	O
most	O
successful	O
speech	O
recognition	O
algorithms	O
.	O
The	O
doubly	O
stochastic	O
property	O
in	O
HMM	O
provides	O
better	O
flexibility	O
in	O
absorbing	O
acoustic	O
as	O
well	O
as	O
temporal	O
variations	O
associated	O
with	O
speech	O
signals	O
.	O
This	O
usually	O
results	O
in	O
improved	O
recognition	O
accuracy	O
.	O
Concerning	O
the	O
language	O
model	O
,	O
a	O
stochastic	O
model	O
,	O
called	O
k-gram	O
language	O
model	O
which	O
is	O
detailed	O
in	O
F.	B-Citation
Jelink	I-Citation
,	I-Citation
"	I-Citation
The	I-Citation
Development	I-Citation
of	I-Citation
an	I-Citation
Experimental	I-Citation
Discrete	I-Citation
Dictation	I-Citation
Recognizer	I-Citation
"	I-Citation
,	I-Citation
Proc.	I-Citation
IEEE	I-Citation
,	I-Citation
vol.	I-Citation
73	I-Citation
,	I-Citation
pp.	I-Citation
1616	I-Citation
-	I-Citation
1624	I-Citation
,	I-Citation
1985	I-Citation
,	O
has	O
been	O
successfully	O
applied	O
in	O
practical	O
large	O
vocabulary	O
voice	O
recognition	O
systems	O
.	O
While	O
in	O
the	O
small	O
vocabulary	O
case	O
,	O
a	O
deterministic	O
grammar	O
has	O
been	O
formulated	O
as	O
a	O
finite	O
state	O
network	O
(	O
FSN	O
)	O
in	O
the	O
application	O
of	O
airline	O
and	O
reservation	O
and	O
information	O
system	O
(	O
see	O
Rabiner	B-Citation
,	I-Citation
L.	I-Citation
R	I-Citation
and	I-Citation
Levinson	I-Citation
,	I-Citation
S.	I-Citation
Z.	I-Citation
,	I-Citation
A	I-Citation
Speaker-Independent	I-Citation
,	I-Citation
Syntax-Directed	I-Citation
,	I-Citation
Connected	I-Citation
Word	I-Citation
Recognition	I-Citation
System	I-Citation
Based	I-Citation
on	I-Citation
Hidden	I-Citation
Markov	I-Citation
Model	I-Citation
and	I-Citation
Level	I-Citation
Building	I-Citation
,	I-Citation
IEEE	I-Citation
Trans	I-Citation
.	I-Citation
on	I-Citation
IASSP	I-Citation
,	I-Citation
Vol.	I-Citation
33	I-Citation
,	I-Citation
No.	I-Citation
3	I-Citation
,	I-Citation
June	I-Citation
1985	I-Citation
.	O
)	O
Statistically	O
,	O
in	O
order	O
to	O
minimize	O
the	O
probability	O
of	O
recognition	O
error	O
,	O
the	O
voice	O
recognition	O
problem	O
can	O
be	O
formalized	O
as	O
follows	O
:	O
with	O
acoustic	O
evidence	O
observation	O
O	O
,	O
the	O
operations	O
of	O
voice	O
recognition	O
are	O
to	O
find	O
the	O
most	O
likely	O
word	O
string	O
W	O
*	O
such	O
that	O
W	O
*	O
=	O
arg	O
max	O
P	O
(	O
W.vertline.O	O
)	O
(	O
1	O
)	O
where	O
the	O
maximization	O
is	O
over	O
all	O
possible	O
word	O
strings	O
W.	O
In	O
accordance	O
with	O
Bayes	O
rule	O
,	O
the	O
posteriori	O
probability	O
P	O
(	O
W.vertline.O	O
)	O
in	O
the	O
above	O
equation	O
can	O
be	O
rewritten	O
as	O
:	O
#	O
#	O
EQU2	O
#	O
#	O
Since	O
P	O
(	O
O	O
)	O
is	O
irrelevant	O
to	O
recognition	O
,	O
the	O
word	O
string	O
estimate	O
can	O
be	O
obtained	O
alternatively	O
as	O
:	O
W	O
*	O
=	O
arg	O
max	O
P	O
(	O
W	O
)	O
P	O
(	O
O.vertline.W	O
)	O
(	O
3	O
)	O
Here	O
P	O
(	O
W	O
)	O
represents	O
the	O
a	O
priori	O
probability	O
that	O
the	O
word	O
string	O
W	O
will	O
be	O
uttered	O
,	O
and	O
P	O
(	O
O.vertline.W	O
)	O
is	O
the	O
probability	O
that	O
the	O
acoustic	O
evidence	O
O	O
will	O
be	O
observed	O
given	O
that	O
the	O
speaker	O
uttered	O
the	O
word	O
sequence	O
W.	O
P	O
(	O
O.vertline.W	O
)	O
is	O
determined	O
by	O
acoustic	O
pattern	O
matching	O
,	O
while	O
the	O
a	O
priori	O
probability	O
P	O
(	O
W	O
)	O
is	O
defined	O
by	O
language	O
model	O
utilized	O
.	O

In	O
connected	O
word	O
recognition	O
,	O
if	O
the	O
vocabulary	O
is	O
small	O
(	O
less	O
than	O
100	O
)	O
,	O
a	O
deterministic	O
grammar	O
can	O
be	O
used	O
to	O
rigidly	O
govern	O
which	O
words	O
can	O
logically	O
follow	O
other	O
words	O
to	O
form	O
legal	O
sentences	O
in	O
the	O
language	O
.	O
The	O
deterministic	O
grammar	O
can	O
be	O
incorporated	O
in	O
the	O
acoustic	O
matching	O
algorithm	O
implicitly	O
to	O
constrain	O
the	O
search	O
space	O
of	O
potential	O
words	O
and	O
to	O
reduce	O
the	O
computation	O
dramatically	O
.	O
However	O
,	O
when	O
the	O
vocabulary	O
size	O
is	O
either	O
medium	O
(	O
greater	O
than	O
100	O
but	O
less	O
than	O
1000	O
)	O
or	O
large	O
(	O
greater	O
than	O
1000	O
)	O
,	O
the	O
probability	O
of	O
the	O
word	O
sequence	O
,	O
W	O
=	O
(	O
w.sub.1,w.sub.2	O
,	O
.	O
.	O
.	O
,	O
w	O
.	O
sub	O
.	O
n	O
)	O
,	O
can	O
be	O
obtained	O
by	O
stochastic	O
language	O
modeling	O
.	O
From	O
simple	O
probability	O
theory	O
,	O
the	O
prior	O
probability	O
,	O
P	O
(	O
W	O
)	O
can	O
be	O
decomposed	O
as	O
#	O
#	O
EQU3	O
#	O
#	O
where	O
P	O
(	O
w	O
.	O
sub	O
.	O
i	O
.vertline.w.sub.1,w.sub.2	O
,	O
.	O
.	O
.	O
,	O
w	O
.	O
sub	O
.	O
i-1	O
)	O
is	O
the	O
probability	O
that	O
w	O
.	O
sub	O
.	O
i	O
will	O
be	O
spoken	O
given	O
that	O
the	O
word	O
sequence	O
(	O
w.sub.1,w.sub.2	O
,	O
.	O
.	O
.	O
,	O
w	O
.	O
sub	O
.	O
i-1	O
)	O
precedes	O
it	O
.	O
The	O
choice	O
of	O
w	O
.	O
sub	O
.	O
i	O
depends	O
on	O
the	O
entire	O
past	O
history	O
of	O
input	O
words	O
.	O
For	O
a	O
vocabulary	O
of	O
size	O
V	O
,	O
it	O
requires	O
V.	O
sup	O
.	O
i	O
values	O
to	O
specify	O
P	O
(	O
w	O
.	O
sub	O
.	O
i	O
.vertline.w.sub.1,w.sub.2	O
,	O
.	O
.	O
.	O
,	O
w	O
.	O
sub	O
.	O
i-1	O
)	O
completely	O
.	O
Even	O
for	O
the	O
mid	O
vocabulary	O
size	O
,	O
this	O
requires	O
a	O
formidable	O
number	O
of	O
samples	O
to	O
train	O
the	O
language	O
model	O
.	O
An	O
inaccurate	O
estimate	O
of	O
P	O
(	O
w	O
.	O
sub	O
.	O
i	O
.vertline.w.sub.1,w.sub.2	O
,	O
.	O
.	O
.	O
,	O
w	O
.	O
sub	O
.	O
i-1	O
)	O
due	O
to	O
insufficient	O
training	O
data	O
will	O
depreciate	O
the	O
results	O
of	O
original	O
acoustic	O
matching	O
.	O

A	O
practical	O
solution	O
to	O
the	O
above	O
problems	O
is	O
to	O
assume	O
that	O
w	O
.	O
sub	O
.	O
i	O
only	O
depends	O
on	O
(	O
k-1	O
)	O
preceding	O
words	O
,	O
w.sub.i-1,w.sub.i-2	O
,	O
.	O
.	O
.	O
,w.sub.i-k+1	O
.	O
A	O
stochastic	O
language	O
model	O
can	O
be	O
completely	O
described	O
in	O
terms	O
of	O
P	O
(	O
w	O
.	O
sub	O
.	O
i	O
.vertline.w.sub.1,w.sub.2	O
,	O
.	O
.	O
.	O
,w.sub.i-k+1	O
)	O
from	O
which	O
k-gram	O
language	O
model	O
is	O
derived	O
.	O
Since	O
most	O
of	O
the	O
word	O
strings	O
will	O
never	O
occur	O
in	O
the	O
language	O
if	O
k	O
>	O
3	O
,	O
unigram	O
(	O
k	O
=	O
1	O
)	O
,	O
bigram	O
(	O
k	O
=	O
2	O
)	O
and	O
trigram	O
(	O
k	O
=	O
3	O
)	O
are	O
the	O
most	O
powerful	O
stochastic	O
language	O
models	O
that	O
take	O
grammar	O
into	O
consideration	O
statistically	O
.	O
Language	O
modeling	O
contains	O
both	O
syntactic	O
and	O
semantic	O
information	O
which	O
is	O
valuable	O
in	O
recognition	O
,	O
but	O
these	O
probabilities	O
must	O
be	O
trained	O
from	O
a	O
large	O
collection	O
of	O
speech	O
data	O
.	O
When	O
the	O
available	O
training	O
data	O
are	O
relatively	O
limited	O
,	O
such	O
as	O
K-grams	O
may	O
never	O
occur	O
in	O
the	O
data	O
,	O
P	O
(	O
w	O
.	O
sub	O
.	O
i	O
.vertline.w.sub.i-2,w.sub.i-1	O
)	O
can	O
be	O
estimated	O
directly	O
from	O
bigram	O
probability	O
P	O
(	O
w	O
.	O
sub	O
.	O
i	O
.vertline.w.sub.i-1	O
.	O
Details	O
of	O
this	O
process	O
can	O
be	O
found	O
in	O
F.	B-Citation
Jelink	I-Citation
,	I-Citation
"	I-Citation
The	I-Citation
Development	I-Citation
of	I-Citation
An	I-Citation
Experimental	I-Citation
Discrete	I-Citation
Dictation	I-Citation
Recognizer	I-Citation
"	I-Citation
,	I-Citation
Proc.	I-Citation
IEEE	I-Citation
,	I-Citation
vol.	I-Citation
73	I-Citation
,	I-Citation
pp.	I-Citation
1616	I-Citation
-	I-Citation
1624	I-Citation
,	I-Citation
1985	I-Citation
.	O
In	O
connected	O
word	O
recognition	O
,	O
whole	O
word	O
model	O
is	O
used	O
as	O
the	O
basic	O
speech	O
unit	O
,	O
while	O
in	O
continuous	O
voice	O
recognition	O
,	O
subband	O
units	O
,	O
such	O
as	O
phonemes	O
,	O
syllables	O
or	O
demisyllables	O
may	O
be	O
used	O
as	O
the	O
basic	O
speech	O
unit	O
.	O
The	O
word	O
decoder	O
will	O
be	O
modified	O
accordingly	O
.	O

Conventional	O
voice	O
recognition	O
systems	O
integrate	O
acoustic	O
processor	O
and	O
word	O
decoders	O
without	O
taking	O
into	O
account	O
their	O
separability	O
,	O
the	O
limitations	O
of	O
application	O
systems	O
(	O
such	O
as	O
power	O
consumption	O
,	O
memory	O
availability	O
,	O
etc	O
.	O
)	O
and	O
communication	O
channel	O
characteristics	O
.	O
This	O
motivates	O
the	O
interest	O
in	O
devising	O
a	O
distributed	O
voice	O
recognition	O
system	O
with	O
these	O
two	O
components	O
appropriately	O
separated	O
.	O

SUMMARY	O
OF	O
THE	O
INVENTION	O
The	O
present	O
invention	O
is	O
a	O
novel	O
and	O
improved	O
distributed	O
voice	O
recognition	O
system	O
,	O
in	O
which	O
(	O
i	O
)	O
the	O
front	O
end	O
acoustic	O
processor	O
can	O
be	O
LPC	O
based	O
or	O
filter	O
bank	O
based	O
;	O
(	O
ii	O
)	O
the	O
acoustic	O
pattern	O
matching	O
in	O
the	O
word	O
decoder	O
can	O
be	O
based	O
on	O
hidden	O
Markov	O
model	O
(	O
HMM	O
)	O
,	O
dynamic	O
time	O
warping	O
(	O
DTW	O
)	O
or	O
even	O
neural	O
networks	O
(	O
NN	O
)	O
;	O
and	O
(	O
iii	O
)	O
for	O
the	O
connected	O
or	O
continuous	O
word	O
recognition	O
purpose	O
,	O
the	O
language	O
model	O
can	O
be	O
based	O
on	O
deterministic	O
or	O
stochastic	O
grammars	O
.	O
The	O
present	O
invention	O
differs	O
from	O
the	O
usual	O
voice	O
recognizer	O
in	O
improving	O
system	O
performance	O
by	O
appropriately	O
separating	O
the	O
components	O
:	O
feature	O
extraction	O
and	O
word	O
decoding	O
.	O
As	O
demonstrated	O
in	O
next	O
examples	O
,	O
if	O
LPC	O
based	O
features	O
,	O
such	O
as	O
cepstrum	O
coefficients	O
,	O
are	O
to	O
be	O
sent	O
over	O
communication	O
channel	O
,	O
a	O
transformation	O
between	O
LPC	O
and	O
LSP	O
can	O
be	O
used	O
to	O
alleviate	O
the	O
noise	O
effects	O
on	O
feature	O
sequence	O
.	O

BRIEF	O
DESCRIPTION	O
OF	O
THE	O
DRAWINGS	O
The	O
features	O
,	O
objects	O
,	O
and	O
advantages	O
of	O
the	O
present	O
invention	O
will	O
become	O
more	O
apparent	O
from	O
the	O
detailed	O
description	O
set	O
forth	O
below	O
when	O
taken	O
in	O
conjunction	O
with	O
the	O
drawings	O
in	O
which	O
like	O
reference	O
characters	O
identify	O
correspondingly	O
throughout	O
and	O
wherein	O
:	O
FIG	O
.	O
1	O
is	O
a	O
block	O
diagram	O
of	O
a	O
traditional	O
speech	O
recognition	O
system	O
;	O
FIG	O
.	O
2	O
is	O
a	O
block	O
diagram	O
of	O
an	O
exemplary	O
implementation	O
of	O
the	O
present	O
invention	O
in	O
a	O
wireless	O
communication	O
environment	O
;	O
FIG	O
.	O
3	O
is	O
a	O
general	O
block	O
diagram	O
of	O
the	O
present	O
invention	O
;	O
FIG	O
.	O
4	O
is	O
a	O
block	O
diagram	O
of	O
an	O
exemplary	O
embodiment	O
of	O
the	O
transform	O
element	O
and	O
inverse	O
transform	O
element	O
of	O
the	O
present	O
invention	O
;	O
and	O
FIG	O
.	O
5	O
is	O
a	O
block	O
diagram	O
of	O
a	O
preferred	O
embodiment	O
of	O
the	O
present	O
invention	O
comprising	O
a	O
local	O
word	O
detector	O
in	O
addition	O
to	O
a	O
remote	O
word	O
detector	O
.	O

DETAILED	O
DESCRIPTION	O
OF	O
THE	O
PREFERRED	O
EMBODIMENTS	O
In	O
a	O
standard	O
voice	O
recognizer	O
,	O
either	O
in	O
recognition	O
or	O
in	O
training	O
,	O
most	O
of	O
the	O
computational	O
complexity	O
is	O
concentrated	O
in	O
the	O
word	O
decoder	O
subsystem	O
of	O
the	O
voice	O
recognizer	O
.	O
In	O
the	O
implementation	O
of	O
voice	O
recognizers	O
with	O
distributed	O
system	O
architecture	O
,	O
it	O
is	O
often	O
desirable	O
to	O
place	O
the	O
word	O
decoding	O
task	O
at	O
the	O
subsystem	O
which	O
can	O
absorb	O
the	O
computational	O
load	O
appropriately	O
.	O
Whereas	O
the	O
acoustic	O
processor	O
should	O
reside	O
as	O
close	O
to	O
the	O
speech	O
source	O
as	O
possible	O
to	O
reduce	O
the	O
effects	O
of	O
quantization	O
errors	O
introduced	O
by	O
signal	O
processing	O
and/or	O
channel	O
induced	O
errors	O
.	O

An	O
exemplary	O
implementation	O
of	O
the	O
present	O
invention	O
is	O
illustrated	O
in	O
FIG	O
.	O
2	O
.	O
In	O
the	O
exemplary	O
embodiment	O
,	O
the	O
environment	O
is	O
a	O
wireless	O
communication	O
system	O
comprising	O
a	O
portable	O
cellular	O
telephone	O
or	O
personal	O
communications	O
device	O
40	O
and	O
a	O
central	O
communications	O
center	O
referred	O
to	O
as	O
a	O
cell	O
base	O
station	O
42	O
.	O
In	O
the	O
exemplary	O
embodiment	O
the	O
distributed	O
VR	O
system	O
is	O
presented	O
.	O
In	O
the	O
distributed	O
VR	O
the	O
acoustic	O
processor	O
or	O
feature	O
extraction	O
element	O
22	O
resides	O
in	O
personal	O
communication	O
device	O
40	O
and	O
word	O
decoder	O
48	O
resides	O
in	O
the	O
central	O
communications	O
center	O
.	O
If	O
,	O
instead	O
of	O
distributed	O
VR	O
,	O
VR	O
is	O
implemented	O
solely	O
in	O
portable	O
cellular	O
phone	O
it	O
would	O
be	O
highly	O
infeasible	O
even	O
for	O
medium	O
size	O
vocabulary	O
,	O
connected	O
word	O
recognition	O
due	O
to	O
high	O
computation	O
cost	O
.	O
On	O
the	O
other	O
hand	O
,	O
if	O
VR	O
resides	O
merely	O
at	O
the	O
base	O
station	O
,	O
the	O
accuracy	O
can	O
be	O
decreased	O
dramatically	O
due	O
to	O
the	O
degradation	O
of	O
speech	O
signals	O
associated	O
with	O
speech	O
codec	O
and	O
channel	O
effects	O
.	O
Evidently	O
,	O
there	O
are	O
three	O
advantages	O
to	O
the	O
proposed	O
distributed	O
system	O
design	O
.	O
The	O
first	O
is	O
the	O
reduction	O
in	O
cost	O
of	O
the	O
cellular	O
telephone	O
due	O
to	O
the	O
word	O
decoder	O
hardware	O
that	O
is	O
no	O
longer	O
resident	O
in	O
the	O
telephone	O
40	O
.	O
The	O
second	O
is	O
a	O
reduction	O
of	O
the	O
drain	O
on	O
the	O
battery	O
(	O
not	O
shown	O
)	O
of	O
portable	O
telephone	O
40	O
that	O
would	O
result	O
from	O
locally	O
performing	O
the	O
computationally	O
intensive	O
word	O
decoder	O
operation	O
.	O
The	O
third	O
is	O
the	O
expected	O
improvement	O
in	O
recognition	O
accuracy	O
in	O
addition	O
to	O
the	O
flexibility	O
and	O
extendibility	O
of	O
the	O
distributed	O
system	O
.	O

The	O
speech	O
is	O
provided	O
to	O
microphone	O
20	O
which	O
converts	O
the	O
speech	O
signal	O
into	O
electrical	O
signals	O
which	O
are	O
provided	O
to	O
feature	O
extraction	O
element	O
22	O
.	O
The	O
signals	O
from	O
microphone	O
20	O
may	O
be	O
analog	O
or	O
digital	O
.	O
If	O
the	O
signals	O
are	O
analog	O
,	O
then	O
an	O
analog	O
to	O
digital	O
converter	O
(	O
not	O
shown	O
)	O
may	O
be	O
needed	O
to	O
be	O
interposed	O
between	O
microphone	O
20	O
and	O
feature	O
extraction	O
element	O
22	O
.	O
The	O
speech	O
signals	O
are	O
provided	O
to	O
feature	O
extraction	O
element	O
22	O
.	O
Feature	O
extraction	O
element	O
22	O
extracts	O
relevant	O
characteristics	O
of	O
the	O
input	O
speech	O
that	O
will	O
be	O
used	O
to	O
decode	O
the	O
linguistic	O
interpretation	O
of	O
the	O
input	O
speech	O
.	O
One	O
example	O
of	O
characteristics	O
that	O
can	O
be	O
used	O
to	O
estimate	O
speech	O
is	O
the	O
frequency	O
characteristics	O
of	O
an	O
input	O
speech	O
frame	O
.	O
This	O
is	O
frequently	O
provided	O
as	O
linear	O
predictive	O
coding	O
parameters	O
of	O
the	O
input	O
frame	O
of	O
speech	O
.	O
The	O
extracted	O
features	O
of	O
the	O
speech	O
are	O
then	O
provided	O
to	O
transmitter	O
24	O
which	O
codes	O
,	O
modulates	O
and	O
amplifies	O
the	O
extracted	O
feature	O
signal	O
and	O
provides	O
the	O
features	O
through	O
duplexer	O
26	O
to	O
antenna	O
28	O
,	O
where	O
the	O
speech	O
features	O
are	O
transmitted	O
to	O
cellular	O
base	O
station	O
or	O
central	O
communications	O
center	O
42	O
.	O
Various	O
types	O
of	O
digital	O
coding	O
,	O
modulation	O
,	O
and	O
transmission	O
schemes	O
well	O
known	O
in	O
the	O
art	O
may	O
be	O
employed	O
.	O

At	O
central	O
communications	O
center	O
42	O
,	O
the	O
transmitted	O
features	O
are	O
received	O
at	O
antenna	O
44	O
and	O
provided	O
to	O
receiver	O
46	O
.	O
Receiver	O
46	O
may	O
perform	O
the	O
functions	O
of	O
demodulating	O
and	O
decoding	O
the	O
received	O
transmitted	O
features	O
which	O
it	O
provides	O
to	O
word	O
decoder	O
48	O
.	O
Word	O
decoder	O
48	O
determines	O
,	O
from	O
the	O
speech	O
features	O
,	O
a	O
linguistic	O
estimate	O
of	O
the	O
speech	O
and	O
provides	O
an	O
action	O
signal	O
to	O
transmitter	O
50	O
.	O
Transmitter	O
50	O
performs	O
the	O
functions	O
of	O
amplification	O
,	O
modulation	O
and	O
coding	O
of	O
the	O
action	O
signal	O
,	O
and	O
provides	O
the	O
amplified	O
signal	O
to	O
antenna	O
52	O
,	O
which	O
transmits	O
the	O
estimated	O
words	O
or	O
a	O
command	O
signal	O
to	O
portable	O
phone	O
40	O
.	O
Transmitter	O
50	O
may	O
also	O
employ	O
well	O
known	O
digital	O
coding	O
,	O
modulation	O
or	O
transmission	O
techniques	O
.	O

At	O
portable	O
phone	O
40	O
,	O
the	O
estimated	O
words	O
or	O
command	O
signals	O
are	O
received	O
at	O
antenna	O
28	O
,	O
which	O
provides	O
the	O
received	O
signal	O
through	O
duplexer	O
26	O
to	O
receiver	O
30	O
which	O
demodulates	O
,	O
decodes	O
the	O
signal	O
and	O
then	O
provides	O
the	O
command	O
signal	O
or	O
estimated	O
words	O
to	O
control	O
element	O
38	O
.	O
In	O
response	O
to	O
the	O
received	O
command	O
signal	O
or	O
estimated	O
words	O
,	O
control	O
element	O
38	O
provides	O
the	O
intended	O
response	O
(	O
e.g.	O
,	O
dialing	O
a	O
phone	O
number	O
,	O
providing	O
information	O
to	O
display	O
screen	O
on	O
the	O
portable	O
phone	O
,	O
etc	O
.	O
)	O
.	O

The	O
same	O
system	O
represented	O
in	O
FIG	O
.	O
2	O
could	O
also	O
serve	O
in	O
a	O
slightly	O
different	O
way	O
in	O
that	O
the	O
information	O
sent	O
back	O
from	O
central	O
communications	O
center	O
42	O
need	O
not	O
be	O
an	O
interpretation	O
of	O
the	O
transmitted	O
speech	O
,	O
rather	O
the	O
information	O
sent	O
back	O
from	O
central	O
communications	O
center	O
42	O
may	O
be	O
a	O
response	O
to	O
the	O
decoded	O
message	O
sent	O
by	O
the	O
portable	O
phone	O
.	O
For	O
example	O
,	O
one	O
may	O
inquire	O
of	O
messages	O
on	O
a	O
remote	O
answering	O
machine	O
(	O
not	O
shown	O
)	O
coupled	O
via	O
a	O
communications	O
network	O
to	O
central	O
communications	O
center	O
42	O
,	O
in	O
which	O
case	O
the	O
signal	O
transmitted	O
from	O
central	O
communications	O
center	O
42	O
to	O
portable	O
telephone	O
40	O
may	O
be	O
the	O
messages	O
from	O
the	O
answering	O
machine	O
in	O
this	O
implementation	O
.	O
A	O
second	O
control	O
element	O
49	O
would	O
be	O
collocated	O
in	O
the	O
central	O
communications	O
center	O
.	O

The	O
significance	O
of	O
placing	O
the	O
feature	O
extraction	O
element	O
22	O
in	O
portable	O
phone	O
40	O
instead	O
of	O
at	O
central	O
communications	O
center	O
42	O
is	O
as	O
follows	O
.	O
If	O
the	O
acoustic	O
processor	O
is	O
placed	O
at	O
central	O
communications	O
center	O
42	O
,	O
as	O
opposed	O
to	O
distributed	O
VR	O
,	O
a	O
low	O
bandwidth	O
digital	O
radio	O
channel	O
may	O
require	O
a	O
vocoder	O
(	O
at	O
the	O
first	O
subsystem	O
)	O
which	O
limits	O
resolution	O
of	O
features	O
vectors	O
due	O
to	O
quantization	O
distortion	O
.	O
However	O
,	O
by	O
putting	O
the	O
acoustic	O
processor	O
in	O
the	O
portable	O
or	O
cellular	O
phone	O
,	O
one	O
can	O
dedicate	O
entire	O
channel	O
bandwidth	O
to	O
feature	O
transmission	O
.	O
Usually	O
,	O
the	O
extracted	O
acoustic	O
feature	O
vector	O
requires	O
less	O
bandwidth	O
than	O
the	O
speech	O
signal	O
for	O
transmission	O
.	O
Since	O
recognition	O
accuracy	O
is	O
highly	O
dependent	O
on	O
degradation	O
of	O
input	O
speech	O
signal	O
,	O
one	O
should	O
provide	O
feature	O
extraction	O
element	O
22	O
as	O
close	O
to	O
the	O
user	O
as	O
possible	O
so	O
that	O
feature	O
extraction	O
element	O
22	O
extracts	O
feature	O
vectors	O
based	O
on	O
microphone	O
speech	O
,	O
instead	O
of	O
(	O
vocoded	O
)	O
telephone	O
speech	O
which	O
may	O
be	O
additionally	O
corrupted	O
in	O
transmission	O
.	O

In	O
real	O
applications	O
,	O
voice	O
recognizers	O
are	O
designed	O
to	O
operate	O
under	O
ambient	O
conditions	O
,	O
such	O
as	O
background	O
noise	O
.	O
Thus	O
,	O
it	O
is	O
important	O
to	O
consider	O
the	O
problem	O
of	O
voice	O
recognition	O
in	O
the	O
presence	O
of	O
noise	O
.	O
It	O
has	O
been	O
shown	O
that	O
,	O
if	O
the	O
training	O
of	O
vocabulary	O
(	O
reference	O
patterns	O
)	O
is	O
performed	O
in	O
the	O
exact	O
(	O
or	O
approximate	O
)	O
same	O
environment	O
as	O
the	O
test	O
condition	O
,	O
voice	O
recognizers	O
not	O
only	O
can	O
provide	O
good	O
performance	O
even	O
in	O
very	O
noisy	O
environments	O
,	O
but	O
can	O
reduce	O
the	O
degradation	O
in	O
recognition	O
accuracy	O
due	O
to	O
noise	O
significantly	O
.	O
The	O
mismatch	O
between	O
training	O
and	O
test	O
conditions	O
accounts	O
for	O
one	O
of	O
the	O
major	O
degradation	O
factors	O
in	O
recognition	O
performance	O
.	O
With	O
the	O
assumption	O
that	O
acoustic	O
features	O
can	O
traverse	O
communication	O
channels	O
more	O
reliably	O
than	O
speech	O
signals	O
(	O
since	O
acoustic	O
features	O
require	O
less	O
bandwidth	O
than	O
speech	O
signals	O
for	O
transmission	O
as	O
mentioned	O
previously	O
)	O
,	O
the	O
proposed	O
distributed	O
voice	O
recognition	O
system	O
is	O
advantageous	O
in	O
providing	O
matching	O
conditions	O
.	O
If	O
a	O
voice	O
recognizer	O
is	O
implemented	O
remotely	O
,	O
the	O
matching	O
conditions	O
can	O
be	O
badly	O
broken	O
due	O
mainly	O
to	O
channel	O
variations	O
such	O
as	O
fading	O
encountered	O
in	O
wireless	O
communications	O
.	O
Implementing	O
VR	O
locally	O
may	O
avoid	O
these	O
effects	O
if	O
the	O
huge	O
training	O
computations	O
can	O
be	O
absorbed	O
locally	O
.	O
Unfortunately	O
,	O
in	O
many	O
applications	O
,	O
this	O
is	O
not	O
possible	O
.	O
Obviously	O
,	O
distributed	O
voice	O
recognition	O
implementation	O
can	O
avoid	O
mismatch	O
conditions	O
induced	O
by	O
channel	O
perplexity	O
and	O
compensate	O
for	O
the	O
shortcomings	O
of	O
centralized	O
implementations	O
.	O

Referring	O
to	O
FIG	O
.	O
3	O
,	O
the	O
digital	O
speech	O
samples	O
are	O
provided	O
to	O
feature	O
extraction	O
element	O
51	O
which	O
provides	O
the	O
features	O
over	O
communication	O
channel	O
56	O
to	O
word	O
estimation	O
element	O
62	O
where	O
an	O
estimated	O
word	O
string	O
is	O
determined	O
.	O
The	O
speech	O
signals	O
are	O
provided	O
to	O
acoustic	O
processor	O
52	O
which	O
determines	O
potential	O
features	O
for	O
each	O
speech	O
frame	O
.	O
Since	O
word	O
decoder	O
requires	O
acoustic	O
feature	O
sequence	O
as	O
input	O
for	O
both	O
recognition	O
and	O
training	O
tasks	O
,	O
these	O
acoustic	O
features	O
need	O
to	O
be	O
transmitted	O
across	O
the	O
communication	O
channel	O
56	O
.	O
However	O
,	O
not	O
all	O
potential	O
features	O
used	O
in	O
typical	O
voice	O
recognition	O
systems	O
are	O
suitable	O
for	O
transmission	O
through	O
noisy	O
channels	O
.	O
In	O
some	O
cases	O
,	O
transform	O
element	O
54	O
is	O
required	O
to	O
facilitate	O
source	O
encoding	O
and	O
to	O
reduce	O
the	O
effects	O
of	O
channel	O
noise	O
.	O
One	O
example	O
of	O
LPC	O
based	O
acoustic	O
features	O
which	O
are	O
widely	O
used	O
in	O
voice	O
recognizers	O
is	O
cepstrum	O
coefficients	O
,	O
[	O
c	O
.	O
sub	O
.	O
i	O
]	O
.	O
They	O
can	O
be	O
obtained	O
directly	O
from	O
LPC	O
coefficients	O
[	O
a	O
.	O
sub	O
.	O
i	O
]	O
as	O
follows	O
:	O
#	O
#	O
EQU4	O
#	O
#	O
where	O
P	O
is	O
the	O
order	O
of	O
LPC	O
filter	O
used	O
and	O
Q	O
is	O
the	O
size	O
of	O
cepstrum	O
feature	O
vector	O
.	O
Since	O
cepstrum	O
feature	O
vectors	O
change	O
rapidly	O
,	O
it	O
is	O
not	O
easy	O
to	O
compress	O
a	O
sequence	O
of	O
frames	O
of	O
cepstrum	O
coefficients	O
.	O
However	O
,	O
there	O
exists	O
a	O
transformation	O
between	O
LPCs	O
and	O
line	O
spectrum	O
pair	O
(	O
LSP	O
)	O
frequencies	O
which	O
changes	O
slowly	O
and	O
can	O
be	O
efficiently	O
encoded	O
by	O
a	O
delta	O
pulse	O
coded	O
modulation	O
(	O
DPCM	O
)	O
scheme	O
.	O
Since	O
cepstrum	O
coefficients	O
can	O
be	O
derived	O
directly	O
from	O
LPC	O
coefficients	O
,	O
LPCs	O
are	O
transformed	O
into	O
LSPs	O
by	O
transform	O
element	O
54	O
which	O
are	O
then	O
encoded	O
to	O
traverse	O
the	O
communication	O
channel	O
56	O
.	O
At	O
remote	O
word	O
estimation	O
element	O
62	O
,	O
the	O
transformed	O
potential	O
features	O
are	O
inverse	O
transformed	O
by	O
inverse	O
transform	O
element	O
60	O
to	O
provide	O
acoustic	O
features	O
to	O
word	O
decoder	O
64	O
which	O
in	O
response	O
provides	O
an	O
estimated	O
word	O
string	O
.	O

An	O
exemplary	O
embodiment	O
of	O
the	O
transform	O
element	O
54	O
is	O
illustrated	O
in	O
FIG	O
.	O
4	O
as	O
transform	O
subsystem	O
70	O
.	O
In	O
FIG	O
.	O
4	O
,	O
the	O
LPC	O
coefficients	O
from	O
acoustic	O
processor	O
52	O
are	O
provided	O
to	O
LPC	O
to	O
LSP	O
transformation	O
element	O
72	O
.	O
Within	O
LPC	O
to	O
LSP	O
element	O
72	O
the	O
LSP	O
coefficients	O
can	O
be	O
determined	O
as	O
follows	O
.	O
For	O
Pth	O
order	O
LPC	O
coefficients	O
,	O
the	O
corresponding	O
LSP	O
frequencies	O
are	O
obtained	O
as	O
the	O
P	O
roots	O
which	O
reside	O
between	O
0	O
and	O
.	O
pi	O
.	O
of	O
the	O
following	O
equations	O
:	O
P	O
(	O
w	O
)	O
=	O
cos	O
5w	O
+	O
p.	O
sub	O
.	O
1	O
cos	O
4w	O
+	O
.	O
.	O
.	O
+	O
p.	O
sub	O
.	O
5	O
/2	O
(	O
7	O
)	O
Q	O
(	O
w	O
)	O
=	O
cos	O
5w	O
+	O
q	O
.	O
sub	O
.	O
1	O
cos	O
4w	O
+	O
.	O
.	O
.	O
+	O
q	O
.	O
sub	O
.	O
5	O
/2	O
(	O
8	O
)	O
where	O
p.	O
sub	O
.	O
i	O
and	O
q	O
.	O
sub	O
.	O
i	O
can	O
be	O
computed	O
recursively	O
as	O
follows	O
:	O
p.	O
sub	O
.	O
0	O
=	O
q	O
.	O
sub	O
.	O
0	O
=	O
1	O
(	O
9	O
)	O
p.	O
sub	O
.	O
i	O
=	O
-	O
a	O
.	O
sub	O
.	O
i	O
-	O
a	O
.	O
sub	O
.	O
P-i	O
-	O
p.sub.i-1,1.ltoreq.i.ltoreq.P/2	O
(	O
10	O
)	O
q	O
.	O
sub	O
.	O
i	O
=	O
-	O
a	O
.	O
sub	O
.	O
i	O
+	O
a	O
.	O
sub	O
.	O
P-i	O
-	O
q.sub.i-1,1.ltoreq.i.ltoreq.P/2	O
(	O
11	O
)	O
The	O
LSP	O
frequencies	O
are	O
provided	O
to	O
DPCM	O
element	O
74	O
where	O
they	O
are	O
encoded	O
for	O
transmission	O
over	O
communication	O
channel	O
76	O
.	O

At	O
inverse	O
transformation	O
element	O
78	O
,	O
the	O
received	O
signal	O
from	O
the	O
channel	O
is	O
passed	O
through	O
inverse	O
DPCM	O
element	O
80	O
and	O
LSP	O
to	O
LPC	O
element	O
82	O
to	O
recover	O
the	O
LSP	O
frequencies	O
of	O
the	O
speech	O
signal	O
.	O
The	O
inverse	O
process	O
of	O
LPC	O
to	O
LSP	O
element	O
72	O
is	O
performed	O
by	O
LSP	O
to	O
LPC	O
element	O
82	O
which	O
converts	O
the	O
LSP	O
frequencies	O
back	O
into	O
LPC	O
coefficients	O
for	O
use	O
in	O
deriving	O
the	O
cepstrum	O
coefficients	O
.	O
LSP	O
to	O
LPC	O
element	O
82	O
performs	O
the	O
conversion	O
as	O
follows	O
:	O
#	O
#	O
EQU5	O
#	O
#	O
The	O
LPC	O
coefficients	O
are	O
then	O
provided	O
to	O
LPC	O
to	O
cepstrum	O
element	O
84	O
which	O
provides	O
the	O
cepstrum	O
coefficients	O
to	O
word	O
decoder	O
64	O
in	O
accordance	O
with	O
equations	O
5	O
and	O
6	O
.	O

Since	O
the	O
word	O
decoder	O
relies	O
solely	O
on	O
an	O
acoustic	O
feature	O
sequence	O
,	O
which	O
can	O
be	O
prone	O
to	O
noise	O
if	O
transmitted	O
directly	O
through	O
the	O
communication	O
channel	O
,	O
a	O
potential	O
acoustic	O
feature	O
sequence	O
is	O
derived	O
and	O
transformed	O
in	O
the	O
subsystem	O
51	O
as	O
depicted	O
in	O
FIG	O
.	O
3	O
into	O
an	O
alternative	O
representation	O
that	O
facilitates	O
transmission	O
.	O
The	O
acoustic	O
feature	O
sequence	O
for	O
use	O
in	O
word	O
decoder	O
is	O
obtained	O
afterwards	O
through	O
inverse	O
transformation	O
.	O
Hence	O
,	O
in	O
distributed	O
implementation	O
of	O
VR	O
,	O
the	O
feature	O
sequence	O
sent	O
through	O
the	O
air	O
(	O
channel	O
)	O
can	O
be	O
different	O
from	O
the	O
one	O
really	O
used	O
in	O
word	O
decoder	O
.	O
It	O
is	O
anticipated	O
that	O
the	O
output	O
from	O
transform	O
element	O
70	O
can	O
be	O
further	O
encoded	O
by	O
any	O
error	O
protection	O
schemes	O
that	O
are	O
known	O
in	O
the	O
art	O
.	O

In	O
FIG	O
.	O
5	O
,	O
an	O
improved	O
embodiment	O
of	O
the	O
present	O
invention	O
is	O
illustrated	O
.	O
In	O
wireless	O
communication	O
applications	O
,	O
users	O
may	O
desire	O
not	O
to	O
occupy	O
the	O
communication	O
channel	O
for	O
a	O
small	O
number	O
of	O
simple	O
,	O
but	O
commonly	O
used	O
voiced	O
commands	O
,	O
in	O
part	O
due	O
to	O
expensive	O
channel	O
access	O
.	O
This	O
can	O
be	O
achieved	O
by	O
further	O
distributing	O
the	O
word	O
decoding	O
function	O
between	O
handset	O
100	O
and	O
base	O
station	O
110	O
in	O
the	O
sense	O
that	O
a	O
voice	O
recognition	O
with	O
a	O
relatively	O
small	O
vocabulary	O
size	O
is	O
implemented	O
locally	O
at	O
handset	O
while	O
a	O
second	O
voice	O
recognition	O
system	O
with	O
larger	O
vocabulary	O
size	O
resides	O
remotely	O
at	O
base	O
station	O
.	O
They	O
both	O
share	O
the	O
same	O
acoustic	O
processor	O
at	O
handset	O
.	O
The	O
vocabulary	O
table	O
in	O
local	O
word	O
decoder	O
contains	O
most	O
widely	O
used	O
words	O
or	O
word	O
strings	O
.	O
The	O
vocabulary	O
table	O
in	O
remote	O
word	O
decoder	O
,	O
on	O
the	O
other	O
hand	O
,	O
contains	O
regular	O
words	O
or	O
word	O
strings	O
.	O
Based	O
on	O
this	O
infrastructure	O
,	O
as	O
illustrated	O
in	O
FIG	O
.	O
5	O
,	O
the	O
average	O
time	O
that	O
channel	O
is	O
busy	O
may	O
be	O
lessened	O
and	O
the	O
average	O
recognition	O
accuracy	O
increased	O
.	O

Moreover	O
,	O
there	O
will	O
exist	O
two	O
groups	O
of	O
voiced	O
commands	O
available	O
,	O
one	O
,	O
called	O
special	O
voiced	O
command	O
,	O
corresponds	O
to	O
the	O
commands	O
recognizable	O
by	O
local	O
VR	O
and	O
the	O
other	O
,	O
called	O
regular	O
voiced	O
command	O
,	O
corresponds	O
to	O
those	O
not	O
recognized	O
by	O
the	O
local	O
VR	O
.	O
Whenever	O
a	O
special	O
voiced	O
command	O
is	O
issued	O
,	O
the	O
real	O
acoustic	O
features	O
are	O
extracted	O
for	O
local	O
word	O
decoder	O
and	O
voice	O
recognition	O
function	O
is	O
performed	O
locally	O
without	O
accessing	O
communication	O
channel	O
.	O
When	O
a	O
regular	O
voiced	O
command	O
is	O
issued	O
,	O
transformed	O
acoustic	O
feature	O
vectors	O
are	O
transmitted	O
through	O
channel	O
and	O
word	O
decoding	O
is	O
done	O
remotely	O
at	O
base	O
station	O
.	O

Since	O
the	O
acoustic	O
features	O
need	O
not	O
be	O
transformed	O
,	O
nor	O
be	O
coded	O
,	O
for	O
any	O
special	O
voiced	O
command	O
and	O
vocabulary	O
size	O
is	O
small	O
for	O
local	O
VR	O
,	O
the	O
required	O
computation	O
will	O
be	O
much	O
less	O
than	O
that	O
of	O
the	O
remote	O
one	O
(	O
the	O
computation	O
associated	O
with	O
the	O
search	O
for	O
correct	O
word	O
string	O
over	O
possible	O
vocabularies	O
is	O
proportional	O
to	O
vocabulary	O
size	O
)	O
.	O
Additionally	O
,	O
the	O
local	O
voice	O
recognizer	O
may	O
be	O
modeled	O
with	O
a	O
simplified	O
version	O
of	O
HMM	O
(	O
such	O
as	O
with	O
a	O
lower	O
number	O
of	O
states	O
,	O
a	O
lower	O
number	O
of	O
mixture	O
components	O
for	O
state	O
output	O
probabilities	O
,	O
etc	O
.	O
)	O
compared	O
to	O
remote	O
VR	O
,	O
since	O
the	O
acoustic	O
feature	O
will	O
be	O
fed	O
directly	O
to	O
local	O
VR	O
without	O
potential	O
corruption	O
in	O
the	O
channel	O
.	O
This	O
will	O
enable	O
a	O
local	O
,	O
though	O
,	O
limited	O
vocabulary	O
,	O
implementation	O
of	O
VR	O
at	O
the	O
handset	O
where	O
the	O
computational	O
load	O
is	O
limited	O
.	O
It	O
is	O
envisioned	O
that	O
the	O
distributed	O
VR	O
structure	O
can	O
also	O
be	O
used	O
in	O
other	O
target	O
applications	O
different	O
than	O
wireless	O
communication	O
systems	O
.	O

Referring	O
to	O
FIG	O
.	O
5	O
,	O
speech	O
signals	O
are	O
provide	O
to	O
acoustic	O
processor	O
102	O
,	O
which	O
then	O
extracts	O
features	O
,	O
for	O
example	O
LPC	O
based	O
feature	O
parameters	O
,	O
from	O
the	O
speech	O
signal	O
.	O
These	O
features	O
are	O
then	O
provided	O
to	O
local	O
word	O
decoder	O
106	O
which	O
searches	O
to	O
recognize	O
the	O
input	O
speech	O
signal	O
from	O
its	O
small	O
vocabulary	O
.	O
If	O
it	O
fails	O
to	O
decode	O
the	O
input	O
word	O
string	O
and	O
determines	O
that	O
remote	O
VR	O
should	O
decode	O
it	O
,	O
it	O
signals	O
transform	O
element	O
104	O
which	O
prepares	O
the	O
features	O
for	O
transmission	O
.	O
The	O
transformed	O
features	O
are	O
then	O
transmitted	O
over	O
communication	O
channel	O
108	O
to	O
remote	O
word	O
decoder	O
110	O
.	O
The	O
transformed	O
features	O
are	O
received	O
at	O
inverse	O
transform	O
element	O
112	O
,	O
which	O
performs	O
the	O
inverse	O
operation	O
of	O
transform	O
element	O
104	O
and	O
provides	O
the	O
acoustic	O
features	O
to	O
remote	O
word	O
decoder	O
element	O
114	O
which	O
in	O
response	O
provides	O
the	O
estimate	O
remote	O
word	O
string	O
.	O

The	O
previous	O
description	O
of	O
the	O
preferred	O
embodiments	O
is	O
provided	O
to	O
enable	O
any	O
person	O
skilled	O
in	O
the	O
art	O
to	O
make	O
or	O
use	O
the	O
present	O
invention	O
.	O
The	O
various	O
modifications	O
to	O
these	O
embodiments	O
will	O
be	O
readily	O
apparent	O
to	O
those	O
skilled	O
in	O
the	O
art	O
,	O
and	O
the	O
generic	O
principles	O
defined	O
herein	O
may	O
be	O
applied	O
to	O
other	O
embodiments	O
without	O
the	O
use	O
of	O
the	O
inventive	O
faculty	O
.	O
Thus	O
,	O
the	O
present	O
invention	O
is	O
not	O
intended	O
to	O
be	O
limited	O
to	O
the	O
embodiments	O
shown	O
herein	O
but	O
is	O
to	O
be	O
accorded	O
the	O
widest	O
scope	O
consistent	O
with	O
the	O
principles	O
and	O
novel	O
features	O
disclosed	O
herein	O
.	O

1	O
.	O
A	O
remote	O
station	O
for	O
use	O
in	O
a	O
mobile	O
communications	O
system	O
,	O
comprising	O
:	O
feature	O
extraction	O
means	O
located	O
at	O
the	O
remote	O
station	O
for	O
receiving	O
a	O
frame	O
of	O
speech	O
samples	O
and	O
extracting	O
a	O
set	O
of	O
parameters	O
for	O
speech	O
recognition	O
;	O
first	O
word	O
decoder	O
means	O
for	O
receiving	O
said	O
set	O
of	O
parameters	O
and	O
for	O
extracting	O
the	O
meaning	O
of	O
said	O
speech	O
from	O
said	O
parameters	O
in	O
accordance	O
with	O
a	O
small	O
vocabulary	O
;	O
and	O
transmission	O
means	O
for	O
wirelessly	O
transmitting	O
a	O
set	O
of	O
parameters	O
that	O
cannot	O
be	O
decoded	O
by	O
said	O
first	O
word	O
decoder	O
means	O
to	O
a	O
receiving	O
station	O
having	O
second	O
word	O
decoder	O
means	O
for	O
extracting	O
the	O
meaning	O
of	O
said	O
speech	O
from	O
the	O
transmitted	O
parameters	O
in	O
accordance	O
with	O
a	O
larger	O
vocabulary	O
.	O

2	O
.	O
The	O
remote	O
station	O
of	O
claim	O
1	O
,	O
further	O
comprising	O
a	O
microphone	O
for	O
receiving	O
an	O
acoustical	O
signal	O
and	O
for	O
providing	O
said	O
acoustical	O
signal	O
to	O
said	O
feature	O
extraction	O
means	O
.	O

3	O
.	O
The	O
remote	O
station	O
of	O
claim	O
1	O
,	O
further	O
comprising	O
transform	O
means	O
interposed	O
between	O
said	O
feature	O
extraction	O
means	O
and	O
said	O
transmitter	O
means	O
for	O
receiving	O
said	O
set	O
of	O
parameters	O
and	O
for	O
transforming	O
said	O
set	O
of	O
parameters	O
into	O
an	O
alternative	O
representation	O
of	O
said	O
set	O
of	O
parameters	O
in	O
accordance	O
with	O
a	O
predetermined	O
transformation	O
format	O
.	O

4	O
.	O
The	O
remote	O
station	O
of	O
claim	O
1	O
,	O
wherein	O
said	O
set	O
of	O
parameters	O
comprises	O
linear	O
prediction	O
coefficients	O
.	O

5	O
.	O
The	O
remote	O
station	O
of	O
claim	O
1	O
,	O
wherein	O
said	O
set	O
of	O
parameters	O
comprises	O
line	O
spectral	O
pair	O
values	O
.	O

6	O
.	O
The	O
remote	O
station	O
of	O
claim	O
3	O
,	O
wherein	O
said	O
set	O
of	O
parameters	O
comprises	O
linear	O
prediction	O
coefficients	O
and	O
said	O
predetermined	O
transformation	O
format	O
is	O
a	O
linear	O
prediction	O
coefficient	O
to	O
line	O
spectral	O
pair	O
transformation	O
.	O

7	O
.	O
The	O
remote	O
station	O
of	O
claim	O
1	O
,	O
further	O
comprising	O
receiver	O
means	O
for	O
receiving	O
a	O
response	O
signal	O
in	O
accordance	O
with	O
a	O
speech	O
recognition	O
operation	O
upon	O
said	O
frame	O
of	O
speech	O
.	O

8	O
.	O
The	O
remote	O
station	O
of	O
claim	O
7	O
,	O
further	O
comprising	O
control	O
means	O
for	O
receiving	O
said	O
response	O
signal	O
and	O
for	O
providing	O
a	O
control	O
signal	O
in	O
accordance	O
with	O
said	O
response	O
signal	O
.	O

9	O
.	O
The	O
remote	O
station	O
of	O
claim	O
1	O
,	O
further	O
comprising	O
a	O
transform	O
element	O
interposed	O
between	O
and	O
having	O
an	O
input	O
coupled	O
to	O
an	O
output	O
of	O
said	O
feature	O
extraction	O
means	O
and	O
having	O
an	O
output	O
coupled	O
to	O
an	O
input	O
of	O
said	O
transmission	O
means	O
.	O

10	O
.	O
The	O
remote	O
station	O
of	O
claim	O
7	O
,	O
further	O
comprising	O
a	O
control	O
element	O
having	O
an	O
input	O
coupled	O
to	O
said	O
receiver	O
output	O
and	O
having	O
an	O
output	O
for	O
providing	O
a	O
control	O
signal	O
in	O
accordance	O
with	O
said	O
response	O
signal	O
.	O

11	O
.	O
A	O
central	O
communications	O
station	O
for	O
use	O
in	O
a	O
mobile	O
communications	O
system	O
,	O
comprising	O
:	O
a	O
word	O
decoder	O
located	O
at	O
said	O
central	O
communications	O
station	O
for	O
receiving	O
a	O
set	O
of	O
speech	O
parameters	O
from	O
a	O
remote	O
station	O
physically	O
separated	O
from	O
said	O
central	O
communications	O
station	O
and	O
communicating	O
therewith	O
by	O
wireless	O
communications	O
means	O
,	O
for	O
performing	O
a	O
speech	O
recognition	O
operation	O
on	O
said	O
set	O
of	O
speech	O
parameters	O
using	O
a	O
regular	O
vocabulary	O
associated	O
with	O
said	O
word	O
decoder	O
located	O
at	O
said	O
central	O
communications	O
station	O
,	O
wherein	O
said	O
speech	O
parameters	O
are	O
not	O
recognizable	O
by	O
a	O
local	O
vocabulary	O
associated	O
with	O
a	O
word	O
decoder	O
located	O
in	O
a	O
remote	O
station	O
;	O
and	O
a	O
signal	O
generator	O
for	O
generating	O
a	O
response	O
signal	O
based	O
on	O
a	O
result	O
of	O
said	O
speech	O
recognition	O
operation	O
.	O

12	O
.	O
The	O
central	O
communications	O
station	O
of	O
claim	O
11	O
,	O
further	O
comprising	O
a	O
receiver	O
having	O
an	O
input	O
for	O
receiving	O
a	O
remote	O
station	O
speech	O
parameter	O
signal	O
and	O
for	O
providing	O
said	O
remote	O
station	O
speech	O
parameters	O
to	O
said	O
word	O
decoder	O
means	O
.	O

13	O
.	O
The	O
central	O
communications	O
station	O
of	O
claim	O
11	O
,	O
further	O
comprising	O
control	O
means	O
having	O
an	O
input	O
coupled	O
to	O
said	O
word	O
decoder	O
output	O
and	O
having	O
an	O
output	O
for	O
providing	O
a	O
control	O
signal	O
.	O

14	O
.	O
A	O
distributed	O
voice	O
recognition	O
system	O
,	O
comprising	O
:	O
a	O
local	O
word	O
decoder	O
located	O
at	O
a	O
subscriber	O
station	O
for	O
receiving	O
extracted	O
acoustic	O
features	O
of	O
a	O
first	O
frame	O
of	O
speech	O
samples	O
and	O
for	O
decoding	O
said	O
acoustic	O
features	O
in	O
accordance	O
with	O
a	O
small	O
vocabulary	O
;	O
and	O
a	O
remote	O
word	O
decoder	O
located	O
at	O
a	O
central	O
processing	O
station	O
physically	O
separated	O
from	O
said	O
subscriber	O
station	O
for	O
receiving	O
extracted	O
acoustic	O
features	O
of	O
a	O
second	O
frame	O
and	O
for	O
decoding	O
,	O
in	O
accordance	O
with	O
a	O
regular	O
vocabulary	O
,	O
larger	O
than	O
said	O
small	O
vocabulary	O
,	O
said	O
acoustic	O
features	O
of	O
said	O
second	O
frame	O
which	O
cannot	O
be	O
decoded	O
by	O
said	O
local	O
word	O
decoder	O
.	O

15	O
.	O
The	O
system	O
of	O
claim	O
14	O
further	O
comprising	O
a	O
preprocessor	O
for	O
extracting	O
said	O
acoustic	O
features	O
of	O
said	O
frame	O
of	O
speech	O
samples	O
in	O
accordance	O
with	O
a	O
predetermined	O
feature	O
extraction	O
format	O
and	O
for	O
providing	O
said	O
acoustic	O
features	O
.	O

16	O
.	O
The	O
system	O
of	O
claim	O
15	O
wherein	O
said	O
acoustic	O
features	O
are	O
linear	O
predictive	O
coding	O
(	O
LPC	O
)	O
based	O
parameters	O
.	O

17	O
.	O
The	O
system	O
of	O
claim	O
15	O
wherein	O
said	O
acoustic	O
features	O
are	O
cepstral	O
coefficients	O
.	O

18	O
.	O
The	O
system	O
of	O
claim	O
15	O
wherein	O
said	O
preprocessor	O
comprises	O
a	O
voice	O
coder	O
(	O
vocoder	O
)	O
.	O

19	O
.	O
The	O
system	O
of	O
claim	O
18	O
wherein	O
said	O
vocoder	O
is	O
a	O
code	O
excited	O
linear	O
prediction	O
(	O
CELP	O
)	O
vocoder	O
.	O

20	O
.	O
The	O
system	O
of	O
claim	O
18	O
wherein	O
said	O
vocoder	O
is	O
a	O
linear	O
predictive	O
coding	O
(	O
LPC	O
)	O
based	O
vocoder	O
.	O

21	O
.	O
The	O
system	O
of	O
claim	O
18	O
wherein	O
said	O
vocoder	O
is	O
a	O
multi-band	O
excitation	O
(	O
MBE	O
)	O
based	O
vocoder	O
.	O

22	O
.	O
The	O
system	O
of	O
claim	O
18	O
wherein	O
said	O
vocoder	O
is	O
an	O
ADPCM	O
vocoder	O
.	O

23	O
.	O
The	O
system	O
of	O
claim	O
14	O
further	O
comprising	O
:	O
a	O
transform	O
element	O
located	O
at	O
said	O
subscriber	O
station	O
for	O
receiving	O
said	O
acoustic	O
features	O
and	O
for	O
converting	O
said	O
acoustic	O
features	O
into	O
transformed	O
features	O
in	O
accordance	O
with	O
a	O
predetermined	O
transform	O
format	O
,	O
said	O
transformed	O
features	O
transmitted	O
through	O
a	O
communication	O
channel	O
to	O
said	O
central	O
processing	O
station	O
;	O
and	O
an	O
inverse	O
transform	O
element	O
located	O
at	O
said	O
central	O
processing	O
station	O
for	O
receiving	O
said	O
transformed	O
features	O
and	O
for	O
converting	O
said	O
transformed	O
features	O
into	O
estimated	O
acoustic	O
features	O
in	O
accordance	O
with	O
a	O
predetermined	O
inverse	O
transform	O
format	O
,	O
and	O
for	O
providing	O
said	O
estimated	O
acoustic	O
features	O
to	O
said	O
remote	O
word	O
decoder	O
.	O

24	O
.	O
The	O
system	O
of	O
claim	O
23	O
wherein	O
said	O
acoustic	O
features	O
are	O
linear	O
predictive	O
coding	O
(	O
LPC	O
)	O
based	O
parameters	O
,	O
said	O
predetermined	O
transform	O
format	O
converts	O
said	O
LPC	O
based	O
parameters	O
to	O
line	O
spectral	O
pair	O
(	O
LSP	O
)	O
frequencies	O
,	O
and	O
said	O
inverse	O
transform	O
format	O
converts	O
said	O
LSP	O
frequencies	O
into	O
LPC	O
based	O
parameters	O
.	O

25	O
.	O
The	O
system	O
of	O
claim	O
14	O
wherein	O
said	O
local	O
word	O
decoder	O
performs	O
acoustic	O
pattern	O
matching	O
based	O
on	O
a	O
hidden	O
Markov	O
model	O
(	O
HMM	O
)	O
.	O

26	O
.	O
The	O
system	O
of	O
claim	O
14	O
wherein	O
said	O
remote	O
word	O
decoder	O
performs	O
acoustic	O
pattern	O
matching	O
based	O
on	O
a	O
hidden	O
Markov	O
model	O
(	O
HMM	O
)	O
.	O

27	O
.	O
The	O
system	O
of	O
claim	O
14	O
wherein	O
said	O
local	O
word	O
decoder	O
performs	O
acoustic	O
pattern	O
matching	O
based	O
on	O
Dynamic	O
Time	O
Warping	O
(	O
DTW	O
)	O
.	O

28	O
.	O
The	O
system	O
of	O
claim	O
14	O
wherein	O
said	O
remote	O
word	O
decoder	O
performs	O
acoustic	O
pattern	O
matching	O
based	O
on	O
Dynamic	O
Time	O
Warping	O
(	O
DTW	O
)	O
.	O

29	O
.	O
The	O
system	O
of	O
claim	O
14	O
wherein	O
said	O
subscriber	O
station	O
communicates	O
with	O
said	O
central	O
processing	O
station	O
by	O
wireless	O
communication	O
means	O
.	O

30	O
.	O
The	O
system	O
of	O
claim	O
29	O
wherein	O
said	O
wireless	O
communication	O
means	O
comprises	O
a	O
CDMA	O
communication	O
system	O
.	O

