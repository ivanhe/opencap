US	O
6381572	O
B1	O
20020430	O

US	O
09288973	O
19990409	O

09	O
eng	O
eng	O

JP	O
10099051	O
19980410	O

20020430	O

20020430	O

7G	O
10L	O
17/00	O
A	O
7	O
G	O
10	O
L	O
17	O
00	O
A	O

G10L	O
15/00	O
20060101C	O
I20051008RMEP	O

20060101	O

C	O
G	O
10	O
L	O
15	O
00	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/02	O
20060101ALI20060310RMJP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
02	O
L	O
I	O

20060310	O

JP	O

R	O
M	O

G10L	O
15/20	O
20060101A	O
I20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
20	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
21/00	O
20060101CLI20060310RMJP	O

20060101	O

C	O
G	O
10	O
L	O
21	O
00	O
L	O
I	O

20060310	O

JP	O

R	O
M	O

G10L	O
21/02	O
20060101ALI20060310RMJP	O

20060101	O

A	O
G	O
10	O
L	O
21	O
02	O
L	O
I	O

20060310	O

JP	O

R	O
M	O

US	O

704/246	O
704	O
246	O

704/250	O
704	O
250	O

704/E15.039	O
704	O
E15	O
.	O
039	O

G10L	O
15/20	O
G	O
10	O
L	O
15	O
20	O

S10L	O
21	O
:	O
02M1	O
S	O
10	O
L	O
21	O
02	O

M	O
1	O

US	O

704/246	O
704	O
246	O

US	O

704/250	O
704	O
250	O

12	O
Method	O
of	O
modifying	O
feature	O
parameter	O
for	O
speech	O
recognition	O
,	O
method	O
of	O
speech	O
recognition	O
and	O
speech	O
recognition	O
apparatus	O

US	O
5293588	O
A	O
Satoh	O
et_al.	O

19940308	O

19910409	O

US	O
5717820	O
A	O
Hamasaki	O
et_al.	O

19980210	O

19950307	O

US	O
5793891	O
A	O
Takahashi	O
et_al.	O

19980811	O

19950703	O

US	O
5794194	O
A	O
Takebayashi	O
et_al.	O

19980811	O

19970203	O

US	O
5864809	O
A	O
Suzuki	O
19990126	O

19971023	O

US	O
5960395	O
A	O
Tzirkel-Hancock	O
19990928	O

19970206	O

US	O
6006175	O
A	O
Holzrichter	O
et_al.	O

19991221	O

19960206	O

US	O
6263308	O
B1	O
Heckerman	O
et_al.	O

20010717	O

20000320	O

Han	B-Citation
et	I-Citation
al	I-Citation
,	I-Citation
Discriminative	I-Citation
Learning	I-Citation
of	I-Citation
Additive	I-Citation
Noise	I-Citation
and	I-Citation
Channel	I-Citation
Distortion	I-Citation
for	I-Citation
Robust	I-Citation
Speech	I-Citation
Recognition	I-Citation
,	I-Citation
pp	I-Citation
81	I-Citation
-	I-Citation
84	I-Citation
,	I-Citation
IEEE	I-Citation
1998	I-Citation
.	O
*	O

Vaseghi	B-Citation
et	I-Citation
al	I-Citation
,	I-Citation
“	I-Citation
Noise	I-Citation
Compensation	I-Citation
Methods	I-Citation
for	I-Citation
Hidden	I-Citation
Markov	I-Citation
Model	I-Citation
Speech	I-Citation
Recognition	I-Citation
in	I-Citation
Adverse	I-Citation
Environments	I-Citation
”	I-Citation
,	I-Citation
1997	I-Citation
IEEE	I-Citation
,	I-Citation
pp11-21	I-Citation
.	O
*	O

Hwang	B-Citation
et	I-Citation
al	I-Citation
,	I-Citation
“	I-Citation
Feature	I-Citation
Adaptation	I-Citation
Using	I-Citation
Deviation	I-Citation
Vector	I-Citation
for	I-Citation
Robust	I-Citation
Speech	I-Citation
Recognition	I-Citation
in	I-Citation
Noisy	I-Citation
Environments	I-Citation
”	I-Citation
,	I-Citation
1997	I-Citation
IEEE	I-Citation
,	I-Citation
pp	I-Citation
1227	I-Citation
-	I-Citation
1230	I-Citation
.	O
*	O

Openshaw	B-Citation
,	I-Citation
“	I-Citation
Reducing	I-Citation
the	I-Citation
Environmental	I-Citation
Sensitivity	I-Citation
of	I-Citation
Cepstral	I-Citation
Features	I-Citation
for	I-Citation
Speaker	I-Citation
Recognition	I-Citation
”	I-Citation
,	I-Citation
pp721-724	I-Citation
,	I-Citation
ICSP	I-Citation
1996	I-Citation
.	O
*	O

Vaseghi	B-Citation
,	I-Citation
“	I-Citation
Speech	I-Citation
Modelling	I-Citation
using	I-Citation
Cepstral-Time	I-Citation
Feature	I-Citation
Matrices	I-Citation
in	I-Citation
Hidden	I-Citation
Markov	I-Citation
Models	I-Citation
”	I-Citation
,	I-Citation
IEE	I-Citation
Proceedings	I-Citation
,	I-Citation
1993	I-Citation
,	I-Citation
pp317-320	I-Citation
.	O

US	O
7191105	O
B2	O
20070313	O

20030122	O

WO	O
2009136953	O
A1	O
20091112	O

20080609	O

US	O
7369991	O
B2	O
20080506	O

20030304	O

US	O
7321853	O
B2	O
20080122	O

20060224	O

US	O
7089177	O
B2	O
20060808	O

20050803	O

US	O
6999924	O
B2	O
20060214	O

20020711	O

Pioneer	O
Electronic	O
Corporation	O
03	O

Tokyo-to	O
JP	O

Ishimitsu	O
,	O
Shunsuke	O

Kawagoe	O
JP	O

Fujita	O
,	O
Ikuo	O

Kawagoe	O
JP	O

Finnegan	O
,	O
Henderson	O
,	O
Farabow	O
,	O
Garrett	O
&	O
Dunner	O
,	O
L.	O
L.	O
P.	O

Tsang	O
Fan	O
3645	O

Opsasnick	O
Michael	O
N.	O

US	O
6381572	O
B1	O
20020430	O

19990409	O

JP	O
11296192	O
A	O
19991029	O

19980410	O

JP	O
11296192	O
A	O
19991029	O

19980410	O

US	O
6381572	O
B1	O
20020430	O

19990409	O

A	O
method	O
of	O
modifying	O
feature	O
parameters	O
for	O
a	O
speech	O
recognition	O
is	O
provided	O
.	O
This	O
method	O
is	O
provided	O
with	O
:	O
a	O
process	O
of	O
extracting	O
the	O
feature	O
parameter	O
from	O
an	O
input	O
speech	O
in	O
a	O
real	O
environment	O
;	O
a	O
process	O
of	O
reading	O
a	O
first	O
speech	O
transfer	O
characteristic	O
corresponding	O
to	O
an	O
environment	O
in	O
which	O
a	O
reference	O
pattern	O
for	O
the	O
speech	O
recognition	O
is	O
generated	O
,	O
from	O
a	O
first	O
memory	O
device	O
;	O
a	O
process	O
of	O
reading	O
a	O
second	O
speech	O
transfer	O
characteristic	O
corresponding	O
to	O
the	O
real	O
environment	O
from	O
a	O
second	O
memory	O
device	O
;	O
a	O
process	O
of	O
modifying	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
to	O
convert	O
the	O
extracted	O
feature	O
parameter	O
corresponding	O
to	O
the	O
real	O
environment	O
into	O
a	O
modified	O
feature	O
parameter	O
corresponding	O
to	O
the	O
environment	O
in	O
which	O
the	O
reference	O
pattern	O
is	O
generated	O
.	O

19990409	O

AS	O
ASSIGNMENT	O
N	O
US	O
6381572B1	O
PIONEER	O
ELECTRONIC	O
CORPORATION	O
,	O
JAPAN	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNORS:ISHIMITSU	O
,	O
SHUNSUKE;FUJITA	O
,	O
IKUTO;REEL/FRAME:009894/0327	O

19990406	O

20051007	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
6381572B1	O
4	O

20091207	O

REMI	O
MAINTENANCE	O
FEE	O
REMINDER	O
MAILED	O
N	O
US	O
6381572B1	O

20100430	O

LAPS	O
-	O
LAPSE	O
FOR	O
FAILURE	O
TO	O
PAY	O
MAINTENANCE	O
FEES	O
N	O
US	O
6381572B1	O

20100622	O

FP	O
-	O
EXPIRED	O
DUE	O
TO	O
FAILURE	O
TO	O
PAY	O
MAINTENANCE	O
FEE	O
N	O
US	O
6381572B1	O

20100430	O

BACKGROUND	O
OF	O
THE	O
INVENTION	O
1	O
.	O
Field	O
of	O
the	O
Invention	O
The	O
present	O
invention	O
relates	O
to	O
a	O
speech	O
recognition	O
apparatus	O
and	O
method	O
.	O
More	O
specifically	O
,	O
the	O
present	O
invention	O
relates	O
to	O
a	O
method	O
of	O
modifying	O
feature	O
parameters	O
of	O
an	O
input	O
speech	O
for	O
a	O
speech	O
recognition	O
.	O

2	O
.	O
Description	O
of	O
the	O
Related	O
Art	O
Speech	O
recognition	O
apparatuses	O
which	O
recognize	O
an	O
input	O
speech	O
are	O
well	O
known	O
.	O
A	O
typical	O
speech	O
recognition	O
apparatus	O
are	O
usually	O
mounted	O
in	O
a	O
car	O
,	O
or	O
installed	O
in	O
a	O
telephone	O
.	O
Therefore	O
,	O
it	O
works	O
in	O
such	O
various	O
environments	O
.	O
So	O
the	O
speech	O
recognition	O
apparatuses	O
are	O
needed	O
to	O
work	O
with	O
a	O
good	O
performance	O
in	O
such	O
various	O
environments	O
.	O

In	O
such	O
various	O
environments	O
,	O
a	O
speech	O
produced	O
by	O
a	O
speaker	O
is	O
distorted	O
while	O
it	O
is	O
transferred	O
to	O
an	O
input	O
device	O
of	O
the	O
speech	O
recognition	O
apparatus	O
,	O
because	O
of	O
a	O
variety	O
of	O
sound	O
fields	O
characteristics	O
or	O
fluctuation	O
of	O
telephone	O
line	O
characteristics	O
On	O
the	O
other	O
hand	O
,	O
a	O
transfer	O
function	O
corresponding	O
to	O
registered	O
patterns	O
in	O
a	O
dictionary	O
for	O
the	O
speech	O
recognition	O
is	O
effective	O
in	O
an	O
ideal	O
environment	O
in	O
which	O
the	O
dictionary	O
is	O
made	O
.	O
But	O
,	O
it	O
is	O
not	O
so	O
effective	O
in	O
a	O
real	O
environment	O
.	O
Therefore	O
,	O
the	O
accuracy	O
of	O
the	O
speech	O
recognition	O
is	O
degrading	O
because	O
of	O
a	O
mismatch	O
between	O
the	O
transfer	O
function	O
in	O
a	O
real	O
environment	O
and	O
the	O
transfer	O
function	O
in	O
an	O
ideal	O
environment	O
.	O

SUMMARY	O
OF	O
THE	O
INVENTION	O
It	O
is	O
therefore	O
an	O
object	O
of	O
the	O
present	O
invention	O
to	O
provide	O
a	O
speech	O
recognition	O
method	O
and	O
apparatus	O
,	O
which	O
can	O
recognize	O
a	O
speech	O
in	O
a	O
real	O
environment	O
without	O
great	O
degradation	O
of	O
an	O
accuracy	O
of	O
the	O
speech	O
recognition	O
.	O

The	O
above	O
object	O
can	O
be	O
achieved	O
by	O
a	O
first	O
method	O
of	O
modifying	O
feature	O
parameter	O
for	O
the	O
speech	O
recognition	O
according	O
to	O
one	O
aspect	O
of	O
the	O
present	O
invention	O
.	O
This	O
invention	O
is	O
provided	O
with	O
:	O
a	O
process	O
of	O
extracting	O
the	O
feature	O
parameter	O
from	O
an	O
input	O
speech	O
in	O
a	O
real	O
environment	O
;	O
a	O
process	O
of	O
reading	O
a	O
first	O
speech	O
transfer	O
characteristic	O
corresponding	O
to	O
an	O
environment	O
in	O
which	O
a	O
reference	O
pattern	O
for	O
the	O
speech	O
recognition	O
is	O
generated	O
,	O
from	O
a	O
first	O
memory	O
device	O
;	O
a	O
process	O
of	O
reading	O
a	O
second	O
speech	O
transfer	O
characteristic	O
corresponding	O
to	O
the	O
real	O
environment	O
from	O
a	O
second	O
memory	O
device	O
;	O
and	O
a	O
process	O
of	O
modifying	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
to	O
convert	O
the	O
extracted	O
feature	O
parameter	O
corresponding	O
to	O
the	O
real	O
environment	O
into	O
a	O
modified	O
feature	O
parameter	O
corresponding	O
to	O
the	O
environment	O
in	O
which	O
the	O
reference	O
pattern	O
is	O
generated	O
.	O

According	O
to	O
the	O
method	O
of	O
modifying	O
feature	O
parameter	O
for	O
the	O
speech	O
recognition	O
of	O
the	O
present	O
invention	O
,	O
a	O
speech	O
spoken	O
by	O
a	O
speaker	O
is	O
inputted	O
into	O
a	O
speech	O
recognition	O
apparatus	O
,	O
and	O
a	O
feature	O
parameter	O
is	O
extracted	O
.	O
This	O
extracted	O
feature	O
parameter	O
,	O
which	O
includes	O
distortion	O
resulted	O
from	O
a	O
speech	O
transfer	O
characteristic	O
,	O
are	O
modified	O
according	O
to	O
a	O
first	O
speech	O
transfer	O
characteristic	O
,	O
which	O
corresponds	O
to	O
an	O
environment	O
in	O
which	O
the	O
reference	O
pattern	O
for	O
the	O
speech	O
recognition	O
is	O
generated	O
,	O
and	O
a	O
second	O
speech	O
transfer	O
characteristic	O
,	O
which	O
corresponds	O
to	O
a	O
real	O
environment	O
.	O
Then	O
,	O
a	O
modified	O
feature	O
parameter	O
,	O
which	O
corresponds	O
to	O
the	O
environment	O
in	O
which	O
the	O
reference	O
pattern	O
is	O
made	O
,	O
is	O
generated	O
.	O
Therefore	O
,	O
distortion	O
of	O
the	O
feature	O
parameter	O
caused	O
by	O
a	O
real	O
environment	O
can	O
be	O
removed	O
,	O
and	O
modified	O
feature	O
parameter	O
,	O
which	O
can	O
contribute	O
to	O
an	O
improvement	O
of	O
an	O
accuracy	O
of	O
the	O
speech	O
recognition	O
,	O
are	O
derived	O
.	O

According	O
to	O
another	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
in	O
the	O
above-stated	O
first	O
method	O
,	O
the	O
feature	O
parameter	O
may	O
be	O
expressed	O
in	O
a	O
frequency	O
domain	O
,	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
may	O
be	O
a	O
first	O
transfer	O
function	O
and	O
a	O
second	O
transfer	O
function	O
in	O
the	O
frequency	O
domain	O
respectively	O
,	O
and	O
the	O
modifying	O
process	O
may	O
modify	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
a	O
formula	O
:	O
F	O
·	O
C1/C2where	O
the	O
F	O
is	O
the	O
extracted	O
feature	O
parameter	O
,	O
the	O
C1is	O
the	O
first	O
transfer	O
function	O
,	O
and	O
the	O
C2is	O
the	O
second	O
transfer	O
function	O
.	O

In	O
this	O
aspect	O
,	O
the	O
feature	O
parameters	O
as	O
a	O
target	O
of	O
the	O
modifying	O
process	O
are	O
expressed	O
in	O
a	O
frequency	O
domain	O
,	O
and	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
are	O
expressed	O
as	O
transfer	O
functions	O
in	O
the	O
frequency	O
domain	O
respectively	O
.	O
The	O
modification	O
of	O
the	O
extracted	O
feature	O
parameter	O
is	O
performed	O
with	O
a	O
multiplication	O
and	O
a	O
division	O
using	O
these	O
two	O
transfer	O
functions	O
.	O
Therefore	O
,	O
an	O
influence	O
of	O
the	O
real	O
environment	O
can	O
be	O
removed	O
,	O
and	O
a	O
processing	O
time	O
can	O
be	O
reduced	O
compared	O
with	O
using	O
the	O
feature	O
parameter	O
expressed	O
in	O
a	O
time	O
domain	O
.	O

According	O
to	O
further	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
in	O
the	O
above-stated	O
first	O
method	O
,	O
the	O
feature	O
parameter	O
may	O
be	O
expressed	O
in	O
a	O
cepstrum	O
domain	O
,	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
may	O
be	O
a	O
first	O
transfer	O
function	O
and	O
a	O
second	O
transfer	O
function	O
in	O
the	O
cepstrum	O
domain	O
respectively	O
,	O
and	O
the	O
modifying	O
process	O
may	O
modify	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
a	O
formula	O
:	O
F	O
+	O
C1	O
−	O
C2	O
,	O
where	O
the	O
F	O
is	O
the	O
extracted	O
feature	O
parameter	O
,	O
the	O
C1is	O
the	O
first	O
transfer	O
function	O
,	O
and	O
the	O
C2is	O
the	O
second	O
transfer	O
function	O
.	O

In	O
this	O
aspect	O
,	O
the	O
feature	O
parameter	O
as	O
a	O
target	O
of	O
the	O
modifying	O
process	O
is	O
expressed	O
in	O
a	O
cepstrum	O
domain	O
,	O
and	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
are	O
expressed	O
as	O
transfer	O
functions	O
in	O
the	O
cepstrum	O
domain	O
respectively	O
.	O
The	O
modification	O
of	O
the	O
extracted	O
feature	O
parameter	O
is	O
performed	O
with	O
an	O
addition	O
and	O
a	O
subtraction	O
using	O
these	O
two	O
transfer	O
functions	O
.	O
Therefore	O
,	O
an	O
influence	O
of	O
the	O
real	O
environment	O
can	O
be	O
removed	O
,	O
and	O
a	O
processing	O
time	O
can	O
be	O
reduced	O
compared	O
with	O
using	O
the	O
feature	O
parameter	O
expressed	O
in	O
a	O
time	O
domain	O
or	O
a	O
frequency	O
domain	O
.	O

The	O
above	O
object	O
can	O
be	O
also	O
achieved	O
by	O
a	O
second	O
method	O
of	O
speech	O
recognition	O
according	O
to	O
further	O
aspect	O
of	O
the	O
present	O
invention	O
.	O
This	O
invention	O
is	O
provided	O
with	O
:	O
a	O
process	O
of	O
extracting	O
a	O
feature	O
parameter	O
from	O
an	O
input	O
speech	O
in	O
a	O
real	O
environment	O
;	O
a	O
process	O
of	O
reading	O
a	O
first	O
speech	O
transfer	O
characteristic	O
corresponding	O
to	O
an	O
environment	O
in	O
which	O
a	O
reference	O
pattern	O
for	O
the	O
speech	O
recognition	O
is	O
generated	O
,	O
from	O
a	O
first	O
memory	O
device	O
;	O
a	O
process	O
of	O
reading	O
a	O
second	O
speech	O
transfer	O
characteristic	O
corresponding	O
to	O
the	O
real	O
environment	O
from	O
a	O
second	O
memory	O
device	O
;	O
a	O
process	O
of	O
modifying	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
to	O
convert	O
the	O
extracted	O
feature	O
parameter	O
corresponding	O
to	O
the	O
real	O
environment	O
into	O
a	O
modified	O
feature	O
parameter	O
corresponding	O
to	O
the	O
environment	O
in	O
which	O
the	O
reference	O
pattern	O
is	O
generated	O
;	O
and	O
a	O
process	O
of	O
calculating	O
an	O
output	O
probability	O
using	O
the	O
modified	O
feature	O
parameter	O
and	O
the	O
reference	O
pattern	O
;	O
a	O
process	O
of	O
recognizing	O
the	O
input	O
speech	O
using	O
the	O
calculated	O
output	O
probability	O
.	O

According	O
to	O
this	O
method	O
of	O
speech	O
recognition	O
of	O
the	O
present	O
invention	O
,	O
a	O
speech	O
spoken	O
by	O
a	O
speaker	O
is	O
inputted	O
into	O
a	O
speech	O
recognition	O
apparatus	O
,	O
and	O
,	O
feature	O
parameter	O
is	O
extracted	O
.	O
This	O
extracted	O
feature	O
parameter	O
,	O
which	O
includes	O
distortion	O
resulted	O
from	O
a	O
speech	O
transfer	O
characteristic	O
,	O
is	O
modified	O
according	O
to	O
a	O
first	O
speech	O
transfer	O
characteristic	O
,	O
which	O
corresponds	O
to	O
an	O
environment	O
in	O
which	O
the	O
reference	O
pattern	O
is	O
generated	O
,	O
and	O
a	O
second	O
speech	O
transfer	O
characteristic	O
,	O
which	O
corresponds	O
to	O
the	O
real	O
environment	O
.	O
Then	O
,	O
a	O
modified	O
feature	O
parameter	O
,	O
which	O
correspond	O
to	O
the	O
environment	O
in	O
which	O
the	O
reference	O
pattern	O
is	O
made	O
,	O
is	O
generated	O
.	O
And	O
the	O
input	O
speech	O
is	O
recognized	O
on	O
the	O
basis	O
of	O
the	O
calculation	O
using	O
the	O
modified	O
feature	O
parameter	O
and	O
the	O
reference	O
pattern	O
.	O
Therefore	O
,	O
distortion	O
of	O
the	O
extracted	O
feature	O
parameter	O
caused	O
by	O
the	O
real	O
environment	O
can	O
be	O
removed	O
,	O
and	O
an	O
accuracy	O
of	O
the	O
speech	O
recognition	O
can	O
be	O
improved	O
.	O

According	O
to	O
further	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
in	O
the	O
above-stated	O
second	O
method	O
,	O
the	O
feature	O
parameter	O
may	O
be	O
expressed	O
in	O
a	O
frequency	O
domain	O
,	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
may	O
be	O
a	O
first	O
transfer	O
function	O
and	O
a	O
second	O
transfer	O
function	O
in	O
the	O
frequency	O
domain	O
respectively	O
,	O
and	O
the	O
modifying	O
process	O
may	O
modify	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
a	O
formula	O
:	O
F	O
·	O
C1/C2where	O
the	O
F	O
is	O
the	O
extracted	O
feature	O
parameter	O
,	O
the	O
C1is	O
the	O
first	O
transfer	O
function	O
,	O
and	O
the	O
C2is	O
the	O
second	O
transfer	O
function	O
.	O

In	O
this	O
aspect	O
,	O
the	O
feature	O
parameter	O
as	O
a	O
target	O
of	O
the	O
modifying	O
process	O
is	O
expressed	O
in	O
a	O
frequency	O
domain	O
,	O
and	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
are	O
expressed	O
as	O
transfer	O
functions	O
in	O
the	O
frequency	O
domain	O
respectively	O
.	O
The	O
modification	O
of	O
the	O
extracted	O
feature	O
parameter	O
is	O
performed	O
with	O
a	O
multiplication	O
and	O
a	O
division	O
using	O
these	O
two	O
transfer	O
functions	O
.	O
And	O
the	O
output	O
probability	O
is	O
calculated	O
,	O
and	O
then	O
,	O
the	O
input	O
speech	O
is	O
recognized	O
.	O
Therefore	O
,	O
distortion	O
caused	O
by	O
an	O
influence	O
of	O
the	O
real	O
environment	O
can	O
be	O
removed	O
,	O
and	O
an	O
accuracy	O
of	O
the	O
speech	O
recognition	O
can	O
be	O
improved	O
with	O
simple	O
and	O
rapid	O
processes	O
.	O

According	O
to	O
further	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
in	O
the	O
above-stated	O
second	O
method	O
,	O
the	O
feature	O
parameter	O
may	O
be	O
expressed	O
in	O
a	O
cepstrum	O
domain	O
,	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
may	O
be	O
a	O
first	O
transfer	O
function	O
and	O
a	O
second	O
transfer	O
function	O
in	O
the	O
cepstrum	O
domain	O
respectively	O
,	O
and	O
the	O
modifying	O
process	O
may	O
modify	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
a	O
formula	O
:	O
F	O
+	O
C1	O
−	O
C2	O
,	O
where	O
the	O
F	O
is	O
the	O
extracted	O
feature	O
parameter	O
,	O
the	O
C1is	O
the	O
first	O
transfer	O
function	O
,	O
and	O
the	O
C2is	O
the	O
second	O
transfer	O
function	O
.	O

In	O
this	O
aspect	O
,	O
the	O
feature	O
parameter	O
as	O
a	O
target	O
of	O
the	O
modifying	O
process	O
is	O
expressed	O
in	O
a	O
cepstrum	O
domain	O
,	O
and	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
are	O
expressed	O
as	O
transfer	O
functions	O
in	O
the	O
cepstrum	O
domain	O
respectively	O
.	O
The	O
modification	O
of	O
the	O
extracted	O
feature	O
parameter	O
is	O
performed	O
with	O
an	O
addition	O
and	O
a	O
subtraction	O
using	O
these	O
two	O
transfer	O
functions	O
.	O
And	O
the	O
output	O
probability	O
is	O
calculated	O
,	O
and	O
then	O
,	O
the	O
input	O
speech	O
is	O
recognized	O
.	O
Therefore	O
,	O
distortion	O
of	O
an	O
influence	O
of	O
the	O
real	O
environment	O
can	O
be	O
removed	O
,	O
and	O
an	O
accuracy	O
of	O
the	O
speech	O
recognition	O
can	O
be	O
further	O
improved	O
with	O
simple	O
and	O
rapid	O
processes	O
.	O

The	O
above	O
object	O
can	O
be	O
also	O
achieved	O
by	O
a	O
speech	O
recognition	O
apparatus	O
according	O
to	O
the	O
present	O
invention	O
.	O
This	O
invention	O
is	O
provided	O
with	O
:	O
an	O
extracting	O
device	O
for	O
extracting	O
a	O
feature	O
parameter	O
from	O
an	O
input	O
speech	O
in	O
a	O
real	O
environment	O
;	O
a	O
first	O
memory	O
device	O
for	O
storing	O
a	O
first	O
speech	O
transfer	O
characteristic	O
corresponding	O
to	O
an	O
environment	O
in	O
which	O
a	O
reference	O
pattern	O
for	O
the	O
speech	O
recognition	O
is	O
generated	O
;	O
a	O
second	O
memory	O
device	O
for	O
storing	O
a	O
second	O
speech	O
transfer	O
characteristic	O
corresponding	O
to	O
the	O
real	O
environment	O
;	O
a	O
modifying	O
device	O
for	O
modifying	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
to	O
convert	O
the	O
extracted	O
feature	O
parameter	O
corresponding	O
to	O
the	O
real	O
environment	O
into	O
a	O
modified	O
feature	O
parameter	O
corresponding	O
to	O
the	O
environment	O
in	O
which	O
the	O
reference	O
pattern	O
is	O
generated	O
;	O
a	O
calculating	O
device	O
for	O
calculating	O
an	O
output	O
probability	O
using	O
the	O
modified	O
feature	O
parameter	O
and	O
the	O
reference	O
pattern	O
;	O
and	O
a	O
recognizing	O
device	O
for	O
recognizing	O
the	O
input	O
speech	O
using	O
the	O
calculated	O
output	O
probability	O
.	O

By	O
this	O
apparatus	O
,	O
distortion	O
of	O
the	O
feature	O
parameter	O
caused	O
by	O
the	O
real	O
environment	O
can	O
be	O
removed	O
,	O
and	O
an	O
accuracy	O
of	O
the	O
speech	O
recognition	O
apparatus	O
can	O
be	O
improved	O
in	O
various	O
environments	O
.	O

According	O
to	O
further	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
in	O
the	O
above-stated	O
apparatus	O
,	O
the	O
feature	O
parameter	O
may	O
be	O
expressed	O
in	O
a	O
frequency	O
domain	O
,	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
may	O
be	O
a	O
first	O
transfer	O
function	O
and	O
a	O
second	O
transfer	O
function	O
in	O
the	O
frequency	O
domain	O
respectively	O
,	O
and	O
the	O
modifying	O
process	O
may	O
modify	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
a	O
formula	O
:	O
F	O
·	O
C1/C2where	O
the	O
F	O
is	O
the	O
extracted	O
feature	O
parameter	O
,	O
the	O
C1is	O
the	O
first	O
transfer	O
function	O
,	O
and	O
the	O
C2is	O
the	O
second	O
transfer	O
function	O
.	O

In	O
the	O
apparatus	O
according	O
to	O
this	O
aspect	O
,	O
distortion	O
caused	O
by	O
an	O
influence	O
of	O
the	O
real	O
environment	O
can	O
be	O
removed	O
,	O
and	O
therefore	O
,	O
an	O
accuracy	O
of	O
the	O
speech	O
recognition	O
apparatus	O
can	O
be	O
improved	O
in	O
various	O
environments	O
with	O
simple	O
and	O
rapid	O
processes	O
.	O

According	O
to	O
further	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
in	O
the	O
above-stated	O
apparatus	O
,	O
the	O
feature	O
parameter	O
may	O
be	O
expressed	O
in	O
a	O
cepstrum	O
domain	O
,	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
may	O
be	O
a	O
first	O
transfer	O
function	O
and	O
a	O
second	O
transfer	O
function	O
in	O
the	O
cepstrum	O
domain	O
respectively	O
,	O
and	O
the	O
modifying	O
process	O
may	O
modify	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
a	O
formula	O
:	O
F	O
+	O
C1	O
−	O
C2	O
,	O
where	O
the	O
F	O
is	O
the	O
extracted	O
feature	O
parameter	O
,	O
the	O
C2is	O
the	O
first	O
transfer	O
function	O
,	O
and	O
the	O
C2is	O
the	O
second	O
transfer	O
function	O
.	O

In	O
this	O
apparatus	O
,	O
accordingly	O
to	O
this	O
aspect	O
,	O
distortion	O
of	O
an	O
influence	O
of	O
the	O
real	O
environment	O
can	O
be	O
removed	O
,	O
and	O
therefore	O
,	O
an	O
accuracy	O
of	O
the	O
speech	O
recognition	O
apparatus	O
can	O
be	O
further	O
improved	O
in	O
various	O
environments	O
with	O
simple	O
and	O
rapid	O
processes	O
.	O

BRIEF	O
DESCRIPTION	O
OF	O
THE	O
DRAWINGS	O

FIG	O
.	O
1is	O
a	O
block	O
diagram	O
showing	O
a	O
speech	O
recognition	O
apparatus	O
of	O
an	O
embodiment	O
of	O
the	O
present	O
invention	O
.	O

DETAILED	O
DESCRIPTION	O
OF	O
THE	O
PREFERRED	O
EMBODIMENTS	O
Referring	O
to	O
the	O
accompanying	O
drawings	O
,	O
embodiments	O
of	O
the	O
present	O
invention	O
will	O
be	O
described	O
.	O
In	O
the	O
description	O
set	O
forth	O
hereinafter	O
,	O
the	O
present	O
invention	O
is	O
applied	O
to	O
a	O
speech	O
recognition	O
apparatus	O
.	O

FIG	O
.	O
1shows	O
a	O
block	O
diagram	O
of	O
a	O
speech	O
recognition	O
apparatus100of	O
an	O
embodiment	O
of	O
the	O
present	O
invention	O
.	O
As	O
shown	O
inFIG	O
.	O
1	O
,	O
the	O
speech	O
recognition	O
apparatus100is	O
provided	O
with	O
a	O
microphone1	O
,	O
a	O
feature	O
extracting	O
portion2	O
,	O
a	O
modifying	O
portion3	O
,	O
a	O
modifying	O
data	O
memory4	O
,	O
a	O
dictionary5	O
,	O
a	O
calculating	O
portion6and	O
a	O
determining	O
portion7	O
.	O

The	O
microphone1is	O
an	O
input	O
device	O
for	O
an	O
input	O
speech	O
produced	O
by	O
a	O
speaker	O
,	O
and	O
the	O
microphone1outputs	O
a	O
speech	O
signal	O
.	O

The	O
feature	O
extracting	O
portion2operates	O
to	O
extract	O
feature	O
parameters	O
of	O
the	O
speech	O
signal	O
.	O
More	O
concretely	O
,	O
the	O
feature	O
extracting	O
portion2converts	O
the	O
speech	O
signal	O
into	O
a	O
digital	O
speech	O
signal	O
,	O
divides	O
the	O
digital	O
speech	O
signal	O
into	O
frames	O
each	O
having	O
predetermined	O
frame	O
length	O
,	O
and	O
analyzes	O
the	O
digital	O
speech	O
signal	O
to	O
thereby	O
extract	O
the	O
feature	O
parameters	O
.	O

The	O
feature	O
parameters	O
extracted	O
by	O
the	O
feature	O
extracting	O
portion2are	O
necessary	O
for	O
distinguishing	O
phonological	O
units	O
of	O
a	O
speech	O
sounds	O
which	O
compose	O
a	O
speech	O
.	O
The	O
extracted	O
feature	O
parameters	O
can	O
be	O
utilized	O
efficiently	O
for	O
the	O
speech	O
recognition	O
,	O
because	O
the	O
speech	O
signal	O
includes	O
a	O
lot	O
of	O
lengthy	O
information	O
.	O

Parameters	O
in	O
a	O
frequency	O
domain	O
are	O
generally	O
used	O
as	O
the	O
feature	O
parameters	O
.	O
Namely	O
,	O
the	O
nature	O
of	O
phonological	O
units	O
of	O
speech	O
sounds	O
in	O
a	O
speech	O
is	O
expressed	O
by	O
a	O
spectral	O
envelope	O
and	O
its	O
fluctuation	O
in	O
time	O
,	O
so	O
the	O
spectral	O
envelope	O
in	O
the	O
frequency	O
domain	O
can	O
be	O
utilized	O
as	O
the	O
feature	O
parameters	O
of	O
the	O
speech	O
recognition	O
.	O

When	O
the	O
spectral	O
envelope	O
is	O
analyzed	O
,	O
a	O
speech	O
,	O
which	O
actually	O
is	O
a	O
nonstationary	O
signal	O
,	O
is	O
regarded	O
as	O
a	O
stationary	O
signal	O
within	O
a	O
short	O
interval	O
.	O
Therefore	O
,	O
time	O
length	O
of	O
one	O
analyzing	O
frame	O
should	O
be	O
set	O
at	O
a	O
value	O
of	O
the	O
order	O
of	O
10	O
millisecond	O
,	O
and	O
an	O
order	O
for	O
analyzing	O
should	O
be	O
set	O
so	O
small	O
that	O
a	O
spectral	O
fine	O
structure	O
dose	O
not	O
appear	O
.	O

Then	O
,	O
the	O
feature	O
parameters	O
,	O
which	O
are	O
expressed	O
in	O
the	O
frequency	O
domain	O
,	O
can	O
also	O
be	O
expressed	O
in	O
a	O
cepstrum	O
domain	O
.	O
Here	O
the	O
cepstrum	O
,	O
which	O
can	O
be	O
obtained	O
by	O
inverse	O
Fourier	O
transform	O
of	O
the	O
spectrum	O
on	O
the	O
logarithm	O
,	O
has	O
a	O
characteristic	O
which	O
approximates	O
an	O
auditory	O
characteristic	O
of	O
a	O
human	O
,	O
and	O
can	O
express	O
the	O
spectral	O
envelope	O
and	O
the	O
spectral	O
fine	O
structure	O
separately	O
.	O

The	O
feature	O
parameters	O
expressed	O
in	O
the	O
frequency	O
domain	O
or	O
the	O
cepstrum	O
domain	O
is	O
inputted	O
to	O
the	O
modifying	O
portion3	O
.	O
The	O
modifying	O
portion3operates	O
in	O
the	O
following	O
manner	O
,	O
in	O
order	O
to	O
remove	O
an	O
influence	O
of	O
distortion	O
of	O
a	O
transmission	O
path	O
.	O

The	O
speech	O
recognition	O
apparatus100is	O
used	O
in	O
various	O
conditions	O
,	O
such	O
as	O
,	O
mounted	O
in	O
a	O
car	O
,	O
or	O
installed	O
in	O
a	O
telephone	O
apparatus	O
.	O
So	O
speech	O
recognition	O
processes	O
are	O
carried	O
out	O
in	O
different	O
environments	O
depending	O
on	O
an	O
area	O
or	O
a	O
shape	O
of	O
the	O
inside	O
space	O
of	O
a	O
car	O
or	O
a	O
room	O
,	O
and	O
therefore	O
its	O
sound	O
fields	O
characteristics	O
are	O
changeable	O
.	O
On	O
the	O
other	O
hand	O
,	O
if	O
a	O
speech	O
signal	O
is	O
transmitted	O
through	O
a	O
telephone	O
line	O
,	O
then	O
speech	O
recognition	O
processes	O
are	O
influenced	O
by	O
transfer	O
characteristics	O
of	O
a	O
telephone	O
line	O
,	O
and	O
influenced	O
by	O
the	O
fluctuation	O
of	O
environmental	O
noise	O
levels	O
.	O

In	O
a	O
real	O
environment	O
,	O
an	O
actual	O
speech	O
produced	O
by	O
a	O
speaker	O
is	O
distorted	O
by	O
influences	O
of	O
such	O
various	O
conditions	O
.	O
Such	O
a	O
distorted	O
speech	O
can	O
be	O
expressed	O
by	O
a	O
mathematical	O
formula	O
in	O
the	O
frequency	O
domain	O
as	O
follows	O
:	O
S	O
(	O
ω	O
)	O
·	O
Cr	O
(	O
ω	O
)	O
(	O
1	O
)	O
where	O
the	O
S	O
(	O
ω	O
)	O
denotes	O
the	O
ideal	O
feature	O
parameter	O
of	O
a	O
speech	O
,	O
and	O
the	O
Cr	O
(	O
ω	O
)	O
denotes	O
the	O
transfer	O
function	O
representing	O
the	O
real	O
environment	O
.	O

Meanwhile	O
,	O
in	O
the	O
dictionary5	O
,	O
reference	O
patterns	O
for	O
the	O
speech	O
recognition	O
(	O
i	O
.	O
e	O
.	O
,	O
speech	O
models	O
)	O
are	O
pre-stored	O
.	O
These	O
patterns	O
are	O
created	O
by	O
recording	O
various	O
speeches	O
in	O
the	O
ideal	O
environment	O
such	O
as	O
an	O
anechoic	O
chamber	O
.	O
For	O
example	O
,	O
the	O
patterns	O
are	O
created	O
as	O
follows	O
:	O
a	O
microphone	O
is	O
set	O
in	O
the	O
anechoic	O
chamber	O
,	O
and	O
then	O
,	O
the	O
distance	O
between	O
the	O
microphone	O
and	O
the	O
speaker	O
is	O
set	O
,	O
and	O
then	O
,	O
the	O
speeches	O
produced	O
by	O
the	O
speaker	O
are	O
actually	O
recorded	O
.	O
The	O
reference	O
patterns	O
are	O
influenced	O
by	O
an	O
transfer	O
function	O
Ct	O
(	O
ω	O
)	O
based	O
upon	O
some	O
inherent	O
factors	O
of	O
the	O
anechoic	O
chamber	O
and	O
other	O
factors	O
of	O
the	O
environment	O
in	O
which	O
recording	O
of	O
the	O
speeches	O
is	O
done	O
.	O

In	O
the	O
modifying	O
process	O
for	O
the	O
speech	O
recognition	O
,	O
a	O
transfer	O
function	O
Cr	O
(	O
ω	O
)	O
)	O
which	O
can	O
reproduce	O
the	O
real	O
environment	O
in	O
which	O
the	O
speech	O
recognition	O
apparatus	O
is	O
utilized	O
is	O
used	O
.	O
This	O
transfer	O
function	O
Cr	O
(	O
ω	O
)	O
is	O
stored	O
in	O
the	O
modifying	O
data	O
memory4	O
.	O
Further	O
,	O
in	O
the	O
modifying	O
process	O
,	O
the	O
transfer	O
function	O
Ct	O
(	O
ω	O
)	O
,	O
which	O
represents	O
the	O
environment	O
in	O
which	O
the	O
reference	O
patterns	O
was	O
recorded	O
(	O
i	O
.	O
e	O
.	O
,	O
anechoic	O
chamber	O
)	O
,	O
is	O
also	O
used	O
.	O
This	O
transfer	O
function	O
Ct	O
(	O
ω	O
)	O
is	O
also	O
stored	O
in	O
the	O
modifying	O
data	O
memory4	O
.	O
If	O
there	O
are	O
various	O
assumed	O
environments	O
,	O
a	O
plurality	O
of	O
predetermined	O
transfer	O
functions	O
Cr	O
′	O
(	O
ω	O
)	O
can	O
be	O
stored	O
in	O
the	O
modifying	O
data	O
memory4	O
,	O
and	O
it	O
is	O
possible	O
to	O
select	O
one	O
of	O
them	O
to	O
set	O
up	O
.	O
In	O
this	O
case	O
,	O
any	O
one	O
of	O
the	O
transfer	O
functions	O
Ct	O
(	O
ω	O
)	O
may	O
be	O
selected	O
in	O
the	O
modifying	O
process	O
.	O

As	O
can	O
be	O
understood	O
from	O
the	O
above	O
,	O
in	O
most	O
cases	O
the	O
transfer	O
function	O
in	O
the	O
real	O
environment	O
Cr	O
(	O
ω	O
)	O
differs	O
from	O
the	O
transfer	O
function	O
in	O
the	O
ideal	O
environment	O
Ct	O
(	O
ω	O
)	O
.	O
As	O
a	O
result	O
,	O
it	O
causes	O
degradation	O
of	O
performance	O
for	O
the	O
speech	O
recognition	O
.	O
In	O
order	O
to	O
prevent	O
this	O
degradation	O
,	O
before	O
a	O
speech	O
recognition	O
process	O
,	O
the	O
modifying	O
portion3of	O
the	O
embodiment	O
performs	O
a	O
modification	O
for	O
the	O
speech	O
,	O
which	O
is	O
spoken	O
in	O
a	O
real	O
environment	O
,	O
and	O
generates	O
the	O
modified	O
feature	O
parameters	O
.	O

In	O
the	O
modifying	O
process	O
,	O
the	O
modifying	O
portion3modifies	O
the	O
feature	O
parameter	O
S	O
(	O
ω	O
)	O
Cr	O
(	O
ω	O
)	O
obtained	O
from	O
the	O
real	O
environment	O
by	O
using	O
the	O
transfer	O
functions	O
Cr	O
(	O
ω	O
)	O
and	O
Ct	O
(	O
ω	O
)	O
stored	O
in	O
the	O
modifying	O
data	O
memory4	O
,	O
in	O
order	O
to	O
generate	O
a	O
modified	O
feature	O
parameter	O
St	O
(	O
ω	O
)	O
.	O
The	O
modifying	O
portion3concretely	O
generates	O
the	O
modified	O
feature	O
parameter	O
St	O
(	O
ω	O
)	O
by	O
using	O
the	O
feature	O
parameter	O
S	O
(	O
ω	O
)	O
·	O
Cr	O
(	O
ω	O
)	O
output	O
from	O
the	O
feature	O
extracting	O
portion2	O
,	O
the	O
transfer	O
functions	O
Cr	O
(	O
ω	O
)	O
and	O
Ct	O
(	O
ω	O
)	O
according	O
to	O
the	O
following	O
equation	O
(	O
2	O
)	O
.	O

St	O
(	O
ω	O
)	O
=	O
{	O
Cr	O
(	O
ω	O
)	O
·	O
S	O
(	O
ω	O
)	O
}	O
Ct	O
(	O
ω	O
)	O
/Cr	O
′	O
(	O
ω	O
)	O
(	O
2	O
)	O
An	O
influence	O
of	O
the	O
real	O
environment	O
on	O
the	O
transfer	O
characteristic	O
are	O
cancelled	O
in	O
the	O
above	O
equation	O
(	O
2	O
)	O
,	O
so	O
feature	O
parameters	O
corresponding	O
to	O
the	O
ideal	O
environment	O
,	O
in	O
which	O
the	O
dictionary5is	O
made	O
,	O
is	O
generated	O
.	O
In	O
the	O
equation	O
(	O
2	O
)	O
,	O
if	O
the	O
transfer	O
function	O
in	O
the	O
real	O
environment	O
Cr	O
(	O
ω	O
)	O
and	O
predetermined	O
transfer	O
function	O
Cr	O
′	O
(	O
ω	O
)	O
are	O
similar	O
to	O
each	O
other	O
,	O
then	O
the	O
modified	O
feature	O
parameter	O
St	O
(	O
ω	O
)	O
having	O
an	O
ideal	O
characteristic	O
are	O
derived	O
.	O

Although	O
the	O
feature	O
parameter	O
obtained	O
from	O
the	O
real	O
environment	O
is	O
modified	O
in	O
the	O
frequency	O
domain	O
in	O
the	O
above-stated	O
modifying	O
process	O
,	O
it	O
may	O
be	O
modified	O
in	O
the	O
cepstrum	O
domain	O
as	O
described	O
below	O
.	O

As	O
described	O
above	O
,	O
the	O
expression	O
in	O
a	O
cepstrum	O
domain	O
is	O
obtained	O
by	O
inverse	O
Fourier	O
transform	O
of	O
the	O
spectrum	O
on	O
the	O
logarithm	O
,	O
so	O
a	O
multiplication	O
and	O
a	O
division	O
in	O
the	O
frequency	O
domain	O
are	O
replaced	O
with	O
an	O
addition	O
and	O
a	O
subtraction	O
in	O
the	O
cepstrum	O
domain	O
respectively	O
.	O
Therefore	O
,	O
the	O
equation	O
(	O
2	O
)	O
is	O
changed	O
into	O
a	O
following	O
manner	O
.	O

Cep	O
[	O
St	O
]	O
=	O
Cep	O
[	O
CrS	O
]	O
+	O
Cep	O
[	O
Ct	O
]	O
−	O
Cep	O
[	O
Cr	O
′	O
]	O
tm	O
(	O
3	O
)	O
In	O
the	O
equation	O
(	O
3	O
)	O
,	O
“	O
Cep	O
”	O
means	O
an	O
amount	O
expressed	O
in	O
the	O
cepstrum	O
domain	O
.	O
Said	O
amounts	O
St	O
(	O
ω	O
)	O
,	O
Cr	O
(	O
ω	O
)	O
·	O
S	O
(	O
ω	O
)	O
,	O
Ct	O
(	O
ω	O
)	O
and	O
Cr	O
′	O
(	O
ω	O
)	O
,	O
which	O
are	O
expressed	O
in	O
the	O
frequency	O
domain	O
,	O
correspond	O
to	O
amounts	O
Cep	O
[	O
St	O
]	O
,	O
Cep	O
[	O
CrS	O
]	O
,	O
Cep	O
[	O
Ct	O
]	O
and	O
Cep	O
[	O
Cr	O
′	O
]	O
,	O
which	O
are	O
expressed	O
in	O
the	O
cepstrum	O
domain	O
,	O
respectively	O
.	O

The	O
equation	O
(	O
3	O
)	O
shows	O
that	O
in	O
case	O
of	O
modifying	O
a	O
large	O
quantity	O
of	O
speech	O
data	O
,	O
a	O
computation	O
time	O
,	O
in	O
which	O
the	O
modification	O
is	O
performed	O
for	O
the	O
feature	O
parameters	O
in	O
a	O
cepstrum	O
domain	O
,	O
become	O
shorter	O
than	O
that	O
in	O
the	O
frequency	O
domain	O
,	O
because	O
of	O
a	O
replacement	O
of	O
a	O
multiplication	O
and	O
a	O
division	O
with	O
an	O
addition	O
and	O
a	O
subtraction	O
.	O

Next	O
,	O
the	O
modified	O
feature	O
parameters	O
are	O
inputted	O
to	O
the	O
calculating	O
portion6	O
,	O
and	O
an	O
output	O
probability	O
is	O
calculated	O
.	O
Namely	O
,	O
a	O
distribution	O
of	O
the	O
feature	O
parameters	O
can	O
be	O
expressed	O
with	O
a	O
polymodal	O
probability	O
density	O
function	O
corresponding	O
to	O
the	O
limited	O
number	O
of	O
phonemes	O
,	O
so	O
the	O
output	O
probability	O
based	O
on	O
a	O
certain	O
center	O
and	O
a	O
certain	O
distribution	O
can	O
be	O
calculated	O
.	O

For	O
example	O
,	O
the	O
output	O
probability	O
is	O
generally	O
calculated	O
using	O
a	O
hidden	O
Markov	O
model	O
(	O
Hereinafter	O
,	O
it	O
is	O
referred	O
as	O
an	O
“	O
HMM	O
”	O
.	O
)	O
.	O
The	O
HMM	O
is	O
a	O
model	O
which	O
has	O
a	O
plurality	O
of	O
states	O
and	O
expresses	O
phonemes	O
and	O
words	O
with	O
state	O
transition	O
probabilities	O
and	O
symbol	O
output	O
probabilities	O
.	O
And	O
the	O
HMMs	O
of	O
necessary	O
phonemes	O
and	O
words	O
are	O
registered	O
with	O
the	O
dictionary5as	O
the	O
reference	O
patterns	O
,	O
and	O
the	O
output	O
probability	O
is	O
calculated	O
using	O
the	O
HMMs	O
,	O
which	O
are	O
outputted	O
from	O
the	O
dictionary5	O
,	O
for	O
each	O
modified	O
feature	O
parameter	O
.	O

Next	O
,	O
the	O
determining	O
portion7determines	O
the	O
biggest	O
probability	O
based	O
on	O
calculated	O
output	O
probabilities	O
,	O
and	O
sets	O
it	O
as	O
a	O
target	O
to	O
recognize	O
.	O
As	O
a	O
result	O
of	O
the	O
determination	O
by	O
the	O
determining	O
portion7	O
,	O
an	O
input	O
speech	O
is	O
recognized	O
,	O
and	O
the	O
result	O
is	O
outputted	O
.	O

As	O
shown	O
in	O
equations	O
(	O
2	O
)	O
and	O
(	O
3	O
)	O
,	O
the	O
modifying	O
process	O
for	O
feature	O
parameters	O
in	O
the	O
frequency	O
domain	O
and	O
the	O
cepstrum	O
domain	O
are	O
described	O
above	O
.	O
However	O
,	O
the	O
present	O
invention	O
can	O
be	O
adapted	O
to	O
a	O
speech	O
recognition	O
using	O
the	O
modifying	O
process	O
in	O
which	O
the	O
feature	O
parameters	O
are	O
expressed	O
in	O
a	O
time	O
domain	O
.	O
In	O
this	O
case	O
,	O
calculations	O
corresponding	O
to	O
the	O
equations	O
(	O
2	O
)	O
and	O
(	O
3	O
)	O
are	O
performed	O
by	O
convolutions	O
.	O
So	O
,	O
an	O
amount	O
of	O
the	O
necessary	O
calculations	O
in	O
the	O
time	O
domain	O
increases	O
more	O
than	O
that	O
in	O
the	O
frequency	O
domain	O
or	O
the	O
cepstrum	O
domain	O
.	O

As	O
described	O
above	O
in	O
detail	O
,	O
according	O
to	O
the	O
speech	O
recognition	O
apparatus	O
of	O
the	O
embodiment	O
,	O
the	O
modifying	O
portion3calculates	O
equations	O
(	O
2	O
)	O
and	O
(	O
3	O
)	O
using	O
the	O
data	O
stored	O
in	O
the	O
modifying	O
data	O
memory4	O
.	O
As	O
a	O
result	O
,	O
the	O
modified	O
feature	O
parameters	O
,	O
which	O
is	O
cancelled	O
the	O
influence	O
of	O
distortion	O
in	O
the	O
real	O
environment	O
,	O
are	O
derived	O
.	O
And	O
the	O
calculating	O
portion6calculates	O
the	O
output	O
probabilities	O
using	O
the	O
data	O
registered	O
in	O
the	O
dictionary5	O
,	O
and	O
the	O
determining	O
portion7recognizes	O
the	O
input	O
speech	O
.	O

Accordingly	O
,	O
the	O
degradation	O
of	O
the	O
accuracy	O
of	O
a	O
speech	O
recognition	O
is	O
prevented	O
more	O
sufficeintly	O
than	O
that	O
without	O
the	O
modifying	O
process	O
,	O
because	O
modified	O
feature	O
parameters	O
have	O
characteristics	O
corresponding	O
to	O
the	O
ideal	O
environment	O
in	O
which	O
the	O
dictionary	O
is	O
made	O
.	O
The	O
calculation	O
using	O
feature	O
parameters	O
expressed	O
in	O
the	O
frequency	O
domain	O
.	O
This	O
is	O
because	O
the	O
former	O
needs	O
the	O
simple	O
calculation	O
including	O
one	O
multiplication	O
and	O
one	O
division	O
only	O
,	O
while	O
the	O
later	O
needs	O
complicated	O
calculating	O
including	O
convolution	O
integrals	O
.	O
The	O
calculation	O
using	O
feature	O
parameters	O
expressed	O
in	O
the	O
cepstrum	O
domain	O
is	O
further	O
easier	O
,	O
than	O
using	O
feature	O
parameters	O
expressed	O
in	O
the	O
frequency	O
domain	O
.	O
This	O
is	O
because	O
the	O
calculation	O
in	O
the	O
cepstrum	O
domain	O
needs	O
one	O
addition	O
and	O
one	O
subtraction	O
only	O
.	O
Furthermore	O
,	O
the	O
degradation	O
of	O
the	O
accuracy	O
of	O
the	O
speech	O
recognition	O
is	O
prevented	O
not	O
only	O
in	O
case	O
of	O
different	O
speakers	O
or	O
different	O
words	O
but	O
also	O
in	O
case	O
of	O
same	O
speakers	O
or	O
same	O
words	O
.	O
As	O
a	O
result	O
of	O
an	O
experiment	O
,	O
the	O
accuracy	O
of	O
the	O
speech	O
recognition	O
of	O
the	O
present	O
embodiment	O
was	O
improved	O
about	O
3	O
percent	O
in	O
a	O
silent	O
environment	O
.	O

A	O
program	O
which	O
operates	O
the	O
speech	O
recognition	O
processes	O
of	O
the	O
present	O
embodiment	O
can	O
be	O
recorded	O
in	O
a	O
computer	O
readable	O
mediums	O
such	O
that	O
ROM	O
,	O
CD-ROM	O
or	O
floppy	O
disk	O
.	O
And	O
the	O
program	O
is	O
installed	O
in	O
the	O
computer	O
with	O
these	O
mediums	O
,	O
and	O
executed	O
the	O
speech	O
recognition	O
processes	O
of	O
the	O
present	O
embodiment	O
.	O

The	O
invention	O
may	O
be	O
embodied	O
in	O
other	O
specific	O
forms	O
without	O
departing	O
from	O
the	O
spirit	O
or	O
essential	O
characteristic	O
thereof	O
.	O
The	O
present	O
embodiments	O
are	O
therefore	O
to	O
be	O
considered	O
in	O
all	O
respects	O
as	O
illustrative	O
and	O
not	O
restrictive	O
,	O
the	O
scope	O
of	O
the	O
invention	O
being	O
indicated	O
by	O
the	O
appended	O
claims	O
rather	O
than	O
by	O
the	O
foregoing	O
description	O
and	O
all	O
changes	O
which	O
come	O
within	O
the	O
meaning	O
and	O
range	O
of	O
equivalency	O
of	O
the	O
claims	O
are	O
therefore	O
intended	O
to	O
be	O
embraced	O
therein	O
.	O

The	O
entire	O
disclosure	O
of	O
Japanese	O
Patent	O
Application	O
No.	O
10	O
-	O
099051	O
filed	O
on	O
Apr.	O
10	O
,	O
1998	O
including	O
the	O
specification	O
,	O
claims	O
,	O
drawings	O
and	O
summary	O
is	O
incorporated	O
herein	O
by	O
reference	O
in	O
its	O
entirety	O

1	O
.	O
A	O
method	O
of	O
modifying	O
a	O
feature	O
parameter	O
for	O
a	O
speech	O
recognition	O
comprising	O
the	O
processes	O
of:extracting	O
the	O
feature	O
parameter	O
from	O
an	O
input	O
speech	O
in	O
a	O
real	O
environment;reading	O
a	O
first	O
speech	O
transfer	O
characteristic	O
corresponding	O
to	O
an	O
environment	O
in	O
which	O
a	O
reference	O
pattern	O
for	O
the	O
speech	O
recognition	O
is	O
generated	O
,	O
from	O
a	O
first	O
memory	O
device;reading	O
a	O
second	O
speech	O
transfer	O
characteristic	O
corresponding	O
to	O
the	O
real	O
environment	O
from	O
a	O
second	O
memory	O
device	O
;	O
andmodifying	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
to	O
convert	O
the	O
extracted	O
feature	O
parameter	O
corresponding	O
to	O
the	O
real	O
environment	O
into	O
a	O
modified	O
feature	O
parameter	O
corresponding	O
to	O
the	O
environment	O
in	O
which	O
the	O
reference	O
pattern	O
is	O
generated	O
.	O

2	O
.	O
The	O
method	O
according	O
toclaim	O
1	O
,	O
wherein	O
the	O
feature	O
parameter	O
is	O
expressed	O
in	O
a	O
frequency	O
domain	O
,	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
are	O
a	O
first	O
transfer	O
function	O
and	O
a	O
second	O
transfer	O
function	O
in	O
the	O
frequency	O
domain	O
respectively	O
,	O
and	O
the	O
modifying	O
process	O
modifies	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
a	O
formula	O
:	O
F	O
·	O
C1/C2	O
where	O
the	O
F	O
is	O
the	O
extracted	O
feature	O
parameter	O
,	O
the	O
C1is	O
the	O
first	O
transfer	O
function	O
,	O
and	O
the	O
C2is	O
the	O
second	O
transfer	O
function	O
.	O

3	O
.	O
The	O
method	O
according	O
toclaim	O
1	O
,	O
wherein	O
the	O
feature	O
parameter	O
is	O
expressed	O
in	O
a	O
cepstrum	O
domain	O
,	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
are	O
a	O
first	O
transfer	O
function	O
and	O
a	O
second	O
transfer	O
function	O
in	O
the	O
cepstrum	O
domain	O
respectively	O
,	O
and	O
the	O
modifying	O
process	O
modifies	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
a	O
formula	O
:	O
F	O
+	O
C1	O
−	O
C2	O
,	O
where	O
the	O
F	O
is	O
the	O
extracted	O
feature	O
parameter	O
,	O
the	O
C1is	O
the	O
first	O
transfer	O
function	O
,	O
and	O
the	O
C2is	O
the	O
second	O
transfer	O
function	O
.	O

4	O
.	O
A	O
method	O
of	O
a	O
speech	O
recognition	O
comprising	O
the	O
processes	O
of:extracting	O
a	O
feature	O
parameter	O
from	O
an	O
input	O
speech	O
in	O
a	O
real	O
environment;reading	O
a	O
first	O
speech	O
transfer	O
characteristic	O
corresponding	O
to	O
an	O
environment	O
in	O
which	O
a	O
reference	O
pattern	O
for	O
the	O
speech	O
recognition	O
is	O
generated	O
,	O
from	O
a	O
first	O
memory	O
device;reading	O
a	O
second	O
speech	O
transfer	O
characteristic	O
corresponding	O
to	O
the	O
real	O
environment	O
from	O
a	O
second	O
memory	O
device;modifying	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
to	O
convert	O
the	O
extracted	O
feature	O
parameter	O
corresponding	O
to	O
the	O
real	O
environment	O
into	O
a	O
modified	O
feature	O
parameter	O
corresponding	O
to	O
the	O
environment	O
in	O
.	O
which	O
the	O
reference	O
pattern	O
is	O
generated;calculating	O
an	O
output	O
probability	O
using	O
the	O
modified	O
feature	O
parameter	O
and	O
the	O
reference	O
pattern	O
;	O
andrecognizing	O
the	O
input	O
speech	O
using	O
the	O
calculated	O
output	O
probability	O
.	O

5	O
.	O
The	O
method	O
according	O
toclaim	O
4	O
,	O
wherein	O
the	O
feature	O
parameter	O
is	O
expressed	O
in	O
a	O
frequency	O
domain	O
,	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
are	O
a	O
first	O
transfer	O
function	O
and	O
a	O
second	O
transfer	O
function	O
in	O
the	O
frequency	O
domain	O
respectively	O
,	O
and	O
the	O
modifying	O
process	O
modifies	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
a	O
formula	O
:	O
F	O
·	O
C1/C2	O
where	O
the	O
F	O
is	O
the	O
extracted	O
feature	O
parameter	O
,	O
the	O
C1is	O
the	O
first	O
transfer	O
function	O
,	O
and	O
the	O
C2is	O
the	O
second	O
transfer	O
function	O
.	O

6	O
.	O
The	O
method	O
according	O
toclaim	O
4	O
,	O
wherein	O
the	O
feature	O
parameter	O
is	O
expressed	O
in	O
a	O
cepstrum	O
domain	O
,	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
are	O
a	O
first	O
transfer	O
function	O
and	O
a	O
second	O
transfer	O
function	O
in	O
the	O
cepstrum	O
domain	O
respectively	O
,	O
and	O
the	O
modifying	O
process	O
modifies	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
a	O
formula	O
:	O
F	O
+	O
C1	O
−	O
C2	O
,	O
where	O
the	O
F	O
is	O
the	O
extracted	O
feature	O
parameter	O
,	O
the	O
C1is	O
the	O
first	O
transfer	O
function	O
,	O
and	O
the	O
C2is	O
the	O
second	O
transfer	O
function	O
.	O

7	O
.	O
A	O
speech	O
recognition	O
apparatus	O
comprising:an	O
extracting	O
device	O
for	O
extracting	O
a	O
feature	O
parameter	O
from	O
an	O
input	O
speech	O
in	O
a	O
real	O
environment;a	O
first	O
memory	O
device	O
for	O
storing	O
a	O
first	O
speech	O
transfer	O
characteristic	O
corresponding	O
to	O
an	O
environment	O
in	O
which	O
a	O
reference	O
pattern	O
for	O
the	O
speech	O
recognition	O
is	O
generated;a	O
second	O
memory	O
device	O
for	O
storing	O
a	O
second	O
speech	O
transfer	O
characteristic	O
corresponding	O
to	O
the	O
real	O
environment;a	O
modifying	O
device	O
for	O
modifying	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
to	O
convert	O
the	O
extracted	O
feature	O
parameter	O
corresponding	O
to	O
the	O
real	O
environment	O
into	O
a	O
modified	O
feature	O
parameter	O
corresponding	O
to	O
the	O
environment	O
in	O
which	O
the	O
reference	O
pattern	O
is	O
generated;a	O
calculating	O
device	O
for	O
calculating	O
an	O
output	O
probability	O
using	O
the	O
modified	O
feature	O
parameter	O
and	O
the	O
reference	O
pattern	O
;	O
anda	O
recognizing	O
device	O
for	O
recognizing	O
the	O
input	O
speech	O
using	O
the	O
calculated	O
output	O
probability	O
.	O

8	O
.	O
The	O
speech	O
recognition	O
apparatus	O
according	O
toclaim	O
7	O
,	O
wherein	O
the	O
feature	O
parameter	O
is	O
expressed	O
in	O
a	O
frequency	O
domain	O
,	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
are	O
a	O
first	O
transfer	O
function	O
and	O
a	O
second	O
transfer	O
function	O
in	O
the	O
frequency	O
domain	O
respectively	O
,	O
and	O
the	O
modifying	O
process	O
modifies	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
a	O
formula	O
:	O
F	O
·	O
C1/C2	O
where	O
the	O
F	O
is	O
the	O
extracted	O
feature	O
parameter	O
,	O
the	O
C1is	O
the	O
first	O
transfer	O
function	O
,	O
and	O
the	O
C2is	O
the	O
second	O
transfer	O
function	O
.	O

9	O
.	O
The	O
speech	O
recognition	O
apparatus	O
according	O
toclaim	O
7	O
,	O
wherein	O
the	O
feature	O
parameter	O
is	O
expressed	O
in	O
a	O
cepstrum	O
domain	O
,	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
are	O
a	O
first	O
transfer	O
function	O
and	O
a	O
second	O
transfer	O
function	O
in	O
the	O
cepstrum	O
domain	O
respectively	O
,	O
and	O
the	O
modifying	O
process	O
modifies	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
a	O
formula	O
:	O
F	O
+	O
C1	O
−	O
C2	O
,	O
where	O
the	O
F	O
is	O
the	O
extracted	O
feature	O
parameter	O
,	O
the	O
C1is	O
the	O
first	O
transfer	O
function	O
,	O
and	O
the	O
C2is	O
the	O
second	O
transfer	O
function	O
.	O

10	O
.	O
A	O
computer	O
readable	O
medium	O
containing	O
a	O
program	O
for	O
performing	O
a	O
speech	O
recognition	O
comprising	O
the	O
processes	O
of:extracting	O
a	O
feature	O
parameter	O
from	O
an	O
input	O
speech	O
in	O
a	O
real	O
environment;reading	O
a	O
first	O
speech	O
transfer	O
characteristic	O
corresponding	O
to	O
an	O
environment	O
in	O
which	O
a	O
reference	O
pattern	O
for	O
the	O
speech	O
recognition	O
is	O
generated	O
,	O
from	O
a	O
first	O
memory	O
device;reading	O
a	O
second	O
speech	O
transfer	O
characteristic	O
corresponding	O
to	O
the	O
real	O
environment	O
from	O
a	O
second	O
memory	O
device;modifying	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
to	O
convert	O
the	O
extracted	O
feature	O
parameter	O
corresponding	O
to	O
the	O
real	O
environment	O
into	O
a	O
modified	O
feature	O
parameter	O
corresponding	O
to	O
the	O
environment	O
in	O
which	O
the	O
reference	O
pattern	O
is	O
generated;calculating	O
an	O
output	O
probability	O
using	O
the	O
modified	O
feature	O
parameter	O
and	O
the	O
reference	O
pattern	O
;	O
andrecognizing	O
the	O
input	O
speech	O
using	O
the	O
calculated	O
output	O
probability	O
.	O

11	O
.	O
The	O
computer	O
readable	O
medium	O
according	O
toclaim	O
10	O
,	O
wherein	O
the	O
feature	O
parameter	O
is	O
expressed	O
in	O
a	O
frequency	O
domain	O
,	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
are	O
a	O
first	O
transfer	O
function	O
and	O
a	O
second	O
transfer	O
function	O
in	O
the	O
frequency	O
domain	O
respectively	O
,	O
and	O
the	O
modifying	O
process	O
modifies	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
a	O
formula	O
:	O
F	O
·	O
C1/C2	O
where	O
the	O
F	O
is	O
the	O
extracted	O
feature	O
parameter	O
,	O
the	O
C1is	O
the	O
first	O
transfer	O
function	O
,	O
and	O
the	O
C2is	O
the	O
second	O
transfer	O
function	O
.	O

12	O
.	O
The	O
computer	O
readable	O
medium	O
according	O
toclaim	O
10	O
,	O
wherein	O
the	O
feature	O
parameter	O
is	O
expressed	O
in	O
a	O
cepstrum	O
domain	O
,	O
the	O
first	O
speech	O
transfer	O
characteristic	O
and	O
the	O
second	O
speech	O
transfer	O
characteristic	O
are	O
a	O
first	O
transfer	O
function	O
and	O
a	O
second	O
transfer	O
function	O
in	O
the	O
cepstrum	O
domain	O
respectively	O
,	O
and	O
the	O
modifying	O
process	O
modifies	O
the	O
extracted	O
feature	O
parameter	O
according	O
to	O
a	O
formula	O
:	O
F	O
+	O
C1	O
−	O
C2	O
,	O
where	O
the	O
F	O
is	O
the	O
extracted	O
feature	O
parameter	O
,	O
the	O
C1is	O
the	O
first	O
transfer	O
function	O
,	O
and	O
the	O
C2is	O
the	O
second	O
transfer	O
function	O
.	O

