US	O
6230126	O
B1	O
20010508	O

US	O
09213248	O
19981217	O

09	O
eng	O
eng	O

JP	O
09364638	O
19971218	O

20010508	O

20010508	O

7G	O
10L	O
15/02	O
A	O
7	O
G	O
10	O
L	O
15	O
02	O
A	O

7G	O
10L	O
15/00	O
B	O
7	O
G	O
10	O
L	O
15	O
00	O
B	O

7G	O
10L	O
15/12	O
B	O
7	O
G	O
10	O
L	O
15	O
12	O
B	O

G10L	O
15/06	O
20060101AFI20051220RMJP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
06	O
F	O
I	O

20051220	O

JP	O

R	O
M	O

G10L	O
15/00	O
20060101C	O
I20051008RMEP	O

20060101	O

C	O
G	O
10	O
L	O
15	O
00	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/00	O
20060101A	O
N20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
00	O
N	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/02	O
20060101A	O
N20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
02	O
N	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/08	O
20060101ALI20051220RMJP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
08	O
L	O
I	O

20051220	O

JP	O

R	O
M	O

G10L	O
15/12	O
20060101A	O
I20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
12	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/22	O
20060101A	O
N20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
22	O
N	O

20051008	O

EP	O

R	O
M	O

US	O

704/231	O
704	O
231	O

704/236	O
704	O
236	O

704/251	O
704	O
251	O

704/252	O
704	O
252	O

704/E15.016	O
704	O
E15	O
.	O
016	O

G10L	O
15/12	O
G	O
10	O
L	O
15	O
12	O

S10L	O
15	O
:	O
00W	O
S	O
10	O
L	O
15	O
00	O

W	O

S10L	O
15	O
:	O
02P	O
S	O
10	O
L	O
15	O
02	O

P	O

S10L	O
15	O
:	O
22U	O
S	O
10	O
L	O
15	O
22	O

U	O

US	O

704/231	O
704	O
231	O

US	O

704/236	O
704	O
236	O

US	O

704/237	O
704	O
237	O

US	O

704/239	O
704	O
239	O

US	O

704/243	O
704	O
243	O

US	O

704/245	O
704	O
245	O

US	O

704/251	O
704	O
251	O

US	O

704/252	O
704	O
252	O

15	O
Word-spotting	O
speech	O
recognition	O
device	O
and	O
system	O

US	O
4513436	O
A	O
Nose	O
et_al.	O

19850423	O

19840223	O

US	O
5159637	O
A	O
Okazaki	O
et_al.	O

19921027	O

19920116	O

US	O
5794194	O
A	O
Takebayashi	O
et_al.	O

19980811	O

19970203	O

US	O
5893058	O
A	O
Kosaka	O
19990406	O

19941114	O

Komori	B-Citation
et	I-Citation
al	I-Citation
,	I-Citation
(	I-Citation
“	I-Citation
A	I-Citation
New	I-Citation
Learning	I-Citation
Algorithm	I-Citation
for	I-Citation
Minimizing	I-Citation
Spotting	I-Citation
Errors	I-Citation
,	I-Citation
”	I-Citation
0	I-Citation
Neural	I-Citation
Networks	I-Citation
for	I-Citation
Processing	I-Citation
,	I-Citation
IEEE	I-Citation
Signal	I-Citation
Processing	I-Citation
Workshop	I-Citation
,	I-Citation
Sep.	I-Citation
1993	I-Citation
)	I-Citation
.	O
*	O

Takebayashi	B-Citation
,	I-Citation
et	I-Citation
al	I-Citation
,	I-Citation
(	I-Citation
“	I-Citation
A	I-Citation
Robust	I-Citation
Speech	I-Citation
Recognition	I-Citation
System	I-Citation
using	I-Citation
Word-Spotting	I-Citation
with	I-Citation
Noise	I-Citation
Immunity	I-Citation
Learning	I-Citation
,	I-Citation
”	I-Citation
Conference	I-Citation
on	I-Citation
Acoustics	I-Citation
,	I-Citation
Speech	I-Citation
and	I-Citation
Signal	I-Citation
Processing	I-Citation
,	I-Citation
ICASSP-91	I-Citation
,	I-Citation
Apr.	I-Citation
1991	I-Citation
)	I-Citation
.	O
*	O

Huang	B-Citation
,	I-Citation
et	I-Citation
al	I-Citation
,	I-Citation
(	I-Citation
“	I-Citation
On	I-Citation
Speaker-Independent	I-Citation
,	I-Citation
-	I-Citation
Dependent	I-Citation
and	I-Citation
—	I-Citation
Adaptive	I-Citation
Speech	I-Citation
Recognition	I-Citation
,	I-Citation
”	I-Citation
IEEE	I-Citation
Transactions	I-Citation
on	I-Citation
Speech	I-Citation
and	I-Citation
Audio	I-Citation
Processing	I-Citation
,	I-Citation
Apr.	I-Citation
1993	I-Citation
)	I-Citation
.	O
*	O

Tetsuya	B-Citation
Muroi	I-Citation
,	I-Citation
et_al.	I-Citation
,	I-Citation
“	I-Citation
Isolated	I-Citation
Spoken	I-Citation
Word	I-Citation
Recognition	I-Citation
By	I-Citation
Duration	I-Citation
Based	I-Citation
State	I-Citation
Transition	I-Citation
Models	I-Citation
”	I-Citation
,	I-Citation
Journal	I-Citation
of	I-Citation
the	I-Citation
Institute	I-Citation
of	I-Citation
Electronics	I-Citation
,	I-Citation
Information	I-Citation
and	I-Citation
Communication	I-Citation
Engineers	I-Citation
,	I-Citation
vol.	I-Citation
J72-D-II	I-Citation
,	I-Citation
Nov.	I-Citation
1989	I-Citation
,	I-Citation
pp.	I-Citation
1769	I-Citation
-	I-Citation
1777	I-Citation
.	O

US	O
7620212	O
B1	O
20091117	O

20030812	O

US	O
6697505	O
B2	O
20040224	O

20030605	O

US	O
6608914	O
B1	O
20030819	O

19981211	O

JP	O
4378284	O
B9	O
20090918	O

20030723	O

US	O
7979278	O
B2	O
20110712	O

20021101	O

Ricoh	O
Company	O
,	O
Ltd.	O

03	O

Tokyo	O
JP	O

Kuroda	O
,	O
Masaru	O

Tokyo	O
JP	O

Oblon	O
,	O
Spivak	O
,	O
McClelland	O
,	O
Maier	O
&	O
Neustadt	O
,	O
P.	O
C.	O

Dorvil	O
Richemond	O
2641	O

Nolan	O
Daniel	O
A.	O

US	O
6230126	O
B1	O
20010508	O

19981217	O

JP	O
11184491	O
A	O
19990709	O

19971218	O

JP	O
11184491	O
A	O
19990709	O

19971218	O

US	O
6230126	O
B1	O
20010508	O

19981217	O

A	O
device	O
for	O
speech	O
recognition	O
includes	O
a	O
dictionary	O
which	O
stores	O
features	O
of	O
recognition	O
objects	O
.	O
The	O
device	O
further	O
includes	O
a	O
matching	O
unit	O
which	O
compares	O
features	O
of	O
input	O
speech	O
with	O
the	O
features	O
of	O
the	O
recognition	O
objects	O
,	O
and	O
a	O
dictionary	O
updating	O
unit	O
which	O
updates	O
time	O
lengths	O
of	O
phonemics	O
in	O
the	O
dictionary	O
based	O
on	O
the	O
input	O
speech	O
when	O
the	O
matching	O
unit	O
finds	O
substantial	O
similarities	O
between	O
the	O
input	O
speech	O
and	O
one	O
of	O
the	O
recognition	O
objects	O
.	O

20010118	O

AS	O
ASSIGNMENT	O
N	O
US	O
6230126B1	O
RICOH	O
COMPANY	O
,	O
LTD.	O
,	O
JAPAN	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNOR:KURODA	O
,	O
MASARU;REEL/FRAME:011451/0902	O

19981208	O

20010516	O

AS	O
ASSIGNMENT	O
N	O
US	O
6230126B1	O
RICOH	O
COMPANY	O
,	O
LTD	O
,	O
JAPAN	O
CORRECTIV;ASSIGNOR:KURODA	O
,	O
MASARU;REEL/FRAME:011810/0200	O

19981208	O

20040922	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
6230126B1	O
4	O

20080924	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
6230126B1	O
8	O

BACKGROUND	O
OF	O
THE	O
INVENTION	O
1	O
.	O
Field	O
of	O
the	O
Invention	O
The	O
present	O
invention	O
relates	O
to	O
a	O
speech-recognition	O
device	O
.	O

2	O
.	O
Description	O
of	O
the	O
Related	O
Art	O
One	O
of	O
the	O
well-known	O
methods	O
of	O
speech	O
recognition	O
is	O
to	O
attend	O
to	O
speech	O
recognition	O
based	O
on	O
speech-frame	O
detection	O
.	O
This	O
scheme	O
determines	O
a	O
start	O
and	O
an	O
end	O
of	O
a	O
speech	O
frame	O
to	O
be	O
recognized	O
by	O
using	O
power	O
information	O
of	O
the	O
speech	O
or	O
the	O
like	O
,	O
and	O
performs	O
a	O
recognition	O
process	O
based	O
on	O
information	O
obtained	O
from	O
the	O
speech	O
frame	O
.	O

FIG	O
.	O
1is	O
a	O
flowchart	O
of	O
a	O
method	O
of	O
recognizing	O
speech	O
based	O
on	O
speech-frame	O
detection	O
.	O
In	O
the	O
speech	O
recognition	O
based	O
on	O
the	O
speech-frame	O
detection	O
,	O
a	O
recognition	O
process	O
is	O
started	O
(	O
step	O
S1	O
)	O
,	O
and	O
speech	O
frames	O
are	O
detected	O
as	O
a	O
speaker	O
produces	O
a	O
speech	O
(	O
step	O
S2	O
)	O
.	O
Speech	O
information	O
obtained	O
from	O
a	O
speech	O
frame	O
is	O
matched	O
against	O
a	O
dictionary	O
pattern	O
(	O
step	O
S3	O
)	O
,	O
and	O
a	O
recognition	O
object	O
(	O
a	O
word	O
in	O
the	O
dictionary	O
)	O
is	O
output	O
as	O
a	O
recognition	O
result	O
when	O
this	O
object	O
exhibits	O
the	O
highest	O
similarity	O
(	O
step	O
S4	O
)	O
.	O
At	O
the	O
step	O
S2	O
,	O
a	O
beginning	O
of	O
a	O
speech	O
frame	O
can	O
be	O
easily	O
detected	O
based	O
on	O
power	O
information	O
.	O
An	O
end	O
of	O
a	O
speech	O
frame	O
,	O
however	O
,	O
is	O
detected	O
when	O
a	O
silence	O
continues	O
to	O
be	O
present	O
for	O
more	O
than	O
a	O
predetermined	O
time	O
period	O
.	O
This	O
measure	O
is	O
taken	O
in	O
order	O
to	O
insure	O
that	O
a	O
silence	O
before	O
a	O
plosive	O
consonant	O
and	O
a	O
silence	O
of	O
a	O
double	O
consonant	O
are	O
differentiated	O
.	O

A	O
period	O
of	O
silence	O
for	O
detecting	O
an	O
end	O
of	O
a	O
speech	O
frame	O
,	O
however	O
,	O
is	O
generally	O
as	O
long	O
as	O
about	O
250	O
msec	O
to	O
350	O
msec	O
because	O
of	O
a	O
need	O
to	O
differentiate	O
a	O
silence	O
of	O
a	O
double	O
consonant	O
.	O
In	O
this	O
scheme	O
,	O
therefore	O
,	O
a	O
recognition	O
result	O
is	O
not	O
available	O
until	O
the	O
end	O
of	O
the	O
time	O
period	O
of	O
250	O
msec	O
to	O
350	O
msec	O
after	O
a	O
completion	O
of	O
speech	O
input	O
.	O
This	O
makes	O
a	O
recognition	O
system	O
which	O
is	O
slow	O
in	O
response	O
.	O
If	O
the	O
period	O
of	O
silence	O
for	O
detecting	O
the	O
end	O
of	O
a	O
speech	O
frame	O
is	O
shortened	O
for	O
the	O
sake	O
of	O
faster	O
response	O
,	O
an	O
erroneous	O
recognition	O
result	O
may	O
be	O
obtained	O
because	O
the	O
result	O
of	O
a	O
double	O
consonant	O
comes	O
out	O
before	O
the	O
end	O
of	O
a	O
speech	O
.	O

It	O
is	O
often	O
observed	O
that	O
a	O
speaker	O
makes	O
redundant	O
sounds	O
irrelevant	O
to	O
recognition	O
of	O
speech	O
as	O
in	O
a	O
situation	O
where	O
he/she	O
may	O
say	O
“	O
ah	O
”	O
,	O
“	O
oh	O
”	O
,	O
etc	O
.	O
Since	O
matching	O
with	O
a	O
dictionary	O
is	O
started	O
at	O
a	O
beginning	O
of	O
a	O
speech	O
frame	O
when	O
the	O
speech	O
frame	O
is	O
subjected	O
to	O
a	O
recognition	O
process	O
,	O
such	O
redundant	O
voices	O
as	O
“	O
ah	O
”	O
and	O
“	O
oh	O
”	O
hinder	O
detection	O
of	O
similarities	O
,	O
and	O
result	O
in	O
an	O
erroneous	O
recognition	O
result	O
.	O

A	O
word	O
spotting	O
scheme	O
is	O
designed	O
to	O
counter	O
various	O
drawbacks	O
described	O
above	O
.	O
FIG	O
.	O
2is	O
a	O
flowchart	O
of	O
a	O
process	O
of	O
a	O
word	O
spotting	O
scheme	O
.	O
In	O
this	O
scheme	O
,	O
a	O
recognition	O
process	O
is	O
started	O
(	O
step	O
S11	O
)	O
,	O
and	O
speech	O
information	O
is	O
matched	O
against	O
a	O
dictionary	O
without	O
detecting	O
a	O
speech	O
frame	O
as	O
a	O
speaker	O
makes	O
a	O
speech	O
(	O
step	O
S12	O
)	O
.	O
Then	O
,	O
a	O
check	O
is	O
made	O
as	O
to	O
whether	O
a	O
detected	O
similarity	O
measure	O
exceeds	O
a	O
predetermined	O
threshold	O
value	O
(	O
step	O
S13	O
)	O
.	O
If	O
it	O
does	O
not	O
,	O
a	O
procedure	O
goes	O
back	O
to	O
the	O
step	O
S12to	O
continue	O
matching	O
of	O
speech	O
information	O
against	O
the	O
dictionary	O
.	O
If	O
the	O
similarity	O
measure	O
exceeds	O
the	O
threshold	O
at	O
the	O
step	O
S13	O
,	O
a	O
recognition	O
object	O
corresponding	O
to	O
this	O
similarity	O
measure	O
is	O
output	O
as	O
a	O
recognition	O
result	O
(	O
step	O
S14	O
)	O
.	O
The	O
word	O
spotting	O
scheme	O
does	O
not	O
require	O
detection	O
of	O
a	O
speech	O
frame	O
,	O
so	O
that	O
it	O
facilitates	O
implementation	O
of	O
a	O
system	O
having	O
a	O
faster	O
response	O
.	O
Also	O
,	O
the	O
word	O
spotting	O
scheme	O
takes	O
redundant	O
words	O
away	O
from	O
a	O
speech	O
before	O
outputting	O
recognition	O
results	O
,	O
thereby	O
providing	O
a	O
better	O
recognition	O
result	O
.	O

The	O
word	O
spotting	O
scheme	O
has	O
its	O
own	O
drawback	O
as	O
described	O
in	O
the	O
following	O
.	O
In	O
the	O
word	O
spotting	O
scheme	O
,	O
no	O
speech	O
frame	O
is	O
detected	O
,	O
and	O
matching	O
against	O
a	O
dictionary	O
is	O
conducted	O
consecutively	O
.	O
If	O
a	O
result	O
of	O
the	O
matching	O
exceeds	O
a	O
threshold	O
,	O
a	O
recognition	O
result	O
is	O
obtained	O
.	O
Otherwise	O
,	O
the	O
matching	O
process	O
is	O
continued	O
.	O
Since	O
the	O
matching	O
process	O
is	O
kept	O
running	O
regardless	O
of	O
the	O
speaker	O
's	O
action	O
,	O
the	O
recognition	O
result	O
obtained	O
from	O
this	O
process	O
may	O
be	O
output	O
even	O
when	O
the	O
speaker	O
is	O
not	O
voicing	O
a	O
word	O
to	O
be	O
recognized	O
.	O
This	O
is	O
called	O
fountaining	O
.	O
For	O
example	O
,	O
fountaining	O
is	O
observed	O
when	O
the	O
speaker	O
is	O
not	O
talking	O
to	O
the	O
recognition	O
device	O
but	O
is	O
talking	O
with	O
someone	O
around	O
him/her	O
.	O

A	O
method	O
of	O
implementing	O
the	O
word	O
spotting	O
scheme	O
can	O
be	O
found	O
,	O
for	O
example	O
,	O
in	O
“	B-Citation
Method	I-Citation
of	I-Citation
Recognizing	I-Citation
Word	I-Citation
Speech	I-Citation
Using	I-Citation
a	I-Citation
State	I-Citation
Transition	I-Citation
Model	I-Citation
of	I-Citation
Continuous	I-Citation
Time	I-Citation
Control	I-Citation
Type	I-Citation
”	I-Citation
,	I-Citation
Journal	I-Citation
of	I-Citation
the	I-Citation
Institute	I-Citation
of	I-Citation
Electronics	I-Citation
,	I-Citation
Information	I-Citation
and	I-Citation
Communication	I-Citation
Engineers	I-Citation
,	I-Citation
vol.	I-Citation
J72-D-II	I-Citation
,	I-Citation
No.	I-Citation
11	I-Citation
,	I-Citation
pp.1769-1777	I-Citation
(	I-Citation
1989	I-Citation
)	I-Citation
.	O
According	O
to	O
the	O
method	O
disclosed	O
in	O
this	O
document	O
,	O
data	O
indicative	O
of	O
a	O
time	O
length	O
is	O
attached	O
to	O
phonemics	O
in	O
a	O
dictionary	O
or	O
codebook	O
.	O
As	O
a	O
result	O
,	O
an	O
improved	O
recognition	O
performance	O
is	O
obtained	O
while	O
reducing	O
the	O
amount	O
of	O
computation	O
.	O
In	O
this	O
method	O
,	O
however	O
,	O
a	O
dictionary	O
of	O
recognized	O
words	O
is	O
compiled	O
by	O
connecting	O
phonemics	O
using	O
an	O
average	O
time	O
length	O
of	O
each	O
phonemic	O
.	O
Because	O
of	O
this	O
,	O
a	O
long	O
word	O
in	O
the	O
dictionary	O
may	O
not	O
correspond	O
to	O
an	O
actually	O
spoken	O
word	O
in	O
terms	O
of	O
the	O
time	O
length	O
of	O
the	O
word	O
.	O
This	O
is	O
because	O
there	O
is	O
a	O
psychological	O
tendency	O
that	O
a	O
speaker	O
tries	O
to	O
speak	O
a	O
shorter	O
word	O
and	O
a	O
longer	O
word	O
in	O
an	O
equal	O
time	O
length	O
.	O
Further	O
,	O
when	O
the	O
speaker	O
is	O
excited	O
,	O
speech	O
may	O
become	O
faster	O
,	O
and	O
voice	O
may	O
be	O
raised	O
.	O
In	O
such	O
situations	O
,	O
a	O
speech-recognition	O
device	O
may	O
experience	O
a	O
degradation	O
in	O
matched	O
similarity	O
measures	O
,	O
and	O
may	O
suffer	O
a	O
drop	O
in	O
a	O
recognition	O
performance	O
.	O
If	O
the	O
speech-recognition	O
device	O
uses	O
the	O
time	O
length	O
as	O
a	O
parameter	O
,	O
a	O
speed	O
of	O
making	O
a	O
speech	O
for	O
a	O
given	O
speaker	O
may	O
be	O
far	O
different	O
from	O
a	O
time	O
length	O
stored	O
in	O
a	O
standard	O
dictionary	O
.	O

In	O
this	O
manner	O
,	O
the	O
related-art	O
voice-recognition	O
device	O
compiles	O
words	O
of	O
a	O
dictionary	O
by	O
connecting	O
phonemics	O
using	O
an	O
average	O
time	O
length	O
of	O
each	O
phonemic	O
.	O
Because	O
of	O
this	O
,	O
there	O
may	O
be	O
a	O
discrepancy	O
in	O
a	O
time	O
length	O
between	O
a	O
word	O
in	O
the	O
dictionary	O
and	O
an	O
actually	O
spoken	O
word	O
,	O
resulting	O
in	O
a	O
degradation	O
in	O
recognition	O
performance	O
.	O

Accordingly	O
,	O
there	O
is	O
a	O
need	O
for	O
a	O
speech-recognition	O
device	O
which	O
can	O
enhance	O
a	O
recognition	O
performance	O
by	O
updating	O
time-length	O
parameters	O
in	O
a	O
standard	O
dictionary	O
in	O
accordance	O
with	O
a	O
time	O
length	O
of	O
an	O
actually	O
spoken	O
word	O
.	O

SUMMARY	O
OF	O
THE	O
INVENTION	O
Accordingly	O
,	O
it	O
is	O
a	O
general	O
object	O
of	O
the	O
present	O
invention	O
to	O
provide	O
a	O
speech-recognition	O
device	O
which	O
can	O
satisfy	O
the	O
need	O
described	O
above	O
.	O

It	O
is	O
another	O
and	O
more	O
specific	O
object	O
of	O
the	O
present	O
invention	O
to	O
provide	O
a	O
speech-recognition	O
device	O
which	O
can	O
enhance	O
a	O
recognition	O
performance	O
by	O
updating	O
time-length	O
parameters	O
in	O
a	O
standard	O
dictionary	O
in	O
accordance	O
with	O
a	O
time	O
length	O
of	O
an	O
actually	O
spoken	O
word	O
.	O

In	O
order	O
to	O
achieve	O
the	O
above	O
objects	O
according	O
to	O
the	O
present	O
invention	O
,	O
a	O
device	O
for	O
speech	O
recognition	O
includes	O
a	O
dictionary	O
which	O
stores	O
features	O
of	O
recognition	O
objects	O
,	O
a	O
matching	O
unit	O
which	O
compares	O
features	O
of	O
input	O
speech	O
with	O
the	O
features	O
of	O
the	O
recognition	O
objects	O
,	O
and	O
a	O
dictionary	O
updating	O
unit	O
which	O
updates	O
time	O
lengths	O
of	O
phonemics	O
in	O
the	O
dictionary	O
based	O
on	O
the	O
input	O
speech	O
when	O
the	O
matching	O
unit	O
finds	O
substantial	O
similarities	O
between	O
the	O
input	O
speech	O
and	O
one	O
of	O
the	O
recognition	O
objects	O
.	O

According	O
to	O
another	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
the	O
device	O
as	O
described	O
above	O
further	O
includes	O
a	O
feature-extraction	O
unit	O
which	O
extracts	O
the	O
features	O
of	O
input	O
speech	O
from	O
the	O
input	O
speech	O
without	O
detecting	O
speech	O
frames	O
,	O
and	O
wherein	O
the	O
matching	O
unit	O
compares	O
the	O
features	O
of	O
input	O
speech	O
with	O
the	O
features	O
of	O
the	O
recognition	O
objects	O
so	O
as	O
to	O
produce	O
a	O
similarity	O
measure	O
continuously	O
without	O
breaks	O
of	O
speech	O
frames	O
,	O
and	O
the	O
dictionary	O
updating	O
unit	O
updates	O
the	O
time	O
lengths	O
of	O
phonemics	O
when	O
the	O
similarity	O
measure	O
exceeds	O
a	O
predetermined	O
threshold	O
.	O

According	O
to	O
another	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
the	O
device	O
as	O
described	O
above	O
is	O
such	O
that	O
the	O
dictionary	O
updating	O
unit	O
compares	O
a	O
sum	O
of	O
the	O
time	O
lengths	O
of	O
phonemics	O
constituting	O
the	O
one	O
of	O
the	O
recognition	O
objects	O
with	O
an	O
actual	O
time	O
length	O
of	O
the	O
input	O
speech	O
corresponding	O
to	O
the	O
one	O
of	O
the	O
recognition	O
objects	O
,	O
and	O
updates	O
the	O
time	O
lengths	O
of	O
the	O
phonemics	O
in	O
the	O
dictionary	O
based	O
on	O
a	O
difference	O
between	O
the	O
sum	O
and	O
the	O
actual	O
time	O
length	O
.	O

According	O
to	O
another	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
the	O
device	O
as	O
described	O
above	O
is	O
such	O
that	O
the	O
dictionary	O
updating	O
unit	O
obtains	O
a	O
difference	O
between	O
a	O
time	O
length	O
of	O
a	O
phonemic	O
in	O
the	O
dictionary	O
and	O
a	O
time	O
length	O
of	O
the	O
same	O
phonemic	O
in	O
the	O
input	O
speech	O
,	O
and	O
adds	O
the	O
difference	O
to	O
the	O
time	O
length	O
of	O
the	O
phonemic	O
in	O
the	O
dictionary	O
so	O
as	O
to	O
update	O
the	O
time	O
lengths	O
of	O
phonemics	O
in	O
the	O
dictionary	O
.	O

According	O
to	O
another	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
the	O
device	O
as	O
described	O
above	O
is	O
such	O
that	O
the	O
dictionary	O
updating	O
unit	O
obtains	O
a	O
difference	O
between	O
a	O
time	O
length	O
of	O
a	O
phonemic	O
in	O
the	O
dictionary	O
and	O
a	O
time	O
length	O
of	O
the	O
same	O
phonemic	O
in	O
the	O
input	O
speech	O
,	O
and	O
adds	O
the	O
difference	O
multiplied	O
by	O
a	O
given	O
weighting	O
factor	O
to	O
the	O
time	O
length	O
of	O
the	O
phonemic	O
in	O
the	O
dictionary	O
so	O
as	O
to	O
update	O
the	O
time	O
lengths	O
of	O
phonemics	O
in	O
the	O
dictionary	O
.	O

According	O
to	O
another	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
the	O
device	O
as	O
described	O
above	O
is	O
such	O
that	O
the	O
weighting	O
factor	O
varies	O
depending	O
on	O
how	O
great	O
the	O
substantial	O
similarities	O
are	O
.	O

According	O
to	O
another	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
the	O
device	O
as	O
described	O
above	O
is	O
such	O
that	O
the	O
weighting	O
factor	O
varies	O
depending	O
on	O
how	O
many	O
times	O
the	O
matching	O
unit	O
correctly	O
finds	O
the	O
substantial	O
similarities	O
with	O
respect	O
to	O
the	O
one	O
of	O
the	O
recognition	O
objects	O
.	O

According	O
to	O
another	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
the	O
device	O
as	O
described	O
above	O
is	O
such	O
that	O
the	O
dictionary	O
updating	O
unit	O
updates	O
a	O
time	O
length	O
of	O
a	O
given	O
phonemic	O
in	O
the	O
dictionary	O
not	O
only	O
with	O
respect	O
to	O
the	O
one	O
of	O
the	O
recognition	O
objects	O
but	O
also	O
with	O
respect	O
to	O
all	O
the	O
recognition	O
objects	O
that	O
include	O
the	O
given	O
phonemic	O
.	O

The	O
objects	O
presented	O
earlier	O
can	O
also	O
be	O
achieved	O
by	O
a	O
method	O
of	O
conducting	O
speech	O
recognition	O
.	O
The	O
method	O
includes	O
the	O
steps	O
of	O
a	O
)	O
storing	O
features	O
of	O
recognition	O
objects	O
in	O
a	O
dictionary	O
,	O
b	O
)	O
comparing	O
features	O
of	O
input	O
speech	O
with	O
the	O
features	O
of	O
the	O
recognition	O
objects	O
,	O
and	O
c	O
)	O
updating	O
time	O
lengths	O
of	O
phonemics	O
in	O
the	O
dictionary	O
based	O
on	O
the	O
input	O
speech	O
when	O
the	O
step	O
b	O
)	O
finds	O
substantial	O
similarities	O
between	O
the	O
input	O
speech	O
and	O
one	O
of	O
the	O
recognition	O
objects	O
.	O

According	O
to	O
another	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
the	O
method	O
as	O
described	O
above	O
further	O
includes	O
a	O
step	O
of	O
extracting	O
the	O
features	O
of	O
input	O
speech	O
from	O
the	O
input	O
speech	O
without	O
detecting	O
speech	O
frames	O
,	O
and	O
wherein	O
the	O
step	O
b	O
)	O
compares	O
the	O
features	O
of	O
input	O
speech	O
with	O
the	O
features	O
of	O
the	O
recognition	O
objects	O
so	O
as	O
to	O
produce	O
a	O
similarity	O
measure	O
continuously	O
without	O
breaks	O
of	O
speech	O
frames	O
,	O
and	O
the	O
step	O
c	O
)	O
updates	O
the	O
time	O
lengths	O
of	O
phonemics	O
when	O
the	O
similarity	O
measure	O
exceeds	O
a	O
predetermined	O
threshold	O
.	O

According	O
to	O
another	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
the	O
method	O
as	O
described	O
above	O
is	O
such	O
that	O
the	O
step	O
c	O
)	O
compares	O
a	O
sum	O
of	O
the	O
time	O
lengths	O
of	O
phonemics	O
constituting	O
the	O
one	O
of	O
the	O
recognition	O
objects	O
with	O
an	O
actual	O
time	O
length	O
of	O
the	O
input	O
speech	O
corresponding	O
to	O
the	O
one	O
of	O
the	O
recognition	O
objects	O
,	O
and	O
updates	O
the	O
time	O
lengths	O
of	O
the	O
phonemics	O
in	O
the	O
dictionary	O
based	O
on	O
a	O
difference	O
between	O
the	O
sum	O
and	O
the	O
actual	O
time	O
length	O
.	O

The	O
objects	O
presented	O
earlier	O
can	O
also	O
be	O
achieved	O
by	O
a	O
machine-readable	O
memory	O
medium	O
having	O
a	O
program	O
embodied	O
therein	O
for	O
causing	O
a	O
computer	O
to	O
perform	O
a	O
speech	O
recognition	O
.	O
The	O
program	O
includes	O
a	O
dictionary	O
configured	O
to	O
store	O
features	O
of	O
recognition	O
objects	O
,	O
a	O
matching	O
unit	O
configured	O
to	O
compare	O
features	O
of	O
input	O
speech	O
with	O
the	O
features	O
of	O
the	O
recognition	O
objects	O
,	O
and	O
a	O
dictionary	O
updating	O
unit	O
configured	O
to	O
update	O
time	O
lengths	O
of	O
phonemics	O
in	O
the	O
dictionary	O
based	O
on	O
the	O
input	O
speech	O
when	O
the	O
matching	O
unit	O
finds	O
substantial	O
similarities	O
between	O
the	O
input	O
speech	O
and	O
one	O
of	O
the	O
recognition	O
objects	O
.	O

According	O
to	O
another	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
the	O
machine-readable	O
memory	O
medium	O
as	O
described	O
above	O
is	O
such	O
that	O
the	O
program	O
further	O
comprises	O
a	O
feature-extraction	O
unit	O
configured	O
to	O
extract	O
the	O
features	O
of	O
input	O
speech	O
from	O
the	O
input	O
speech	O
without	O
detecting	O
speech	O
frames	O
,	O
and	O
wherein	O
the	O
matching	O
unit	O
compares	O
the	O
features	O
of	O
input	O
speech	O
with	O
the	O
features	O
of	O
the	O
recognition	O
objects	O
so	O
as	O
to	O
produce	O
a	O
similarity	O
measure	O
continuously	O
without	O
breaks	O
of	O
speech	O
frames	O
,	O
and	O
the	O
dictionary	O
updating	O
unit	O
updates	O
the	O
time	O
lengths	O
of	O
phonemics	O
when	O
the	O
similarity	O
measure	O
exceeds	O
a	O
predetermined	O
threshold	O
.	O

According	O
to	O
another	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
the	O
machine-readable	O
memory	O
medium	O
as	O
described	O
above	O
is	O
such	O
that	O
the	O
dictionary	O
updating	O
unit	O
compares	O
a	O
sum	O
of	O
the	O
time	O
lengths	O
of	O
phonemics	O
constituting	O
the	O
one	O
of	O
the	O
recognition	O
objects	O
with	O
an	O
actual	O
time	O
length	O
of	O
the	O
input	O
speech	O
corresponding	O
to	O
the	O
one	O
of	O
the	O
recognition	O
objects	O
,	O
and	O
updates	O
the	O
time	O
lengths	O
of	O
the	O
phonemics	O
in	O
the	O
dictionary	O
based	O
on	O
a	O
difference	O
between	O
the	O
sum	O
and	O
the	O
actual	O
time	O
length	O
.	O

Other	O
objects	O
and	O
further	O
features	O
of	O
the	O
present	O
invention	O
will	O
be	O
apparent	O
from	O
the	O
following	O
detailed	O
description	O
when	O
read	O
in	O
conjunction	O
with	O
the	O
accompanying	O
drawings	O
.	O

BRIEF	O
DESCRIPTION	O
OF	O
THE	O
DRAWINGS	O
A	O
more	O
complete	O
appreciation	O
of	O
the	O
present	O
invention	O
and	O
many	O
of	O
the	O
attendant	O
advantages	O
thereof	O
will	O
be	O
readily	O
obtained	O
as	O
the	O
same	O
becomes	O
better	O
understood	O
by	O
reference	O
to	O
the	O
following	O
detailed	O
description	O
when	O
considered	O
in	O
connection	O
with	O
the	O
accompanying	O
drawings	O
,	O
wherein	O
:	O

FIG	O
.	O
1is	O
a	O
flowchart	O
of	O
a	O
method	O
of	O
recognizing	O
speech	O
based	O
on	O
speech-frame	O
detection	O
;	O

FIG	O
.	O
2is	O
a	O
flowchart	O
of	O
a	O
process	O
of	O
a	O
word	O
spotting	O
scheme	O
;	O

FIG	O
.	O
3is	O
a	O
block	O
diagram	O
of	O
a	O
speech-recognition	O
device	O
according	O
to	O
the	O
present	O
invention	O
.	O

FIG	O
.	O
4is	O
an	O
illustrative	O
drawing	O
showing	O
an	O
example	O
of	O
parameters	O
stored	O
in	O
a	O
dictionary	O
;	O

FIG	O
.	O
5is	O
a	O
flowchart	O
for	O
explaining	O
operations	O
of	O
the	O
speech-recognition	O
device	O
ofFIG	O
.	O
3	O
;	O

FIG	O
.	O
6is	O
a	O
chart	O
showing	O
a	O
similarity	O
measure	O
obtained	O
when	O
a	O
speaker	O
pronounces	O
a	O
Japanese	O
word	O
“	O
sokan	O
”	O
;	O

FIG	O
.	O
7is	O
another	O
chart	O
showing	O
a	O
similarity	O
measure	O
obtained	O
when	O
a	O
speaker	O
pronounces	O
the	O
Japanese	O
word	O
“	O
sokan	O
”	O
;	O
and	O

FIG	O
.	O
8is	O
a	O
block	O
diagram	O
showing	O
a	O
hardware	O
configuration	O
of	O
the	O
speech-recognition	O
device	O
shown	O
in	O
FIG	O
.	O
3	O
.	O

DESCRIPTION	O
OF	O
THE	O
PREFERRED	O
EMBODIMENTS	O
In	O
the	O
following	O
,	O
embodiments	O
of	O
the	O
present	O
invention	O
will	O
be	O
described	O
with	O
reference	O
to	O
the	O
accompanying	O
drawings	O
.	O

FIG	O
.	O
3is	O
a	O
block	O
diagram	O
of	O
a	O
speech-recognition	O
device	O
according	O
to	O
the	O
present	O
invention	O
.	O
InFIG	O
.	O
3	O
,	O
the	O
speech-recognition	O
device	O
includes	O
an	O
input	O
unit	O
(	O
e.g.	O
,	O
microphone	O
)	O
1	O
,	O
a	O
feature-extraction	O
unit2	O
,	O
a	O
standard	O
dictionary3	O
,	O
a	O
matching	O
unit4	O
,	O
a	O
result	O
outputting	O
unit5	O
,	O
and	O
a	O
dictionary	O
updating	O
unit6	O
.	O
The	O
input	O
unit1is	O
used	O
for	O
inputting	O
speech	O
.	O
The	O
feature-extraction	O
unit2extracts	O
features	O
from	O
the	O
speech	O
input	O
via	O
the	O
input	O
unit1	O
.	O
The	O
standard	O
dictionary3stores	O
standard	O
features	O
of	O
all	O
the	O
recognition	O
objects	O
.	O
The	O
matching	O
unit4matches	O
the	O
features	O
of	O
the	O
input	O
speech	O
with	O
the	O
standard	O
features	O
of	O
the	O
recognition	O
objects	O
stored	O
in	O
the	O
standard	O
dictionary3when	O
the	O
features	O
of	O
the	O
input	O
speech	O
are	O
extracted	O
by	O
the	O
feature-extraction	O
unit2	O
,	O
thereby	O
obtaining	O
a	O
similarity	O
measure	O
with	O
respect	O
to	O
each	O
recognition	O
object	O
.	O
Based	O
on	O
the	O
matching	O
results	O
,	O
the	O
result	O
outputting	O
unit5outputs	O
a	O
recognition	O
object	O
as	O
a	O
recognition	O
result	O
when	O
this	O
recognition	O
object	O
has	O
the	O
highest	O
similarity	O
measure	O
that	O
exceeds	O
a	O
predetermined	O
threshold	O
value	O
(	O
i	O
.	O
e	O
.	O
,	O
a	O
rejecting	O
threshold	O
value	O
)	O
,	O
for	O
example	O
.	O
The	O
dictionary	O
updating	O
unit6updates	O
the	O
standard	O
dictionary3	O
.	O

The	O
feature-extraction	O
unit2	O
,	O
the	O
standard	O
dictionary3	O
,	O
the	O
matching	O
unit4	O
,	O
and	O
the	O
result	O
outputting	O
unit5employ	O
a	O
recognition	O
process	O
based	O
on	O
the	O
word	O
spotting	O
scheme	O
,	O
for	O
example	O
,	O
which	O
does	O
not	O
need	O
detection	O
of	O
speech	O
frames	O
.	O
As	O
a	O
recognition	O
process	O
based	O
on	O
the	O
word	O
spotting	O
scheme	O
,	O
a	O
word-speech-recognition	O
scheme	O
using	O
a	O
state-transition	O
model	O
of	O
a	O
consecutive-time-control	O
type	O
may	O
be	O
employed	O
.	O
When	O
this	O
word-speech-recognition	O
scheme	O
is	O
used	O
,	O
a	O
detected	O
similarity	O
measure	O
becomes	O
a	O
maximum	O
near	O
an	O
end	O
of	O
a	O
speech	O
,	O
and	O
a	O
detection	O
of	O
a	O
peak	O
of	O
the	O
similarity	O
measure	O
leads	O
to	O
outputting	O
of	O
a	O
recognition	O
result	O
.	O

In	O
what	O
follows	O
,	O
a	O
description	O
will	O
be	O
given	O
with	O
regard	O
to	O
operations	O
of	O
the	O
speech-recognition	O
device	O
of	O
FIG	O
.	O
3	O
.	O

When	O
speech	O
is	O
input	O
to	O
the	O
input	O
unit1	O
,	O
the	O
input	O
speech	O
is	O
converted	O
into	O
a	O
predetermined	O
set	O
of	O
features	O
by	O
the	O
feature-extraction	O
unit2	O
.	O
The	O
features	O
which	O
can	O
be	O
used	O
in	O
the	O
present	O
invention	O
include	O
TSP	O
indicative	O
of	O
power	O
in	O
a	O
frequency	O
domain	O
and	O
cepstrum	O
.	O

The	O
features	O
of	O
the	O
input	O
speech	O
obtained	O
by	O
the	O
feature-extracting	O
unit2are	O
then	O
compared	O
with	O
the	O
standard	O
features	O
of	O
each	O
recognition	O
object	O
stored	O
in	O
the	O
standard	O
dictionary3	O
.	O
This	O
comparison	O
is	O
made	O
by	O
the	O
matching	O
unit4	O
,	O
and	O
a	O
similarity	O
measure	O
is	O
obtained	O
with	O
respect	O
to	O
each	O
recognition	O
object	O
.	O
Based	O
on	O
the	O
matching	O
results	O
of	O
the	O
matching	O
unit4	O
,	O
the	O
result	O
outputting	O
unit5outputs	O
a	O
recognition	O
object	O
as	O
a	O
recognition	O
result	O
when	O
this	O
recognition	O
object	O
has	O
the	O
highest	O
similarity	O
measure	O
that	O
exceeds	O
a	O
predetermined	O
threshold	O
value	O
(	O
i	O
.	O
e	O
.	O
,	O
a	O
rejecting	O
threshold	O
value	O
)	O
,	O
for	O
example	O
.	O

The	O
standard	O
dictionary3stores	O
a	O
representation	O
of	O
a	O
state	O
corresponding	O
to	O
each	O
phonemic	O
to	O
be	O
recognized	O
.	O
In	O
order	O
to	O
represent	O
this	O
state	O
,	O
a	O
feature	O
vector	O
Yj	O
and	O
a	O
time	O
length	O
Lj	O
are	O
used	O
as	O
parameters	O
.	O
Here	O
,	O
the	O
time	O
length	O
Lj	O
is	O
an	O
average	O
time	O
length	O
defined	O
for	O
each	O
phonemic	O
.	O

Take	O
an	O
example	O
of	O
a	O
Japanese	O
word	O
“	O
soqkan	O
”	O
(	O
meaning	O
“	O
correlation	O
”	O
)	O
.	O
Each	O
phonemic	O
of	O
this	O
word	O
,	O
i	O
.	O
e	O
.	O
,	O
/s/	O
,	O
/o/	O
,	O
/q/	O
,	O
/ka/	O
,	O
and	O
/n/	O
,	O
has	O
a	O
corresponding	O
state	O
thereof	O
,	O
and	O
five	O
states	O
in	O
total	O
(	O
State1through	O
State5	O
)	O
are	O
represented	O
as	O
shown	O
in	O
FIG	O
.	O
4	O
.	O
Each	O
state	O
representing	O
features	O
of	O
a	O
corresponding	O
phonemic	O
is	O
defined	O
by	O
a	O
pair	O
of	O
a	O
feature	O
vector	O
Yj	O
(	O
j	O
=	O
1	O
to	O
5	O
)	O
and	O
a	O
time	O
length	O
Lj	O
(	O
j	O
=	O
1	O
to	O
5	O
)	O
.	O
A	O
total	O
time	O
length	O
of	O
this	O
word	O
is	O
represented	O
as	O
a	O
sum	O
of	O
Lj	O
.	O
That	O
is	O
,	O
the	O
total	O
time	O
length	O
is	O
represented	O
by	O
a	O
sum	O
of	O
the	O
time	O
lengths	O
Lj	O
added	O
together	O
with	O
respect	O
to	O
all	O
the	O
phonemics	O
included	O
in	O
the	O
word	O
.	O

Ltot	O
=	O
ΣLj	O
As	O
previously	O
described	O
,	O
however	O
,	O
a	O
time	O
length	O
of	O
an	O
actually	O
spoken	O
word	O
may	O
not	O
be	O
equal	O
to	O
the	O
sum	O
Ltot	O
of	O
the	O
time	O
lengths	O
of	O
all	O
the	O
phonemics	O
in	O
the	O
word	O
calculated	O
based	O
on	O
the	O
standard	O
dictionary	O
.	O

In	O
order	O
to	O
obviate	O
this	O
problem	O
,	O
the	O
speech-recognition	O
device	O
ofFIG	O
.	O
3is	O
provided	O
with	O
the	O
dictionary	O
updating	O
unit6for	O
updating	O
the	O
standard	O
dictionary3	O
.	O
The	O
dictionary	O
updating	O
unit6updates	O
the	O
standard	O
dictionary	O
as	O
it	O
becomes	O
necessary	O
in	O
accordance	O
with	O
fluctuation	O
of	O
the	O
speaker	O
's	O
speech	O
(	O
i	O
.	O
e	O
.	O
,	O
fluctuation	O
in	O
speech	O
speed	O
)	O
.	O

In	O
detail	O
,	O
the	O
dictionary	O
updating	O
unit6compares	O
a	O
time	O
length	O
of	O
an	O
actually	O
spoken	O
word	O
with	O
a	O
sum	O
of	O
time	O
lengths	O
of	O
all	O
the	O
phonemics	O
included	O
in	O
a	O
recognition	O
object	O
in	O
the	O
standard	O
dictionary3when	O
the	O
recognition	O
object	O
has	O
a	O
similarity	O
measure	O
exceeding	O
the	O
threshold	O
value	O
according	O
to	O
the	O
matching	O
by	O
the	O
matching	O
unit4	O
.	O
This	O
is	O
performed	O
where	O
a	O
time	O
length	O
of	O
each	O
phonemic	O
is	O
provided	O
in	O
the	O
standard	O
dictionary3	O
.	O
Based	O
on	O
the	O
comparison	O
,	O
the	O
dictionary	O
updating	O
unit6updates	O
a	O
time	O
length	O
of	O
a	O
phonemic	O
in	O
the	O
standard	O
dictionary3with	O
respect	O
to	O
all	O
the	O
phonemics	O
included	O
in	O
the	O
recognition	O
object	O
.	O

FIG	O
.	O
5is	O
a	O
flowchart	O
for	O
explaining	O
operations	O
of	O
the	O
speech-recognition	O
device	O
of	O
FIG	O
.	O
3	O
.	O
As	O
shown	O
inFIG	O
.	O
5	O
,	O
a	O
recognition	O
process	O
is	O
started	O
(	O
step	O
S21	O
)	O
,	O
and	O
speech	O
information	O
is	O
matched	O
against	O
the	O
dictionary3without	O
detecting	O
a	O
speech	O
frame	O
as	O
a	O
speaker	O
makes	O
a	O
speech	O
(	O
step	O
S22	O
)	O
.	O
Then	O
,	O
a	O
check	O
is	O
made	O
as	O
to	O
whether	O
a	O
detected	O
similarity	O
measure	O
exceeds	O
a	O
predetermined	O
threshold	O
value	O
(	O
step	O
S23	O
)	O
.	O
If	O
it	O
does	O
not	O
,	O
a	O
procedure	O
goes	O
back	O
to	O
the	O
step	O
S22to	O
continue	O
matching	O
of	O
speech	O
information	O
against	O
the	O
dictionary3	O
.	O
If	O
the	O
similarity	O
measure	O
exceeds	O
the	O
threshold	O
at	O
the	O
step	O
S23	O
,	O
a	O
recognition	O
object	O
corresponding	O
to	O
this	O
similarity	O
measure	O
is	O
output	O
as	O
a	O
recognition	O
result	O
(	O
step	O
S24	O
)	O
.	O
The	O
outputting	O
of	O
the	O
recognition	O
result	O
is	O
carried	O
out	O
after	O
detecting	O
a	O
peak	O
of	O
the	O
similarity	O
measure	O
following	O
a	O
moment	O
at	O
which	O
the	O
similarity	O
measure	O
exceeds	O
a	O
threshold	O
value	O
SMth	O
.	O

When	O
the	O
similarity	O
measure	O
exceeds	O
the	O
threshold	O
value	O
,	O
a	O
comparison	O
is	O
made	O
between	O
a	O
time	O
length	O
of	O
a	O
word	O
obtained	O
as	O
the	O
recognition	O
result	O
and	O
a	O
time	O
length	O
of	O
an	O
actually	O
spoken	O
word	O
(	O
step	O
S25	O
)	O
.	O
Here	O
,	O
the	O
time	O
length	O
of	O
the	O
word	O
obtained	O
as	O
the	O
recognition	O
result	O
is	O
a	O
sum	O
of	O
time	O
lengths	O
of	O
all	O
the	O
phonemics	O
included	O
in	O
the	O
word	O
in	O
the	O
standard	O
dictionary3	O
.	O
Based	O
on	O
the	O
comparison	O
(	O
i	O
.	O
e	O
.	O
,	O
based	O
on	O
a	O
difference	O
between	O
the	O
two	O
lengths	O
)	O
,	O
the	O
time-length	O
parameters	O
Lj	O
regarding	O
phonemics	O
stored	O
in	O
the	O
standard	O
dictionary3are	O
modified	O
(	O
step	O
S26	O
)	O
.	O
Namely	O
,	O
a	O
difference	O
between	O
the	O
time	O
length	O
of	O
the	O
word	O
obtained	O
as	O
the	O
recognition	O
result	O
and	O
the	O
time	O
length	O
of	O
the	O
actually	O
spoken	O
word	O
is	O
introduced	O
to	O
the	O
time-length	O
parameters	O
Lj	O
of	O
the	O
phonemics	O
stored	O
in	O
the	O
standard	O
dictionary3	O
.	O

When	O
there	O
is	O
a	O
difference	O
dt	O
between	O
a	O
time-length	O
parameter	O
Lj	O
of	O
a	O
given	O
phonemic	O
and	O
an	O
actually	O
spoken	O
time	O
length	O
of	O
this	O
phonemic	O
,	O
the	O
difference	O
dt	O
may	O
be	O
directly	O
introduced	O
to	O
the	O
time-length	O
parameter	O
Lj	O
such	O
that	O
Lj	O
=	O
Lj	O
+	O
dt	O
.	O
This	O
is	O
one	O
way	O
to	O
modify	O
the	O
time-length	O
parameters	O
Lj	O
stored	O
in	O
the	O
standard	O
dictionary3	O
.	O
Alternatively	O
,	O
the	O
difference	O
dt	O
between	O
the	O
time	O
length	O
Lj	O
and	O
the	O
actually	O
spoken	O
time	O
length	O
may	O
be	O
multiplied	O
by	O
a	O
coefficient	O
(	O
adjusting	O
value	O
)	O
K	O
,	O
so	O
that	O
Kdt	O
is	O
used	O
for	O
modifying	O
the	O
time	O
length	O
of	O
the	O
phonemic	O
.	O
Namely	O
,	O
the	O
time-length	O
parameter	O
Lj	O
in	O
the	O
standard	O
dictionary3may	O
be	O
modified	O
for	O
a	O
given	O
phonemic	O
such	O
that	O
Lj	O
=	O
Lj	O
+	O
Kdt	O
.	O

In	O
summary	O
,	O
the	O
dictionary	O
updating	O
unit6may	O
update	O
a	O
time-length	O
parameter	O
Lj	O
of	O
a	O
given	O
phonemic	O
included	O
in	O
the	O
recognition	O
object	O
(	O
word	O
)	O
in	O
the	O
standard	O
dictionary	O
by	O
using	O
a	O
product	O
of	O
the	O
time	O
difference	O
and	O
a	O
predetermined	O
adjusting	O
value	O
.	O

Further	O
,	O
the	O
dictionary	O
updating	O
unit6may	O
modify	O
a	O
time	O
length	O
of	O
a	O
given	O
phonemic	O
in	O
the	O
standard	O
dictionary	O
even	O
when	O
the	O
same	O
phonemic	O
of	O
a	O
different	O
recognition	O
object	O
is	O
subjected	O
to	O
the	O
updating	O
process	O
.	O

In	O
what	O
follows	O
,	O
the	O
present	O
invention	O
will	O
be	O
described	O
by	O
using	O
a	O
specific	O
example.FIG	O
.	O
6is	O
a	O
chart	O
showing	O
a	O
similarity	O
measure	O
obtained	O
when	O
a	O
speaker	O
pronounces	O
the	O
Japanese	O
word	O
“	O
sokan	O
”	O
.	O
This	O
word	O
is	O
represented	O
as	O
“	O
/s/	O
,	O
/o/	O
,	O
/q/	O
,	O
/ka/	O
,	O
/n/	O
”	O
(	O
/q/	O
indicates	O
a	O
period	O
of	O
silence	O
)	O
.	O
FIG	O
.	O
7is	O
another	O
chart	O
showing	O
a	O
similarity	O
measure	O
obtained	O
when	O
a	O
speaker	O
pronounces	O
the	O
Japanese	O
word	O
“	O
sokan	O
”	O
.	O

FIG	O
.	O
6shows	O
a	O
case	O
in	O
which	O
no	O
difference	O
is	O
found	O
between	O
a	O
time	O
length	O
of	O
an	O
actually	O
spoken	O
word	O
and	O
a	O
sum	O
of	O
time	O
lengths	O
Lj	O
(	O
j	O
=	O
1	O
to	O
5	O
)	O
of	O
all	O
the	O
phonemics	O
included	O
in	O
the	O
word	O
in	O
the	O
standard	O
dictionary3	O
.	O
In	O
this	O
case	O
,	O
the	O
similarity	O
measure	O
is	O
gradually	O
built	O
up	O
,	O
and	O
exceeds	O
the	O
threshold	O
value	O
SMth	O
before	O
reaching	O
a	O
level	O
sm1as	O
shown	O
in	O
the	O
figure	O
.	O

FIG	O
.	O
7	O
,	O
on	O
the	O
other	O
hand	O
,	O
demonstrates	O
a	O
case	O
in	O
which	O
a	O
pronunciation	O
of	O
the	O
word	O
“	O
sokan	O
”	O
has	O
a	O
spoken	O
time	O
length	O
of	O
/o/	O
thereof	O
longer	O
than	O
a	O
time	O
length	O
of	O
a	O
phonemic	O
/o/	O
stored	O
in	O
the	O
standard	O
dictionary3	O
.	O
At	O
a	O
time	O
t1inFIG	O
.	O
7	O
,	O
a	O
time	O
period	O
corresponding	O
to	O
a	O
time	O
length	O
L2of	O
the	O
phonemic	O
/o/	O
in	O
the	O
standard	O
dictionary3overlaps	O
a	O
time	O
period	O
of	O
the	O
actually	O
spoken	O
phonemic	O
/o/	O
.	O
In	O
an	O
example	O
shown	O
inFIG	O
.	O
7	O
,	O
however	O
,	O
the	O
actually	O
spoken	O
phonemic	O
/o/	O
further	O
extends	O
after	O
the	O
time	O
t1	O
,	O
so	O
that	O
the	O
time	O
length	O
of	O
the	O
actually	O
pronounced	O
phonemic	O
/o/	O
differs	O
from	O
the	O
time	O
length	O
L2of	O
the	O
phonemic	O
/o/	O
in	O
the	O
standard	O
dictionary3	O
.	O
Because	O
of	O
this	O
,	O
the	O
similarity	O
measure	O
decreases	O
toward	O
a	O
time	O
t2	O
.	O
After	O
the	O
time	O
t2	O
,	O
however	O
,	O
subsequent	O
phonemics	O
/q/	O
,	O
/ka/	O
,	O
and	O
/n/	O
serve	O
to	O
increase	O
the	O
similarity	O
measure	O
.	O
The	O
similarity	O
measure	O
finally	O
reaches	O
a	O
level	O
sm2after	O
exceeding	O
the	O
threshold	O
value	O
SMth	O
.	O
The	O
level	O
sm2is	O
lower	O
than	O
the	O
level	O
sml	O
,	O
which	O
is	O
expected	O
to	O
be	O
reached	O
in	O
an	O
ideal	O
situation	O
.	O
The	O
difference	O
between	O
the	O
level	O
sm2and	O
the	O
level	O
sml	O
is	O
equal	O
to	O
2	O
×	O
dsm	O
.	O
Despite	O
the	O
attained	O
level	O
lower	O
than	O
expected	O
,	O
the	O
similarity	O
measure	O
is	O
still	O
larger	O
than	O
the	O
threshold	O
value	O
SMth	O
,	O
so	O
that	O
a	O
recognition	O
result	O
is	O
output	O
in	O
the	O
case	O
of	O
FIG	O
.	O
7	O
.	O
Here	O
,	O
the	O
outputting	O
of	O
the	O
recognition	O
result	O
is	O
carried	O
out	O
after	O
detecting	O
a	O
peak	O
of	O
the	O
similarity	O
measure	O
following	O
a	O
moment	O
at	O
which	O
the	O
similarity	O
measure	O
exceeds	O
a	O
threshold	O
value	O
SMth	O
.	O

In	O
the	O
case	O
ofFIG	O
.	O
7	O
,	O
the	O
actually	O
pronounced	O
length	O
of	O
the	O
phonemic	O
/o/	O
is	O
longer	O
than	O
the	O
time	O
length	O
of	O
the	O
phonemic	O
/o/	O
in	O
the	O
standard	O
dictionary3by	O
a	O
time	O
difference	O
dt	O
,	O
so	O
that	O
the	O
time-length	O
parameter	O
L2of	O
the	O
phonemic	O
/o/	O
in	O
the	O
word	O
“	O
sokan	O
”	O
stored	O
in	O
the	O
standard	O
dictionary3is	O
modified	O
(	O
updated	O
)	O
accordingly	O
.	O
Updating	O
of	O
the	O
parameter	O
L2in	O
the	O
dictionary3may	O
be	O
conducted	O
concurrently	O
with	O
the	O
outputting	O
of	O
the	O
recognition	O
result	O
.	O
In	O
the	O
updating	O
process	O
,	O
the	O
time-length	O
parameter	O
L2has	O
the	O
time	O
difference	O
dt	O
added	O
thereto	O
,	O
so	O
that	O
a	O
new	O
time-length	O
parameter	O
L2	O
′	O
is	O
obtained	O
.	O
This	O
is	O
represented	O
as	O
:	O
L2	O
′	O
=	O
L2	O
+	O
dt	O
Alternatively	O
,	O
the	O
time	O
difference	O
dt	O
may	O
be	O
multiplied	O
by	O
the	O
predetermined	O
adjusting	O
value	O
K	O
in	O
order	O
to	O
obtain	O
the	O
new	O
time-length	O
parameter	O
L2	O
′	O
as	O
shown	O
in	O
the	O
following	O
.	O

L2	O
′	O
=	O
L2	O
+	O
K	O
·	O
dt	O
Here	O
,	O
K	O
may	O
have	O
a	O
value	O
ranging	O
from	O
about	O
0	O
.	O
1	O
to	O
about	O
0	O
.	O
7	O
,	O
and	O
may	O
vary	O
adaptively	O
depending	O
on	O
how	O
many	O
times	O
a	O
correct	O
recognition	O
result	O
is	O
obtained	O
with	O
respect	O
to	O
a	O
given	O
recognition	O
object	O
as	O
well	O
as	O
how	O
great	O
the	O
similarity	O
measure	O
is	O
.	O

In	O
this	O
manner	O
,	O
the	O
present	O
invention	O
compares	O
an	O
average	O
time	O
length	O
of	O
an	O
actual	O
pronunciation	O
with	O
a	O
time	O
length	O
obtained	O
from	O
the	O
dictionary	O
in	O
accordance	O
with	O
the	O
result	O
of	O
speech	O
recognition	O
,	O
and	O
dynamically	O
modifies	O
time-length	O
parameters	O
of	O
the	O
dictionary	O
so	O
as	O
to	O
enhance	O
performance	O
.	O

The	O
time-length	O
parameters	O
stored	O
in	O
the	O
standard	O
dictionary3have	O
a	O
one-to-one	O
correspondence	O
with	O
phonemics	O
.	O
That	O
is	O
,	O
a	O
time-length	O
parameter	O
is	O
provided	O
in	O
the	O
standard	O
dictionary	O
with	O
respect	O
to	O
each	O
phonemic	O
.	O
Accordingly	O
,	O
when	O
a	O
time-length	O
parameter	O
is	O
updated	O
in	O
the	O
standard	O
dictionary3with	O
respect	O
to	O
a	O
given	O
phonemic	O
in	O
a	O
given	O
recognition	O
object	O
(	O
recognized	O
word	O
)	O
,	O
the	O
same	O
phonemic	O
in	O
other	O
recognition	O
objects	O
(	O
other	O
words	O
)	O
stored	O
in	O
the	O
standard	O
dictionary3can	O
have	O
a	O
time-length	O
parameter	O
thereof	O
also	O
updated	O
.	O
For	O
example	O
,	O
the	O
word	O
“	O
koshin	O
”	O
is	O
stored	O
in	O
the	O
dictionary	O
,	O
and	O
includes	O
the	O
phonemic	O
/o/	O
as	O
does	O
the	O
word	O
“	O
sokan	O
”	O
.	O
When	O
the	O
time-length	O
parameter	O
of	O
the	O
phonemic	O
/o/	O
is	O
updated	O
in	O
the	O
word	O
“	O
sokan	O
”	O
,	O
the	O
time-length	O
parameter	O
of	O
the	O
phonemic	O
/o/	O
in	O
the	O
word	O
“	O
koshin	O
”	O
may	O
also	O
be	O
changed	O
at	O
the	O
same	O
time	O
so	O
as	O
to	O
provide	O
a	O
new	O
time-length	O
parameter	O
.	O

FIG	O
.	O
8is	O
a	O
block	O
diagram	O
showing	O
a	O
hardware	O
configuration	O
of	O
the	O
speech-recognition	O
device	O
shown	O
in	O
FIG	O
.	O
3	O
.	O
As	O
shown	O
inFIG	O
.	O
8	O
,	O
the	O
speech-recognition	O
device	O
may	O
be	O
implemented	O
by	O
using	O
a	O
personal	O
computer	O
or	O
the	O
like	O
,	O
and	O
includes	O
a	O
CPU21	O
,	O
a	O
ROM22	O
,	O
a	O
RAM	O
23	O
,	O
an	O
input	O
device24	O
,	O
and	O
a	O
result	O
outputting	O
device26	O
.	O
The	O
CPU21attends	O
to	O
overall	O
control	O
.	O
The	O
ROM22stores	O
control	O
programs	O
and	O
the	O
like	O
used	O
by	O
the	O
CPU21	O
.	O
The	O
RAM23is	O
used	O
as	O
a	O
work	O
area	O
used	O
by	O
the	O
CPU21	O
.	O
The	O
input	O
device24is	O
used	O
for	O
inputting	O
speech	O
.	O
The	O
result	O
outputting	O
device	O
(	O
e.g.	O
,	O
display	O
or	O
printer	O
)	O
26outputs	O
speech-recognition	O
results	O
.	O

The	O
CPU21provides	O
functions	O
of	O
the	O
feature-extraction	O
unit2	O
,	O
the	O
matching	O
unit4	O
,	O
the	O
dictionary	O
updating	O
unit6	O
,	O
etc	O
.	O
,	O
shown	O
in	O
FIG	O
.	O
3	O
.	O

The	O
functions	O
of	O
the	O
feature-extraction	O
unit2	O
,	O
the	O
matching	O
unit4	O
,	O
the	O
dictionary	O
updating	O
unit6	O
,	O
etc	O
.	O
,	O
are	O
implemented	O
via	O
software	O
,	O
which	O
is	O
supplied	O
in	O
a	O
software	O
package	O
stored	O
in	O
a	O
memory	O
medium	O
such	O
as	O
a	O
CD-ROM	O
.	O
InFIG	O
.	O
8	O
,	O
therefore	O
,	O
the	O
speech-recognition	O
device	O
further	O
includes	O
a	O
media	O
driving	O
unit31	O
,	O
which	O
drives	O
a	O
memory	O
medium30when	O
it	O
is	O
set	O
in	O
the	O
driving	O
unit31	O
.	O

In	O
other	O
words	O
,	O
the	O
speech-recognition	O
device	O
according	O
to	O
the	O
present	O
invention	O
may	O
be	O
implemented	O
such	O
that	O
a	O
general-purpose	O
computer	O
system	O
has	O
programs	O
loaded	O
thereto	O
from	O
a	O
memory	O
medium	O
such	O
as	O
a	O
CD-ROM	O
,	O
and	O
allows	O
a	O
micro-processor	O
thereof	O
to	O
execute	O
a	O
speech-recognition	O
process	O
.	O
In	O
this	O
configuration	O
,	O
programs	O
for	O
performing	O
a	O
speech-recognition	O
process	O
of	O
the	O
present	O
invention	O
(	O
i	O
.	O
e	O
.	O
;	O
programs	O
used	O
by	O
the	O
hardware	O
system	O
)	O
are	O
supplied	O
by	O
way	O
of	O
a	O
memory	O
medium	O
.	O
The	O
memory	O
medium	O
for	O
storing	O
the	O
programs	O
is	O
not	O
limited	O
to	O
a	O
CD-ROM	O
,	O
but	O
includes	O
a	O
ROM	O
,	O
a	O
RAM	O
,	O
a	O
flexible	O
disc	O
,	O
a	O
memory	O
card	O
,	O
etc	O
.	O
The	O
programs	O
stored	O
in	O
the	O
memory	O
medium	O
are	O
installed	O
in	O
a	O
memory	O
device	O
built	O
in	O
as	O
part	O
of	O
the	O
hardware	O
system	O
.	O
An	O
example	O
of	O
such	O
a	O
memory	O
device	O
is	O
a	O
hard	O
drive	O
.	O
The	O
programs	O
are	O
executed	O
so	O
as	O
to	O
provide	O
functions	O
of	O
the	O
speech-recognition	O
process	O
.	O

The	O
programs	O
for	O
providing	O
the	O
speech-recognition	O
process	O
of	O
the	O
present	O
invention	O
may	O
be	O
supplied	O
via	O
a	O
communication	O
channel	O
from	O
a	O
server	O
,	O
for	O
example	O
,	O
in	O
stead	O
of	O
being	O
supplied	O
in	O
the	O
form	O
of	O
memory	O
medium	O
.	O

Further	O
,	O
the	O
present	O
invention	O
is	O
not	O
limited	O
to	O
these	O
embodiments	O
,	O
but	O
various	O
variations	O
and	O
modifications	O
may	O
be	O
made	O
without	O
departing	O
from	O
the	O
scope	O
of	O
the	O
present	O
invention	O
.	O

1	O
.	O
A	O
device	O
for	O
speech	O
recognition	O
,	O
comprising:a	O
codebook	O
which	O
stores	O
features	O
of	O
recognition	O
objects	O
and	O
time	O
lengths	O
of	O
phonemics	O
,	O
wherein	O
the	O
time	O
lengths	O
are	O
an	O
average	O
time	O
length	O
defined	O
for	O
each	O
phonemic	O
;	O
a	O
matching	O
unit	O
which	O
compares	O
features	O
of	O
input	O
speech	O
with	O
the	O
features	O
of	O
the	O
recognition	O
objects	O
;	O
anda	O
codebook	O
updating	O
unit	O
which	O
updates	O
the	O
time	O
lengths	O
of	O
phonemics	O
in	O
said	O
codebook	O
based	O
on	O
the	O
input	O
speech	O
when	O
said	O
matching	O
unit	O
finds	O
substantial	O
similarities	O
between	O
the	O
input	O
speech	O
and	O
one	O
of	O
the	O
recognition	O
objects	O
.	O

2	O
.	O
The	O
device	O
as	O
claimed	O
in	O
claim1	O
,	O
further	O
comprisinga	O
feature-extraction	O
unit	O
which	O
extracts	O
the	O
features	O
of	O
input	O
speech	O
from	O
the	O
input	O
speech	O
without	O
detecting	O
speech	O
frames	O
,	O
andwherein	O
said	O
matching	O
unit	O
compares	O
the	O
features	O
of	O
input	O
speech	O
with	O
the	O
features	O
of	O
the	O
recognition	O
objects	O
so	O
as	O
to	O
produce	O
a	O
similarity	O
measure	O
continuously	O
without	O
breaks	O
of	O
speech	O
frames	O
,	O
and	O
said	O
dictionary	O
updating	O
unit	O
updates	O
the	O
time	O
lengths	O
of	O
phonemics	O
when	O
the	O
similarity	O
measure	O
exceeds	O
a	O
predetermined	O
threshold	O
.	O

3	O
.	O
The	O
device	O
as	O
claimed	O
in	O
claim1	O
,	O
wherein	O
said	O
dictionary	O
updating	O
unit	O
compares	O
a	O
sum	O
of	O
the	O
time	O
lengths	O
of	O
phonemics	O
constituting	O
said	O
one	O
of	O
the	O
recognition	O
objects	O
with	O
an	O
actual	O
time	O
length	O
of	O
the	O
input	O
speech	O
corresponding	O
to	O
said	O
one	O
of	O
the	O
recognition	O
objects	O
,	O
and	O
updates	O
the	O
time	O
lengths	O
of	O
the	O
phonemics	O
in	O
said	O
dictionary	O
based	O
on	O
a	O
difference	O
between	O
the	O
sum	O
and	O
the	O
actual	O
time	O
length	O
.	O

4	O
.	O
The	O
device	O
as	O
claimed	O
in	O
claim1	O
,	O
wherein	O
said	O
dictionary	O
updating	O
unit	O
obtains	O
a	O
difference	O
between	O
a	O
time	O
length	O
of	O
a	O
phonemic	O
in	O
said	O
dictionary	O
and	O
a	O
time	O
length	O
of	O
the	O
same	O
phonemic	O
in	O
the	O
input	O
speech	O
,	O
and	O
adds	O
the	O
difference	O
to	O
the	O
time	O
length	O
of	O
the	O
phonemic	O
in	O
said	O
dictionary	O
so	O
as	O
to	O
update	O
the	O
time	O
lengths	O
of	O
phonemics	O
in	O
said	O
dictionary	O
.	O

5	O
.	O
The	O
device	O
as	O
claimed	O
in	O
claim1	O
,	O
wherein	O
said	O
dictionary	O
updating	O
unit	O
obtains	O
a	O
difference	O
between	O
a	O
time	O
length	O
of	O
a	O
phonemic	O
in	O
said	O
dictionary	O
and	O
a	O
time	O
length	O
of	O
the	O
same	O
phonemic	O
in	O
the	O
input	O
speech	O
,	O
and	O
adds	O
the	O
difference	O
multiplied	O
by	O
a	O
given	O
weighting	O
factor	O
to	O
the	O
time	O
length	O
of	O
the	O
phonemic	O
in	O
said	O
dictionary	O
so	O
as	O
to	O
update	O
the	O
time	O
lengths	O
of	O
phonemics	O
in	O
said	O
dictionary	O
.	O

6	O
.	O
The	O
device	O
as	O
claimed	O
in	O
claim5	O
,	O
wherein	O
said	O
weighting	O
factor	O
varies	O
depending	O
on	O
how	O
great	O
the	O
substantial	O
similarities	O
are	O
.	O

7	O
.	O
The	O
device	O
as	O
claimed	O
in	O
claim5	O
,	O
wherein	O
said	O
weighting	O
factor	O
varies	O
depending	O
on	O
how	O
many	O
times	O
the	O
matching	O
unit	O
correctly	O
finds	O
the	O
substantial	O
similarities	O
with	O
respect	O
to	O
said	O
one	O
of	O
the	O
recognition	O
objects	O
.	O

8	O
.	O
The	O
device	O
as	O
claimed	O
in	O
claim1	O
,	O
wherein	O
said	O
dictionary	O
updating	O
unit	O
updates	O
a	O
time	O
length	O
of	O
a	O
given	O
phonemic	O
in	O
said	O
dictionary	O
not	O
only	O
with	O
respect	O
to	O
said	O
one	O
of	O
the	O
recognition	O
objects	O
but	O
also	O
with	O
respect	O
to	O
all	O
the	O
recognition	O
objects	O
that	O
include	O
the	O
given	O
phonemic	O
.	O

9	O
.	O
A	O
device	O
for	O
speech	O
recognition	O
,	O
comprising:codebook	O
means	O
for	O
storing	O
features	O
of	O
recognition	O
objects	O
and	O
time	O
lengths	O
of	O
phonemics	O
,	O
wherein	O
the	O
time	O
lengths	O
are	O
an	O
average	O
time	O
length	O
defined	O
for	O
each	O
phonemic;matching	O
means	O
for	O
comparing	O
features	O
of	O
input	O
speech	O
with	O
the	O
features	O
of	O
recognition	O
objects	O
;	O
andcodebook	O
updating	O
means	O
for	O
updating	O
the	O
time	O
lengths	O
of	O
phonemics	O
in	O
said	O
codebook	O
means	O
based	O
on	O
the	O
input	O
speech	O
when	O
said	O
matching	O
means	O
finds	O
substantial	O
similarities	O
between	O
the	O
input	O
speech	O
and	O
one	O
of	O
the	O
recognition	O
objects	O
.	O

10	O
.	O
A	O
method	O
of	O
conducting	O
speech	O
recognition	O
,	O
comprising	O
the	O
steps	O
of	O
:	O
a	O
)	O
storing	O
features	O
of	O
recognition	O
objects	O
and	O
time	O
lengths	O
of	O
phonemics	O
in	O
a	O
codebook	O
,	O
wherein	O
the	O
time	O
length	O
are	O
an	O
average	O
time	O
length	O
defined	O
for	O
each	O
phonemic	O
;	O
b	O
)	O
comparing	O
features	O
of	O
input	O
speech	O
with	O
the	O
features	O
of	O
the	O
recognition	O
objects	O
;	O
andc	O
)	O
updating	O
the	O
time	O
lengths	O
of	O
phonemics	O
in	O
said	O
codebook	O
based	O
on	O
the	O
input	O
speech	O
when	O
said	O
step	O
b	O
)	O
finds	O
substantial	O
similarities	O
between	O
the	O
input	O
speech	O
and	O
one	O
of	O
the	O
recognition	O
objects	O
.	O

11	O
.	O
The	O
method	O
as	O
claimed	O
in	O
claim10	O
,	O
further	O
comprising	O
a	O
step	O
ofextracting	O
the	O
features	O
of	O
input	O
speech	O
from	O
the	O
input	O
speech	O
without	O
detecting	O
speech	O
frames	O
,	O
andwherein	O
said	O
step	O
b	O
)	O
compares	O
the	O
features	O
of	O
input	O
speech	O
with	O
the	O
features	O
of	O
the	O
recognition	O
objects	O
so	O
as	O
to	O
produce	O
a	O
similarity	O
measure	O
continuously	O
without	O
breaks	O
of	O
speech	O
frames	O
,	O
and	O
said	O
step	O
c	O
)	O
updates	O
the	O
time	O
lengths	O
of	O
phonemics	O
when	O
the	O
similarity	O
measure	O
exceeds	O
a	O
predetermined	O
threshold	O
.	O

12	O
.	O
The	O
method	O
as	O
claimed	O
in	O
claim10	O
,	O
wherein	O
said	O
step	O
c	O
)	O
compares	O
a	O
sum	O
of	O
the	O
time	O
lengths	O
of	O
phonemics	O
constituting	O
said	O
one	O
of	O
the	O
recognition	O
objects	O
with	O
an	O
actual	O
time	O
length	O
of	O
the	O
input	O
speech	O
corresponding	O
to	O
said	O
one	O
of	O
the	O
recognition	O
objects	O
,	O
and	O
updates	O
the	O
time	O
lengths	O
of	O
the	O
phonemics	O
in	O
said	O
dictionary	O
based	O
on	O
a	O
difference	O
between	O
the	O
sum	O
and	O
the	O
actual	O
time	O
length	O
.	O

13	O
.	O
A	O
machine-readable	O
memory	O
medium	O
having	O
a	O
program	O
embodied	O
therein	O
for	O
causing	O
a	O
computer	O
to	O
perform	O
a	O
speech	O
recognition	O
,	O
said	O
program	O
comprising:a	O
codebook	O
configured	O
to	O
store	O
features	O
of	O
recognition	O
objects	O
and	O
time	O
lengths	O
of	O
phonemics	O
,	O
wherein	O
the	O
time	O
lengths	O
are	O
an	O
average	O
time	O
length	O
defined	O
for	O
each	O
phonemic	O
;	O
a	O
matching	O
unit	O
configured	O
to	O
compare	O
features	O
of	O
input	O
speech	O
with	O
the	O
features	O
of	O
the	O
recognition	O
objects	O
;	O
anda	O
codebook	O
updating	O
unit	O
configured	O
to	O
update	O
the	O
time	O
lengths	O
of	O
phonemics	O
in	O
said	O
codebook	O
based	O
on	O
the	O
input	O
speech	O
when	O
said	O
matching	O
unit	O
finds	O
substantial	O
similarities	O
between	O
the	O
input	O
speech	O
and	O
one	O
of	O
the	O
recognition	O
objects	O
.	O

14	O
.	O
The	O
machine-readable	O
memory	O
medium	O
as	O
claimed	O
in	O
claim13	O
,	O
wherein	O
said	O
program	O
further	O
comprisesa	O
feature-extraction	O
unit	O
configured	O
to	O
extract	O
the	O
features	O
of	O
input	O
speech	O
from	O
the	O
input	O
speech	O
without	O
detecting	O
speech	O
frames	O
,	O
andwherein	O
said	O
matching	O
unit	O
compares	O
the	O
features	O
of	O
input	O
speech	O
with	O
the	O
features	O
of	O
the	O
recognition	O
objects	O
so	O
as	O
to	O
produce	O
a	O
similarity	O
measure	O
continuously	O
without	O
breaks	O
of	O
speech	O
frames	O
,	O
and	O
said	O
dictionary	O
updating	O
unit	O
updates	O
the	O
time	O
lengths	O
of	O
phonemics	O
when	O
the	O
similarity	O
measure	O
exceeds	O
a	O
predetermined	O
threshold	O
.	O

15	O
.	O
The	O
machine-readable	O
memory	O
medium	O
as	O
claimed	O
in	O
claim13	O
,	O
wherein	O
said	O
dictionary	O
updating	O
unit	O
compares	O
a	O
sum	O
of	O
the	O
time	O
lengths	O
of	O
phonemics	O
constituting	O
said	O
one	O
of	O
the	O
recognition	O
objects	O
with	O
an	O
actual	O
time	O
length	O
of	O
the	O
input	O
speech	O
corresponding	O
to	O
said	O
one	O
of	O
the	O
recognition	O
objects	O
,	O
and	O
updates	O
the	O
time	O
lengths	O
of	O
the	O
phonemics	O
in	O
said	O
dictionary	O
based	O
on	O
a	O
difference	O
between	O
the	O
sum	O
and	O
the	O
actual	O
time	O
length	O
.	O

