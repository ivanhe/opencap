US	O
6009390	O
A	O
19991228	O

US	O
08927883	O
19970911	O

eng	O
eng	O

19991228	O

19991228	O

6G	O
10L	O
7/08	O
A	O
6	O
G	O
10	O
L	O
7	O
08	O
A	O

G10L	O
15/00	O
20060101C	O
I20051008RMEP	O

20060101	O

C	O
G	O
10	O
L	O
15	O
00	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/14	O
20060101A	O
I20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
14	O
I	O

20051008	O

EP	O

R	O
M	O

US	O

704/240	O
704	O
240	O

704/245	O
704	O
245	O

704/E15.033	O
704	O
E15	O
.	O
033	O

G10L	O
15/14M4	O
G	O
10	O
L	O
15	O
14	O

M	O
4	O

US	O

704/240	O
704	O
240	O

US	O

704/243	O
704	O
243	O

US	O

704/245	O
704	O
245	O

US	O

704/255	O
704	O
255	O

US	O

704/256	O
704	O
256	O

32	O
Technique	O
for	O
selective	O
use	O
of	O
Gaussian	O
kernels	O
and	O
mixture	O
component	O
weights	O
of	O
tied-mixture	O
hidden	O
Markov	O
models	O
for	O
speech	O
recognition	O

US	O
5473728	O
A	O
Luginbuhl	O
et_al.	O

19951205	O

19930224	O

US	O

704/243	O
704	O
243	O

US	O
5794198	O
A	O
Takahashi	O
et_al.	O

19980811	O

19951024	O

US	O

704/256	O
704	O
256	O

US	O
5825978	O
A	O
Digalakis	O
et_al.	O

19981020	O

19940718	O

US	O

704/256	O
704	O
256	O

Digalakis	B-Citation
,	I-Citation
V.	I-Citation
V.	I-Citation
;	I-Citation
Rtischev	I-Citation
,	I-Citation
D.	I-Citation
;	I-Citation
Neumeyer	I-Citation
,	I-Citation
L.	I-Citation
G.	I-Citation
,	I-Citation
Speaker	I-Citation
adaptation	I-Citation
using	I-Citation
constrained	I-Citation
estimation	I-Citation
of	I-Citation
Gaussian	I-Citation
mixtures	I-Citation
,	I-Citation
Speech	I-Citation
and	I-Citation
Audio	I-Citation
Processing	I-Citation
,	I-Citation
IEEE	I-Citation
Transactions	I-Citation
on	I-Citation
,	I-Citation
vol.	I-Citation
3	I-Citation
,	I-Citation
No.	I-Citation
5	I-Citation
,	I-Citation
Sep.	I-Citation
1995	I-Citation
,	I-Citation
pp.	I-Citation
357	I-Citation
366	I-Citation
.	O

Digalakis	B-Citation
,	I-Citation
V.	I-Citation
V.	I-Citation
;	I-Citation
Monaco	I-Citation
,	I-Citation
P.	I-Citation
;	I-Citation
and	I-Citation
Murveit	I-Citation
,	I-Citation
H.	I-Citation
,	I-Citation
Genones	I-Citation
:	I-Citation
generalized	I-Citation
mixture	I-Citation
tying	I-Citation
in	I-Citation
continuous	I-Citation
hidden	I-Citation
Markov	I-Citation
model	I-Citation
based	I-Citation
speech	I-Citation
recognizers	I-Citation
,	I-Citation
Speech	I-Citation
and	I-Citation
Audio	I-Citation
Processing	I-Citation
,	I-Citation
IEEE	I-Citation
Transactions	I-Citation
on	I-Citation
,	I-Citation
vol.	I-Citation
4	I-Citation
4	I-Citation
,	I-Citation
pp.	I-Citation
281	I-Citation
289	I-Citation
,	I-Citation
Jul.	I-Citation
1996	I-Citation
.	O

J.	B-Citation
R.	I-Citation
Bellegarda	I-Citation
et_al.	I-Citation
,	I-Citation
Tied	I-Citation
Mixture	I-Citation
Continuous	I-Citation
Parameter	I-Citation
Modeling	I-Citation
for	I-Citation
Speech	I-Citation
Recognition	I-Citation
,	I-Citation
IEEE	I-Citation
Trans	I-Citation
.	I-Citation
Acoustics	I-Citation
Speech	I-Citation
Signal	I-Citation
Process	I-Citation
.	I-Citation
,	I-Citation
vol.	I-Citation
38	I-Citation
,	I-Citation
No.	I-Citation
12	I-Citation
,	I-Citation
1990	I-Citation
,	I-Citation
pp.	I-Citation
2033	I-Citation
2045	I-Citation
.	O

X.	B-Citation
D.	I-Citation
Huang	I-Citation
et_al.	I-Citation
,	I-Citation
Semi	I-Citation
Continuous	I-Citation
Hidden	I-Citation
Markov	I-Citation
Models	I-Citation
for	I-Citation
Speech	I-Citation
Signals	I-Citation
,	I-Citation
Computer	I-Citation
Speech	I-Citation
and	I-Citation
Language	I-Citation
,	I-Citation
vol.	I-Citation
3	I-Citation
,	I-Citation
1989	I-Citation
,	I-Citation
pp.	I-Citation
239	I-Citation
251	I-Citation
.	O

E.	B-Citation
Bocchieri	I-Citation
,	I-Citation
A	I-Citation
study	I-Citation
of	I-Citation
the	I-Citation
Beam	I-Citation
Search	I-Citation
Algorithm	I-Citation
for	I-Citation
Large	I-Citation
Vocabulary	I-Citation
Continuous	I-Citation
Speech	I-Citation
Recognition	I-Citation
and	I-Citation
Methods	I-Citation
for	I-Citation
Improved	I-Citation
Efficiency	I-Citation
,	I-Citation
Proceedings	I-Citation
Eurospeech	I-Citation
,	I-Citation
1993	I-Citation
,	I-Citation
pp.	I-Citation
1521	I-Citation
1524	I-Citation
.	O

Y.	B-Citation
Linde	I-Citation
et_al.	I-Citation
,	I-Citation
An	I-Citation
Algorithm	I-Citation
for	I-Citation
Vector	I-Citation
Quantizer	I-Citation
Design	I-Citation
,	I-Citation
IEEE	I-Citation
Trans	I-Citation
.	I-Citation
Communications	I-Citation
,	I-Citation
vol.	I-Citation
COM	I-Citation
28	I-Citation
,	I-Citation
Jan.	I-Citation
1980	I-Citation
,	I-Citation
pp.	I-Citation
84	I-Citation
95	I-Citation
.	O

E.	B-Citation
Bocchieri	I-Citation
,	I-Citation
Vector	I-Citation
Quantization	I-Citation
for	I-Citation
the	I-Citation
Efficient	I-Citation
Computation	I-Citation
of	I-Citation
Continuous	I-Citation
Density	I-Citation
Likelihoods	I-Citation
,	I-Citation
IEEE	I-Citation
,	I-Citation
1993	I-Citation
,	I-Citation
pp.	I-Citation
II	I-Citation
692	I-Citation
695	I-Citation
.	O

P.	B-Citation
Lockwood	I-Citation
et_al.	I-Citation
,	I-Citation
Experiments	I-Citation
with	I-Citation
a	I-Citation
Non	I-Citation
Linear	I-Citation
Spectral	I-Citation
Subtractor	I-Citation
(	I-Citation
NSS	I-Citation
)	I-Citation
,	I-Citation
Hidden	I-Citation
Markov	I-Citation
Models	I-Citation
and	I-Citation
the	I-Citation
Projection	I-Citation
,	I-Citation
for	I-Citation
Robust	I-Citation
Speech	I-Citation
Recognition	I-Citation
in	I-Citation
Cars	I-Citation
,	I-Citation
Proceedings	I-Citation
Eurospeech	I-Citation
,	I-Citation
1993	I-Citation
,	I-Citation
pp.	I-Citation
79	I-Citation
82	I-Citation
.	O

US	O
6092042	O
A	O
20000718	O

19980331	O

US	O
7184959	O
B2	O
20070227	O

20031015	O

US	O
7539617	O
B2	O
20090526	O

20030701	O

US	O
7652593	O
B1	O
20100126	O

20061005	O

US	O
7688225	O
B1	O
20100330	O

20071022	O

US	O
7439981	O
B2	O
20081021	O

20041021	O

US	O
7315307	O
B2	O
20080101	O

20040520	O

US	O
7315308	O
B2	O
20080101	O

20040521	O

US	O
7269556	O
B2	O
20070911	O

20030326	O

US	O
7239324	O
B2	O
20070703	O

20020215	O

US	O
7216066	O
B2	O
20070508	O

20060222	O

US	O
7020587	O
B1	O
20060328	O

20000630	O

US	O
6804648	O
B1	O
20041012	O

19990325	O

US	O
6816085	O
B1	O
20041109	O

20001117	O

US	O
6725195	O
B2	O
20040420	O

20011022	O

US	O
6741966	O
B2	O
20040525	O

20010122	O

US	O
6246982	O
B1	O
20010612	O

19990126	O

US	O
7778831	O
B2	O
20100817	O

20060221	O

US	O
7813925	O
B2	O
20101012	O

20060406	O

US	O
7912715	O
B2	O
20110322	O

20030327	O

US	O
7970613	O
B2	O
20110628	O

20051112	O

WO	O
2011071560	O
A1	O
20110616	O

20100623	O

US	O
8010358	O
B2	O
20110830	O

20060221	O

Lucent	O
Technologies	O
Inc.	O

Murray	O
Hill	O
US	O

Gupta	O
Sunil	O
K.	O

Edison	O
US	O

Haimi-Cohen	O
Raziel	O

Springfield	O
US	O

Soong	O
Frank	O
K.	O

Warren	O
US	O

Hudspeth	O
;	O
David	O
R.	O

Storm	O
;	O
Donald	O
L.	O

US	O
6009390	O
A	O
19991228	O

19970911	O

US	O
6009390	O
A	O
19991228	O

19970911	O

In	O
a	O
speech	O
recognition	O
system	O
,	O
tied-mixture	O
hidden	O
Markov	O
models	O
(	O
HMMs	O
)	O
are	O
used	O
to	O
match	O
,	O
in	O
the	O
maximum	O
likelihood	O
sense	O
,	O
the	O
phonemes	O
of	O
spoken	O
words	O
given	O
the	O
acoustic	O
input	O
thereof	O
.	O
In	O
a	O
well	O
known	O
manner	O
,	O
such	O
speech	O
recognition	O
requires	O
computation	O
of	O
state	O
observation	O
likelihoods	O
(	O
SOLs	O
)	O
.	O
Because	O
of	O
the	O
use	O
of	O
HMMs	O
,	O
each	O
SOL	O
computation	O
involves	O
a	O
substantial	O
number	O
of	O
Gaussian	O
kernels	O
and	O
mixture	O
component	O
weights	O
.	O
In	O
accordance	O
with	O
the	O
invention	O
,	O
the	O
number	O
of	O
Gaussian	O
kernels	O
is	O
cut	O
down	O
to	O
reduce	O
the	O
computational	O
complexity	O
and	O
increase	O
the	O
efficiency	O
of	O
memory	O
access	O
to	O
the	O
kernels	O
.	O
For	O
example	O
,	O
only	O
the	O
non-zero	O
mixture	O
component	O
weights	O
and	O
the	O
Gaussian	O
kernels	O
associated	O
therewith	O
are	O
considered	O
in	O
the	O
SOL	O
computation	O
.	O
In	O
accordance	O
with	O
an	O
aspect	O
of	O
the	O
invention	O
,	O
only	O
a	O
subset	O
of	O
the	O
Gaussian	O
kernels	O
of	O
significant	O
values	O
,	O
regardless	O
of	O
the	O
values	O
of	O
the	O
associated	O
mixture	O
component	O
weights	O
,	O
are	O
considered	O
in	O
the	O
SOL	O
computation	O
.	O
In	O
accordance	O
with	O
another	O
aspect	O
of	O
the	O
invention	O
,	O
at	O
least	O
some	O
of	O
the	O
mixture	O
component	O
weights	O
are	O
quantized	O
to	O
reduce	O
memory	O
space	O
needed	O
to	O
store	O
them	O
.	O
As	O
such	O
,	O
the	O
computational	O
complexity	O
and	O
memory	O
access	O
efficiency	O
are	O
further	O
improved	O
.	O

19970911	O

AS	O
ASSIGNMENT	O
N	O
US	O
6009390A	O
LUCENT	O
TECHNOLOGIES	O
INC.	O
,	O
NEW	O
JERSEY	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNORS:GUPTA	O
,	O
SUNIL	O
K.;HAIMI-COHEN	O
,	O
RAZIEL;SOONG	O
,	O
FRANK	O
K.;REEL/FRAME:008798/0298	O

19970908	O

20010405	O

AS	O
ASSIGNMENT	O
N	O
US	O
6009390A	O
THE	O
CHASE	O
MANHATTAN	O
BANK	O
,	O
AS	O
COLLATERAL	O
AGENT	O
,	O
TEX	O
CONDITIONAL	O
ASSIGNMENT	O
OF	O
AND	O
SECURITY	O
INTEREST	O
IN	O
PATENT	O
RIGHTS;ASSIGNOR:LUCENT	O
TECHNOLOGIES	O
INC.	O
(	O
DE	O
CORPORATION	O
)	O
;REEL/FRAME:011722/0048	O

20010222	O

20030501	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
6009390A	O
4	O

20061206	O

AS	O
ASSIGNMENT	O
C	O
US	O
6009390A	O
LUCENT	O
TECHNOLOGIES	O
INC.	O
,	O
NEW	O
JERSEY	O
TERMINATION	O
AND	O
RELEASE	O
OF	O
SECURITY	O
INTEREST	O
IN	O
PATENT	O
RIGHTS;ASSIGNOR:JPMORGAN	O
CHASE	O
BANK	O
,	O
N.	O
A.	O
(	O
FORMERLY	O
KNOWN	O
AS	O
THE	O
CHASE	O
MANHATTAN	O
BANK	O
)	O
,	O
AS	O
ADMINISTRATIVE	O
AGENT;REEL/FRAME:018590/0047	O

20061130	O

20070622	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
6009390A	O
8	O

20110623	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
6009390A	O
12	O

This	O
application	O
claims	O
priority	O
of	O
U.S.	O
Provisional	O
Application	O
No.	O
60/025	O
,	O
300	O
,	O
filed	O
on	O
Sep.	O
18	O
,	O
1996	O
,	O
now	O
abandoned	O
.	O

FIELD	O
OF	O
THE	O
INVENTION	O
The	O
invention	O
relates	O
to	O
speech	O
recognition	O
systems	O
and	O
methods	O
,	O
and	O
more	O
particularly	O
to	O
systems	O
and	O
methods	O
for	O
recognizing	O
speech	O
based	O
on	O
tied-mixture	O
hidden	O
Markov	O
models	O
(	O
HMMs	O
)	O
.	O

BACKGROUND	O
OF	O
THE	O
INVENTION	O
In	O
communication	O
,	O
data	O
processing	O
and	O
similar	O
systems	O
,	O
a	O
user	O
interface	O
using	O
audio	O
facilities	O
is	O
often	O
advantageous	O
especially	O
when	O
it	O
is	O
anticipated	O
that	O
the	O
user	O
would	O
be	O
physically	O
engaged	O
in	O
an	O
activity	O
(	O
e.g.	O
,	O
driving	O
a	O
car	O
)	O
while	O
he/she	O
is	O
operating	O
one	O
such	O
system	O
.	O
Techniques	O
for	O
recognizing	O
human	O
speech	O
in	O
such	O
systems	O
to	O
perform	O
certain	O
tasks	O
have	O
been	O
developed	O
.	O

In	O
accordance	O
with	O
one	O
such	O
technique	O
,	O
input	O
speech	O
is	O
analyzed	O
in	O
signal	O
frames	O
,	O
represented	O
by	O
feature	O
vectors	O
corresponding	O
to	O
phonemes	O
making	O
up	O
individual	O
words	O
.	O
The	O
phonemes	O
are	O
characterized	O
by	O
hidden	O
Markov	O
models	O
(	O
HMMs	O
)	O
,	O
and	O
a	O
Viterbi	O
algorithm	O
is	O
used	O
to	O
identify	O
a	O
sequence	O
of	O
HMMs	O
which	O
best	O
matches	O
,	O
in	O
a	O
maximum	O
likelihood	O
sense	O
,	O
a	O
respective	O
concatenation	O
of	O
phonemes	O
corresponding	O
to	O
an	O
unknown	O
,	O
spoken	O
utterance	O
.	O
The	O
Viterbi	O
algorithm	O
forms	O
a	O
plurality	O
of	O
sequences	O
of	O
tentative	O
decisions	O
as	O
to	O
what	O
the	O
uttered	O
phonemes	O
were	O
.	O
These	O
sequences	O
of	O
tentative	O
decisions	O
define	O
the	O
so-called	O
"	O
survival	O
paths	O
.	O
"	O
The	O
theory	O
of	O
the	O
Viterbi	O
algorithm	O
predicts	O
that	O
these	O
survival	O
paths	O
merge	O
to	O
the	O
"	O
maximum-likelihood	O
path	O
"	O
going	O
back	O
in	O
time	O
.	O
See	O
G.	B-Citation
D.	I-Citation
Forney	I-Citation
,	I-Citation
"	I-Citation
The	I-Citation
Viterbi	I-Citation
Algorithm	I-Citation
,	I-Citation
"	I-Citation
Proceedings	I-Citation
of	I-Citation
the	I-Citation
IEEE	I-Citation
,	I-Citation
Vol.	I-Citation
761	I-Citation
,	I-Citation
No.	I-Citation
3	I-Citation
,	I-Citation
March	I-Citation
1973	I-Citation
,	I-Citation
pp.	I-Citation
268	I-Citation
-	I-Citation
278	I-Citation
.	O
In	O
this	O
instance	O
,	O
such	O
a	O
maximum-likelihood	O
path	O
corresponds	O
to	O
a	O
particular	O
concatenation	O
of	O
phonemes	O
which	O
maximizes	O
a	O
cumulative	O
conditional	O
probability	O
that	O
it	O
matches	O
the	O
unknown	O
,	O
spoken	O
utterance	O
given	O
the	O
acoustic	O
input	O
thereof	O
.	O

In	O
practice	O
,	O
in	O
each	O
state	O
where	O
a	O
tentative	O
decision	O
is	O
made	O
,	O
a	O
state	O
observation	O
likelihood	O
(	O
SOL	O
)	O
measure	O
,	O
indicating	O
the	O
probability	O
that	O
a	O
respective	O
phoneme	O
was	O
uttered	O
during	O
the	O
signal	O
frame	O
period	O
,	O
is	O
derived	O
from	O
an	O
HMM	O
.	O
As	O
the	O
tentative	O
decisions	O
are	O
made	O
along	O
a	O
sequence	O
,	O
the	O
SOL	O
measures	O
are	O
accumulated	O
.	O
Based	O
on	O
the	O
respective	O
cumulative	O
SOL	O
measures	O
of	O
the	O
tentative	O
decision	O
sequences	O
,	O
a	O
dynamic	O
programming	O
methodology	O
is	O
used	O
to	O
identify	O
the	O
maximum-likelihood	O
phoneme	O
concatenation	O
corresponding	O
to	O
the	O
unknown	O
,	O
spoken	O
utterance	O
.	O

The	O
SOL	O
measures	O
may	O
be	O
derived	O
from	O
well-known	O
continuous-density	O
HMMs	O
which	O
offer	O
high	O
recognition	O
accuracy	O
.	O
However	O
,	O
such	O
a	O
derivation	O
requires	O
intensive	O
computations	O
involving	O
a	O
large	O
number	O
of	O
Gaussian	O
kernels	O
which	O
are	O
state	O
dependent	O
.	O
As	O
a	O
result	O
,	O
the	O
derivation	O
incurs	O
high	O
computational	O
cost	O
,	O
and	O
substantial	O
overheads	O
in	O
memory	O
storage	O
and	O
access	O
.	O

Attempts	O
have	O
been	O
made	O
to	O
improve	O
the	O
efficiency	O
of	O
the	O
derivation	O
of	O
the	O
SOL	O
measures	O
.	O
One	O
such	O
attempt	O
involves	O
use	O
of	O
tied-mixture	O
HMMs	O
,	O
also	O
known	O
as	O
semi-continuous	O
HMMs	O
.	O
For	O
details	O
on	O
the	O
tied-mixture	O
HMMs	O
,	O
one	O
may	O
refer	O
to	O
:	O
X.	B-Citation
Huang	I-Citation
et_al.	I-Citation
,	I-Citation
"	I-Citation
Semi-Continuous	I-Citation
Hidden	I-Citation
Markov	I-Citation
Models	I-Citation
for	I-Citation
Speech	I-Citation
Signals	I-Citation
,	I-Citation
"	I-Citation
Computer	I-Citation
Speech	I-Citation
and	I-Citation
Language	I-Citation
,	I-Citation
vol.	I-Citation
3	I-Citation
,	I-Citation
1989	I-Citation
,	I-Citation
pp.	I-Citation
239	I-Citation
-	I-Citation
251	I-Citation
;	O
and	O
J.	B-Citation
R.	I-Citation
Bellegarda	I-Citation
et_al.	I-Citation
,	I-Citation
"	I-Citation
Tied	I-Citation
Mixture	I-Citation
Continuous	I-Citation
Parameter	I-Citation
Modeling	I-Citation
for	I-Citation
Speech	I-Citation
Recognition	I-Citation
,	I-Citation
"	I-Citation
IEEE	I-Citation
Trans	I-Citation
.	I-Citation
Acoustics	I-Citation
Speech	I-Citation
Signal	I-Citation
Process	I-Citation
,	I-Citation
vol.	I-Citation
38	I-Citation
,	I-Citation
no.	I-Citation
12	I-Citation
,	I-Citation
1990	I-Citation
,	I-Citation
pp.	I-Citation
2033	I-Citation
-	I-Citation
2045	I-Citation
.	O
Although	O
they	O
do	O
not	O
offer	O
the	O
recognition	O
accuracy	O
as	O
high	O
as	O
the	O
continuous-density	O
HMMS	O
,	O
the	O
tied-mixture	O
HMMs	O
all	O
share	O
the	O
same	O
collection	O
of	O
Gaussian	O
kernels	O
,	O
which	O
are	O
state	O
independent	O
.	O
As	O
a	O
result	O
,	O
among	O
other	O
things	O
,	O
less	O
storage	O
for	O
the	O
Gaussian	O
kernels	O
is	O
required	O
for	O
the	O
SOL	O
derivation	O
using	O
the	O
tied-mixture	O
HMMs	O
.	O

SUMMARY	O
OF	O
THE	O
INVENTION	O
Despite	O
the	O
above-identified	O
advantage	O
of	O
using	O
tied-mixture	O
HMMs	O
,	O
the	O
computation	O
of	O
the	O
SOL	O
measure	O
based	O
on	O
such	O
HMMs	O
is	O
undesirably	O
complex	O
,	O
stemming	O
from	O
use	O
of	O
a	O
large	O
number	O
of	O
mixture	O
component	O
weights	O
associated	O
with	O
the	O
Gaussian	O
kernels	O
in	O
the	O
computation	O
.	O
This	O
large	O
number	O
of	O
weights	O
is	O
attributed	O
to	O
the	O
fact	O
that	O
the	O
weights	O
are	O
state	O
dependent	O
.	O
Concomitantly	O
,	O
the	O
large	O
number	O
of	O
mixture	O
component	O
weights	O
require	O
substantial	O
storage	O
space	O
.	O
As	O
a	O
result	O
,	O
the	O
weights	O
,	O
together	O
with	O
the	O
Gaussian	O
kernels	O
,	O
are	O
normally	O
stored	O
in	O
an	O
off-chip	O
memory	O
(	O
i	O
.	O
e	O
.	O
,	O
external	O
to	O
a	O
processor	O
chip	O
)	O
which	O
affords	O
slow	O
memory	O
access	O
.	O

In	O
accordance	O
with	O
the	O
invention	O
,	O
at	O
least	O
some	O
of	O
the	O
mixture	O
component	O
weights	O
are	O
quantized	O
,	O
and	O
can	O
be	O
efficiently	O
stored	O
as	O
codewords	O
representing	O
the	O
quantized	O
weights	O
.	O
To	O
that	O
end	O
,	O
one	O
or	O
more	O
of	O
the	O
mixture	O
component	O
weights	O
whose	O
values	O
are	O
different	O
from	O
a	O
selected	O
constant	O
value	O
(	O
i	O
.	O
e	O
.	O
,	O
a	O
quantization	O
level	O
value	O
)	O
are	O
set	O
to	O
the	O
selected	O
constant	O
value	O
in	O
the	O
SOL	O
computation	O
.	O
In	O
accordance	O
with	O
an	O
aspect	O
of	O
the	O
invention	O
,	O
only	O
the	O
non-zero	O
weights	O
and	O
the	O
Gaussian	O
kernels	O
associated	O
therewith	O
are	O
considered	O
in	O
the	O
SOL	O
computation	O
.	O
In	O
accordance	O
with	O
another	O
aspect	O
of	O
the	O
invention	O
,	O
only	O
a	O
subset	O
of	O
the	O
Gaussian	O
kernels	O
of	O
significant	O
values	O
,	O
regardless	O
of	O
the	O
values	O
of	O
the	O
associated	O
weights	O
,	O
are	O
considered	O
in	O
the	O
SOL	O
computation	O
.	O

Thus	O
,	O
with	O
the	O
inventive	O
techniques	O
,	O
the	O
complexity	O
of	O
the	O
SOL	O
computation	O
is	O
reduced	O
because	O
of	O
use	O
of	O
a	O
relatively	O
small	O
number	O
of	O
the	O
mixture	O
component	O
weights	O
and	O
Gaussian	O
kernels	O
,	O
with	O
respect	O
to	O
the	O
traditional	O
SOL	O
computation	O
.	O
In	O
addition	O
,	O
relative	O
to	O
the	O
prior	O
art	O
technique	O
,	O
the	O
inventive	O
techniques	O
require	O
less	O
storage	O
for	O
the	O
mixture	O
component	O
weights	O
and	O
Gaussian	O
kernels	O
,	O
thereby	O
affording	O
more	O
efficient	O
memory	O
access	O
.	O

BRIEF	O
DESCRIPTION	O
OF	O
THE	O
DRAWING	O
In	O
the	O
drawing	O
,	O
FIG	O
.	O
1	O
is	O
a	O
block	O
diagram	O
of	O
a	O
speech	O
recognition	O
system	O
in	O
accordance	O
with	O
the	O
invention	O
;	O
FIG	O
.	O
2	O
is	O
a	O
flow	O
chart	O
depicting	O
a	O
first	O
process	O
which	O
may	O
be	O
employed	O
in	O
the	O
system	O
of	O
FIG	O
.	O
1	O
for	O
selecting	O
Gaussian	O
kernels	O
in	O
deriving	O
an	O
SOL	O
measure	O
in	O
accordance	O
with	O
a	O
first	O
aspect	O
of	O
the	O
invention	O
;	O
FIG	O
.	O
3	O
is	O
a	O
flow	O
chart	O
depicting	O
a	O
second	O
process	O
which	O
may	O
be	O
employed	O
in	O
the	O
system	O
of	O
FIG	O
.	O
1	O
for	O
using	O
non-zero	O
mixture	O
component	O
weights	O
in	O
deriving	O
an	O
SOL	O
measure	O
in	O
accordance	O
with	O
a	O
second	O
aspect	O
of	O
the	O
invention	O
;	O
FIG	O
.	O
4	O
is	O
a	O
flow	O
chart	O
depicting	O
a	O
third	O
process	O
which	O
may	O
be	O
employed	O
in	O
the	O
system	O
of	O
FIG	O
.	O
1	O
for	O
using	O
selected	O
non-zero	O
mixture	O
component	O
weights	O
in	O
deriving	O
an	O
SOL	O
measure	O
in	O
accordance	O
with	O
a	O
third	O
aspect	O
of	O
the	O
invention	O
;	O
FIG	O
.	O
5	O
is	O
a	O
flow	O
chart	O
depicting	O
a	O
fourth	O
process	O
which	O
may	O
be	O
employed	O
in	O
the	O
system	O
of	O
FIG	O
.	O
1	O
for	O
using	O
selected	O
non-zero	O
mixture	O
component	O
weights	O
,	O
and	O
mixture	O
component	O
weights	O
which	O
are	O
set	O
to	O
a	O
constant	O
value	O
in	O
deriving	O
an	O
SOL	O
measure	O
in	O
accordance	O
with	O
a	O
fourth	O
aspect	O
of	O
the	O
invention	O
;	O
FIG	O
.	O
6	O
is	O
a	O
flow	O
chart	O
depicting	O
a	O
fifth	O
process	O
which	O
may	O
ve	O
employed	O
in	O
the	O
system	O
of	O
FIG	O
.	O
1	O
for	O
using	O
quantized	O
mixture	O
component	O
weights	O
in	O
deriving	O
an	O
SOL	O
measure	O
in	O
accordance	O
with	O
a	O
fifth	O
aspect	O
of	O
the	O
invention	O
;	O
FIG	O
.	O
7	O
is	O
a	O
flow	O
chart	O
depicting	O
a	O
process	O
for	O
determining	O
quantization	O
levels	O
for	O
quantizing	O
mixture	O
component	O
weights	O
used	O
in	O
the	O
fifth	O
process	O
;	O
and	O
FIG	O
.	O
8	O
illustrates	O
a	O
substitute	O
step	O
for	O
use	O
in	O
the	O
process	O
of	O
FIG	O
.	O
7	O
.	O

DETAILED	O
DESCRIPTION	O
FIG	O
.	O
1	O
illustrates	O
speech	O
recognition	O
system	O
100	O
embodying	O
the	O
principles	O
of	O
the	O
invention	O
.	O
As	O
shown	O
in	O
FIG	O
.	O
1	O
,	O
system	O
100	O
includes	O
a	O
number	O
of	O
functional	O
blocks	O
including	O
analog-to-digital	O
(	O
A/D	O
)	O
convertor	O
103	O
,	O
feature	O
extractor	O
105	O
,	O
end-point	O
detector	O
113	O
,	O
speech	O
recognizer	O
117	O
,	O
word	O
model	O
storage	O
125	O
and	O
post-processor	O
130	O
.	O
The	O
functionality	O
of	O
each	O
block	O
of	O
system	O
100	O
may	O
be	O
performed	O
by	O
a	O
respective	O
different	O
processor	O
,	O
or	O
the	O
functionality	O
of	O
several	O
or	O
all	O
blocks	O
may	O
be	O
performed	O
by	O
the	O
same	O
processor	O
.	O
Furthermore	O
,	O
each	O
stage	O
can	O
include	O
multiple	O
processing	O
elements	O
.	O
The	O
stages	O
are	O
pipelined	O
and	O
their	O
operations	O
are	O
performed	O
in	O
a	O
synchronous	O
manner	O
.	O

Specifically	O
,	O
input	O
speech	O
including	O
a	O
sequence	O
of	O
spoken	O
words	O
is	O
provided	O
,	O
through	O
a	O
microphone	O
(	O
not	O
shown	O
)	O
,	O
to	O
A/D	O
convertor	O
103	O
in	O
system	O
100	O
.	O
Convertor	O
103	O
in	O
a	O
conventional	O
manner	O
samples	O
the	O
input	O
speech	O
.	O
The	O
digital	O
samples	O
are	O
then	O
provided	O
to	O
feature	O
extractor	O
105	O
.	O

Upon	O
receiving	O
the	O
digital	O
samples	O
,	O
feature	O
extractor	O
105	O
organizes	O
the	O
received	O
samples	O
in	O
speech	O
frames	O
of	O
,	O
say	O
,	O
20	O
ms	O
long	O
,	O
and	O
derives	O
for	O
each	O
frame	O
a	O
measure	O
of	O
energy	O
in	O
the	O
frame	O
and	O
a	O
set	O
of	O
short	O
spectrum	O
vectors	O
,	O
e.g.	O
,	O
linear-predictive-coding	O
(	O
LPC	O
)	O
parameters	O
.	O
In	O
this	O
instance	O
,	O
the	O
LPC	O
parameters	O
specify	O
a	O
spectrum	O
of	O
an	O
all-pole	O
model	O
which	O
best	O
matches	O
the	O
signal	O
spectrum	O
over	O
a	O
period	O
of	O
time	O
in	O
which	O
the	O
frame	O
of	O
speech	O
samples	O
are	O
accumulated	O
.	O
Based	O
on	O
the	O
LPC	O
parameters	O
,	O
extractor	O
105	O
produces	O
a	O
feature	O
vector	O
per	O
frame	O
,	O
which	O
comprises	O
twelve	O
cepstral	O
features	O
,	O
twelve	O
delta-cepstral	O
features	O
and	O
a	O
delta-energy	O
feature	O
.	O
In	O
a	O
well-known	O
manner	O
,	O
these	O
cepstral	O
and	O
delta-cepstral	O
features	O
characterize	O
the	O
spectrum	O
and	O
its	O
time	O
variation	O
of	O
the	O
speech	O
frame	O
.	O
The	O
delta-energy	O
feature	O
indicates	O
an	O
amount	O
of	O
change	O
in	O
energy	O
in	O
the	O
speech	O
frame	O
from	O
the	O
previous	O
frame	O
.	O

End-point	O
detector	O
113	O
of	O
conventional	O
design	O
uses	O
the	O
delta	O
energy	O
feature	O
,	O
in	O
conjunction	O
with	O
the	O
energy	O
measure	O
by	O
feature	O
extractor	O
105	O
,	O
to	O
determine	O
the	O
beginning	O
and	O
end	O
of	O
a	O
speech	O
signal	O
.	O
It	O
then	O
passes	O
data	O
signals	O
containing	O
the	O
25	O
features	O
in	O
each	O
feature	O
vector	O
onto	O
speech	O
recognizer	O
117	O
,	O
along	O
with	O
any	O
end-point	O
determinations	O
.	O
Recognizer	O
117	O
includes	O
microprocessor	O
140	O
and	O
memory	O
150	O
.	O
Instructed	O
by	O
a	O
control	O
program	O
which	O
is	O
stored	O
in	O
memory	O
150	O
,	O
microprocessor	O
140	O
performs	O
speech	O
recognition	O
to	O
be	O
described	O
.	O
In	O
particular	O
,	O
based	O
on	O
the	O
aforementioned	O
data	O
signals	O
and	O
word	O
models	O
provided	O
by	O
word	O
model	O
storage	O
125	O
,	O
microprocessor	O
140	O
determines	O
what	O
the	O
spoken	O
words	O
were	O
in	O
accordance	O
with	O
the	O
invention	O
.	O

Storage	O
125	O
contains	O
tied-mixture	O
hidden	O
Markov	O
models	O
(	O
HMMs	O
)	O
133	O
for	O
various	O
spoken	O
words	O
which	O
system	O
100	O
is	O
capable	O
of	O
recognizing	O
.	O
Based	O
on	O
the	O
Viterbi	O
algorithm	O
,	O
recognizer	O
117	O
identifies	O
an	O
optimum	O
sequence	O
of	O
HMMs	O
which	O
best	O
matches	O
,	O
in	O
a	O
maximum	O
likelihood	O
sense	O
,	O
a	O
respective	O
concatenation	O
of	O
phonemes	O
corresponding	O
to	O
an	O
unknown	O
spoken	O
utterance	O
.	O
Such	O
an	O
identification	O
process	O
is	O
realized	O
by	O
dynamic	O
programming	O
in	O
a	O
conventional	O
manner	O
.	O
Based	O
on	O
the	O
word	O
models	O
provided	O
by	O
storage	O
125	O
,	O
recognizer	O
117	O
forms	O
a	O
plurality	O
of	O
sequences	O
of	O
tentative	O
decisions	O
as	O
to	O
what	O
the	O
utterance	O
was	O
.	O
These	O
sequences	O
of	O
tentative	O
decisions	O
each	O
have	O
a	O
cumulative	O
state	O
observation	O
likelihood	O
(	O
SOL	O
)	O
measure	O
(	O
generically	O
denoted	O
153	O
)	O
associated	O
therewith	O
,	O
which	O
is	O
updated	O
to	O
reflect	O
the	O
cumulative	O
conditional	O
probability	O
that	O
the	O
decision	O
being	O
made	O
in	O
a	O
sequence	O
would	O
be	O
correct	O
given	O
the	O
acoustic	O
input	O
.	O
In	O
addition	O
,	O
the	O
sequences	O
respectively	O
define	O
the	O
so-called	O
"	O
survival	O
paths	O
.	O
"	O
The	O
theory	O
of	O
the	O
Viterbi	O
algorithm	O
predicts	O
that	O
the	O
survival	O
paths	O
merge	O
to	O
the	O
"	O
maximum-likelihood	O
path	O
"	O
going	O
back	O
in	O
time	O
.	O
The	O
maximum-likelihood	O
path	O
can	O
be	O
retraced	O
to	O
identify	O
the	O
maximum-likelihood	O
,	O
underlying	O
speech	O
unit	O
which	O
maximizes	O
the	O
conditional	O
probability	O
that	O
it	O
matches	O
the	O
unknown	O
,	O
spoken	O
utterance	O
given	O
the	O
acoustic	O
input	O
thereof	O
.	O

Speech	O
recognizer	O
117	O
feeds	O
a	O
series	O
of	O
recognized	O
words	O
to	O
post-processor	O
130	O
for	O
further	O
processing	O
.	O
This	O
further	O
processing	O
may	O
include	O
performing	O
a	O
well-known	O
grammar	O
check	O
on	O
the	O
recognized	O
words	O
to	O
improve	O
the	O
recognition	O
accuracy	O
.	O
However	O
,	O
such	O
further	O
processing	O
is	O
deemed	O
to	O
be	O
outside	O
the	O
scope	O
of	O
the	O
invention	O
,	O
and	O
its	O
detailed	O
description	O
is	O
thus	O
omitted	O
here	O
.	O

The	O
Viterbi	O
update	O
operation	O
performed	O
by	O
microprocessor	O
140	O
in	O
recognizer	O
117	O
at	O
time	O
t	O
may	O
be	O
described	O
using	O
the	O
following	O
expression	O
:	O
#	O
#	O
EQU1	O
#	O
#	O
where	O
log.delta..sub.t	O
(	O
j	O
)	O
represents	O
the	O
cumulative	O
SOL	O
measure	O
(	O
expressed	O
in	O
logarithm	O
)	O
along	O
a	O
survival	O
path	O
that	O
ends	O
in	O
state	O
j	O
at	O
time	O
t	O
,	O
P	O
represents	O
the	O
number	O
of	O
states	O
in	O
the	O
word	O
model	O
under	O
consideration	O
,	O
log	O
a	O
.	O
sub	O
.	O
ij	O
represents	O
the	O
state	O
transition	O
probability	O
(	O
expressed	O
in	O
logarithm	O
)	O
of	O
going	O
from	O
state	O
i	O
to	O
state	O
j	O
,	O
o	O
.	O
sub	O
.	O
t	O
represents	O
the	O
input	O
feature	O
vector	O
to	O
recognizer	O
117	O
at	O
time	O
t	O
,	O
and	O
log	O
b	O
.	O
sub	O
.	O
j	O
(	O
o	O
.	O
sub	O
.	O
t	O
)	O
represents	O
the	O
SOL	O
measure	O
(	O
expressed	O
in	O
logarithm	O
)	O
in	O
state	O
j	O
.	O

Because	O
of	O
use	O
of	O
tied-mixture	O
HMMs	O
in	O
this	O
illustrative	O
embodiment	O
,	O
the	O
SOL	O
measure	O
in	O
state	O
j	O
may	O
be	O
expressed	O
as	O
follows	O
:	O
#	O
#	O
EQU2	O
#	O
#	O
where	O
j	O
represents	O
the	O
state	O
index	O
and	O
1.ltoreq.j.ltoreq.P	O
,	O
S	O
represents	O
the	O
number	O
of	O
streams	O
into	O
which	O
the	O
feature	O
vector	O
o	O
.	O
sub	O
.	O
t	O
is	O
divided	O
,	O
M.	O
sub	O
.	O
s	O
represents	O
the	O
number	O
of	O
mixture	O
components	O
in	O
a	O
tied-mixture	O
HMM	O
for	O
stream	O
s	O
,	O
c	O
.	O
sub	O
.	O
jms	O
represents	O
the	O
weight	O
for	O
mixture	O
component	O
m	O
in	O
state	O
j	O
and	O
stream	O
s	O
.	O
Illustratively	O
,	O
in	O
system	O
100	O
,	O
components	O
of	O
each	O
feature	O
vector	O
are	O
sub-divided	O
into	O
three	O
streams	O
.	O
Specifically	O
,	O
the	O
twelve	O
cepstral	O
features	O
in	O
the	O
feature	O
vector	O
are	O
placed	O
in	O
a	O
first	O
stream	O
,	O
the	O
twelve	O
delta-cepstral	O
features	O
in	O
a	O
second	O
stream	O
,	O
and	O
the	O
delta-energy	O
feature	O
in	O
a	O
third	O
stream	O
.	O
For	O
each	O
stream	O
s	O
,	O
a	O
set	O
of	O
M.	O
sub	O
.	O
s	O
Gaussian	O
kernel	O
likelihoods	O
[	O
o	O
.	O
sub	O
.	O
ts	O
;.mu..sub.ms,.sigma..sup.2.sub.ms	O
]	O
's	O
is	O
used	O
,	O
where	O
.mu..sub.ms	O
and	O
.sigma..sup.2.sub.ms	O
vectors	O
respectively	O
represent	O
the	O
mean	O
and	O
variance	O
characterizing	O
the	O
Gaussian	O
kernels	O
.	O
Each	O
kernel	O
is	O
assumed	O
to	O
have	O
a	O
diagonal	O
covariance	O
matrix	O
,	O
with	O
.sigma..sup.2.sub.ms	O
being	O
the	O
diagonal	O
elements	O
therein	O
.	O
It	O
should	O
be	O
noted	O
that	O
by	O
virtue	O
of	O
the	O
use	O
of	O
the	O
tied-mixture	O
HMMs	O
,	O
all	O
the	O
models	O
desirably	O
share	O
the	O
same	O
sets	O
of	O
Gaussian	O
kernels	O
135	O
,	O
independent	O
of	O
the	O
state	O
.	O
In	O
this	O
instance	O
,	O
256	O
kernels	O
are	O
used	O
for	O
each	O
of	O
the	O
cepstral	O
and	O
delta-cepstral	O
feature	O
streams	O
,	O
and	O
32	O
kernels	O
for	O
delta-energy	O
stream	O
.	O

By	O
rewriting	O
expression	O
(	O
2	O
)	O
as	O
follows	O
:	O
#	O
#	O
EQU3	O
#	O
#	O
it	O
is	O
apparent	O
that	O
the	O
exponential	O
portion	O
of	O
expression	O
(	O
3	O
)	O
is	O
independent	O
of	O
j	O
and	O
thus	O
only	O
needs	O
to	O
be	O
computed	O
once	O
for	O
all	O
states	O
.	O
However	O
,	O
it	O
is	O
also	O
apparent	O
that	O
each	O
SOL	O
computation	O
according	O
to	O
expression	O
(	O
3	O
)	O
requires	O
M	O
evaluations	O
of	O
the	O
kernel	O
log-likelihoods	O
,	O
i	O
.	O
e	O
.	O
,	O
log	O
[	O
o	O
.	O
sub	O
.	O
ts	O
;.mu..sub.ms,.sigma..sup.2.sub.ms	O
]	O
's	O
,	O
each	O
of	O
which	O
is	O
followed	O
by	O
the	O
exponential	O
operation	O
on	O
the	O
evaluated	O
likelihood	O
,	O
where	O
:	O
#	O
#	O
EQU4	O
#	O
#	O
Moreover	O
,	O
S	O
logarithmic	O
operations	O
,	O
M	O
multiplications	O
and	O
[	O
M-1	O
]	O
additions	O
are	O
further	O
needed	O
for	O
the	O
SOL	O
computation	O
.	O

Despite	O
the	O
use	O
of	O
the	O
tied-mixture	O
HMMs	O
here	O
,	O
since	O
the	O
number	O
of	O
the	O
mixture	O
component	O
weights	O
c	O
.	O
sub	O
.	O
jms	O
's	O
involved	O
in	O
the	O
above	O
SOL	O
computation	O
in	O
each	O
state	O
is	O
usually	O
high	O
(	O
544	O
in	O
this	O
instance	O
)	O
,	O
we	O
realize	O
that	O
the	O
computational	O
complexity	O
and	O
memory	O
access	O
requirement	O
in	O
such	O
a	O
computation	O
remain	O
to	O
be	O
less	O
than	O
desirable	O
.	O
Thus	O
,	O
it	O
is	O
an	O
object	O
of	O
the	O
invention	O
to	O
reduce	O
the	O
number	O
of	O
the	O
kernel	O
likelihoods	O
used	O
in	O
the	O
SOL	O
computation	O
to	O
improve	O
the	O
computational	O
complexity	O
and	O
memory	O
access	O
requirement	O
.	O
It	O
is	O
another	O
object	O
of	O
the	O
invention	O
to	O
reduce	O
the	O
number	O
of	O
mixture	O
component	O
weights	O
used	O
in	O
the	O
SOL	O
computation	O
and/or	O
the	O
required	O
storage	O
therefor	O
to	O
achieve	O
lower	O
computational	O
cost	O
and	O
higher	O
memory	O
access	O
efficiency	O
.	O

In	O
accordance	O
with	O
the	O
invention	O
,	O
only	O
a	O
selected	O
number	O
of	O
significant	O
kernel	O
likelihood	O
values	O
are	O
selected	O
for	O
use	O
in	O
computing	O
the	O
SOL	O
.	O
To	O
that	O
end	O
,	O
for	O
each	O
stream	O
s	O
,	O
the	O
kernel	O
likelihoods	O
,	O
denoted	O
L1	O
,	O
L2	O
.	O
.	O
.	O
,	O
and	O
LM	O
.	O
sub	O
.	O
s	O
,	O
are	O
arranged	O
in	O
descending	O
order	O
of	O
their	O
value	O
as	O
indicated	O
at	O
step	O
202	O
in	O
FIG	O
.	O
2	O
.	O
Only	O
the	O
first	O
M.sup.L.sub.s	O
kernel	O
likelihoods	O
having	O
relatively	O
large	O
values	O
are	O
used	O
in	O
the	O
SOL	O
computation	O
at	O
step	O
204	O
,	O
where	O
M.sup.L.sub.s	O
<	O
M.	O
sub	O
.	O
s	O
.	O
In	O
effect	O
,	O
the	O
above	O
expression	O
(	O
2	O
)	O
becomes	O
:	O
#	O
#	O
EQU5	O
#	O
#	O
with	O
the	O
bracketed	O
sum	O
having	O
the	O
index	O
m	O
truncated	O
,	O
where	O
1jP	O
.	O
Accordingly	O
,	O
fewer	O
mixture	O
component	O
weights	O
c	O
.	O
sub	O
.	O
jms	O
are	O
required	O
in	O
the	O
truncated	O
sum	O
in	O
expression	O
(	O
5	O
)	O
,	O
and	O
thus	O
needed	O
to	O
be	O
stored	O
.	O
As	O
such	O
,	O
the	O
inventive	O
technique	O
affords	O
not	O
only	O
a	O
reduction	O
in	O
computational	O
complexity	O
,	O
but	O
also	O
efficient	O
memory	O
access	O
to	O
the	O
requisite	O
mixture	O
component	O
weights	O
.	O

It	O
should	O
be	O
noted	O
that	O
the	O
above	O
approach	O
where	O
all	O
Gaussian	O
kernel	O
likelihoods	O
are	O
required	O
to	O
be	O
evaluated	O
,	O
followed	O
by	O
selection	O
of	O
the	O
most	O
significant	O
M.sup.L.sub.s	O
likelihoods	O
,	O
may	O
be	O
inefficient	O
.	O
Rather	O
,	O
in	O
practice	O
,	O
Gaussian	O
kernels	O
are	O
grouped	O
in	O
a	O
small	O
predetermined	O
number	O
of	O
clusters	O
in	O
storage	O
125	O
.	O
Each	O
cluster	O
is	O
signified	O
by	O
a	O
centroid	O
of	O
all	O
kernels	O
assigned	O
to	O
the	O
cluster	O
.	O
Such	O
a	O
centroid	O
may	O
be	O
evaluated	O
using	O
the	O
well-known	O
LBG	O
algorithm	O
described	O
in	O
:	O
Y.	B-Citation
Linde	I-Citation
et_al.	I-Citation
,	I-Citation
"	I-Citation
An	I-Citation
Algorithm	I-Citation
for	I-Citation
Vector	I-Citation
Quantizer	I-Citation
design	I-Citation
,	I-Citation
"	I-Citation
IEEE	I-Citation
Trans	I-Citation
.	I-Citation
Communications	I-Citation
,	I-Citation
vol.	I-Citation
COM-28	I-Citation
,	I-Citation
pp.	I-Citation
84	I-Citation
-	I-Citation
95	I-Citation
,	I-Citation
January	I-Citation
1980	I-Citation
.	O
In	O
this	O
instance	O
,	O
the	O
centroid	O
is	O
a	O
vector	O
representing	O
a	O
median	O
of	O
the	O
respective	O
means	O
of	O
the	O
kernels	O
belonging	O
to	O
the	O
cluster	O
.	O
Only	O
those	O
kernels	O
of	O
the	O
clusters	O
whose	O
centroids	O
are	O
"	O
close	O
"	O
(	O
within	O
a	O
predetermined	O
Euclidean	O
distance	O
)	O
to	O
the	O
input	O
feature	O
vector	O
are	O
accessed	O
by	O
microprocessor	O
140	O
for	O
evaluation	O
of	O
the	O
corresponding	O
likelihoods	O
.	O
Because	O
not	O
all	O
the	O
kernel	O
likelihoods	O
need	O
to	O
be	O
evaluated	O
,	O
the	O
memory	O
access	O
efficiency	O
and	O
computational	O
complexity	O
are	O
further	O
improved	O
.	O

In	O
a	O
command-word	O
recognition	O
experiment	O
using	O
system	O
100	O
in	O
accordance	O
with	O
the	O
invention	O
,	O
we	O
found	O
that	O
very	O
few	O
,	O
significant	O
kernel	O
likelihoods	O
were	O
required	O
to	O
obtain	O
a	O
recognition	O
accuracy	O
comparable	O
to	O
that	O
of	O
a	O
prior	O
art	O
system	O
using	O
every	O
kernel	O
likelihood	O
.	O
On	O
the	O
other	O
hand	O
,	O
in	O
a	O
more	O
complex	O
connected	O
digit	O
recognition	O
experiment	O
,	O
many	O
more	O
kernel	O
likelihoods	O
,	O
and	O
thus	O
mixture	O
components	O
weights	O
,	O
needed	O
to	O
be	O
retained	O
to	O
achieve	O
the	O
comparable	O
recognition	O
accuracy	O
.	O

Compression	O
techniques	O
for	O
reducing	O
the	O
required	O
storage	O
for	O
mixture	O
component	O
weights	O
in	O
accordance	O
with	O
the	O
invention	O
will	O
now	O
be	O
described	O
.	O
In	O
a	O
tied-mixture	O
HMM	O
based	O
system	O
,	O
such	O
as	O
system	O
100	O
,	O
the	O
requirements	O
of	O
storage	O
(	O
e.g.	O
,	O
storage	O
125	O
)	O
are	O
dominated	O
by	O
the	O
mixture	O
component	O
weights	O
,	O
denoted	O
137	O
.	O
However	O
,	O
we	O
have	O
recognized	O
that	O
the	O
values	O
of	O
many	O
mixture	O
component	O
weights	O
are	O
usually	O
zero	O
or	O
extremely	O
small	O
,	O
stemming	O
from	O
a	O
kernel	O
being	O
too	O
"	O
far	O
"	O
from	O
a	O
state	O
in	O
the	O
feature	O
space	O
or	O
due	O
to	O
a	O
lack	O
of	O
enough	O
training	O
data	O
for	O
that	O
state	O
.	O
For	O
example	O
,	O
in	O
the	O
above	O
command-word	O
and	O
connected	O
digits	O
recognition	O
experiments	O
,	O
we	O
found	O
that	O
the	O
values	O
of	O
40	O
-	O
50	O
%	O
of	O
the	O
mixture	O
component	O
weights	O
were	O
virtually	O
zero	O
.	O

In	O
accordance	O
with	O
a	O
first	O
inventive	O
technique	O
,	O
referred	O
to	O
as	O
the	O
"	O
Lossless	O
Weight	O
Storage	O
Reduction	O
"	O
technique	O
,	O
only	O
non-zero	O
mixture	O
component	O
weights	O
are	O
stored	O
in	O
storage	O
125	O
to	O
save	O
memory	O
space	O
as	O
indicated	O
in	O
step	O
305	O
.	O
For	O
each	O
non-zero	O
weight	O
value	O
,	O
at	O
step	O
309	O
an	O
index	O
to	O
the	O
corresponding	O
kernel	O
likelihood	O
is	O
also	O
stored	O
thus	O
indices	O
139	O
in	O
storage	O
125	O
.	O
As	O
the	O
non-zero	O
weight	O
is	O
retrieved	O
by	O
microprocessor	O
140	O
in	O
the	O
SOL	O
computation	O
,	O
the	O
associated	O
index	O
is	O
used	O
to	O
look	O
up	O
the	O
corresponding	O
kernel	O
likelihood	O
required	O
in	O
the	O
computation	O
as	O
indicated	O
at	O
step	O
311	O
.	O

Let	O
M.sup.z.sub.js	O
represent	O
the	O
number	O
of	O
non-zero	O
weights	O
stored	O
for	O
each	O
stream	O
s	O
and	O
state	O
j	O
.	O
In	O
this	O
instance	O
,	O
the	O
storage	O
requirement	O
(	O
M.	O
sub	O
.	O
w	O
)	O
for	O
the	O
mixture	O
component	O
weights	O
for	O
each	O
word	O
model	O
(	O
w	O
)	O
is	O
as	O
follows	O
:	O
#	O
#	O
EQU6	O
#	O
#	O
where	O
P	O
is	O
the	O
number	O
of	O
states	O
in	O
the	O
model	O
.	O
It	O
should	O
be	O
noted	O
that	O
if	O
the	O
number	O
of	O
Gaussian	O
kernels	O
for	O
each	O
stream	O
does	O
not	O
exceed	O
256	O
,	O
which	O
is	O
the	O
case	O
of	O
system	O
100	O
,	O
each	O
index	O
can	O
be	O
stored	O
as	O
a	O
byte	O
.	O

To	O
reduce	O
M.	O
sub	O
.	O
w	O
,	O
a	O
second	O
inventive	O
compression	O
technique	O
may	O
be	O
applied	O
in	O
system	O
100	O
.	O
In	O
accordance	O
with	O
this	O
inventive	O
technique	O
,	O
a	O
smaller	O
number	O
(	O
say	O
M	O
'	O
.	O
sub	O
.	O
js	O
)	O
of	O
non-zero	O
mixture	O
component	O
weights	O
of	O
significant	O
values	O
are	O
used	O
.	O
Specifically	O
,	O
these	O
weights	O
are	O
first	O
arranged	O
in	O
descending	O
order	O
of	O
their	O
value	O
as	O
indicated	O
at	O
step	O
503	O
in	O
FIG	O
.	O
4	O
.	O
Only	O
the	O
mixture	O
component	O
weights	O
having	O
largest	O
values	O
are	O
included	O
in	O
the	O
SOL	O
computation	O
.	O
In	O
other	O
words	O
,	O
the	O
weights	O
c	O
.	O
sub	O
.	O
jms	O
in	O
equation	O
(	O
3	O
)	O
are	O
replaced	O
at	O
step	O
405	O
by	O
c	O
.	O
sub	O
.	O
jms	O
which	O
are	O
defined	O
as	O
follows	O
:	O
#	O
#	O
EQU7	O
#	O
#	O
Note	O
that	O
in	O
expression	O
(	O
7	O
)	O
the	O
selected	O
M	O
'	O
.	O
sub	O
.	O
js	O
non-zero	O
mixture	O
component	O
weights	O
are	O
normalized	O
so	O
that	O
they	O
sum	O
to	O
one	O
.	O
The	O
kernel	O
likelihoods	O
are	O
indexed	O
as	O
before	O
to	O
facilitate	O
the	O
retrieval	O
thereof	O
,	O
along	O
with	O
the	O
corresponding	O
mixture	O
component	O
weights	O
.	O
In	O
this	O
instance	O
,	O
the	O
required	O
storage	O
for	O
the	O
mixture	O
component	O
weights	O
for	O
each	O
word	O
model	O
w	O
is	O
:	O
#	O
#	O
EQU8	O
#	O
#	O
Since	O
M	O
'	O
.	O
sub	O
.	O
js	O
<	O
M.sup.z.sub.js	O
,	O
it	O
is	O
clear	O
that	O
the	O
memory	O
requirements	O
M.	O
sub	O
.	O
w	O
is	O
reduced	O
.	O
It	O
is	O
also	O
clear	O
that	O
the	O
recognition	O
accuracy	O
varies	O
with	O
the	O
selected	O
value	O
of	O
M	O
'	O
.	O
sub	O
.	O
js	O
.	O

In	O
an	O
alternative	O
embodiment	O
,	O
a	O
variable	O
number	O
of	O
mixture	O
component	O
weights	O
are	O
used	O
in	O
the	O
SOL	O
computation	O
such	O
that	O
the	O
sum	O
of	O
the	O
weights	O
is	O
greater	O
than	O
or	O
equal	O
to	O
a	O
specified	O
value	O
.	O
pi	O
.	O
,	O
which	O
is	O
smaller	O
than	O
one	O
.	O
That	O
is	O
,	O
#	O
#	O
EQU9	O
#	O
#	O
In	O
a	O
speech	O
recognition	O
experiment	O
using	O
the	O
alternative	O
embodiment	O
,	O
if	O
we	O
used	O
a	O
conservative	O
.	O
pi	O
.	O
,	O
we	O
achieved	O
a	O
reduction	O
of	O
about	O
30	O
%	O
in	O
storage	O
for	O
the	O
mixture	O
component	O
weights	O
without	O
significant	O
loss	O
of	O
recognition	O
accuracy	O
,	O
with	O
respect	O
to	O
the	O
system	O
where	O
all	O
non-zero	O
component	O
weights	O
are	O
stored	O
.	O
In	O
any	O
event	O
,	O
it	O
is	O
clear	O
that	O
a	O
large	O
number	O
of	O
mixture	O
component	O
weights	O
need	O
to	O
be	O
retained	O
to	O
maintain	O
such	O
accuracy	O
.	O

However	O
,	O
we	O
have	O
realized	O
that	O
setting	O
small	O
mixture	O
component	O
weights	O
to	O
zero	O
,	O
regardless	O
of	O
the	O
corresponding	O
kernel	O
likelihoods	O
,	O
causes	O
elimination	O
of	O
otherwise	O
non-zero	O
contributions	O
to	O
the	O
SOL	O
computation	O
.	O
In	O
general	O
,	O
the	O
dynamic	O
range	O
of	O
kernel	O
likelihood	O
values	O
is	O
much	O
larger	O
than	O
that	O
of	O
mixture	O
component	O
weight	O
values	O
.	O
Thus	O
,	O
when	O
mixture	O
component	O
weights	O
associated	O
with	O
kernel	O
likelihoods	O
of	O
significant	O
values	O
are	O
artificially	O
set	O
to	O
zero	O
,	O
a	O
substantial	O
error	O
is	O
introduced	O
into	O
the	O
SOL	O
computation	O
.	O
In	O
other	O
words	O
,	O
"	O
good	O
"	O
survival	O
paths	O
are	O
incorrectly	O
pruned	O
in	O
the	O
recognition	O
process	O
,	O
jeopardizing	O
the	O
recognition	O
accuracy	O
.	O

In	O
accordance	O
with	O
an	O
aspect	O
of	O
the	O
invention	O
,	O
such	O
recognition	O
accuracy	O
is	O
improved	O
by	O
approximating	O
small	O
non-zero	O
mixture	O
component	O
weights	O
with	O
a	O
non-zero	O
constant	O
,	O
instead	O
of	O
artificially	O
setting	O
them	O
to	O
zero	O
.	O
To	O
that	O
end	O
,	O
the	O
weights	O
c	O
.	O
sub	O
.	O
jms	O
are	O
arranged	O
in	O
descending	O
order	O
of	O
their	O
value	O
as	O
indicated	O
at	O
step	O
503	O
in	O
FIG	O
.	O
5	O
,	O
and	O
then	O
replaced	O
at	O
step	O
505	O
by	O
c	O
.	O
sub	O
.	O
jms	O
which	O
are	O
defined	O
as	O
follows	O
:	O
#	O
#	O
EQU10	O
#	O
#	O
where	O
M.sup.z.sub.js	O
represents	O
the	O
number	O
of	O
non-zero	O
weights	O
for	O
state	O
j	O
and	O
stream	O
s	O
as	O
described	O
before	O
,	O
and	O
M	O
'	O
.	O
sub	O
.	O
js	O
represents	O
the	O
number	O
of	O
mixture	O
component	O
weights	O
whose	O
actual	O
weights	O
are	O
used	O
.	O
The	O
values	O
of	O
the	O
remaining	O
(	O
M.sup.z.sub.js	O
-	O
M	O
'	O
.	O
sub	O
.	O
js	O
)	O
relatively	O
small	O
weights	O
are	O
each	O
set	O
to	O
a	O
constant	O
,	O
which	O
in	O
this	O
instance	O
is	O
their	O
average	O
value	O
.	O
In	O
this	O
example	O
,	O
the	O
required	O
storage	O
for	O
mixture	O
component	O
weights	O
for	O
each	O
word	O
model	O
w	O
becomes	O
:	O
#	O
#	O
EQU11	O
#	O
#	O
Compared	O
with	O
expression	O
(	O
8	O
)	O
,	O
the	O
additional	O
terms	O
in	O
expression	O
(	O
11	O
)	O
are	O
attributed	O
to	O
the	O
storage	O
requirement	O
of	O
the	O
constant	O
value	O
assigned	O
to	O
the	O
(	O
M.sup.z.sub.js	O
-	O
M	O
'	O
.	O
sub	O
.	O
js	O
)	O
relatively	O
small	O
weights	O
which	O
are	O
not	O
accounted	O
for	O
previously	O
,	O
and	O
the	O
indices	O
for	O
such	O
weights	O
.	O
However	O
,	O
it	O
should	O
be	O
noted	O
that	O
the	O
value	O
of	O
M	O
'	O
.	O
sub	O
.	O
js	O
in	O
expression	O
(	O
10	O
)	O
may	O
be	O
much	O
smaller	O
than	O
that	O
in	O
expression	O
(	O
7	O
)	O
to	O
achieve	O
the	O
same	O
recognition	O
accuracy	O
.	O
As	O
such	O
,	O
the	O
required	O
storage	O
here	O
may	O
actually	O
be	O
less	O
than	O
that	O
in	O
the	O
previous	O
embodiment	O
,	O
despite	O
the	O
additional	O
terms	O
.	O

Referring	O
to	O
expression	O
(	O
10	O
)	O
,	O
the	O
process	O
where	O
those	O
mixture	O
component	O
weights	O
below	O
a	O
specified	O
threshold	O
are	O
each	O
designated	O
the	O
same	O
average	O
value	O
can	O
be	O
envisioned	O
as	O
one	O
of	O
quantization	O
where	O
the	O
weights	O
are	O
quantized	O
using	O
a	O
single	O
quantization	O
level	O
(	O
i	O
.	O
e	O
.	O
,	O
the	O
average	O
weight	O
value	O
)	O
.	O
In	O
accordance	O
with	O
a	O
further	O
aspect	O
of	O
the	O
invention	O
,	O
such	O
a	O
quantization	O
process	O
is	O
extended	O
to	O
include	O
all	O
mixture	O
component	O
weights	O
.	O
That	O
is	O
,	O
all	O
the	O
weights	O
are	O
quantized	O
using	O
a	O
predetermined	O
number	O
of	O
quantization	O
levels	O
as	O
indicated	O
at	O
step	O
603	O
in	O
FIG	O
.	O
6	O
.	O
Each	O
weight	O
is	O
designated	O
at	O
step	O
605	O
the	O
closest	O
quantization	O
level	O
value	O
,	O
and	O
the	O
quantized	O
weight	O
is	O
used	O
in	O
lieu	O
of	O
the	O
actual	O
weight	O
in	O
the	O
SOL	O
computation	O
at	O
step	O
607	O
.	O
Furthermore	O
,	O
each	O
weight	O
can	O
be	O
represented	O
by	O
a	O
codeword	O
indicating	O
the	O
corresponding	O
quantized	O
weight	O
.	O
The	O
number	O
of	O
codewords	O
needed	O
equals	O
the	O
number	O
of	O
quantization	O
levels	O
used	O
.	O
Advantageously	O
,	O
storage	O
of	O
the	O
weights	O
by	O
their	O
codewords	O
,	O
which	O
are	O
integers	O
,	O
are	O
much	O
more	O
efficient	O
than	O
that	O
by	O
their	O
actual	O
weight	O
values	O
as	O
in	O
the	O
prior	O
art	O
,	O
which	O
are	O
floating	O
point	O
numbers	O
.	O
For	O
example	O
,	O
if	O
no	O
more	O
than	O
256	O
quantization	O
levels	O
used	O
,	O
each	O
codeword	O
can	O
be	O
stored	O
as	O
a	O
byte	O
.	O
Despite	O
the	O
fact	O
that	O
a	O
look-up	O
table	O
is	O
also	O
needed	O
here	O
for	O
looking	O
up	O
a	O
given	O
codeword	O
for	O
the	O
corresponding	O
quantized	O
weight	O
,	O
the	O
total	O
required	O
storage	O
is	O
relatively	O
small	O
and	O
can	O
be	O
accommodated	O
by	O
an	O
on-chip	O
memory	O
,	O
thereby	O
affording	O
efficient	O
memory	O
access	O
.	O
Such	O
an	O
on-chip	O
memory	O
may	O
be	O
a	O
random-access	O
memory	O
(	O
RAM	O
)	O
internal	O
to	O
recognizer	O
117	O
.	O

Given	O
the	O
mixture	O
component	O
weights	O
,	O
the	O
values	O
of	O
the	O
quantization	O
levels	O
are	O
determined	O
using	O
the	O
aforementioned	O
LBG	O
algorithm	O
to	O
minimize	O
the	O
total	O
quantization	O
error	O
.	O
FIG	O
.	O
7	O
is	O
a	O
flow	O
chart	O
depicting	O
process	O
200	O
according	O
to	O
the	O
LBG	O
algorithm	O
for	O
obtaining	O
the	O
quantization	O
level	O
values	O
.	O
Process	O
200	O
is	O
an	O
iterative	O
process	O
.	O
To	O
initialize	O
the	O
process	O
,	O
a	O
value	O
between	O
zero	O
and	O
the	O
maximum	O
mixture	O
component	O
weight	O
value	O
is	O
selected	O
as	O
a	O
tentative	O
quantization	O
level	O
value	O
,	O
as	O
indicated	O
at	O
step	O
203	O
.	O
For	O
example	O
,	O
the	O
average	O
weight	O
value	O
may	O
be	O
used	O
as	O
the	O
initial	O
tentative	O
quantization	O
level	O
value	O
.	O

At	O
step	O
205	O
,	O
for	O
each	O
mixture	O
component	O
weight	O
c	O
.	O
sub	O
.	O
i	O
(	O
1.ltoreq.i.ltoreq.I	O
)	O
,	O
the	O
closest	O
tentative	O
quantization	O
level	O
c	O
.	O
sup	O
.	O
i	O
is	O
found	O
among	O
the	O
L	O
tentative	O
quantization	O
levels	O
provided	O
in	O
the	O
previous	O
step	O
(	O
in	O
the	O
first	O
iteration	O
L	O
=	O
1	O
)	O
,	O
where	O
I	O
represents	O
the	O
total	O
number	O
of	O
the	O
mixture	O
component	O
weights	O
in	O
the	O
SOL	O
computation	O
.	O
A	O
function	O
d	O
(	O
c	O
,	O
c	O
.	O
sub	O
.	O
i	O
)	O
is	O
used	O
to	O
measure	O
the	O
distance	O
of	O
a	O
mixture	O
component	O
weight	O
(	O
c	O
.	O
sub	O
.	O
i	O
)	O
from	O
a	O
quantization	O
level	O
(	O
c	O
)	O
,	O
and	O
thus	O
the	O
quantization	O
error	O
associated	O
with	O
c	O
.	O
sub	O
.	O
i	O
.	O
Illustratively	O
,	O
the	O
so-called	O
"	O
.	O
sup	O
.	O
1	O
"	O
distance	O
measure	O
is	O
used	O
in	O
this	O
instance	O
,	O
and	O
accordingly	O
d	O
(	O
c	O
,	O
c	O
.	O
sub	O
.	O
i	O
)	O
equals	O
the	O
absolute	O
value	O
of	O
the	O
difference	O
between	O
c	O
and	O
c	O
.	O
sub	O
.	O
i	O
.	O
That	O
is	O
,	O
d	O
(	O
c	O
,	O
c	O
.	O
sub	O
.	O
i	O
)	O
=	O
.	O
linevert	O
split.cc.sub.i	O
.	O
linevert	O
split	O
.	O
.	O

It	O
should	O
be	O
noted	O
at	O
this	O
point	O
that	O
the	O
so-called	O
"	O
.	O
sup	O
.	O
2	O
"	O
distance	O
measure	O
may	O
also	O
be	O
used	O
,	O
instead	O
,	O
which	O
measures	O
the	O
Euclidean	O
distance	O
between	O
two	O
given	O
points	O
.	O

At	O
step	O
207	O
,	O
a	O
first	O
cumulative	O
quantization	O
error	O
(	O
D1	O
)	O
is	O
computed	O
as	O
follows	O
:	O
#	O
#	O
EQU12	O
#	O
#	O
At	O
step	O
209	O
,	O
by	O
perturbing	O
the	O
L	O
tentative	O
quantization	O
levels	O
in	O
accordance	O
with	O
the	O
LBG	O
algorithm	O
,	O
L	O
centroids	O
c	O
.	O
sub	O
.	O
1	O
,	O
c	O
.	O
sub	O
.	O
2	O
.	O
.	O
.	O
,	O
and	O
c	O
.	O
sub	O
.	O
L	O
corresponding	O
thereto	O
are	O
determined	O
to	O
"	O
minimize	O
"	O
a	O
second	O
cumulative	O
quantization	O
error	O
(	O
D2	O
)	O
.	O
It	O
should	O
be	O
noted	O
that	O
the	O
LBG	O
algorithm	O
only	O
guarantees	O
a	O
local	O
minimum	O
.	O
That	O
is	O
,	O
the	O
"	O
minimized	O
"	O
D2	O
probably	O
can	O
be	O
improved	O
.	O
In	O
any	O
event	O
,	O
the	O
"	O
minimized	O
"	O
D2	O
is	O
computed	O
at	O
step	O
210	O
as	O
follows	O
:	O
#	O
#	O
EQU13	O
#	O
#	O
where	O
c	O
.	O
sup	O
.	O
i	O
represents	O
the	O
centroid	O
to	O
which	O
c	O
.	O
sub	O
.	O
i	O
is	O
closest	O
.	O

At	O
step	O
211	O
,	O
a	O
converging	O
factor	O
(	O
D1-D2	O
)	O
/D1	O
is	O
used	O
to	O
determine	O
whether	O
an	O
additional	O
iteration	O
is	O
needed	O
to	O
improve	O
D2	O
.	O
Specifically	O
,	O
if	O
this	O
factor	O
is	O
greater	O
than	O
or	O
equal	O
to	O
a	O
predetermined	O
threshold	O
.	O
mu	O
.	O
,	O
where	O
0	O
<	O
.	O
mu	O
.	O
<	O
1	O
(	O
e.g.	O
,	O
.	O
mu	O
.	O
=	O
0	O
.	O
01	O
)	O
,	O
process	O
200	O
returns	O
to	O
step	O
205	O
through	O
step	O
212	O
to	O
reduce	O
D2	O
.	O
At	O
step	O
212	O
,	O
the	O
L	O
centroids	O
are	O
designated	O
to	O
be	O
the	O
tentative	O
quantization	O
levels	O
for	O
step	O
205	O
.	O

Otherwise	O
if	O
the	O
converging	O
factor	O
is	O
smaller	O
than	O
.	O
mu	O
.	O
,	O
it	O
is	O
determined	O
at	O
step	O
213	O
whether	O
L	O
equals	O
the	O
desired	O
number	O
of	O
quantization	O
levels	O
,	O
say	O
,	O
Q.	O
If	O
L.	O
noteq	O
.	O
Q	O
(	O
i	O
.	O
e	O
.	O
,	O
L	O
<	O
Q	O
)	O
,	O
process	O
200	O
proceeds	O
to	O
step	O
215	O
where	O
the	O
above	O
centroids	O
are	O
used	O
as	O
tentative	O
quantization	O
levels	O
,	O
and	O
new	O
tentative	O
quantization	O
levels	O
are	O
added	O
to	O
increase	O
L.	O
For	O
example	O
,	O
a	O
way	O
to	O
increase	O
the	O
number	O
of	O
the	O
tentative	O
quantization	O
levels	O
using	O
the	O
centroids	O
is	O
that	O
the	O
first	O
new	O
tentative	O
quantization	O
level	O
is	O
assigned	O
half	O
the	O
value	O
of	O
the	O
smallest	O
centroid	O
.	O
Each	O
additional	O
new	O
tentative	O
quantization	O
level	O
is	O
assigned	O
the	O
average	O
value	O
of	O
a	O
respective	O
pair	O
of	O
contiguous	O
centroids	O
.	O

The	O
increased	O
number	O
of	O
tentative	O
quantization	O
levels	O
are	O
then	O
provided	O
to	O
step	O
205	O
previously	O
described	O
.	O
If	O
L	O
=	O
Q	O
at	O
step	O
213	O
,	O
the	O
centroids	O
from	O
step	O
209	O
are	O
used	O
as	O
the	O
respective	O
Q	O
quantization	O
levels	O
to	O
quantize	O
all	O
the	O
mixture	O
component	O
weights	O
,	O
as	O
indicated	O
at	O
step	O
217	O
.	O
In	O
addition	O
to	O
the	O
Q	O
quantization	O
levels	O
used	O
,	O
a	O
quantization	O
level	O
of	O
value	O
zero	O
is	O
also	O
used	O
.	O
As	O
such	O
,	O
certain	O
small	O
non-zero	O
weights	O
closest	O
to	O
the	O
zero	O
quantization	O
level	O
,	O
together	O
with	O
the	O
zero	O
weights	O
,	O
are	O
quantized	O
to	O
the	O
zero	O
value	O
.	O

Referring	O
back	O
to	O
expression	O
(	O
3	O
)	O
,	O
the	O
mixture	O
components	O
weights	O
c	O
.	O
sub	O
.	O
jms	O
are	O
subject	O
to	O
a	O
logarithmic	O
operation	O
which	O
is	O
a	O
nonlinear	O
operation	O
.	O
This	O
being	O
so	O
,	O
a	O
logarithmic	O
singular	O
effect	O
occurs	O
especially	O
when	O
Q	O
is	O
small	O
.	O
This	O
effect	O
is	O
caused	O
by	O
the	O
logarithmic	O
operation	O
's	O
amplifying	O
the	O
quantization	O
errors	O
of	O
small	O
quantized	O
weights	O
in	O
a	O
disproportionate	O
manner	O
,	O
with	O
respect	O
to	O
those	O
of	O
relatively	O
large	O
quantized	O
weights	O
.	O
To	O
avoid	O
the	O
logarithmic	O
singularity	O
effect	O
,	O
the	O
distance	O
(	O
or	O
quantization	O
error	O
)	O
measure	O
d	O
(	O
c	O
,	O
c	O
)	O
used	O
in	O
process	O
200	O
for	O
deriving	O
the	O
quantization	O
levels	O
should	O
not	O
be	O
the	O
.	O
sup	O
.	O
1	O
distance	O
measure	O
.	O
Rather	O
,	O
the	O
following	O
asymmetric	O
distance	O
measure	O
should	O
be	O
used	O
,	O
instead	O
as	O
shown	O
at	O
step	O
805	O
in	O
FIG	O
.	O
8	O
for	O
substitutingstep	O
205	O
in	O
FIG	O
.	O
7	O
:	O
#	O
#	O
EQU14	O
#	O
#	O
where	O
.	O
rho	O
.	O
=	O
1	O
or	O
2	O
,	O
.xi..gtoreq.1	O
,	O
and	O
.	O
zeta	O
.	O
>	O
0	O
.	O
The	O
parameters	O
.	O
xi	O
.	O
and	O
.	O
zeta	O
.	O
are	O
used	O
to	O
control	O
the	O
behavior	O
of	O
d	O
(	O
c	O
,	O
c	O
)	O
.	O
In	O
addition	O
,	O
the	O
parameter	O
.	O
zeta	O
.	O
determines	O
the	O
range	O
of	O
"	O
very	O
small	O
"	O
weights	O
.	O
If	O
.	O
xi	O
.	O
c	O
<	O
<	O
.	O
zeta	O
.	O
then	O
.	O
linevert	O
split..DELTA.c.linevert	O
split.=.linevert	O
split.c-c.linevert	O
split..ltoreq..linevert	O
split.c.linevert	O
split..ltoreq..xi.c	O
<	O
<	O
.	O
zeta	O
.	O
,	O
and	O
d	O
(	O
c	O
,	O
c	O
)	O
may	O
be	O
approximated	O
as	O
follows	O
:	O
#	O
#	O
EQU15	O
#	O
#	O
On	O
the	O
other	O
hand	O
,	O
if	O
.	O
xi	O
.	O
c	O
>	O
>	O
.	O
zeta	O
.	O
,	O
d	O
(	O
c	O
,	O
c	O
)	O
may	O
be	O
approximated	O
as	O
follows	O
:	O
#	O
#	O
EQU16	O
#	O
#	O
Based	O
on	O
expression	O
(	O
16	O
)	O
,	O
if	O
.	O
linevert	O
split..DELTA.c.linevert	O
split	O
.	O
<	O
<	O
.	O
xi	O
.	O
c	O
,	O
it	O
can	O
be	O
shown	O
that	O
d	O
(	O
c	O
,	O
c	O
)	O
.apprxeq..linevert	O
split.c-c.linevert	O
split..sup..rho	O
.	O
,	O
which	O
is	O
the	O
.	O
sup	O
.	O
.	O
rho	O
.	O
distance	O
.	O
If	O
.	O
linevert	O
split..DELTA.c.linevert	O
split	O
.	O
is	O
not	O
small	O
enough	O
,	O
and	O
.	O
DELTA	O
.	O
c	O
is	O
negative	O
(	O
positive	O
)	O
,	O
it	O
can	O
be	O
shown	O
that	O
the	O
error	O
d	O
(	O
c	O
,	O
c	O
)	O
increases	O
significantly	O
(	O
decreases	O
to	O
a	O
lesser	O
extent	O
)	O
.	O
Thus	O
,	O
the	O
parameter	O
.	O
xi	O
.	O
determines	O
the	O
degree	O
of	O
asymmetry	O
of	O
d	O
(	O
c	O
,	O
c	O
)	O
.	O
Specifically	O
,	O
as	O
.	O
xi	O
.	O
gets	O
larger	O
,	O
d	O
(	O
c	O
,	O
c	O
)	O
becomes	O
less	O
asymmetric	O
.	O

The	O
foregoing	O
merely	O
illustrates	O
the	O
principles	O
of	O
the	O
invention	O
.	O
It	O
will	O
thus	O
be	O
appreciated	O
that	O
a	O
person	O
skilled	O
in	O
the	O
art	O
will	O
be	O
able	O
to	O
devise	O
numerous	O
systems	O
which	O
,	O
although	O
not	O
explicitly	O
shown	O
or	O
described	O
herein	O
,	O
embody	O
the	O
principles	O
of	O
the	O
invention	O
and	O
are	O
thus	O
within	O
its	O
spirit	O
and	O
scope	O
.	O

For	O
example	O
,	O
it	O
will	O
be	O
appreciated	O
that	O
a	O
person	O
skilled	O
in	O
the	O
art	O
may	O
further	O
reduce	O
the	O
complexity	O
of	O
the	O
SOL	O
computation	O
by	O
combining	O
two	O
or	O
more	O
inventive	O
techniques	O
.	O
For	O
instance	O
,	O
the	O
mixture	O
component	O
weight	O
quantization	O
technique	O
described	O
above	O
may	O
be	O
combined	O
with	O
a	O
second	O
technique	O
according	O
to	O
expression	O
(	O
5	O
)	O
wherein	O
only	O
a	O
predetermined	O
number	O
of	O
significant	O
weights	O
are	O
used	O
in	O
the	O
SOL	O
computation	O
.	O
Thus	O
,	O
in	O
accordance	O
with	O
such	O
a	O
combined	O
technique	O
,	O
not	O
all	O
of	O
the	O
mixture	O
components	O
weights	O
are	O
quantized	O
in	O
performing	O
the	O
SOL	O
computation	O
.	O
Rather	O
,	O
those	O
insignificant	O
weights	O
are	O
totally	O
ignored	O
and	O
only	O
the	O
remaining	O
weights	O
are	O
quantized	O
in	O
performing	O
such	O
a	O
computation	O
.	O

1	O
.	O
A	O
speech	O
recognizer	O
comprising	O
:	O
a	O
processor	O
responsive	O
to	O
a	O
representation	O
of	O
speech	O
for	O
deriving	O
at	O
least	O
one	O
state	O
observation	O
likelihood	O
measure	O
,	O
each	O
state	O
observation	O
likelihood	O
measure	O
being	O
a	O
function	O
of	O
at	O
least	O
a	O
plurality	O
of	O
probability	O
kernels	O
and	O
a	O
plurality	O
of	O
weights	O
associated	O
therewith	O
,	O
one	O
or	O
more	O
of	O
the	O
weights	O
whose	O
values	O
are	O
different	O
from	O
a	O
selected	O
constant	O
value	O
being	O
set	O
to	O
the	O
selected	O
constant	O
value	O
in	O
deriving	O
the	O
state	O
observation	O
likelihood	O
measure	O
;	O
and	O
an	O
output	O
for	O
generating	O
signals	O
representative	O
of	O
recognized	O
speech	O
based	O
on	O
the	O
at	O
least	O
one	O
state	O
observation	O
likelihood	O
measure	O
.	O

2	O
.	O
The	O
speech	O
recognizer	O
of	O
claim	O
1	O
wherein	O
the	O
selected	O
constant	O
value	O
is	O
zero	O
.	O

3	O
.	O
The	O
speech	O
recognizer	O
of	O
claim	O
1	O
wherein	O
the	O
values	O
of	O
the	O
one	O
or	O
more	O
of	O
the	O
weights	O
are	O
non-zero	O
,	O
the	O
selected	O
constant	O
value	O
being	O
an	O
average	O
of	O
the	O
values	O
of	O
the	O
one	O
or	O
more	O
of	O
the	O
weights	O
.	O

4	O
.	O
The	O
speech	O
recognizer	O
of	O
claim	O
1	O
wherein	O
selected	O
ones	O
of	O
the	O
plurality	O
of	O
weights	O
,	O
other	O
than	O
the	O
one	O
or	O
more	O
of	O
the	O
weights	O
,	O
are	O
set	O
to	O
at	O
least	O
one	O
other	O
selected	O
constant	O
value	O
in	O
deriving	O
the	O
state	O
observation	O
likelihood	O
measure	O
.	O

5	O
.	O
The	O
speech	O
recognizer	O
of	O
claim	O
4	O
wherein	O
the	O
selected	O
constant	O
value	O
and	O
the	O
at	O
least	O
one	O
other	O
selected	O
constant	O
value	O
are	O
selected	O
in	O
accordance	O
with	O
an	O
LBG	O
algorithm	O
.	O

6	O
.	O
The	O
speech	O
recognizer	O
of	O
claim	O
4	O
wherein	O
the	O
one	O
or	O
more	O
of	O
the	O
weights	O
are	O
closer	O
,	O
in	O
terms	O
of	O
a	O
distance	O
measure	O
,	O
to	O
the	O
selected	O
constant	O
value	O
than	O
to	O
each	O
of	O
the	O
at	O
least	O
one	O
other	O
selected	O
constant	O
value	O
.	O

7	O
.	O
The	O
speech	O
recognizer	O
of	O
claim	O
6	O
wherein	O
said	O
distance	O
measure	O
is	O
an	O
absolute	O
value	O
distance	O
measure	O
.	O

8	O
.	O
The	O
speech	O
recognizer	O
of	O
claim	O
6	O
wherein	O
said	O
distance	O
measure	O
is	O
asymmetric	O
.	O

9	O
.	O
The	O
speech	O
recognizer	O
of	O
claim	O
1	O
wherein	O
said	O
probability	O
kernels	O
are	O
Gaussian	O
kernels	O
in	O
accordance	O
with	O
tied-mixture	O
hidden	O
Markov	O
models	O
(	O
HMMs	O
)	O
.	O

10	O
.	O
The	O
speech	O
recognizer	O
of	O
claim	O
1	O
wherein	O
said	O
weights	O
are	O
mixture	O
component	O
weights	O
in	O
accordance	O
with	O
tied-mixture	O
HMMs	O
.	O

11	O
.	O
A	O
speech	O
recognizer	O
comprising	O
:	O
a	O
processor	O
responsive	O
to	O
a	O
representation	O
of	O
speech	O
for	O
deriving	O
at	O
least	O
one	O
state	O
observation	O
likelihood	O
measure	O
,	O
each	O
state	O
observation	O
likelihood	O
measure	O
being	O
a	O
function	O
of	O
at	O
least	O
a	O
plurality	O
of	O
probability	O
kernels	O
and	O
a	O
plurality	O
of	O
weights	O
,	O
each	O
probability	O
kernel	O
being	O
associated	O
with	O
a	O
respective	O
one	O
of	O
the	O
weights	O
,	O
only	O
the	O
probability	O
kernels	O
associated	O
with	O
those	O
weights	O
whose	O
values	O
are	O
non-zero	O
being	O
identified	O
by	O
respective	O
indexes	O
;	O
a	O
repository	O
for	O
providing	O
each	O
non-zero	O
weight	O
and	O
an	O
index	O
identifying	O
the	O
probability	O
kernel	O
associated	O
with	O
the	O
non-zero	O
weight	O
for	O
deriving	O
the	O
at	O
least	O
one	O
state	O
observation	O
likelihood	O
measure	O
;	O
and	O
an	O
output	O
for	O
generating	O
signals	O
representative	O
of	O
recognized	O
speech	O
based	O
on	O
the	O
at	O
least	O
one	O
state	O
observation	O
likelihood	O
measure	O
.	O

12	O
.	O
The	O
speech	O
recognizer	O
of	O
claim	O
11	O
wherein	O
said	O
probability	O
kernels	O
are	O
Gaussian	O
kernels	O
in	O
accordance	O
with	O
tied-mixture	O
HMMs	O
.	O

13	O
.	O
The	O
speech	O
recognizer	O
of	O
claim	O
11	O
wherein	O
said	O
weights	O
are	O
mixture	O
component	O
weights	O
in	O
accordance	O
with	O
tied-mixture	O
HMMs	O
.	O

14	O
.	O
Apparatus	O
for	O
recognizing	O
speech	O
based	O
on	O
at	O
least	O
one	O
state	O
observation	O
likelihood	O
measure	O
,	O
each	O
state	O
observation	O
likelihood	O
measure	O
being	O
a	O
function	O
of	O
at	O
least	O
a	O
plurality	O
of	O
probability	O
kernels	O
and	O
a	O
plurality	O
of	O
weights	O
associated	O
therewith	O
,	O
comprising	O
:	O
a	O
processor	O
for	O
deriving	O
the	O
at	O
least	O
one	O
state	O
observation	O
likelihood	O
measure	O
based	O
on	O
a	O
subset	O
of	O
the	O
plurality	O
of	O
probability	O
kernels	O
,	O
each	O
probability	O
kernel	O
in	O
the	O
subset	O
being	O
a	O
function	O
of	O
a	O
representation	O
of	O
the	O
speech	O
to	O
be	O
recognized	O
,	O
the	O
number	O
of	O
probability	O
kernels	O
in	O
the	O
subset	O
being	O
predetermined	O
,	O
the	O
predetermined	O
number	O
being	O
smaller	O
than	O
the	O
number	O
of	O
the	O
plurality	O
of	O
probability	O
kernels	O
,	O
the	O
probability	O
kernels	O
in	O
the	O
subset	O
each	O
having	O
a	O
larger	O
value	O
than	O
any	O
of	O
the	O
probability	O
kernels	O
outside	O
the	O
subset	O
;	O
and	O
an	O
output	O
for	O
generating	O
signals	O
representative	O
of	O
recognized	O
speech	O
based	O
on	O
the	O
at	O
least	O
one	O
state	O
observation	O
likelihood	O
measure	O
.	O

15	O
.	O
The	O
apparatus	O
of	O
claim	O
14	O
wherein	O
said	O
probability	O
kernels	O
are	O
Gaussian	O
kernels	O
in	O
accordance	O
with	O
tied-mixture	O
HMMs	O
.	O

16	O
.	O
The	O
apparatus	O
of	O
claim	O
14	O
wherein	O
said	O
weights	O
are	O
mixture	O
component	O
weights	O
in	O
accordance	O
with	O
tied-mixture	O
HMMs	O
.	O

17	O
.	O
A	O
method	O
for	O
recognizing	O
speech	O
comprising	O
:	O
deriving	O
at	O
least	O
one	O
state	O
observation	O
likelihood	O
measure	O
in	O
response	O
to	O
a	O
representation	O
of	O
said	O
speech	O
,	O
each	O
state	O
observation	O
likelihood	O
measure	O
being	O
a	O
function	O
of	O
at	O
least	O
a	O
plurality	O
of	O
probability	O
kernels	O
and	O
a	O
plurality	O
of	O
weights	O
associated	O
therewith	O
,	O
one	O
or	O
more	O
of	O
the	O
weights	O
whose	O
values	O
are	O
different	O
from	O
a	O
selected	O
constant	O
value	O
being	O
set	O
to	O
the	O
selected	O
constant	O
value	O
in	O
deriving	O
the	O
state	O
observation	O
likelihood	O
measure	O
;	O
and	O
generating	O
signals	O
representative	O
of	O
recognized	O
speech	O
based	O
on	O
the	O
at	O
least	O
one	O
state	O
observation	O
likelihood	O
measure	O
.	O

18	O
.	O
The	O
method	O
of	O
claim	O
17	O
wherein	O
the	O
selected	O
constant	O
value	O
is	O
zero	O
.	O

19	O
.	O
The	O
method	O
of	O
claim	O
17	O
wherein	O
the	O
values	O
of	O
the	O
one	O
or	O
more	O
of	O
the	O
weights	O
are	O
non-zero	O
,	O
the	O
selected	O
constant	O
value	O
being	O
an	O
average	O
of	O
the	O
values	O
of	O
the	O
one	O
or	O
more	O
of	O
the	O
weights	O
.	O

20	O
.	O
The	O
method	O
of	O
claim	O
17	O
wherein	O
selected	O
ones	O
of	O
the	O
plurality	O
of	O
weights	O
,	O
other	O
than	O
the	O
one	O
or	O
more	O
of	O
the	O
weights	O
,	O
are	O
set	O
to	O
at	O
least	O
one	O
other	O
selected	O
constant	O
value	O
in	O
deriving	O
the	O
state	O
observation	O
likelihood	O
measure	O
.	O

21	O
.	O
The	O
method	O
of	O
claim	O
20	O
wherein	O
the	O
selected	O
constant	O
value	O
and	O
the	O
at	O
least	O
one	O
other	O
selected	O
constant	O
value	O
are	O
selected	O
in	O
accordance	O
with	O
an	O
LBG	O
algorithm	O
.	O

22	O
.	O
The	O
method	O
of	O
claim	O
20	O
wherein	O
the	O
one	O
or	O
more	O
of	O
the	O
weights	O
are	O
closer	O
,	O
in	O
terms	O
of	O
a	O
distance	O
measure	O
,	O
to	O
the	O
selected	O
constant	O
value	O
than	O
to	O
each	O
of	O
the	O
at	O
least	O
one	O
other	O
selected	O
constant	O
value	O
.	O

23	O
.	O
The	O
method	O
of	O
claim	O
22	O
wherein	O
said	O
distance	O
measure	O
is	O
an	O
absolute	O
value	O
distance	O
measure	O
.	O

24	O
.	O
The	O
method	O
of	O
claim	O
22	O
wherein	O
said	O
distance	O
measure	O
is	O
asymmetric	O
.	O

25	O
.	O
The	O
method	O
of	O
claim	O
17	O
wherein	O
said	O
probability	O
kernels	O
are	O
Gaussian	O
kernels	O
in	O
accordance	O
with	O
tied-mixture	O
hidden	O
Markov	O
models	O
(	O
HMMs	O
)	O
.	O

26	O
.	O
The	O
method	O
of	O
claim	O
17	O
wherein	O
said	O
weights	O
are	O
mixture	O
component	O
weights	O
in	O
accordance	O
with	O
tied-mixture	O
HMMs	O
.	O

27	O
.	O
A	O
method	O
for	O
recognizing	O
speech	O
comprising	O
:	O
deriving	O
at	O
least	O
one	O
state	O
observation	O
likelihood	O
measure	O
in	O
response	O
to	O
a	O
representation	O
of	O
said	O
speech	O
,	O
each	O
state	O
observation	O
likelihood	O
measure	O
being	O
a	O
function	O
of	O
at	O
least	O
a	O
plurality	O
of	O
probability	O
kernels	O
and	O
a	O
plurality	O
of	O
weights	O
,	O
each	O
probability	O
kernel	O
being	O
associated	O
with	O
a	O
respective	O
one	O
of	O
the	O
weights	O
,	O
only	O
the	O
probability	O
kernels	O
associated	O
with	O
those	O
weights	O
whose	O
values	O
are	O
non-zero	O
being	O
identified	O
by	O
respective	O
indexes	O
;	O
providing	O
each	O
non-zero	O
weight	O
and	O
an	O
index	O
identifying	O
the	O
probability	O
kernel	O
associated	O
with	O
the	O
non-zero	O
weight	O
for	O
deriving	O
the	O
at	O
least	O
one	O
state	O
observation	O
likelihood	O
measure	O
;	O
and	O
generating	O
signals	O
representative	O
of	O
recognized	O
speech	O
based	O
on	O
the	O
at	O
least	O
one	O
state	O
observation	O
likelihood	O
measure	O
.	O

28	O
.	O
The	O
method	O
of	O
claim	O
27	O
wherein	O
said	O
probability	O
kernels	O
are	O
Gaussian	O
kernels	O
in	O
accordance	O
with	O
tied-mixture	O
HMMs	O
.	O

29	O
.	O
The	O
method	O
of	O
claim	O
27	O
wherein	O
said	O
weights	O
are	O
mixture	O
component	O
weights	O
in	O
accordance	O
with	O
tied-mixture	O
HMMs	O
.	O

30	O
.	O
A	O
method	O
for	O
recognizing	O
speech	O
based	O
on	O
at	O
least	O
one	O
state	O
observation	O
likelihood	O
measure	O
,	O
each	O
state	O
observation	O
likelihood	O
measure	O
being	O
a	O
function	O
of	O
at	O
least	O
a	O
plurality	O
of	O
probability	O
kernels	O
and	O
a	O
plurality	O
of	O
weights	O
associated	O
therewith	O
,	O
comprising	O
:	O
deriving	O
the	O
at	O
least	O
one	O
state	O
observation	O
likelihood	O
measure	O
based	O
on	O
a	O
subset	O
of	O
the	O
plurality	O
of	O
probability	O
kernels	O
,	O
each	O
probability	O
kernel	O
in	O
the	O
subset	O
being	O
a	O
function	O
of	O
a	O
representation	O
of	O
the	O
speech	O
to	O
be	O
recognized	O
,	O
the	O
number	O
of	O
probability	O
kernels	O
in	O
the	O
subset	O
being	O
predetermined	O
,	O
the	O
predetermined	O
number	O
being	O
smaller	O
than	O
the	O
number	O
of	O
the	O
plurality	O
of	O
probability	O
kernels	O
,	O
the	O
probability	O
kernels	O
in	O
the	O
subset	O
each	O
having	O
a	O
larger	O
value	O
than	O
any	O
of	O
the	O
probability	O
kernels	O
outside	O
the	O
subset	O
;	O
and	O
an	O
output	O
for	O
generating	O
signals	O
representative	O
of	O
recognized	O
speech	O
based	O
on	O
the	O
at	O
least	O
one	O
state	O
observation	O
likelihood	O
measure	O
.	O

31	O
.	O
The	O
method	O
of	O
claim	O
30	O
wherein	O
said	O
probability	O
kernels	O
are	O
Gaussian	O
kernels	O
in	O
accordance	O
with	O
tied-mixture	O
HMMs	O
.	O

32	O
.	O
The	O
method	O
of	O
claim	O
30	O
wherein	O
said	O
weights	O
are	O
mixture	O
component	O
weights	O
in	O
accordance	O
with	O
tied-mixture	O
HMMs	O
.	O

