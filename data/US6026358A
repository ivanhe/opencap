US	O
6026358	O
A	O
20000215	O

US	O
08576585	O
19951221	O

eng	O
eng	O

JP	O
06336135	O
A	O
19941222	O

JP	O
07236061	O
A	O
19950822	O

336135	O

236061	O

JP19940336135	O

JP19950236061	O

20000215	O

20000215	O

7G	O
10L	O
5/04	O
A	O
7	O
G	O
10	O
L	O
5	O
04	O
A	O

7G	O
06E	O
1/00	O
B	O
7	O
G	O
06	O
E	O
1	O
00	O
B	O

G06F	O
15/18	O
20060101CFI20060310RMJP	O

20060101	O

C	O
G	O
06	O
F	O
15	O
18	O
F	O
I	O

20060310	O

JP	O

R	O
M	O

G06F	O
15/18	O
20060101AFI20060310RMJP	O

20060101	O

A	O
G	O
06	O
F	O
15	O
18	O
F	O
I	O

20060310	O

JP	O

R	O
M	O

G06N	O
3/00	O
20060101C	O
I20051008RMEP	O

20060101	O

C	O
G	O
06	O
N	O
3	O
00	O
I	O

20051008	O

EP	O

R	O
M	O

G06N	O
3/00	O
20060101ALI20060310RMJP	O

20060101	O

A	O
G	O
06	O
N	O
3	O
00	O
L	O
I	O

20060310	O

JP	O

R	O
M	O

G06N	O
3/04	O
20060101A	O
I20051008RMEP	O

20060101	O

A	O
G	O
06	O
N	O
3	O
04	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/00	O
20060101C	O
I20051008RMEP	O

20060101	O

C	O
G	O
10	O
L	O
15	O
00	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/02	O
20060101ALI20060310RMJP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
02	O
L	O
I	O

20060310	O

JP	O

R	O
M	O

G10L	O
15/08	O
20060101ALI20060310RMJP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
08	O
L	O
I	O

20060310	O

JP	O

R	O
M	O

G10L	O
15/16	O
20060101A	O
I20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
16	O
I	O

20051008	O

EP	O

R	O
M	O

US	O

704/232	O
704	O
232	O

704/249	O
704	O
249	O

704/E15.017	O
704	O
E15	O
.	O
017	O

G06N	O
3/04T	O
G	O
06	O
N	O
3	O
04	O

T	O

G10L	O
15/16	O
G	O
10	O
L	O
15	O
16	O

US	O

395/2	O
.	O
41	O
395	O
2	O
.	O
41	O

US	O

395/2	O
.	O
31	O
395	O
2	O
.	O
31	O

US	O

395/2	O
.	O
28	O
395	O
2	O
.	O
28	O

US	O

395/2	O
.	O
32	O
395	O
2	O
.	O
32	O

US	O

395/2	O
.	O
12	O
395	O
2	O
.	O
12	O

US	O

395/2	O
.	O
58	O
395	O
2	O
.	O
58	O

US	O

395/23	O
395	O
23	O

US	O

395/24	O
395	O
24	O

US	O

395/2	O
.	O
63	O
395	O
2	O
.	O
63	O

US	O

704/232	O
704	O
232	O

US	O

704/222	O
704	O
222	O

US	O

704/219	O
704	O
219	O

US	O

704/223	O
704	O
223	O

US	O

704/203	O
704	O
203	O

US	O

704/249	O
704	O
249	O

19	O
Neural	O
network	O
,	O
a	O
method	O
of	O
learning	O
of	O
a	O
neural	O
network	O
and	O
phoneme	O
recognition	O
apparatus	O
utilizing	O
a	O
neural	O
network	O

US	O
5175793	O
A	O
Sakamoto	O
et_al.	O

19921229	O

19900131	O

US	O

395/2	O
.	O
09	O
395	O
2	O
.	O
09	O

US	O
5214743	O
A	O
Asai	O
et_al.	O

19930525	O

19901024	O

US	O

395/11	O
395	O
11	O

US	O
5259039	O
A	O
Akamatsu	O
19931102	O

19920921	O

US	O

382/14	O
382	O
14	O

US	O
5381513	O
A	O
Tsuboka	O
19950110	O

19940316	O

US	O

395/2	O
.	O
41	O
395	O
2	O
.	O
41	O

US	O
5481644	O
A	O
Inazumi	O
19960102	O

19930806	O

US	O

395/2	O
.	O
41	O
395	O
2	O
.	O
41	O

Elman	B-Citation
,	I-Citation
Jeffery	I-Citation
L.	I-Citation
,	I-Citation
Finding	I-Citation
Structure	I-Citation
in	I-Citation
Time	I-Citation
,	I-Citation
Cognitive	I-Citation
Science	I-Citation
,	I-Citation
14	I-Citation
,	I-Citation
pp.	I-Citation
179	I-Citation
211	I-Citation
,	I-Citation
Apr.	I-Citation
Jun.	I-Citation
1990	I-Citation
.	O

Fahlman	B-Citation
Scott	I-Citation
E.	I-Citation
,	I-Citation
Faster	I-Citation
Learning	I-Citation
Variations	I-Citation
on	I-Citation
Back	I-Citation
Propagation	I-Citation
:	I-Citation
An	I-Citation
Empirical	I-Citation
Study	I-Citation
,	I-Citation
published	I-Citation
by	I-Citation
Carnegie	I-Citation
Mellon	I-Citation
University	I-Citation
Computer	I-Citation
Science	I-Citation
Department	I-Citation
,	I-Citation
Nov.	I-Citation
1988	I-Citation
.	O

Elman	B-Citation
,	I-Citation
Jeffery	I-Citation
L.	I-Citation
Structured	I-Citation
Representations	I-Citation
and	I-Citation
Connectionist	I-Citation
Models	I-Citation
,	I-Citation
published	I-Citation
by	I-Citation
University	I-Citation
of	I-Citation
California	I-Citation
,	I-Citation
San	I-Citation
Diego	I-Citation
Department	I-Citation
of	I-Citation
Cognitive	I-Citation
Science	I-Citation
(	I-Citation
1989	I-Citation
)	I-Citation
.	O

US	O
7599895	O
B2	O
20091006	O

20060626	O

US	O
7502769	O
B2	O
20090310	O

20051107	O

US	O
7426501	O
B2	O
20080916	O

20031215	O

US	O
7412428	O
B2	O
20080812	O

20031230	O

US	O
7409375	O
B2	O
20080805	O

20050606	O

US	O
7420396	O
B2	O
20080902	O

20060608	O

US	O
7392230	O
B2	O
20080624	O

20031230	O

US	O
7398259	O
B2	O
20080708	O

20041021	O

US	O
7107252	O
B2	O
20060912	O

20050131	O

US	O
7028017	O
B2	O
20060411	O

20050131	O

US	O
7039619	O
B2	O
20060502	O

20050131	O

US	O
6995649	O
B2	O
20060207	O

20050131	O

US	O
6954657	O
B2	O
20051011	O

20010508	O

US	O
6889216	O
B2	O
20050503	O

20020312	O

US	O
7752151	O
B2	O
20100706	O

20080410	O

US	O
7827130	O
B2	O
20101102	O

20090130	O

US	O
7827131	O
B2	O
20101102	O

20080410	O

US	O
7930257	O
B2	O
20110419	O

20071228	O

Justsystem	O
Corporation	O

Tokyo	O
JP	O

Tomabechi	O
Hideto	O

Tokushima	O
JP	O

Greenblum	O
&	O
Bernstein	O
P.	O
L.	O
C.	O

Dorvil	O
;	O
Richemond	O

US	O
6026358	O
A	O
20000215	O

19951221	O

US	O
6026358	O
A	O
20000215	O

19951221	O

JP	O
08227410	O
A	O
19960903	O

19950822	O

A	O
neuron	O
device	O
network	O
is	O
provided	O
with	O
a	O
speech	O
input	O
layer	O
,	O
a	O
context	O
layer	O
,	O
a	O
hidden	O
layer	O
,	O
a	O
speech	O
output	O
layer	O
and	O
a	O
hypothesis	O
layer	O
.	O
A	O
phoneme	O
to	O
be	O
learned	O
is	O
spectral-analyzed	O
by	O
an	O
FFT	O
unit	O
and	O
a	O
vector	O
row	O
at	O
a	O
time	O
point	O
t	O
is	O
input	O
to	O
a	O
speech	O
input	O
layer	O
.	O
Also	O
,	O
a	O
vector	O
state	O
of	O
the	O
hidden	O
layer	O
at	O
a	O
time	O
t-1	O
is	O
input	O
to	O
the	O
context	O
layer	O
,	O
the	O
vector	O
row	O
at	O
a	O
time	O
t	O
+	O
1	O
is	O
input	O
to	O
the	O
speech	O
output	O
layer	O
as	O
an	O
instructor	O
signal	O
,	O
and	O
a	O
code	O
row	O
for	O
hypothesizing	O
the	O
phoneme	O
,	O
or	O
the	O
code	O
row	O
,	O
is	O
input	O
to	O
the	O
hypothesis	O
layer	O
.	O
The	O
time	O
series	O
relation	O
of	O
the	O
vector	O
rows	O
and	O
the	O
phoneme	O
are	O
hypothetically	O
learned	O
.	O
Alternatively	O
,	O
a	O
spectrum	O
,	O
a	O
cepstrum	O
or	O
a	O
speech	O
vector	O
row	O
based	O
on	O
outputs	O
from	O
the	O
hidden	O
layer	O
of	O
an	O
auto-associative	O
neural	O
network	O
is	O
input	O
to	O
the	O
speech	O
input	O
layer	O
,	O
and	O
the	O
code	O
row	O
is	O
output	O
from	O
the	O
hypothesis	O
layer	O
,	O
taking	O
into	O
account	O
the	O
time	O
series	O
relation	O
.	O
The	O
speech	O
is	O
recognized	O
when	O
a	O
CPU	O
reads	O
the	O
stored	O
output	O
values	O
of	O
the	O
hidden	O
layer	O
and	O
the	O
connection	O
weights	O
of	O
the	O
hidden	O
layer	O
and	O
the	O
hypothesis	O
layer	O
from	O
a	O
memory	O
of	O
the	O
neuron	O
device	O
network	O
and	O
calculates	O
output	O
values	O
of	O
the	O
respective	O
neuron	O
devices	O
of	O
the	O
hypothesis	O
layer	O
based	O
on	O
the	O
output	O
values	O
and	O
the	O
connection	O
weights	O
.	O
The	O
corresponding	O
phoneme	O
is	O
determined	O
by	O
collating	O
the	O
output	O
values	O
of	O
the	O
respective	O
neuron	O
devices	O
of	O
the	O
hypothesis	O
layer	O
with	O
the	O
code	O
rows	O
in	O
an	O
instructor	O
signal	O
table	O
.	O

20030806	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
6026358A	O
4	O

20070814	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
6026358A	O
8	O

BACKGROUND	O
OF	O
THE	O
INVENTION	O
1	O
.	O
Field	O
of	O
the	O
Invention	O
The	O
present	O
invention	O
relates	O
to	O
a	O
method	O
and	O
apparatus	O
for	O
speech	O
recognition	O
using	O
a	O
neural	O
network	O
.	O
More	O
particularly	O
,	O
the	O
present	O
invention	O
relates	O
to	O
a	O
neural	O
network	O
,	O
a	O
learning	O
method	O
of	O
the	O
neural	O
network	O
,	O
and	O
a	O
phoneme	O
recognition	O
apparatus	O
using	O
the	O
neural	O
network	O
.	O

2	O
.	O
Background	O
Information	O
Neural	O
networks	O
are	O
a	O
recent	O
technology	O
which	O
mimics	O
the	O
information	O
processing	O
of	O
human	O
cerebral	O
nerves	O
,	O
and	O
have	O
attracted	O
much	O
attention	O
.	O
The	O
neural	O
network	O
is	O
constituted	O
by	O
a	O
neuron	O
device	O
network	O
having	O
a	O
plurality	O
of	O
neuron	O
devices	O
for	O
transmitting	O
data	O
,	O
and	O
a	O
learning	O
controller	O
for	O
controlling	O
learning	O
of	O
the	O
plurality	O
of	O
neuron	O
devices	O
.	O
The	O
neuron	O
device	O
network	O
is	O
generally	O
made	O
up	O
on	O
an	O
input	O
layer	O
to	O
which	O
data	O
are	O
input	O
,	O
an	O
output	O
layer	O
from	O
which	O
data	O
are	O
output	O
based	O
on	O
the	O
inputted	O
data	O
,	O
and	O
one	O
or	O
more	O
hidden	O
layers	O
provided	O
between	O
the	O
input	O
and	O
output	O
layers	O
.	O
Each	O
of	O
the	O
neuron	O
devices	O
provided	O
within	O
the	O
respective	O
layers	O
of	O
the	O
neuron	O
device	O
network	O
is	O
connected	O
with	O
another	O
neuron	O
device	O
with	O
(	O
i	O
.	O
e	O
.	O
,	O
through	O
)	O
a	O
predetermined	O
strength	O
(	O
connection	O
weight	O
)	O
,	O
and	O
an	O
output	O
signal	O
varies	O
in	O
accordance	O
with	O
the	O
values	O
of	O
the	O
connection	O
weights	O
between	O
the	O
devices	O
.	O

In	O
the	O
conventional	O
neural	O
network	O
having	O
the	O
above-described	O
hierarchic	O
structure	O
,	O
a	O
process	O
called	O
"	O
learning	O
"	O
is	O
carried	O
out	O
by	O
changing	O
the	O
connection	O
weight	O
between	O
the	O
respective	O
neuron	O
devices	O
by	O
the	O
learning	O
controller	O
.	O

Learning	O
is	O
performed	O
by	O
supplying	O
analog	O
or	O
binary	O
data	O
(	O
patterns	O
)	O
which	O
correspond	O
to	O
a	O
number	O
of	O
inputs/outputs	O
of	O
the	O
input	O
and	O
output	O
layers	O
.	O
If	O
it	O
is	O
assumed	O
that	O
g1	O
to	O
g6	O
are	O
supplied	O
as	O
input	O
data	O
,	O
then	O
the	O
output	O
signals	O
p1	O
to	O
p3	O
are	O
output	O
from	O
the	O
output	O
layer	O
when	O
g1	O
to	O
g3	O
are	O
received	O
as	O
learning	O
patterns	O
from	O
the	O
input	O
layer	O
.	O
If	O
the	O
correct	O
answers	O
are	O
received	O
from	O
the	O
output	O
signals	O
based	O
on	O
the	O
input	O
signals	O
g4	O
to	O
g6	O
,	O
the	O
signals	O
g4	O
to	O
g6	O
are	O
generally	O
referred	O
to	O
as	O
instructor	O
signals	O
.	O
Further	O
,	O
learning	O
is	O
performed	O
by	O
executing	O
a	O
correction	O
process	O
of	O
the	O
connection	O
weights	O
of	O
the	O
respective	O
neuron	O
devices	O
for	O
a	O
plurality	O
of	O
learning	O
patterns	O
in	O
order	O
to	O
minimize	O
the	O
margin	O
of	O
error	O
of	O
the	O
output	O
signals	O
p1	O
to	O
p3	O
based	O
on	O
the	O
instructor	O
signals	O
g4	O
to	O
g6	O
,	O
or	O
until	O
these	O
two	O
types	O
of	O
signals	O
coincide	O
with	O
each	O
other	O
.	O

Specifically	O
,	O
a	O
process	O
for	O
correcting	O
the	O
connection	O
weights	O
between	O
the	O
respective	O
neuron	O
devices	O
in	O
the	O
neuron	O
device	O
network	O
so	O
that	O
the	O
output	O
signals	O
coincide	O
with	O
the	O
instructor	O
signals	O
,	O
is	O
error	O
back-propagation	O
(	O
often	O
referred	O
to	O
as	O
BP	O
)	O
which	O
has	O
been	O
conventionally	O
used	O
.	O

In	O
order	O
to	O
minimize	O
the	O
margin	O
of	O
error	O
of	O
the	O
output	O
values	O
from	O
the	O
instructor	O
values	O
in	O
the	O
output	O
layer	O
,	O
the	O
error	O
back-propagation	O
is	O
used	O
to	O
correct	O
the	O
connection	O
weights	O
of	O
the	O
respective	O
neuron	O
devices	O
between	O
all	O
of	O
the	O
layers	O
constituting	O
the	O
neural	O
network	O
.	O
That	O
is	O
,	O
the	O
error	O
in	O
the	O
output	O
layer	O
is	O
determined	O
as	O
a	O
product	O
obtained	O
from	O
individual	O
errors	O
generated	O
from	O
the	O
neuron	O
devices	O
in	O
the	O
respective	O
hidden	O
layers	O
,	O
and	O
the	O
connection	O
weight	O
and	O
is	O
corrected	O
so	O
that	O
not	O
only	O
the	O
error	O
from	O
the	O
output	O
layer	O
,	O
but	O
also	O
the	O
error	O
of	O
the	O
neuron	O
devices	O
in	O
the	O
respective	O
hidden	O
layers	O
,	O
which	O
is	O
a	O
cause	O
of	O
the	O
error	O
from	O
the	O
output	O
layer	O
,	O
are	O
minimized	O
.	O
Thus	O
,	O
all	O
errors	O
are	O
computed	O
in	O
accordance	O
with	O
each	O
neuron	O
device	O
in	O
both	O
the	O
output	O
layer	O
and	O
the	O
respective	O
hidden	O
layers	O
.	O

According	O
to	O
error	O
back-propagation	O
processing	O
,	O
individual	O
error	O
values	O
of	O
the	O
neuron	O
devices	O
in	O
the	O
output	O
layer	O
are	O
given	O
as	O
initial	O
conditions	O
,	O
and	O
the	O
processing	O
is	O
executed	O
in	O
the	O
reverse	O
order	O
,	O
namely	O
,	O
a	O
first	O
target	O
of	O
computation	O
is	O
an	O
error	O
value	O
of	O
each	O
neuron	O
device	O
in	O
an	O
nth	O
hidden	O
layer	O
,	O
a	O
second	O
target	O
is	O
an	O
error	O
value	O
of	O
each	O
neuron	O
device	O
in	O
an	O
(	O
n-1	O
)	O
th	O
hidden	O
layer	O
,	O
and	O
the	O
last	O
target	O
is	O
an	O
error	O
value	O
of	O
each	O
neuron	O
device	O
in	O
the	O
first	O
hidden	O
layer	O
.	O
A	O
correction	O
value	O
is	O
calculated	O
based	O
on	O
the	O
thus-obtained	O
error	O
value	O
for	O
each	O
neuron	O
device	O
and	O
the	O
current	O
connection	O
weight	O
.	O

Learning	O
is	O
completed	O
by	O
repeating	O
the	O
above-described	O
learning	O
processing	O
with	O
respect	O
to	O
all	O
of	O
the	O
learning	O
patterns	O
a	O
predetermined	O
number	O
of	O
times	O
,	O
or	O
until	O
the	O
magnitude	O
of	O
error	O
of	O
the	O
output	O
signal	O
from	O
the	O
instructor	O
signal	O
is	O
below	O
a	O
predetermined	O
value	O
.	O

Typically	O
,	O
neural	O
networks	O
have	O
been	O
used	O
in	O
systems	O
for	O
pattern	O
recognition	O
,	O
such	O
as	O
characters	O
or	O
graphics	O
of	O
various	O
data	O
,	O
processes	O
for	O
analyzing	O
or	O
synthesizing	O
voices	O
,	O
or	O
prediction	O
of	O
occurrence	O
of	O
time	O
series	O
patterns	O
of	O
movement	O
.	O

In	O
the	O
conventional	O
neural	O
network	O
,	O
however	O
,	O
these	O
layers	O
of	O
the	O
neuron	O
device	O
network	O
have	O
not	O
been	O
implemented	O
in	O
such	O
a	O
manner	O
that	O
learning	O
can	O
be	O
effectively	O
performed	O
when	O
carrying	O
out	O
speech	O
recognition	O
,	O
character	O
recognition	O
or	O
form	O
recognition	O
.	O
Thus	O
,	O
in	O
the	O
case	O
where	O
the	O
conventional	O
neural	O
network	O
is	O
used	O
in	O
,	O
e.g.	O
,	O
a	O
speech	O
recognition	O
apparatus	O
,	O
an	O
input	O
spectrum	O
is	O
segmented	O
to	O
coincide	O
with	O
a	O
size	O
of	O
the	O
neural	O
network	O
.	O
Therefore	O
,	O
it	O
is	O
difficult	O
to	O
apply	O
the	O
neural	O
network	O
to	O
the	O
recognition	O
of	O
a	O
continuous	O
stream	O
of	O
speech	O
because	O
the	O
uttering	O
speed	O
and	O
a	O
length	O
of	O
each	O
phoneme	O
may	O
vary	O
greatly	O
.	O
At	O
present	O
,	O
speech	O
recognition	O
is	O
performed	O
at	O
each	O
phoneme	O
level	O
after	O
the	O
phoneme	O
is	O
subjected	O
to	O
segment	O
processing	O
to	O
match	O
the	O
size	O
of	O
the	O
neural	O
network	O
.	O

In	O
addition	O
,	O
the	O
input	O
spectrum	O
must	O
be	O
adapted	O
to	O
coincide	O
with	O
an	O
initial	O
position	O
of	O
the	O
speech	O
recognition	O
neural	O
network	O
.	O
Therefore	O
,	O
it	O
is	O
impossible	O
to	O
perform	O
the	O
recognition	O
of	O
a	O
continuous	O
stream	O
of	O
speech	O
when	O
the	O
start	O
time	O
of	O
a	O
phoneme	O
is	O
unpredictable	O
.	O

Further	O
,	O
in	O
the	O
conventional	O
neural	O
network	O
,	O
each	O
spectrum	O
of	O
the	O
phoneme	O
is	O
individually	O
processed	O
during	O
the	O
speech	O
recognition	O
.	O
However	O
,	O
since	O
the	O
state	O
of	O
a	O
current	O
phoneme	O
is	O
affected	O
by	O
the	O
state	O
of	O
a	O
phoneme	O
which	O
immediately	O
precedes	O
the	O
current	O
phoneme	O
during	O
continuous	O
speech	O
recognition	O
,	O
the	O
previous	O
phoneme	O
information	O
cannot	O
be	O
used	O
in	O
speech	O
recognition	O
of	O
the	O
current	O
phoneme	O
by	O
the	O
conventional	O
neural	O
network	O
where	O
each	O
phoneme	O
is	O
individually	O
requested	O
,	O
thus	O
the	O
conventional	O
neural	O
network	O
is	O
not	O
suitable	O
for	O
continuous	O
speech	O
recognition	O
.	O

SUMMARY	O
OF	O
THE	O
INVENTION	O
In	O
view	O
of	O
the	O
foregoing	O
,	O
the	O
present	O
invention	O
,	O
through	O
one	O
or	O
more	O
of	O
its	O
various	O
aspects	O
,	O
embodiments	O
and/or	O
specific	O
features	O
or	O
subcomponents	O
thereof	O
,	O
is	O
thus	O
provided	O
,	O
intended	O
and	O
designed	O
to	O
bring	O
about	O
one	O
or	O
more	O
of	O
the	O
objects	O
and	O
advantages	O
as	O
specifically	O
noted	O
below	O
.	O

It	O
is	O
therefore	O
a	O
first	O
object	O
of	O
the	O
present	O
invention	O
to	O
provide	O
a	O
new	O
learning	O
method	O
for	O
a	O
neural	O
network	O
for	O
processing	O
data	O
in	O
which	O
a	O
set	O
of	O
a	O
plurality	O
of	O
vector	O
rows	O
represents	O
a	O
predefined	O
pattern	O
.	O

Further	O
,	O
a	O
second	O
object	O
of	O
the	O
present	O
invention	O
is	O
to	O
provide	O
a	O
neural	O
network	O
having	O
a	O
processing	O
architecture	O
for	O
data	O
in	O
which	O
a	O
plurality	O
of	O
vector	O
rows	O
represents	O
a	O
predefined	O
pattern	O
.	O

Furthermore	O
,	O
it	O
is	O
a	O
third	O
object	O
of	O
the	O
present	O
invention	O
to	O
provide	O
a	O
speech	O
recognition	O
apparatus	O
capable	O
of	O
recognizing	O
a	O
continuous	O
stream	O
of	O
speech	O
in	O
accordance	O
with	O
each	O
phoneme	O
or	O
a	O
word	O
.	O

According	O
to	O
one	O
aspect	O
of	O
the	O
invention	O
,	O
a	O
method	O
of	O
learning	O
is	O
provided	O
which	O
is	O
characterized	O
by	O
the	O
steps	O
of	O
inputting	O
first	O
vector	O
rows	O
representing	O
data	O
to	O
a	O
data	O
input	O
layer	O
,	O
inputting	O
second	O
vectors	O
rows	O
as	O
a	O
first	O
instructor	O
signal	O
to	O
a	O
first	O
output	O
layer	O
,	O
and	O
inputting	O
a	O
definite	O
meaning	O
as	O
a	O
second	O
instructor	O
signal	O
to	O
a	O
second	O
output	O
layer	O
.	O
Learning	O
is	O
performed	O
for	O
the	O
data	O
by	O
having	O
a	O
plurality	O
of	O
first	O
vector	O
rows	O
represent	O
the	O
definite	O
meaning	O
.	O

According	O
to	O
another	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
a	O
method	O
of	O
learning	O
of	O
a	O
neural	O
network	O
is	O
provided	O
which	O
is	O
characterized	O
by	O
the	O
steps	O
of	O
inputting	O
output	O
vector	O
values	O
of	O
a	O
hidden	O
layer	O
or	O
a	O
first	O
output	O
layer	O
having	O
a	O
plurality	O
of	O
neuron	O
devices	O
,	O
the	O
plurality	O
of	O
neuron	O
devices	O
corresponding	O
to	O
first	O
vector	O
rows	O
of	O
a	O
feedback	O
input	O
layer	O
,	O
the	O
feedback	O
input	O
layer	O
connected	O
with	O
the	O
hidden	O
layer	O
and	O
having	O
a	O
number	O
of	O
neuron	O
devices	O
equal	O
to	O
a	O
number	O
of	O
neuron	O
devices	O
of	O
the	O
hidden	O
layer	O
,	O
inputting	O
second	O
vector	O
rows	O
representing	O
data	O
to	O
a	O
data	O
input	O
layer	O
,	O
inputting	O
third	O
vector	O
rows	O
as	O
a	O
first	O
instructor	O
signal	O
to	O
a	O
first	O
output	O
layer	O
,	O
and	O
inputting	O
a	O
definite	O
meaning	O
as	O
a	O
second	O
instructor	O
signal	O
to	O
a	O
second	O
output	O
layer	O
.	O
Learning	O
is	O
performed	O
for	O
the	O
data	O
by	O
having	O
a	O
plurality	O
of	O
second	O
vector	O
rows	O
representing	O
the	O
definite	O
meaning	O
.	O

Further	O
,	O
according	O
to	O
another	O
aspect	O
of	O
the	O
invention	O
,	O
there	O
is	O
provided	O
a	O
neural	O
network	O
comprising	O
a	O
neuron	O
device	O
network	O
having	O
a	O
data	O
input	O
layer	O
,	O
a	O
hidden	O
layer	O
connected	O
to	O
the	O
data	O
input	O
layer	O
,	O
and	O
an	O
output	O
layer	O
connected	O
to	O
the	O
hidden	O
layer	O
,	O
the	O
output	O
layer	O
comprising	O
a	O
first	O
output	O
layer	O
and	O
a	O
second	O
output	O
layer	O
,	O
a	O
learning	O
device	O
in	O
the	O
neuron	O
device	O
network	O
for	O
learning	O
about	O
data	O
having	O
a	O
plurality	O
of	O
first	O
vector	O
rows	O
representing	O
a	O
definite	O
meaning	O
,	O
an	O
inputting	O
device	O
for	O
inputting	O
the	O
plurality	O
of	O
first	O
vector	O
rows	O
to	O
the	O
data	O
input	O
layer	O
of	O
the	O
neuron	O
device	O
network	O
,	O
and	O
an	O
outputting	O
device	O
for	O
outputting	O
output	O
signals	O
of	O
the	O
second	O
output	O
layer	O
based	O
on	O
input	O
of	O
the	O
plurality	O
of	O
first	O
vector	O
rows	O
by	O
the	O
inputting	O
device	O
.	O
The	O
learning	O
device	O
inputs	O
the	O
plurality	O
of	O
first	O
vector	O
rows	O
to	O
the	O
data	O
input	O
layer	O
,	O
inputs	O
the	O
second	O
vector	O
rows	O
as	O
a	O
first	O
instructor	O
signal	O
to	O
the	O
first	O
output	O
layer	O
and	O
inputs	O
the	O
definite	O
meaning	O
as	O
a	O
second	O
instructor	O
signal	O
to	O
the	O
second	O
output	O
layer	O
.	O

According	O
to	O
yet	O
another	O
aspect	O
of	O
the	O
invention	O
there	O
is	O
provided	O
a	O
neural	O
network	O
comprising	O
a	O
neuron	O
device	O
network	O
comprising	O
an	O
input	O
layer	O
having	O
a	O
data	O
input	O
layer	O
and	O
a	O
feedback	O
input	O
layer	O
,	O
a	O
hidden	O
layer	O
connected	O
to	O
the	O
input	O
layer	O
,	O
and	O
an	O
output	O
layer	O
connected	O
to	O
the	O
hidden	O
layer	O
,	O
the	O
output	O
layer	O
having	O
a	O
first	O
output	O
layer	O
and	O
a	O
second	O
output	O
layer	O
,	O
an	O
inputting	O
device	O
for	O
inputting	O
the	O
plurality	O
of	O
first	O
vector	O
rows	O
to	O
the	O
data	O
input	O
layer	O
of	O
the	O
neuron	O
device	O
network	O
such	O
that	O
the	O
learning	O
device	O
performs	O
the	O
learning	O
,	O
and	O
an	O
outputting	O
device	O
for	O
outputting	O
output	O
signals	O
of	O
the	O
second	O
output	O
layer	O
based	O
on	O
input	O
of	O
the	O
plurality	O
of	O
first	O
vector	O
rows	O
by	O
the	O
inputting	O
device	O
.	O
Also	O
provided	O
is	O
a	O
learning	O
device	O
in	O
the	O
neuron	O
device	O
network	O
for	O
learning	O
about	O
data	O
having	O
a	O
plurality	O
of	O
first	O
vector	O
rows	O
representing	O
a	O
definite	O
meaning	O
by	O
inputting	O
a	O
plurality	O
of	O
second	O
vector	O
values	O
of	O
the	O
hidden	O
layer	O
or	O
the	O
first	O
output	O
layer	O
,	O
to	O
the	O
input	O
layer	O
,	O
inputting	O
the	O
plurality	O
of	O
first	O
vector	O
rows	O
to	O
the	O
data	O
input	O
layer	O
of	O
the	O
input	O
layer	O
,	O
inputting	O
a	O
plurality	O
of	O
third	O
vector	O
rows	O
as	O
a	O
first	O
instructor	O
signal	O
to	O
the	O
first	O
output	O
layer	O
,	O
and	O
inputting	O
the	O
definite	O
meaning	O
as	O
a	O
second	O
instructor	O
signal	O
to	O
the	O
second	O
output	O
layer	O
.	O

According	O
to	O
yet	O
another	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
there	O
is	O
provided	O
a	O
speech	O
recognition	O
apparatus	O
comprising	O
a	O
neural	O
network	O
,	O
a	O
speech	O
inputting	O
device	O
for	O
inputting	O
speech	O
,	O
an	O
analyzing	O
device	O
for	O
analyzing	O
in	O
a	O
time-series	O
,	O
vector	O
rows	O
representing	O
characteristics	O
of	O
the	O
speech	O
input	O
by	O
the	O
speech	O
inputting	O
device	O
,	O
a	O
vector	O
row	O
inputting	O
device	O
for	O
successively	O
inputting	O
the	O
vector	O
rows	O
analyzed	O
by	O
the	O
analyzing	O
device	O
to	O
a	O
data	O
input	O
layer	O
of	O
the	O
neural	O
network	O
,	O
and	O
a	O
phoneme	O
specifying	O
device	O
for	O
specifying	O
a	O
phoneme	O
in	O
accordance	O
with	O
outputs	O
of	O
an	O
output	O
layer	O
of	O
the	O
neural	O
network	O
by	O
successively	O
inputting	O
the	O
vector	O
rows	O
to	O
the	O
data	O
input	O
layer	O
by	O
the	O
vector	O
row	O
inputting	O
device	O
.	O

Further	O
,	O
the	O
analyzing	O
device	O
may	O
use	O
spectral	O
data	O
or	O
cepstrum	O
data	O
of	O
the	O
inputted	O
speech	O
,	O
or	O
output	O
value	O
data	O
of	O
a	O
hidden	O
layer	O
of	O
an	O
auto-associative	O
neural	O
network	O
as	O
the	O
vector	O
rows	O
representing	O
a	O
quantity	O
of	O
characteristics	O
of	O
the	O
speech	O
.	O

The	O
above-listed	O
and	O
other	O
objects	O
,	O
features	O
and	O
advantages	O
will	O
be	O
more	O
fully	O
set	O
forth	O
hereinafter	O
.	O

BRIEF	O
DESCRIPTION	O
OF	O
THE	O
DRAWINGS	O
The	O
present	O
invention	O
is	O
further	O
described	O
in	O
the	O
detailed	O
description	O
which	O
follows	O
,	O
by	O
reference	O
to	O
the	O
noted	O
plurality	O
of	O
drawings	O
by	O
way	O
of	O
non-limiting	O
examples	O
of	O
preferred	O
embodiments	O
of	O
the	O
present	O
invention	O
,	O
in	O
which	O
like	O
reference	O
numerals	O
represent	O
similar	O
parts	O
throughout	O
the	O
several	O
views	O
of	O
the	O
drawings	O
,	O
and	O
wherein	O
:	O
FIG	O
.	O
1	O
is	O
a	O
schematic	O
block	O
diagram	O
showing	O
a	O
speech	O
recognition	O
apparatus	O
using	O
a	O
neutral	O
network	O
according	O
to	O
an	O
embodiment	O
of	O
the	O
present	O
invention	O
;	O
FIG	O
.	O
2	O
is	O
a	O
diagram	O
showing	O
the	O
structure	O
of	O
a	O
neuron	O
device	O
network	O
of	O
the	O
speech	O
recognition	O
apparatus	O
;	O
FIG	O
.	O
3	O
is	O
an	O
explanatory	O
view	O
showing	O
a	O
content	O
of	O
a	O
second	O
instructor	O
signal	O
table	O
with	O
respect	O
to	O
the	O
neuron	O
device	O
network	O
;	O
FIG	O
.	O
4	O
is	O
a	O
chart	O
showing	O
a	O
connection	O
weight	O
table	O
between	O
respective	O
neuron	O
devices	O
of	O
the	O
neuron	O
device	O
network	O
;	O
FIGS	O
.	O
5A	O
and	O
5B	O
show	O
the	O
relationship	O
of	O
the	O
input	O
analog	O
voice	O
and	O
the	O
converted	O
character	O
string	O
voice	O
data	O
in	O
accordance	O
with	O
an	O
aspect	O
of	O
the	O
voice	O
transmission	O
apparatus	O
;	O
FIG	O
.	O
6	O
is	O
an	O
explanatory	O
view	O
showing	O
vector	O
rows	O
pertaining	O
to	O
the	O
speech	O
which	O
has	O
been	O
subjected	O
to	O
spectral	O
analysis	O
by	O
an	O
FFT	O
unit	O
of	O
the	O
speech	O
recognition	O
apparatus	O
;	O
FIG	O
.	O
7	O
is	O
a	O
chart	O
showing	O
a	O
distribution	O
of	O
each	O
phoneme	O
of	O
a	O
word	O
"	O
mae	O
"	O
specified	O
by	O
the	O
speech	O
recognition	O
apparatus	O
;	O
FIG	O
.	O
8	O
is	O
a	O
diagram	O
showing	O
the	O
structure	O
of	O
another	O
example	O
of	O
a	O
neuron	O
device	O
network	O
in	O
the	O
speech	O
recognition	O
apparatus	O
of	O
the	O
present	O
invention	O
;	O
FIG	O
.	O
9	O
is	O
a	O
schematic	O
block	O
diagram	O
showing	O
the	O
system	O
structure	O
of	O
a	O
neural	O
network	O
according	O
to	O
a	O
second	O
embodiment	O
of	O
the	O
present	O
invention	O
;	O
FIG	O
.	O
10	O
is	O
a	O
diagram	O
showing	O
the	O
structure	O
of	O
a	O
cepstrum	O
unit	O
in	O
the	O
second	O
embodiment	O
;	O
FIG	O
.	O
11	O
is	O
a	O
diagram	O
showing	O
the	O
system	O
structure	O
of	O
a	O
neural	O
network	O
used	O
according	O
to	O
a	O
third	O
embodiment	O
of	O
the	O
present	O
invention	O
;	O
FIG	O
.	O
12	O
is	O
a	O
diagram	O
showing	O
the	O
structure	O
of	O
the	O
auto-associative	O
neural	O
network	O
according	O
to	O
the	O
third	O
embodiment	O
;	O
FIGS	O
.	O
13A	O
,	O
13B	O
and	O
13C	O
are	O
graphs	O
showing	O
data	O
obtained	O
by	O
spectral-analyzing	O
the	O
terms	O
"	O
"	O
,	O
"	O
"	O
and	O
"	O
A	O
"	O
according	O
to	O
the	O
third	O
embodiment	O
;	O
and	O
FIG	O
.	O
14	O
is	O
an	O
explanatory	O
view	O
showing	O
a	O
relation	O
among	O
input	O
data	O
,	O
instructor	O
signals	O
and	O
output	O
data	O
when	O
the	O
auto-associative	O
neural	O
network	O
is	O
learning	O
according	O
to	O
the	O
third	O
embodiment	O
.	O

DESCRIPTION	O
OF	O
THE	O
PREFERRED	O
EMBODIMENTS	O
Detailed	O
description	O
will	O
now	O
be	O
given	O
with	O
regard	O
to	O
a	O
neural	O
network	O
,	O
a	O
learning	O
method	O
for	O
the	O
neural	O
network	O
and	O
a	O
speech	O
recognition	O
apparatus	O
using	O
the	O
neural	O
network	O
according	O
to	O
the	O
features	O
of	O
a	O
first	O
embodiment	O
of	O
the	O
present	O
invention	O
with	O
reference	O
to	O
FIGS	O
.	O
1	O
-	O
8	O
.	O

FIG	O
.	O
1	O
illustrates	O
in	O
block	O
diagram	O
fashion	O
,	O
the	O
system	O
structure	O
of	O
a	O
speech	O
recognition	O
apparatus	O
using	O
a	O
neural	O
network	O
according	O
to	O
a	O
first	O
embodiment	O
of	O
the	O
present	O
invention	O
.	O

The	O
speech	O
recognition	O
apparatus	O
is	O
provided	O
with	O
a	O
CPU	O
11	O
for	O
performing	O
the	O
functions	O
of	O
inputting	O
vector	O
rows	O
and	O
instructor	O
signals	O
(	O
vector	O
rows	O
)	O
to	O
an	O
output	O
layer	O
for	O
the	O
learning	O
process	O
of	O
a	O
neuron	O
device	O
network	O
22	O
,	O
and	O
changing	O
connection	O
weights	O
between	O
respective	O
neuron	O
devices	O
based	O
on	O
the	O
learning	O
process	O
.	O
The	O
CPU	O
11	O
first	O
performs	O
various	O
processing	O
and	O
controlling	O
functions	O
,	O
such	O
as	O
speech	O
recognition	O
based	O
on	O
the	O
output	O
signals	O
from	O
the	O
neuron	O
device	O
network	O
22	O
.	O
The	O
CPU	O
11	O
is	O
connected	O
to	O
a	O
read-only	O
memory	O
(	O
ROM	O
)	O
13	O
,	O
a	O
random-access	O
memory	O
(	O
RAM	O
)	O
14	O
,	O
a	O
communication	O
control	O
unit	O
15	O
,	O
a	O
printer	O
16	O
,	O
a	O
display	O
unit	O
17	O
,	O
a	O
keyboard	O
18	O
,	O
an	O
FFT	O
(	O
fast	O
Fourier	O
transform	O
)	O
unit	O
21	O
,	O
a	O
neuron	O
device	O
network	O
22	O
and	O
a	O
graphic	O
reading	O
unit	O
24	O
through	O
a	O
bus	O
line	O
12	O
such	O
as	O
a	O
data	O
bus	O
line	O
.	O
The	O
bus	O
line	O
12	O
may	O
be	O
an	O
ISA	O
,	O
EISA	O
,	O
or	O
PCI	O
bus	O
,	O
for	O
example	O
.	O

The	O
ROM	O
13	O
is	O
a	O
read-only	O
memory	O
storing	O
various	O
programs	O
or	O
data	O
used	O
by	O
the	O
CPU	O
11	O
for	O
performing	O
processing	O
or	O
controlling	O
the	O
learning	O
process	O
,	O
and	O
speech	O
recognition	O
of	O
the	O
neuron	O
device	O
network	O
22	O
.	O
The	O
ROM	O
13	O
stores	O
programs	O
for	O
carrying	O
out	O
the	O
learning	O
process	O
according	O
to	O
error	O
back-propagation	O
for	O
the	O
neuron	O
device	O
network	O
or	O
code	O
rows	O
concerning	O
,	O
for	O
example	O
,	O
80	O
kinds	O
of	O
phonemes	O
for	O
performing	O
speech	O
recognition	O
.	O
The	O
code	O
rows	O
concerning	O
the	O
phonemes	O
are	O
used	O
as	O
second	O
instructor	O
signals	O
and	O
for	O
recognizing	O
phonemes	O
from	O
output	O
signals	O
of	O
the	O
neuron	O
device	O
network	O
.	O
Also	O
,	O
the	O
ROM	O
13	O
stores	O
programs	O
of	O
a	O
transformation	O
system	O
for	O
recognizing	O
speech	O
from	O
recognized	O
phonemes	O
and	O
transforming	O
the	O
recognized	O
speech	O
into	O
a	O
writing	O
(	O
i	O
.	O
e	O
.	O
,	O
written	O
form	O
)	O
represented	O
by	O
characters	O
.	O

A	O
predetermined	O
program	O
stored	O
in	O
the	O
ROM	O
13	O
is	O
downloaded	O
and	O
stored	O
in	O
the	O
RAM	O
14	O
.	O
The	O
RAM	O
14	O
is	O
a	O
random	O
access	O
memory	O
used	O
as	O
a	O
working	O
memory	O
of	O
the	O
CPU	O
11	O
.	O
In	O
the	O
RAM	O
14	O
,	O
a	O
vector	O
row	O
storing	O
area	O
is	O
provided	O
for	O
temporarily	O
storing	O
a	O
power	O
obtained	O
at	O
each	O
point	O
in	O
time	O
for	O
each	O
frequency	O
of	O
the	O
speech	O
signal	O
analyzed	O
by	O
the	O
FFT	O
unit	O
21	O
.	O
A	O
value	O
of	O
the	O
power	O
for	O
each	O
frequency	O
serves	O
as	O
a	O
vector	O
row	O
input	O
to	O
a	O
first	O
input	O
portion	O
of	O
the	O
neuron	O
device	O
network	O
22	O
.	O

Further	O
,	O
in	O
the	O
case	O
where	O
characters	O
or	O
graphics	O
are	O
recognized	O
in	O
the	O
neural	O
network	O
,	O
the	O
image	O
data	O
read	O
by	O
the	O
graphic	O
reading	O
unit	O
24	O
are	O
stored	O
in	O
the	O
RAM	O
14	O
.	O

The	O
communication	O
control	O
unit	O
15	O
transmits	O
and/or	O
receives	O
various	O
data	O
such	O
as	O
recognized	O
speech	O
data	O
to	O
and/or	O
from	O
another	O
communication	O
control	O
unit	O
through	O
a	O
communication	O
network	O
2	O
such	O
as	O
a	O
telephone	O
line	O
network	O
,	O
an	O
ISDN	O
line	O
,	O
a	O
LAN	O
,	O
or	O
a	O
personal	O
computer	O
communication	O
network	O
.	O

The	O
printer	O
16	O
can	O
be	O
provided	O
with	O
a	O
laser	O
printer	O
,	O
a	O
bubble-type	O
printer	O
,	O
a	O
dot	O
matrix	O
printer	O
,	O
or	O
the	O
like	O
,	O
and	O
prints	O
contents	O
of	O
input	O
data	O
or	O
the	O
recognized	O
speech	O
.	O

The	O
display	O
unit	O
17	O
includes	O
an	O
image	O
display	O
portion	O
such	O
as	O
a	O
CRT	O
display	O
or	O
a	O
liquid	O
crystal	O
display	O
,	O
and	O
a	O
display	O
control	O
portion	O
.	O
The	O
display	O
unit	O
17	O
displays	O
the	O
contents	O
of	O
the	O
input	O
data	O
or	O
the	O
recognized	O
speech	O
as	O
well	O
as	O
a	O
direction	O
of	O
an	O
operation	O
required	O
for	O
speech	O
recognition	O
.	O

The	O
keyboard	O
18	O
is	O
an	O
input	O
unit	O
for	O
varying	O
operating	O
parameters	O
or	O
inputting	O
setting	O
conditions	O
of	O
the	O
FFT	O
unit	O
21	O
,	O
or	O
for	O
inputting	O
sentences	O
.	O
The	O
keyboard	O
18	O
is	O
provided	O
with	O
a	O
ten-key	O
numeric	O
pad	O
for	O
inputting	O
numerical	O
figures	O
,	O
character	O
keys	O
for	O
inputting	O
characters	O
,	O
and	O
function	O
keys	O
for	O
performing	O
various	O
functions	O
.	O
A	O
mouse	O
19	O
is	O
connected	O
to	O
the	O
keyboard	O
18	O
and	O
serves	O
as	O
a	O
pointing	O
device	O
.	O

A	O
speech	O
input	O
unit	O
23	O
,	O
such	O
as	O
a	O
microphone	O
is	O
connected	O
to	O
the	O
FFT	O
unit	O
21	O
.	O
The	O
FFT	O
unit	O
21	O
transforms	O
analog	O
speech	O
data	O
input	O
from	O
the	O
voice	O
input	O
unit	O
23	O
into	O
digital	O
data	O
and	O
carries	O
out	O
spectral	O
analysis	O
of	O
the	O
digital	O
data	O
by	O
discrete	O
Fourier	O
transformation	O
.	O
By	O
performing	O
a	O
spectral	O
analysis	O
using	O
the	O
FFT	O
unit	O
21	O
,	O
the	O
vector	O
row	O
based	O
on	O
the	O
powers	O
of	O
the	O
respective	O
frequencies	O
are	O
output	O
at	O
predetermined	O
intervals	O
of	O
time	O
.	O
The	O
FFT	O
unit	O
21	O
performs	O
an	O
analysis	O
of	O
time-series	O
vector	O
rows	O
which	O
represent	O
characteristics	O
of	O
the	O
inputted	O
speech	O
.	O
The	O
vector	O
rows	O
output	O
by	O
the	O
FFT	O
21	O
are	O
stored	O
in	O
the	O
vector	O
row	O
storing	O
area	O
in	O
the	O
RAM	O
14	O
.	O

The	O
graphic	O
reading	O
unit	O
24	O
,	O
provided	O
with	O
devices	O
such	O
as	O
a	O
CCD	O
(	O
Charged	O
Coupled	O
Device	O
)	O
,	O
is	O
used	O
for	O
reading	O
images	O
such	O
as	O
characters	O
or	O
graphics	O
recorded	O
on	O
paper	O
or	O
the	O
like	O
.	O
The	O
image	O
data	O
read	O
by	O
the	O
image	O
reading	O
unit	O
24	O
are	O
stored	O
in	O
the	O
RAM	O
14	O
.	O

FIG	O
.	O
2	O
shows	O
the	O
structure	O
of	O
the	O
neuron	O
device	O
network	O
22	O
of	O
FIG	O
.	O
1	O
.	O

As	O
shown	O
in	O
FIG	O
.	O
2	O
,	O
the	O
neuron	O
device	O
network	O
22	O
comprises	O
three	O
groups	O
consisting	O
of	O
five	O
layers	O
.	O
That	O
is	O
,	O
a	O
first	O
group	O
is	O
an	O
input	O
layer	O
31	O
having	O
a	O
speech	O
input	O
layer	O
32	O
functioning	O
as	O
a	O
data	O
input	O
layer	O
and	O
a	O
context	O
layer	O
33	O
functioning	O
as	O
a	O
feedback	O
input	O
layer	O
.	O
A	O
second	O
group	O
has	O
a	O
hidden	O
layer	O
34	O
.	O
A	O
third	O
group	O
is	O
an	O
output	O
layer	O
36	O
having	O
a	O
speech	O
output	O
layer	O
37	O
functioning	O
as	O
a	O
first	O
output	O
layer	O
and	O
a	O
hypothesis	O
layer	O
38	O
functioning	O
as	O
a	O
second	O
output	O
layer	O
.	O

The	O
neuron	O
device	O
network	O
22	O
has	O
a	O
memory	O
(	O
not	O
shown	O
)	O
for	O
storing	O
values	O
of	O
respective	O
neuron	O
devices	O
constituting	O
the	O
input	O
layer	O
31	O
,	O
the	O
hidden	O
layer	O
34	O
and	O
the	O
output	O
layer	O
36	O
.	O

In	O
the	O
speech	O
recognition	O
apparatus	O
according	O
to	O
the	O
first	O
embodiment	O
,	O
the	O
speech	O
input	O
layer	O
32	O
has	O
30	O
neuron	O
devices	O
In1	O
to	O
In30	O
.	O
Further	O
,	O
the	O
hidden	O
layer	O
34	O
has	O
200	O
neuron	O
devices	O
Hi1	O
to	O
Hi200	O
.	O
The	O
context	O
layer	O
33	O
has	O
200	O
neuron	O
devices	O
Co1	O
to	O
Co200	O
,	O
which	O
is	O
the	O
same	O
number	O
of	O
devices	O
as	O
that	O
of	O
the	O
hidden	O
layer	O
34	O
.	O
The	O
speech	O
output	O
layer	O
37	O
has	O
30	O
neuron	O
devices	O
Ou1	O
to	O
Ou30	O
,	O
which	O
is	O
the	O
same	O
number	O
of	O
devices	O
as	O
that	O
of	O
the	O
speech	O
input	O
layer	O
32	O
.	O
The	O
hypothesis	O
layer	O
38	O
has	O
eight	O
neuron	O
devices	O
Hy1	O
to	O
Hy8	O
.	O

The	O
hypothesis	O
layer	O
38	O
has	O
eight	O
neuron	O
devices	O
because	O
in	O
this	O
example	O
the	O
80	O
phonemes	O
to	O
be	O
recognized	O
can	O
be	O
effectively	O
encoded	O
with	O
eight	O
neuron	O
devices	O
.	O
The	O
spoken	O
or	O
written	O
language	O
to	O
be	O
recognized	O
will	O
determine	O
a	O
number	O
of	O
phonemes	O
and	O
a	O
number	O
of	O
neuron	O
devices	O
required	O
for	O
encoding	O
the	O
number	O
of	O
phonemes	O
.	O
Further	O
,	O
a	O
number	O
of	O
phonemes	O
in	O
Japanese	O
,	O
the	O
language	O
of	O
the	O
present	O
example	O
,	O
is	O
not	O
necessarily	O
restricted	O
to	O
80	O
,	O
and	O
any	O
number	O
of	O
phonemes	O
and	O
neuron	O
devices	O
may	O
be	O
used	O
.	O

In	O
addition	O
,	O
the	O
hypothesis	O
layer	O
38	O
may	O
be	O
provided	O
with	O
the	O
same	O
number	O
of	O
the	O
neuron	O
devices	O
as	O
the	O
number	O
of	O
phonemes	O
.	O
In	O
other	O
words	O
,	O
if	O
a	O
number	O
of	O
phonemes	O
is	O
80	O
,	O
the	O
hypothesis	O
layer	O
38	O
may	O
be	O
provided	O
with	O
80	O
neuron	O
devices	O
Hy1	O
to	O
Hy80	O
in	O
accordance	O
with	O
the	O
respective	O
phonemes	O
.	O
In	O
regard	O
to	O
second	O
instructor	O
signals	O
provided	O
to	O
the	O
hypothesis	O
layer	O
38	O
by	O
the	O
input	O
layer	O
32	O
,	O
only	O
one	O
bit	O
(	O
neuron	O
device	O
)	O
corresponding	O
with	O
each	O
phoneme	O
is	O
"	O
1	O
"	O
and	O
each	O
of	O
other	O
bits	O
is	O
"	O
0	O
"	O
.	O
For	O
example	O
,	O
as	O
illustrated	O
in	O
FIG	O
.	O
3	O
,	O
the	O
signal	O
"	O
100000	O
.	O
.	O
.	O
0	O
"	O
is	O
obtained	O
for	O
the	O
phoneme	O
"	O
a	O
"	O
and	O
the	O
signal	O
"	O
010000	O
.	O
.	O
.	O
0	O
"	O
is	O
obtained	O
for	O
the	O
phoneme	O
"	O
i	O
"	O
.	O
By	O
doing	O
so	O
,	O
CPU	O
processing	O
burden	O
associated	O
with	O
the	O
learning	O
process	O
is	O
increased	O
,	O
but	O
a	O
given	O
phoneme	O
can	O
be	O
more	O
easily	O
distinguished	O
from	O
other	O
phonemes	O
in	O
speech	O
recognition	O
carried	O
out	O
after	O
learning	O
.	O

The	O
input	O
layer	O
31	O
,	O
the	O
hidden	O
layer	O
34	O
and	O
the	O
output	O
layer	O
36	O
of	O
the	O
neuron	O
device	O
network	O
22	O
are	O
capable	O
of	O
forward-propagation	O
activation	O
and	O
back-propagation	O
learning	O
.	O
A	O
complete	O
connection	O
is	O
made	O
between	O
the	O
input	O
layer	O
31	O
and	O
the	O
hidden	O
layer	O
34	O
,	O
and	O
between	O
the	O
hidden	O
layer	O
34	O
and	O
the	O
output	O
layer	O
36	O
.	O
That	O
is	O
,	O
all	O
the	O
neuron	O
devices	O
of	O
the	O
input	O
layer	O
31	O
are	O
connected	O
with	O
all	O
the	O
neuron	O
devices	O
of	O
the	O
hidden	O
layer	O
34	O
,	O
and	O
all	O
the	O
neuron	O
devices	O
of	O
the	O
hidden	O
layer	O
34	O
are	O
connected	O
with	O
all	O
the	O
neuron	O
devices	O
of	O
the	O
output	O
layer	O
56	O
.	O

Further	O
,	O
during	O
the	O
process	O
of	O
learning	O
in	O
the	O
neuron	O
device	O
network	O
,	O
vector	O
rows	O
of	O
speech	O
at	O
a	O
time	O
t	O
which	O
have	O
been	O
subjected	O
to	O
spectral	O
analysis	O
by	O
the	O
FFT	O
unit	O
21	O
are	O
sequentially	O
input	O
to	O
the	O
speech	O
input	O
layer	O
32	O
.	O
At	O
time	O
t	O
,	O
the	O
context	O
layer	O
33	O
receives	O
vector	O
states	O
of	O
the	O
neuron	O
devices	O
Hi1	O
to	O
Hi200	O
of	O
the	O
hidden	O
layer	O
34	O
obtained	O
upon	O
completion	O
of	O
learning	O
at	O
time	O
t-1	O
prior	O
to	O
time	O
t	O
.	O
The	O
vector	O
rows	O
obtained	O
at	O
time	O
t	O
+	O
1	O
,	O
which	O
are	O
to	O
be	O
supplied	O
to	O
the	O
speech	O
input	O
layer	O
32	O
subsequently	O
,	O
are	O
input	O
to	O
the	O
speech	O
output	O
layer	O
37	O
as	O
first	O
instructor	O
signals	O
.	O

The	O
second	O
instructor	O
signals	O
are	O
input	O
to	O
the	O
hypothesis	O
layer	O
38	O
as	O
code	O
rows	O
for	O
hypothesizing	O
a	O
definite	O
meaning	O
A	O
(	O
for	O
example	O
,	O
a	O
phoneme	O
which	O
should	O
be	O
recognized	O
)	O
represented	O
by	O
the	O
vector	O
rows	O
input	O
to	O
the	O
voice	O
input	O
layer	O
32	O
at	O
times	O
before	O
and	O
after	O
time	O
t	O
.	O

In	O
this	O
manner	O
,	O
according	O
to	O
the	O
present	O
embodiment	O
,	O
the	O
vector	O
row	O
at	O
the	O
current	O
time	O
(	O
time	O
t	O
)	O
is	O
input	O
to	O
the	O
speech	O
input	O
layer	O
32	O
;	O
the	O
vector	O
value	O
at	O
the	O
past	O
time	O
(	O
time	O
t-1	O
)	O
in	O
the	O
hidden	O
layer	O
34	O
is	O
input	O
to	O
the	O
context	O
layer	O
33	O
;	O
and	O
the	O
vector	O
row	O
at	O
the	O
future	O
time	O
(	O
time	O
t	O
+	O
1	O
)	O
is	O
input	O
to	O
the	O
speech	O
output	O
layer	O
37	O
.	O
Therefore	O
,	O
the	O
time	O
series	O
relation	O
of	O
the	O
vector	O
rows	O
is	O
learned	O
in	O
accordance	O
with	O
each	O
power	O
P	O
(	O
at	O
time	O
tn	O
)	O
which	O
has	O
been	O
subjected	O
to	O
spectral	O
analysis	O
for	O
each	O
phoneme	O
.	O
That	O
is	O
,	O
each	O
connection	O
weight	O
of	O
the	O
speech	O
input	O
layer	O
32	O
,	O
the	O
hidden	O
layer	O
34	O
and	O
the	O
speech	O
output	O
layer	O
37	O
is	O
learned	O
as	O
a	O
value	O
which	O
includes	O
the	O
time	O
series	O
relation	O
from	O
the	O
past	O
(	O
t-1	O
)	O
,	O
the	O
present	O
(	O
t	O
)	O
and	O
the	O
future	O
(	O
t	O
+	O
1	O
)	O
.	O

Further	O
,	O
each	O
power	O
P	O
(	O
at	O
time	O
tn	O
)	O
concerning	O
the	O
same	O
phoneme	O
is	O
input	O
to	O
the	O
speech	O
input	O
layer	O
32	O
for	O
learning	O
,	O
as	O
the	O
same	O
second	O
instructor	O
signal	O
is	O
input	O
to	O
the	O
hypothesis	O
layer	O
38	O
.	O
Consequently	O
,	O
the	O
time	O
series	O
relation	O
of	O
the	O
input	O
vector	O
rows	O
and	O
the	O
phonemes	O
(	O
code	O
rows	O
)	O
having	O
this	O
relationship	O
are	O
also	O
hypothetically	O
learned	O
.	O

Therefore	O
,	O
in	O
case	O
of	O
speech	O
recognition	O
,	O
when	O
the	O
vector	O
row	O
related	O
to	O
speech	O
which	O
has	O
been	O
subjected	O
to	O
spectral	O
analysis	O
is	O
input	O
to	O
the	O
speech	O
input	O
layer	O
32	O
,	O
the	O
vector	O
rows	O
are	O
output	O
from	O
the	O
hypothesis	O
layer	O
38	O
taking	O
into	O
account	O
the	O
time	O
series	O
relation	O
of	O
the	O
vector	O
row	O
.	O

FIG	O
.	O
3	O
shows	O
the	O
contents	O
of	O
the	O
second	O
instructor	O
signal	O
table	O
.	O

As	O
shown	O
in	O
FIG	O
.	O
3	O
,	O
the	O
second	O
instructor	O
signal	O
is	O
designated	O
by	O
a	O
code	O
row	O
consisting	O
of	O
8	O
bits	O
in	O
such	O
a	O
manner	O
that	O
a	O
phoneme	O
"	O
a	O
"	O
corresponds	O
to	O
"	O
10000000	O
"	O
;	O
a	O
phoneme	O
"	O
i	O
"	O
,	O
"	O
01000000	O
"	O
;	O
and	O
a	O
phoneme	O
"	O
u	O
"	O
,	O
"	O
00100000	O
"	O
.	O
Each	O
bit	O
of	O
the	O
code	O
represented	O
by	O
the	O
second	O
instructor	O
signal	O
is	O
supplied	O
to	O
the	O
respective	O
neuron	O
devices	O
Hy1	O
to	O
Hy8	O
of	O
the	O
hypothesis	O
layer	O
38	O
.	O
The	O
second	O
instructor	O
signal	O
for	O
each	O
phoneme	O
is	O
stored	O
in	O
the	O
ROM	O
13	O
.	O

Note	O
that	O
each	O
code	O
row	O
of	O
the	O
second	O
instructor	O
signal	O
illustrated	O
in	O
FIG	O
.	O
3	O
is	O
shown	O
as	O
an	O
example	O
in	O
the	O
present	O
embodiment	O
,	O
and	O
any	O
other	O
code	O
row	O
may	O
be	O
similarly	O
used	O
.	O
Further	O
,	O
although	O
a	O
number	O
of	O
neuron	O
devices	O
of	O
the	O
hypothesis	O
layer	O
38	O
is	O
determined	O
in	O
accordance	O
with	O
a	O
number	O
of	O
phonemes	O
,	O
the	O
code	O
row	O
may	O
be	O
represented	O
by	O
a	O
number	O
of	O
bits	O
corresponding	O
with	O
a	O
number	O
of	O
neuron	O
devices	O
.	O

FIG	O
.	O
4	O
illustrates	O
a	O
connection	O
weight	O
table	O
for	O
storing	O
the	O
connection	O
weights	O
between	O
the	O
respective	O
neuron	O
devices	O
in	O
such	O
a	O
neuron	O
device	O
network	O
22	O
.	O
As	O
illustrated	O
in	O
the	O
figure	O
,	O
the	O
connection	O
weight	O
between	O
each	O
of	O
the	O
neuron	O
devices	O
of	O
the	O
hidden	O
layer	O
34	O
and	O
neuron	O
devices	O
of	O
the	O
speech	O
input	O
layer	O
32	O
,	O
the	O
context	O
layer	O
33	O
,	O
the	O
speech	O
output	O
layer	O
37	O
,	O
and	O
the	O
hypothesis	O
layer	O
38	O
,	O
respectively	O
,	O
is	O
specified	O
in	O
the	O
table	O
.	O
For	O
example	O
,	O
the	O
connection	O
weight	O
between	O
neuron	O
device	O
Hi1	O
of	O
the	O
hidden	O
layer	O
34	O
and	O
neuron	O
device	O
Ou2	O
of	O
the	O
speech	O
output	O
layer	O
37	O
is	O
WO12	O
.	O

The	O
neuron	O
device	O
network	O
22	O
is	O
provided	O
with	O
a	O
memory	O
(	O
not	O
shown	O
)	O
for	O
storing	O
the	O
connection	O
weights	O
.	O
Further	O
,	O
a	O
learning	O
function	O
of	O
the	O
neuron	O
device	O
network	O
shown	O
in	O
FIG	O
.	O
2	O
is	O
carried	O
out	O
by	O
the	O
CPU	O
11	O
by	O
varying	O
the	O
connection	O
weights	O
in	O
this	O
table	O
in	O
accordance	O
with	O
a	O
predetermined	O
error	O
back-propagation	O
method	O
.	O

The	O
operation	O
of	O
the	O
first	O
embodiment	O
of	O
the	O
present	O
invention	O
will	O
now	O
be	O
described	O
.	O

A	O
Learning	O
Function	O
of	O
the	O
Neural	O
Network	O
When	O
the	O
learning	O
process	O
of	O
the	O
neural	O
network	O
is	O
carried	O
out	O
,	O
a	O
user	O
first	O
specifies	O
a	O
learning	O
mode	O
by	O
an	O
operation	O
of	O
the	O
keyboard	O
18	O
or	O
checking	O
check	O
boxes	O
or	O
icons	O
displayed	O
on	O
the	O
display	O
unit	O
17	O
using	O
the	O
mouse	O
19	O
.	O

After	O
specifying	O
the	O
learning	O
mode	O
,	O
the	O
user	O
sequentially	O
inputs	O
characters	O
corresponding	O
to	O
the	O
predetermined	O
80	O
phonemes	O
from	O
the	O
keyboard	O
18	O
,	O
and	O
then	O
inputs	O
the	O
sounds	O
associated	O
with	O
each	O
of	O
the	O
phonemes	O
to	O
the	O
speech	O
input	O
apparatus	O
23	O
.	O
Note	O
that	O
the	O
individual	O
phonemes	O
to	O
be	O
input	O
and	O
uttered	O
by	O
the	O
user	O
may	O
be	O
sequentially	O
displayed	O
on	O
the	O
display	O
unit	O
17	O
.	O

Referring	O
to	O
FIG	O
.	O
5A	O
,	O
upon	O
inputting	O
an	O
analog	O
signal	O
pattern	O
for	O
,	O
e.g.	O
,	O
a	O
phoneme	O
"	O
a	O
"	O
,	O
the	O
speech	O
input	O
unit	O
23	O
supplies	O
the	O
analog	O
signal	O
to	O
the	O
FFT	O
unit	O
21	O
.	O
The	O
FFT	O
unit	O
21	O
samples	O
the	O
supplied	O
analog	O
speech	O
data	O
at	O
22	O
kHz	O
and	O
A/D-converts	O
the	O
data	O
into	O
Pulse	O
Code	O
Modulation	O
(	O
PCM	O
)	O
data	O
consisting	O
of	O
16	O
bits	O
.	O
The	O
obtained	O
PCM	O
data	O
are	O
then	O
stored	O
in	O
a	O
memory	O
(	O
not	O
shown	O
)	O
in	O
the	O
FFT	O
unit	O
21	O
.	O

Subsequently	O
,	O
in	O
the	O
FFT	O
unit	O
21	O
,	O
the	O
digital	O
speech	O
data	O
"	O
a	O
"	O
are	O
subjected	O
to	O
spectral	O
analysis	O
by	O
the	O
fast	O
Fourier	O
transform	O
(	O
FFT	O
)	O
processing	O
at	O
each	O
time	O
tn	O
(	O
n	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
n	O
)	O
in	O
accordance	O
with	O
time	O
windows	O
such	O
as	O
a	O
square	O
window	O
,	O
a	O
Hamming	O
window	O
and	O
a	O
Hanning	O
window	O
and	O
parameters	O
such	O
as	O
a	O
number	O
of	O
points	O
.	O
As	O
shown	O
in	O
FIG	O
.	O
5B	O
,	O
the	O
FFT	O
unit	O
21	O
calculates	O
the	O
power	O
P	O
(	O
at	O
time	O
tn	O
)	O
with	O
respect	O
to	O
each	O
frequency	O
(	O
F1	O
to	O
F30	O
)	O
of	O
the	O
speech	O
data	O
at	O
time	O
tn	O
.	O
As	O
shown	O
in	O
FIG	O
.	O
6	O
,	O
the	O
vector	O
row	O
constituted	O
by	O
the	O
power	O
P	O
(	O
at	O
time	O
tn	O
)	O
with	O
respect	O
to	O
each	O
frequency	O
is	O
stored	O
in	O
the	O
vector	O
row	O
storing	O
area	O
in	O
the	O
RAM	O
14	O
for	O
each	O
time	O
tn	O
.	O

When	O
spectral	O
analysis	O
of	O
the	O
input	O
phoneme	O
by	O
the	O
FFT	O
unit	O
21	O
is	O
completed	O
,	O
the	O
CPU	O
11	O
executes	O
the	O
learning	O
process	O
of	O
the	O
neuron	O
device	O
network	O
22	O
in	O
accordance	O
with	O
the	O
vector	O
rows	O
stored	O
in	O
the	O
RAM	O
14	O
.	O

A	O
description	O
will	O
now	O
be	O
given	O
of	O
the	O
learning	O
process	O
referring	O
to	O
the	O
example	O
phoneme	O
"	O
a	O
"	O
at	O
time	O
tn	O
.	O
In	O
this	O
example	O
,	O
the	O
CPU	O
11	O
first	O
inputs	O
to	O
the	O
neuron	O
devices	O
Co1	O
to	O
Co200	O
of	O
the	O
context	O
layer	O
33	O
and	O
the	O
states	O
of	O
the	O
neuron	O
devices	O
Hi1	O
to	O
Hi2000	O
of	O
the	O
hidden	O
layer	O
34	O
at	O
time	O
tn	O
before	O
starting	O
the	O
learning	O
process	O
,	O
i	O
.	O
e	O
.	O
,	O
the	O
vector	O
row	O
in	O
the	O
hidden	O
layer	O
at	O
the	O
time	O
point	O
when	O
the	O
learning	O
at	O
time	O
tn-1	O
is	O
completed	O
.	O

The	O
CPU	O
11	O
then	O
reads	O
from	O
the	O
RAM	O
14	O
the	O
vector	O
row	O
P	O
(	O
tn	O
)	O
associated	O
with	O
the	O
phoneme	O
"	O
a	O
"	O
at	O
time	O
tn	O
and	O
inputs	O
the	O
vector	O
row	O
P	O
(	O
tn	O
)	O
to	O
each	O
neuron	O
devices	O
In1	O
to	O
In30	O
of	O
the	O
speech	O
input	O
layer	O
32	O
.	O
Each	O
of	O
the	O
neuron	O
devices	O
In1	O
to	O
In30	O
is	O
provided	O
for	O
each	O
of	O
the	O
frequencies	O
F1	O
to	O
F30	O
outputted	O
by	O
the	O
FFT	O
unit	O
21	O
.	O

Further	O
,	O
the	O
vector	O
row	O
P	O
(	O
tn	O
+	O
1	O
)	O
at	O
time	O
tn	O
+	O
1	O
following	O
the	O
time	O
tn	O
is	O
input	O
as	O
the	O
first	O
instructor	O
signal	O
to	O
the	O
neuron	O
devices	O
Ou1	O
to	O
Ou30	O
of	O
the	O
speech	O
output	O
layer	O
37	O
.	O
The	O
code	O
row	O
"	O
10000000	O
"	O
illustrated	O
in	O
FIG	O
.	O
3	O
of	O
the	O
input	O
phoneme	O
"	O
a	O
"	O
is	O
also	O
input	O
as	O
the	O
second	O
instructor	O
signal	O
to	O
the	O
respective	O
neuron	O
devices	O
Hy1	O
to	O
Hy8	O
of	O
the	O
hypothesis	O
layer	O
38	O
.	O

Upon	O
completing	O
the	O
input	O
of	O
the	O
vector	O
row	O
to	O
the	O
input	O
layer	O
31	O
and	O
input	O
of	O
the	O
first	O
instructor	O
signal	O
to	O
the	O
output	O
layer	O
36	O
,	O
the	O
CPU	O
11	O
continues	O
the	O
learning	O
process	O
by	O
using	O
the	O
current	O
connection	O
weights	O
between	O
the	O
respective	O
neuron	O
devices	O
of	O
the	O
input	O
layer	O
31	O
,	O
the	O
hidden	O
layer	O
34	O
and	O
the	O
output	O
layer	O
36	O
,	O
and	O
then	O
updates	O
each	O
connection	O
weight	O
after	O
learning	O
.	O

Note	O
that	O
learning	O
is	O
carried	O
out	O
in	O
accordance	O
with	O
the	O
error	O
back-propagation	O
of	O
the	O
present	O
invention	O
.	O
An	O
example	O
of	O
a	O
learning	O
expression	O
is	O
.	O
DELTA	O
.	O
w	O
(	O
t	O
)	O
=	O
[	O
S	O
(	O
t	O
)	O
/	O
[	O
S	O
(	O
t-1	O
)	O
-	O
S	O
(	O
t	O
)	O
]	O
]	O
.times..DELTA.w	O
(	O
t-1	O
)	O
,	O
and	O
details	O
of	O
the	O
learning	O
expression	O
and	O
the	O
learning	O
algorithm	O
are	O
described	O
in	O
Technical	B-Citation
Report	I-Citation
#CMU-CS-88-162	I-Citation
"	I-Citation
An	I-Citation
Empirical	I-Citation
Study	I-Citation
of	I-Citation
Learning	I-Citation
Speed	I-Citation
in	I-Citation
Back-propagation	I-Citation
Networks	I-Citation
"	I-Citation
by	I-Citation
S.	I-Citation
Fahlman	I-Citation
,	I-Citation
issued	I-Citation
in	I-Citation
September	I-Citation
1988	I-Citation
,	I-Citation
Carnegie	I-Citation
Mellon	I-Citation
University	I-Citation
,	O
which	O
is	O
expressly	O
incorporated	O
herein	O
by	O
reference	O
in	O
its	O
entirety	O
.	O

Further	O
,	O
learning	O
may	O
be	O
carried	O
out	O
by	O
applying	O
back-propagation	O
of	O
a	O
feed	O
forward	O
network	O
to	O
a	O
discrete-time	O
recurrent	O
network	O
as	O
described	O
in	O
"	B-Citation
Finding	I-Citation
Structure	I-Citation
in	I-Citation
Time	I-Citation
"	I-Citation
by	I-Citation
J.	I-Citation
L.	I-Citation
Elman	I-Citation
,	I-Citation
Cognitive	I-Citation
Science	I-Citation
,	I-Citation
14	I-Citation
,	I-Citation
pp.	I-Citation
179	I-Citation
-	I-Citation
211	I-Citation
(	I-Citation
1990	I-Citation
)	I-Citation
,	O
which	O
is	O
expressly	O
incorporated	O
herein	O
by	O
reference	O
in	O
its	O
entirety	O
.	O

Furthermore	O
,	O
learning	O
is	O
not	O
restricted	O
to	O
the	O
above-described	O
methods	O
,	O
and	O
it	O
may	O
be	O
performed	O
in	O
accordance	O
with	O
other	O
methods	O
similar	O
to	O
those	O
noted	O
above	O
.	O

When	O
learning	O
about	O
the	O
phoneme	O
"	O
a	O
"	O
at	O
time	O
t	O
is	O
completed	O
,	O
learning	O
at	O
the	O
time	O
t	O
+	O
1	O
is	O
carried	O
out	O
.	O
In	O
this	O
case	O
,	O
the	O
vector	O
row	O
of	O
the	O
hidden	O
layer	O
34	O
for	O
the	O
time	O
when	O
the	O
learning	O
at	O
time	O
tn	O
is	O
completed	O
is	O
input	O
to	O
the	O
context	O
layer	O
33	O
.	O
Similar	O
to	O
learning	O
at	O
the	O
time	O
tn	O
,	O
the	O
vector	O
row	O
P	O
(	O
tn	O
+	O
1	O
)	O
at	O
the	O
time	O
tn	O
+	O
1	O
is	O
read	O
from	O
the	O
RAM	O
14	O
to	O
be	O
input	O
to	O
the	O
speech	O
input	O
layer	O
32	O
.	O
In	O
addition	O
,	O
the	O
vector	O
row	O
P	O
(	O
tn	O
+	O
2	O
)	O
at	O
time	O
tn	O
+	O
2	O
is	O
input	O
as	O
the	O
first	O
instructor	O
signal	O
to	O
the	O
speech	O
output	O
layer	O
37	O
.	O

The	O
hypothesis	O
layer	O
38	O
continues	O
to	O
receive	O
the	O
same	O
code	O
"	O
10000000	O
"	O
associated	O
with	O
"	O
a	O
"	O
as	O
the	O
second	O
instructor	O
signal	O
while	O
the	O
learning	O
process	O
about	O
the	O
input	O
phoneme	O
"	O
a	O
"	O
is	O
carried	O
out	O
.	O

When	O
the	O
learning	O
process	O
at	O
time	O
t	O
+	O
1	O
is	O
completed	O
and	O
the	O
connection	O
weight	O
values	O
shown	O
in	O
FIG	O
.	O
4	O
are	O
updated	O
,	O
learning	O
about	O
the	O
phoneme	O
"	O
a	O
"	O
is	O
completed	O
by	O
performing	O
the	O
learning	O
process	O
with	O
regard	O
to	O
all	O
of	O
the	O
vector	O
rows	O
which	O
have	O
been	O
subjected	O
to	O
spectral	O
analysis	O
.	O

The	O
learning	O
process	O
with	O
regard	O
to	O
all	O
the	O
phonemes	O
such	O
as	O
"	O
i	O
"	O
,	O
"	O
u	O
"	O
,	O
"	O
e	O
"	O
,	O
"	O
o	O
"	O
and	O
others	O
is	O
similarly	O
carried	O
out	O
as	O
described	O
above	O
.	O

Recognition	O
of	O
an	O
Input	O
Speech	O
With	O
regard	O
to	O
this	O
example	O
,	O
it	O
is	O
assumed	O
that	O
a	O
sound	O
,	O
e.g.	O
,	O
"	O
mae	O
,	O
"	O
is	O
input	O
from	O
the	O
speech	O
input	O
unit	O
23	O
after	O
the	O
above-mentioned	O
learning	O
process	O
is	O
completed	O
.	O
Spectral	O
analysis	O
of	O
the	O
input	O
sound	O
is	O
then	O
performed	O
in	O
the	O
FFT	O
unit	O
21	O
.	O

The	O
CPU	O
11	O
subsequently	O
inputs	O
the	O
vector	O
row	O
of	O
the	O
hidden	O
layer	O
34	O
at	O
time	O
tn-1	O
to	O
the	O
context	O
layer	O
33	O
and	O
thereafter	O
inputs	O
to	O
the	O
speech	O
input	O
layer	O
32	O
a	O
vector	O
P	O
(	O
tn	O
)	O
which	O
consists	O
of	O
the	O
power	O
with	O
respect	O
to	O
each	O
frequency	O
at	O
current	O
time	O
tn	O
.	O
The	O
CPU	O
11	O
reads	O
the	O
respective	O
connection	O
weights	O
(	O
FIG	O
.	O
4	O
)	O
between	O
the	O
input	O
layer	O
31	O
and	O
the	O
hidden	O
layer	O
34	O
from	O
the	O
memory	O
of	O
the	O
neuron	O
device	O
network	O
22	O
and	O
calculates	O
output	O
values	O
of	O
the	O
respective	O
neuron	O
devices	O
Hi1	O
to	O
Hi200	O
of	O
the	O
hidden	O
layer	O
34	O
based	O
on	O
the	O
respective	O
connection	O
weights	O
and	O
input	O
values	O
of	O
the	O
input	O
layer	O
31	O
.	O
The	O
output	O
values	O
are	O
stored	O
in	O
a	O
memory	O
(	O
not	O
shown	O
)	O
of	O
the	O
neuron	O
device	O
network	O
22	O
.	O
The	O
vector	O
values	O
of	O
the	O
hidden	O
layer	O
34	O
are	O
input	O
to	O
the	O
context	O
layer	O
33	O
which	O
relate	O
to	O
the	O
vector	O
row	O
P	O
(	O
tn	O
+	O
1	O
)	O
at	O
a	O
time	O
following	O
time	O
tn	O
.	O

The	O
CPU	O
11	O
then	O
reads	O
the	O
stored	O
output	O
values	O
of	O
the	O
hidden	O
layer	O
34	O
and	O
the	O
connection	O
weights	O
of	O
the	O
hidden	O
layer	O
34	O
and	O
the	O
hypothesis	O
layer	O
38	O
from	O
the	O
memory	O
of	O
the	O
neuron	O
device	O
network	O
22	O
and	O
calculates	O
output	O
values	O
of	O
the	O
respective	O
neuron	O
devices	O
Hy1	O
to	O
Hy8	O
of	O
the	O
hypothesis	O
layer	O
38	O
based	O
on	O
the	O
output	O
values	O
and	O
the	O
connection	O
weights	O
.	O
The	O
corresponding	O
phoneme	O
is	O
determined	O
by	O
collating	O
the	O
output	O
values	O
of	O
the	O
respective	O
neuron	O
devices	O
Hy1	O
to	O
Hy8	O
with	O
the	O
code	O
rows	O
in	O
the	O
second	O
instructor	O
signal	O
table	O
stored	O
in	O
the	O
ROM	O
13	O
.	O
The	O
determined	O
phoneme	O
is	O
stored	O
in	O
the	O
RAM	O
14	O
.	O

Since	O
the	O
phoneme	O
is	O
specified	O
each	O
time	O
the	O
vector	O
row	O
P	O
(	O
tn	O
)	O
is	O
input	O
to	O
the	O
speech	O
input	O
unit	O
32	O
in	O
a	O
time	O
series	O
,	O
a	O
plurality	O
of	O
phoneme	O
rows	O
are	O
generated	O
.	O
For	O
example	O
,	O
if	O
a	O
sound	O
"	O
iro	O
"	O
is	O
input	O
,	O
"	O
iiiiirrrooooo	O
"	O
is	O
obtained	O
.	O
Therefore	O
,	O
the	O
CPU	O
11	O
recognizes	O
the	O
input	O
sound	O
as	O
"	O
iro	O
"	O
based	O
on	O
the	O
phoneme	O
rows	O
stored	O
in	O
the	O
RAM	O
14	O
.	O

When	O
an	O
input	O
command	O
is	O
issued	O
from	O
the	O
keyboard	O
18	O
,	O
the	O
CPU	O
11	O
then	O
transforms	O
the	O
recognized	O
sound	O
into	O
a	O
writing	O
represented	O
by	O
characters	O
in	O
accordance	O
with	O
the	O
transformation	O
system	O
such	O
as	O
the	O
Japanese	O
transformation	O
system	O
described	O
above	O
.	O
Although	O
the	O
present	O
invention	O
is	O
described	O
as	O
recognizing	O
spoken	O
and	O
written	O
Japanese	O
,	O
the	O
present	O
invention	O
can	O
be	O
adapted	O
to	O
recognize	O
any	O
language	O
.	O
The	O
transformed	O
writing	O
is	O
displayed	O
on	O
the	O
display	O
unit	O
17	O
and	O
simultaneously	O
stored	O
in	O
the	O
RAM	O
14	O
.	O
Further	O
,	O
in	O
response	O
to	O
commands	O
from	O
the	O
keyboard	O
18	O
,	O
the	O
data	O
are	O
transmitted	O
to	O
various	O
communication	O
control	O
units	O
such	O
as	O
a	O
personal	O
computer	O
or	O
a	O
word	O
processor	O
through	O
the	O
communication	O
control	O
unit	O
15	O
and	O
the	O
communication	O
network	O
2	O
.	O

FIG	O
.	O
7	O
illustrates	O
a	O
result	O
of	O
the	O
recognition	O
of	O
each	O
phoneme	O
in	O
the	O
sound	O
"	O
mae	O
"	O
.	O
Additionally	O
,	O
during	O
the	O
learning	O
process	O
,	O
the	O
vector	O
rows	O
shown	O
in	O
FIG	O
.	O
7	O
are	O
adopted	O
as	O
codes	O
for	O
respective	O
phonemes	O
input	O
as	O
the	O
second	O
instructor	O
signal	O
to	O
the	O
hypothesis	O
layer	O
38	O
.	O
Further	O
,	O
the	O
outputs	O
of	O
the	O
respective	O
neuron	O
devices	O
Hy1	O
to	O
Hy8	O
are	O
supplied	O
as	O
second	O
output	O
signals	O
if	O
these	O
outputs	O
exceed	O
a	O
predetermined	O
threshold	O
value	O
.	O
The	O
outputs	O
of	O
the	O
respective	O
neuron	O
devices	O
Hy1	O
to	O
Hy8	O
are	O
not	O
supplied	O
if	O
they	O
are	O
below	O
the	O
threshold	O
value	O
.	O
This	O
condition	O
is	O
represented	O
by	O
a	O
reference	O
character	O
"	O
-	O
"	O
.	O

As	O
illustrated	O
in	O
the	O
right-most	O
column	O
of	O
FIG	O
.	O
7	O
,	O
phonemes	O
"	O
m	O
"	O
,	O
"	O
a	O
"	O
and	O
"	O
e	O
"	O
can	O
be	O
determined	O
as	O
having	O
a	O
correspondence	O
with	O
input	O
of	O
the	O
vector	O
rows	O
at	O
each	O
time	O
tn	O
.	O
The	O
input	O
sound	O
is	O
recognized	O
as	O
"	O
mae	O
"	O
from	O
these	O
phonemes	O
.	O

As	O
shown	O
in	O
FIG	O
.	O
7	O
,	O
the	O
sound	O
is	O
specified	O
from	O
the	O
respective	O
phoneme	O
rows	O
by	O
the	O
outputs	O
from	O
the	O
neuron	O
devices	O
Hy1	O
to	O
Hy8	O
at	O
each	O
time	O
tn	O
.	O
If	O
a	O
plurality	O
of	O
identical	O
phonemes	O
,	O
e.g.	O
,	O
four	O
or	O
more	O
identical	O
phonemes	O
are	O
continuously	O
specified	O
,	O
these	O
phonemes	O
are	O
judged	O
to	O
be	O
effective	O
and	O
speech	O
recognition	O
is	O
carried	O
out	O
.	O
For	O
example	O
,	O
as	O
shown	O
in	O
FIG	O
.	O
7	O
,	O
the	O
phoneme	O
"	O
m	O
"	O
specified	O
at	O
time	O
t1	O
and	O
the	O
phoneme	O
"	O
e	O
"	O
specified	O
at	O
time	O
t35	O
are	O
not	O
obtained	O
for	O
at	O
least	O
three	O
consecutive	O
times	O
thereafter	O
,	O
therefore	O
they	O
are	O
excluded	O
from	O
a	O
target	O
of	O
speech	O
recognition	O
.	O

The	O
phoneme	O
may	O
be	O
judged	O
to	O
be	O
effective	O
when	O
that	O
phoneme	O
is	O
continuously	O
specified	O
not	O
only	O
four	O
times	O
or	O
more	O
,	O
but	O
also	O
two	O
,	O
three	O
,	O
five	O
,	O
ten	O
or	O
any	O
other	O
number	O
of	O
times	O
.	O
Further	O
,	O
a	O
number	O
for	O
judging	O
that	O
phoneme	O
as	O
effective	O
may	O
be	O
specified	O
by	O
a	O
user	O
based	O
on	O
an	O
input	O
from	O
the	O
keyboard	O
.	O

As	O
shown	O
by	O
the	O
character	O
"	O
?	O
"	O
in	O
the	O
right-most	O
column	O
in	O
FIG	O
.	O
7	O
,	O
when	O
performing	O
speech	O
recognition	O
,	O
the	O
phoneme	O
sometimes	O
cannot	O
be	O
determined	O
during	O
an	O
initial	O
stage	O
when	O
the	O
vector	O
rows	O
which	O
have	O
been	O
subjected	O
to	O
spectral	O
analysis	O
are	O
input	O
,	O
or	O
when	O
shifting	O
from	O
one	O
phoneme	O
to	O
another	O
phoneme	O
.	O
However	O
,	O
the	O
speech	O
can	O
be	O
easily	O
recognized	O
by	O
the	O
phonemes	O
which	O
are	O
thereafter	O
continuously	O
specified	O
as	O
shown	O
in	O
the	O
figure	O
.	O

Also	O
,	O
the	O
phonemes	O
sometimes	O
cannot	O
be	O
determined	O
at	O
the	O
first	O
stage	O
where	O
the	O
vector	O
rows	O
which	O
have	O
been	O
subjected	O
to	O
spectral	O
analysis	O
are	O
input	O
because	O
it	O
may	O
be	O
considered	O
that	O
the	O
learning	O
process	O
is	O
insufficient	O
or	O
incomplete	O
.	O
When	O
performing	O
the	O
learning	O
process	O
,	O
the	O
relation	O
of	O
time	O
series	O
of	O
the	O
past	O
,	O
the	O
present	O
and	O
the	O
future	O
are	O
factored	O
into	O
the	O
learning	O
.	O
However	O
,	O
during	O
the	O
first	O
state	O
,	O
the	O
information	O
related	O
to	O
the	O
time	O
series	O
of	O
the	O
past	O
is	O
insufficient	O
or	O
does	O
not	O
exist	O
.	O
Therefore	O
the	O
learning	O
process	O
for	O
determining	O
phonemes	O
cannot	O
be	O
performed	O
due	O
to	O
the	O
missing	O
information	O
.	O

Further	O
,	O
the	O
phonemes	O
cannot	O
be	O
specified	O
when	O
shifting	O
from	O
one	O
phoneme	O
to	O
another	O
phoneme	O
,	O
because	O
it	O
may	O
be	O
determined	O
by	O
the	O
CPU	O
11	O
that	O
the	O
learning	O
process	O
takes	O
place	O
with	O
respect	O
to	O
the	O
individual	O
phonemes	O
,	O
and	O
the	O
time	O
series	O
relationship	O
of	O
the	O
respective	O
phonemes	O
are	O
not	O
the	O
present	O
target	O
of	O
the	O
learning	O
process	O
.	O

According	O
to	O
this	O
embodiment	O
,	O
since	O
learning	O
is	O
carried	O
out	O
by	O
taking	O
the	O
time	O
series	O
relation	O
of	O
the	O
spectra	O
of	O
the	O
respective	O
phonemes	O
into	O
consideration	O
,	O
a	O
voice	O
of	O
one	O
speaker	O
uttering	O
phonemes	O
for	O
learning	O
,	O
and	O
that	O
of	O
another	O
speaker	O
can	O
be	O
correctly	O
recognized	O
.	O
Therefore	O
,	O
speaker-independent	O
recognition	O
is	O
possible	O
.	O

Further	O
,	O
determining	O
a	O
starting	O
point	O
of	O
a	O
particular	O
phoneme	O
to	O
be	O
recognized	O
has	O
been	O
a	O
problem	O
when	O
performing	O
speech	O
recognition	O
in	O
prior	O
systems	O
,	O
but	O
according	O
to	O
the	O
embodiment	O
of	O
the	O
present	O
invention	O
,	O
a	O
starting	O
point	O
of	O
the	O
phoneme	O
does	O
not	O
have	O
to	O
be	O
specified	O
.	O

Furthermore	O
,	O
when	O
executing	O
continuous	O
speech	O
recognition	O
in	O
accordance	O
with	O
each	O
phoneme	O
,	O
speech	O
can	O
be	O
recognized	O
irrespective	O
of	O
the	O
uttering	O
time	O
of	O
each	O
phoneme	O
,	O
which	O
may	O
vary	O
greatly	O
from	O
speaker	O
to	O
speaker	O
.	O
For	O
example	O
,	O
when	O
pronouncing	O
a	O
word	O
"	O
haru	O
"	O
with	O
a	O
sound	O
"	O
ha	O
"	O
being	O
prolonged	O
,	O
only	O
a	O
plurality	O
of	O
phonemes	O
"	O
a	O
"	O
are	O
specified	O
like	O
"	O
hhhh	O
.	O
.	O
.	O
aaaaaaaaaa	O
.	O
.	O
.	O
rrrr	O
.	O
.	O
.	O
uuuuu	O
.	O
.	O
.	O
"	O
,	O
and	O
the	O
word	O
"	O
haru	O
"	O
can	O
be	O
easily	O
recognized	O
.	O

Moreover	O
,	O
according	O
to	O
the	O
first	O
embodiment	O
,	O
a	O
plurality	O
of	O
vector	O
rows	O
P	O
(	O
tn	O
)	O
at	O
multiple	O
time	O
points	O
tn	O
are	O
input	O
for	O
each	O
phoneme	O
,	O
and	O
the	O
phoneme	O
is	O
specified	O
at	O
each	O
time	O
point	O
.	O
Because	O
the	O
state	O
of	O
each	O
phoneme	O
is	O
affected	O
by	O
the	O
state	O
of	O
the	O
previous	O
phoneme	O
in	O
continuous	O
speech	O
recognition	O
,	O
the	O
phonemes	O
can	O
be	O
distinguished	O
when	O
shifting	O
from	O
one	O
phoneme	O
to	O
another	O
phoneme	O
.	O
In	O
other	O
words	O
,	O
by	O
the	O
output	O
of	O
the	O
symbol	O
"	O
?	O
"	O
(	O
in	O
the	O
right-most	O
column	O
in	O
FIG	O
.	O
7	O
)	O
successive	O
phonemes	O
can	O
be	O
distinguished	O
.	O
Thereafter	O
,	O
the	O
same	O
phonemes	O
are	O
continuously	O
specified	O
,	O
and	O
the	O
speech	O
can	O
be	O
easily	O
recognized	O
even	O
during	O
continuous	O
speech	O
recognition	O
.	O

In	O
the	O
embodiment	O
described	O
above	O
,	O
since	O
a	O
recurrent	O
type	O
neutral	O
network	O
is	O
adopted	O
,	O
the	O
vector	O
values	O
in	O
the	O
hidden	O
layer	O
34	O
are	O
fed	O
back	O
as	O
inputs	O
to	O
the	O
context	O
layer	O
33	O
.	O
However	O
,	O
the	O
present	O
invention	O
,	O
is	O
not	O
restricted	O
to	O
this	O
configuration	O
and	O
,	O
for	O
example	O
,	O
the	O
vector	O
values	O
in	O
the	O
speech	O
output	O
layer	O
37	O
may	O
be	O
fed	O
back	O
to	O
the	O
context	O
layer	O
33	O
.	O
In	O
this	O
case	O
,	O
a	O
number	O
of	O
neuron	O
devices	O
Co	O
in	O
the	O
context	O
layer	O
33	O
must	O
be	O
equal	O
to	O
a	O
number	O
of	O
neuron	O
devices	O
Ou	O
in	O
the	O
speech	O
output	O
layer	O
37	O
.	O
If	O
the	O
vector	O
values	O
fed	O
back	O
as	O
inputs	O
to	O
the	O
context	O
layer	O
34	O
are	O
not	O
above	O
the	O
threshold	O
values	O
shown	O
in	O
FIG	O
.	O
7	O
,	O
the	O
output	O
values	O
of	O
the	O
respective	O
neuron	O
devices	O
in	O
the	O
speech	O
output	O
layer	O
37	O
are	O
used	O
as	O
the	O
vector	O
values	O
.	O

In	O
addition	O
,	O
in	O
the	O
embodiment	O
mentioned	O
above	O
,	O
although	O
a	O
recurrent	O
type	O
neutral	O
network	O
is	O
employed	O
,	O
a	O
neuron	O
device	O
network	O
without	O
a	O
context	O
layer	O
may	O
be	O
used	O
in	O
the	O
present	O
invention	O
.	O
In	O
this	O
case	O
,	O
the	O
vector	O
row	O
at	O
time	O
t	O
is	O
input	O
to	O
the	O
speech	O
input	O
layer	O
32	O
;	O
the	O
vector	O
row	O
at	O
next	O
time	O
t	O
+	O
1	O
input	O
as	O
the	O
first	O
instructor	O
signal	O
to	O
the	O
speech	O
output	O
layer	O
37	O
;	O
and	O
a	O
definite	O
meaning	O
represented	O
by	O
a	O
set	O
of	O
time	O
points	O
tn	O
is	O
input	O
as	O
the	O
second	O
instructor	O
signal	O
to	O
the	O
hypothesis	O
layer	O
38	O
.	O

If	O
there	O
is	O
no	O
context	O
layer	O
,	O
the	O
time	O
series	O
relation	O
based	O
on	O
the	O
information	O
of	O
the	O
past	O
(	O
time	O
t-1	O
)	O
is	O
not	O
learned	O
.	O
However	O
,	O
since	O
the	O
time	O
series	O
relation	O
based	O
on	O
the	O
present	O
(	O
time	O
t	O
)	O
and	O
the	O
future	O
(	O
time	O
t	O
+	O
1	O
)	O
is	O
locally	O
learned	O
,	O
the	O
speech	O
can	O
be	O
sufficiently	O
recognized	O
.	O
In	O
this	O
case	O
,	O
the	O
processing	O
necessary	O
for	O
learning	O
and	O
speech	O
recognition	O
is	O
reduced	O
and	O
the	O
speed	O
of	O
processing	O
can	O
be	O
improved	O
.	O

Further	O
,	O
in	O
this	O
embodiment	O
,	O
although	O
speech	O
recognition	O
is	O
effected	O
by	O
hypothetically	O
learning	O
both	O
the	O
time	O
series	O
relation	O
of	O
the	O
vector	O
rows	O
to	O
be	O
input	O
and	O
the	O
phonemes	O
(	O
code	O
rows	O
)	O
,	O
the	O
target	O
of	O
learning	O
is	O
not	O
restricted	O
to	O
the	O
phonemes	O
having	O
the	O
time	O
series	O
relation	O
in	O
the	O
present	O
invention	O
.	O
The	O
invention	O
may	O
be	O
used	O
in	O
learning	O
,	O
recognition	O
and	O
prediction	O
of	O
a	O
definite	O
meaning	O
represented	O
by	O
a	O
set	O
of	O
plurality	O
of	O
vector	O
rows	O
Fn	O
(	O
n	O
=	O
1	O
,	O
2	O
,	O
3	O
,	O
.	O
.	O
.	O
)	O
having	O
a	O
predetermined	O
relationship	O
.	O

For	O
example	O
,	O
learning	O
and	O
prediction	O
of	O
occurrence	O
of	O
time	O
series	O
patterns	O
of	O
movements	O
,	O
as	O
well	O
as	O
speech	O
recognition	O
,	O
may	O
be	O
carried	O
out	O
.	O

Furthermore	O
,	O
it	O
may	O
be	O
possible	O
to	O
perform	O
learning	O
and	O
recognition	O
of	O
a	O
specific	O
meaning	O
represented	O
by	O
a	O
set	O
of	O
a	O
plurality	O
of	O
vector	O
rows	O
having	O
a	O
spatial	O
relation	O
or	O
a	O
frequency	O
relation	O
as	O
well	O
as	O
the	O
time	O
series	O
relation	O
.	O
For	O
instance	O
,	O
characters	O
may	O
be	O
recognized	O
by	O
learning	O
the	O
spatial	O
relation	O
that	O
the	O
characters	O
have	O
.	O

Moreover	O
,	O
the	O
embodiment	O
has	O
been	O
described	O
as	O
to	O
speech	O
recognition	O
according	O
to	O
each	O
phoneme	O
,	O
but	O
speech	O
recognition	O
may	O
be	O
effected	O
in	O
accordance	O
with	O
each	O
word	O
.	O
In	O
this	O
case	O
,	O
the	O
code	O
row	O
representing	O
a	O
word	O
is	O
used	O
as	O
the	O
second	O
instructor	O
signal	O
for	O
a	O
definite	O
meaning	O
represented	O
by	O
the	O
vector	O
row	O
.	O

In	O
addition	O
,	O
although	O
learning	O
of	O
the	O
neuron	O
device	O
network	O
22	O
is	O
carried	O
out	O
by	O
the	O
CPU	O
11	O
in	O
accordance	O
with	O
the	O
learning	O
program	O
stored	O
in	O
the	O
ROM	O
13	O
,	O
and	O
speech	O
is	O
recognized	O
by	O
the	O
neuron	O
device	O
network	O
22	O
after	O
learning	O
in	O
this	O
embodiment	O
,	O
speaker-independent	O
recognition	O
of	O
a	O
continuous	O
stream	O
of	O
speech	O
is	O
possible	O
with	O
a	O
high	O
recognition	O
rate	O
,	O
thus	O
re-learning	O
for	O
an	O
individual	O
speaker	O
is	O
unnecessary	O
.	O
Thus	O
,	O
the	O
speech	O
recognition	O
apparatus	O
does	O
not	O
have	O
to	O
be	O
provided	O
with	O
a	O
learning	O
function	O
,	O
and	O
may	O
use	O
a	O
neuron	O
device	O
network	O
consisting	O
of	O
the	O
context	O
layer	O
33	O
,	O
the	O
hidden	O
layer	O
34	O
and	O
the	O
hypothesis	O
layer	O
38	O
having	O
connection	O
weights	O
determined	O
by	O
a	O
learning	O
process	O
of	O
another	O
apparatus	O
.	O
In	O
this	O
case	O
,	O
the	O
neuron	O
device	O
network	O
22	O
may	O
be	O
implemented	O
using	O
hardware	O
having	O
connection	O
weights	O
which	O
have	O
been	O
previously	O
learned	O
.	O

Further	O
,	O
although	O
each	O
phoneme	O
during	O
learning	O
,	O
and	O
speech	O
during	O
speech	O
recognition	O
are	O
subjected	O
to	O
spectral	O
analysis	O
in	O
accordance	O
with	O
the	O
fast	O
Fourier	O
transformation	O
in	O
the	O
FFT	O
unit	O
in	O
the	O
embodiment	O
described	O
above	O
,	O
spectral	O
analysis	O
may	O
be	O
carried	O
out	O
in	O
accordance	O
with	O
any	O
other	O
algorithm	O
.	O
For	O
example	O
,	O
spectral	O
analysis	O
may	O
be	O
performed	O
in	O
accordance	O
with	O
DCT	O
(	O
discrete	O
cosine	O
transformation	O
)	O
or	O
the	O
like	O
.	O

Furthermore	O
,	O
the	O
above	O
embodiment	O
has	O
been	O
described	O
where	O
only	O
one	O
kind	O
of	O
learning	O
is	O
effected	O
with	O
respect	O
to	O
,	O
for	O
example	O
,	O
a	O
phoneme	O
"	O
a	O
"	O
which	O
is	O
a	O
vowel	O
,	O
but	O
various	O
kinds	O
of	O
learning	O
may	O
be	O
enabled	O
in	O
the	O
present	O
invention	O
.	O
For	O
instance	O
,	O
in	O
case	O
of	O
a	O
phoneme	O
"	O
a	O
"	O
,	O
learning	O
about	O
"	O
a	O
"	O
taken	O
out	O
from	O
(	O
i	O
.	O
e	O
.	O
,	O
as	O
used	O
in	O
)	O
each	O
sound	O
"	O
ma	O
"	O
,	O
"	O
na	O
"	O
or	O
"	O
ka	O
"	O
,	O
as	O
well	O
as	O
an	O
independent	O
vowel	O
"	O
a	O
"	O
,	O
may	O
be	O
performed	O
.	O
In	O
addition	O
,	O
in	O
case	O
of	O
a	O
consonant	O
,	O
e.g.	O
,	O
"	O
m	O
"	O
,	O
phoneme	O
"	O
m	O
"	O
may	O
be	O
taken	O
out	O
from	O
each	O
sound	O
"	O
ma	O
"	O
,	O
"	O
mi	O
"	O
or	O
"	O
mu	O
"	O
and	O
learning	O
may	O
be	O
carried	O
out	O
thereabout	O
.	O
Learning	O
about	O
each	O
phoneme	O
connected	O
to	O
any	O
of	O
various	O
other	O
phonemes	O
can	O
be	O
carried	O
out	O
,	O
thus	O
improving	O
the	O
recognition	O
rate	O
.	O

FIG	O
.	O
8	O
illustrates	O
the	O
structure	O
of	O
the	O
neuron	O
device	O
network	O
according	O
to	O
a	O
second	O
example	O
of	O
the	O
present	O
invention	O
of	O
the	O
first	O
embodiment	O
.	O

As	O
shown	O
in	O
FIG	O
.	O
8	O
,	O
the	O
neuron	O
device	O
network	O
comprises	O
a	O
recurrent	O
cascade	O
type	O
neural	O
network	O
in	O
the	O
second	O
example	O
of	O
this	O
embodiment	O
.	O

The	O
recurrent	O
cascade	O
type	O
neuron	O
device	O
network	O
is	O
provided	O
with	O
a	O
speech	O
input	O
layer	O
52	O
and	O
an	O
output	O
layer	O
56	O
which	O
has	O
a	O
speech	O
output	O
layer	O
57	O
and	O
a	O
hypothesis	O
layer	O
58	O
.	O
Each	O
of	O
the	O
neuron	O
devices	O
In1	O
to	O
In30	O
of	O
speech	O
input	O
layer	O
52	O
is	O
connected	O
to	O
each	O
of	O
the	O
neuron	O
devices	O
Ou1	O
to	O
Ou30	O
of	O
the	O
output	O
layer	O
56	O
.	O
This	O
type	O
of	O
connection	O
of	O
the	O
neuron	O
devices	O
is	O
a	O
complete	O
connection	O
.	O

Further	O
,	O
the	O
neuron	O
device	O
network	O
is	O
provided	O
with	O
a	O
cascade	O
hidden	O
layer	O
54	O
consisting	O
of	O
80	O
hidden	O
layers	O
5401	O
to	O
5480	O
corresponding	O
to	O
all	O
of	O
the	O
phonemes	O
,	O
and	O
a	O
cascade	O
context	O
layer	O
53	O
consisting	O
of	O
80	O
context	O
layers	O
5301	O
to	O
5380	O
corresponding	O
to	O
the	O
respective	O
hidden	O
layers	O
5401	O
to	O
5480	O
of	O
the	O
cascade	O
hidden	O
layer	O
54	O
.	O
Each	O
of	O
the	O
hidden	O
layers	O
5401	O
to	O
5480	O
has	O
a	O
different	O
number	O
of	O
neuron	O
devices	O
in	O
accordance	O
with	O
the	O
corresponding	O
number	O
of	O
phonemes	O
.	O
Each	O
of	O
the	O
context	O
layers	O
5301	O
to	O
5380	O
is	O
provided	O
with	O
the	O
same	O
number	O
of	O
neuron	O
devices	O
as	O
that	O
of	O
the	O
corresponding	O
hidden	O
layers	O
5401	O
to	O
5480	O
.	O

In	O
this	O
example	O
,	O
each	O
of	O
the	O
speech	O
input	O
layer	O
52	O
and	O
the	O
speech	O
output	O
layer	O
57	O
has	O
30	O
neuron	O
devices	O
,	O
and	O
the	O
hypothesis	O
layer	O
58	O
has	O
8	O
neuron	O
devices	O
.	O
However	O
,	O
similar	O
to	O
the	O
embodiment	O
shown	O
in	O
FIG	O
.	O
2	O
,	O
the	O
number	O
of	O
neuron	O
devices	O
may	O
be	O
any	O
other	O
number	O
so	O
long	O
as	O
the	O
number	O
of	O
neuron	O
devices	O
of	O
the	O
speech	O
input	O
layer	O
52	O
equals	O
that	O
of	O
the	O
neuron	O
devices	O
in	O
the	O
speech	O
output	O
layer	O
57	O
.	O

The	O
connection	O
between	O
the	O
hidden	O
layers	O
of	O
cascade	O
hidden	O
layer	O
54	O
and	O
the	O
hidden	O
layers	O
of	O
cascade	O
context	O
layer	O
53	O
is	O
not	O
a	O
complete	O
connection	O
.	O
The	O
respective	O
hidden	O
layers	O
5401	O
to	O
5480	O
are	O
connected	O
only	O
with	O
their	O
corresponding	O
cascade	O
context	O
layers	O
5301	O
to	O
5380	O
.	O
That	O
is	O
,	O
the	O
hidden	O
layer	O
5401	O
is	O
connected	O
with	O
the	O
corresponding	O
cascade	O
context	O
layer	O
5301	O
but	O
not	O
completely	O
connected	O
with	O
other	O
context	O
layers	O
5302	O
to	O
5380	O
.	O
Similarly	O
,	O
other	O
hidden	O
devices	O
5402	O
to	O
5480	O
are	O
connected	O
with	O
only	O
the	O
corresponding	O
context	O
layers	O
.	O

Further	O
,	O
the	O
cascade	O
hidden	O
layer	O
54	O
is	O
completely	O
connected	O
with	O
both	O
the	O
speech	O
input	O
layer	O
52	O
and	O
the	O
output	O
layer	O
56	O
.	O

Note	O
that	O
the	O
neuron	O
devices	O
constituting	O
the	O
respective	O
hidden	O
layers	O
5401	O
to	O
5480	O
are	O
independent	O
from	O
each	O
other	O
in	O
this	O
embodiment	O
.	O
However	O
,	O
any	O
adjacent	O
neuron	O
device	O
may	O
be	O
connected	O
with	O
another	O
adjacent	O
neuron	O
device	O
in	O
order	O
to	O
input	O
an	O
output	O
of	O
one	O
neuron	O
device	O
to	O
another	O
neuron	O
device	O
.	O

In	O
the	O
neuron	O
device	O
network	O
having	O
such	O
a	O
configuration	O
,	O
the	O
vector	O
rows	O
of	O
the	O
speech	O
obtained	O
at	O
time	O
t	O
which	O
have	O
been	O
subjected	O
to	O
spectral	O
analysis	O
by	O
the	O
FFT	O
unit	O
21	O
during	O
learning	O
are	O
sequentially	O
input	O
to	O
the	O
speech	O
input	O
layer	O
52	O
.	O
The	O
vector	O
states	O
of	O
the	O
hidden	O
layers	O
5401	O
to	O
5480	O
,	O
which	O
are	O
obtained	O
after	O
learning	O
at	O
previous	O
time	O
t-1	O
is	O
completed	O
,	O
are	O
input	O
to	O
the	O
respective	O
context	O
layers	O
5301	O
to	O
5380	O
.	O
The	O
vector	O
rows	O
at	O
time	O
t	O
+	O
1	O
,	O
which	O
are	O
to	O
be	O
subsequently	O
supplied	O
to	O
the	O
speech	O
input	O
layer	O
52	O
,	O
are	O
input	O
to	O
the	O
speech	O
output	O
layer	O
57	O
as	O
the	O
first	O
instructor	O
signals	O
.	O

The	O
second	O
instructor	O
signals	O
are	O
input	O
to	O
the	O
hypothesis	O
layer	O
58	O
,	O
as	O
code	O
rows	O
hypothesizing	O
phonemes	O
represented	O
by	O
the	O
vector	O
rows	O
which	O
are	O
input	O
to	O
the	O
speech	O
input	O
layer	O
52	O
at	O
intervals	O
of	O
time	O
before	O
and	O
after	O
time	O
t	O
.	O

For	O
example	O
,	O
when	O
learning	O
a	O
phoneme	O
"	O
a	O
"	O
,	O
learning	O
is	O
carried	O
out	O
by	O
varying	O
only	O
connection	O
weights	O
between	O
the	O
hidden	O
layer	O
5401	O
and	O
the	O
output	O
layer	O
56	O
,	O
the	O
hidden	O
layer	O
5401	O
and	O
the	O
context	O
layer	O
5301	O
,	O
the	O
hidden	O
layer	O
5401	O
and	O
the	O
speech	O
input	O
layer	O
52	O
,	O
and	O
the	O
speech	O
input	O
layer	O
52	O
and	O
the	O
output	O
layer	O
56	O
.	O
That	O
is	O
,	O
the	O
connection	O
weights	O
are	O
not	O
varied	O
between	O
the	O
hidden	O
layers	O
5402	O
to	O
5480	O
and	O
the	O
speech	O
input	O
layer	O
52	O
,	O
the	O
hidden	O
layers	O
5402	O
to	O
5480	O
and	O
the	O
context	O
layers	O
5302	O
to	O
5380	O
,	O
and	O
the	O
hidden	O
layers	O
5402	O
to	O
5480	O
and	O
the	O
output	O
layer	O
56	O
.	O

When	O
learning	O
a	O
next	O
phoneme	O
"	O
i	O
"	O
,	O
the	O
connection	O
weights	O
between	O
the	O
hidden	O
layer	O
5401	O
and	O
the	O
respective	O
layers	O
are	O
fixed	O
.	O
Further	O
,	O
an	O
output	O
from	O
the	O
hidden	O
layer	O
5401	O
whose	O
connection	O
weights	O
are	O
fixed	O
,	O
as	O
well	O
as	O
an	O
output	O
from	O
the	O
hidden	O
layer	O
5402	O
corresponding	O
with	O
the	O
phoneme	O
"	O
i	O
"	O
,	O
is	O
input	O
to	O
the	O
output	O
layer	O
56	O
during	O
the	O
learning	O
process	O
associated	O
with	O
the	O
phoneme	O
"	O
i	O
"	O
.	O
Output	O
values	O
of	O
the	O
output	O
layer	O
56	O
obtained	O
in	O
response	O
to	O
this	O
set	O
of	O
inputs	O
are	O
compared	O
with	O
values	O
of	O
the	O
instructor	O
signals	O
and	O
learned	O
.	O

In	O
this	O
manner	O
,	O
the	O
output	O
from	O
the	O
hidden	O
layer	O
5401	O
,	O
whose	O
connection	O
weights	O
are	O
fixed	O
,	O
is	O
regarded	O
as	O
noise	O
when	O
learning	O
the	O
phoneme	O
"	O
i	O
"	O
.	O
However	O
,	O
learning	O
of	O
the	O
connection	O
weights	O
of	O
the	O
hidden	O
layer	O
5402	O
,	O
during	O
which	O
the	O
noise	O
is	O
eliminated	O
,	O
is	O
carried	O
out	O
by	O
using	O
the	O
noise	O
associated	O
with	O
the	O
learning	O
of	O
the	O
next	O
phoneme	O
"	O
i	O
"	O
.	O

Similarly	O
,	O
when	O
learning	O
the	O
next	O
phoneme	O
"	O
u	O
"	O
,	O
outputs	O
from	O
the	O
hidden	O
layers	O
5401	O
to	O
5402	O
corresponding	O
with	O
the	O
determined	O
phonemes	O
"	O
a	O
"	O
and	O
"	O
i	O
"	O
are	O
input	O
to	O
the	O
output	O
layer	O
56	O
.	O
The	O
connection	O
weight	O
of	O
these	O
two	O
layers	O
5401	O
and	O
5402	O
is	O
fixed	O
.	O

In	O
the	O
recurrent	O
cascade	O
type	O
neuron	O
device	O
network	O
having	O
above-described	O
configuration	O
,	O
since	O
a	O
pair	O
in	O
each	O
of	O
the	O
hidden	O
layer	O
and	O
the	O
context	O
layer	O
are	O
provided	O
for	O
each	O
phoneme	O
and	O
completely	O
separated	O
from	O
other	O
hidden	O
layers	O
or	O
context	O
layers	O
,	O
each	O
phoneme	O
can	O
be	O
learned	O
at	O
high	O
speed	O
.	O

As	O
a	O
variation	O
of	O
this	O
embodiment	O
,	O
the	O
learning	O
process	O
of	O
the	O
hidden	O
layer	O
and	O
the	O
context	O
layer	O
pair	O
corresponding	O
with	O
each	O
phoneme	O
may	O
be	O
independently	O
carried	O
out	O
by	O
using	O
a	O
separate	O
computer	O
system	O
or	O
the	O
like	O
,	O
and	O
the	O
cascade	O
hidden	O
layer	O
and	O
the	O
cascade	O
context	O
layer	O
may	O
be	O
designed	O
by	O
combining	O
pairs	O
of	O
the	O
hidden	O
layer	O
and	O
the	O
context	O
layer	O
after	O
completion	O
of	O
each	O
learning	O
cycle	O
.	O

In	O
this	O
case	O
,	O
since	O
each	O
hidden	O
layer	O
independently	O
learns	O
about	O
only	O
its	O
corresponding	O
phoneme	O
,	O
learning	O
during	O
which	O
the	O
noise	O
is	O
eliminated	O
from	O
the	O
hidden	O
layers	O
corresponding	O
with	O
other	O
phonemes	O
cannot	O
be	O
performed	O
.	O
Thus	O
,	O
an	O
additional	O
hidden	O
layer	O
is	O
provided	O
by	O
which	O
a	O
signal	O
for	O
eliminating	O
the	O
noise	O
of	O
each	O
phoneme	O
is	O
input	O
to	O
the	O
output	O
layer	O
56	O
.	O
Further	O
,	O
learning	O
about	O
all	O
the	O
phonemes	O
is	O
again	O
performed	O
with	O
the	O
connection	O
weights	O
of	O
the	O
respective	O
hidden	O
layers	O
5401	O
to	O
5480	O
which	O
have	O
been	O
already	O
learned	O
and	O
fixed	O
.	O

In	O
this	O
case	O
,	O
an	O
output	O
from	O
an	O
additionally-provided	O
hidden	O
layer	O
is	O
a	O
value	O
for	O
eliminating	O
the	O
noise	O
.	O
For	O
example	O
,	O
when	O
the	O
learning	O
process	O
related	O
to	O
the	O
phoneme	O
"	O
a	O
"	O
is	O
carried	O
out	O
again	O
,	O
the	O
connection	O
weights	O
of	O
the	O
additional	O
hidden	O
layer	O
are	O
learned	O
,	O
so	O
that	O
a	O
value	O
of	O
the	O
total	O
of	O
outputs	O
from	O
the	O
hidden	O
layers	O
5402	O
to	O
5480	O
whose	O
connection	O
weights	O
are	O
fixed	O
,	O
is	O
0	O
.	O

Although	O
the	O
cascade	O
context	O
layer	O
53	O
is	O
provided	O
to	O
the	O
neuron	O
device	O
network	O
shown	O
in	O
FIG	O
.	O
8	O
,	O
the	O
neuron	O
device	O
network	O
may	O
not	O
have	O
the	O
cascade	O
context	O
layer	O
53	O
.	O
In	O
this	O
case	O
,	O
the	O
respective	O
neuron	O
devices	O
constituting	O
the	O
hidden	O
layers	O
5401	O
to	O
5480	O
of	O
the	O
cascade	O
hidden	O
layer	O
54	O
are	O
configured	O
to	O
feed-back	O
their	O
values	O
as	O
inputs	O
thereto	O
.	O
In	O
other	O
words	O
,	O
when	O
processing	O
the	O
inputs	O
obtained	O
at	O
time	O
t	O
,	O
the	O
values	O
at	O
time	O
t-1	O
of	O
the	O
neuron	O
devices	O
,	O
as	O
well	O
as	O
the	O
inputs	O
at	O
time	O
t	O
supplied	O
from	O
the	O
speech	O
input	O
layer	O
52	O
,	O
are	O
fed	O
back	O
and	O
input	O
to	O
the	O
neuron	O
devices	O
of	O
the	O
respective	O
hidden	O
layers	O
.	O

According	O
to	O
this	O
embodiment	O
,	O
since	O
calculation	O
of	O
the	O
connection	O
weights	O
between	O
the	O
cascade	O
context	O
layer	O
53	O
and	O
the	O
cascade	O
hidden	O
layer	O
54	O
is	O
unnecessary	O
while	O
taking	O
the	O
information	O
of	O
the	O
past	O
(	O
time	O
t-1	O
)	O
into	O
consideration	O
,	O
the	O
processing	O
speed	O
can	O
be	O
improved	O
.	O

In	O
the	O
neuron	O
device	O
network	O
shown	O
in	O
FIGS	O
.	O
2	O
and	O
8	O
and	O
variations	O
thereof	O
,	O
description	O
has	O
been	O
set	O
forth	O
where	O
the	O
connection	O
made	O
between	O
the	O
respective	O
layers	O
is	O
a	O
complete	O
connection	O
,	O
but	O
the	O
present	O
invention	O
is	O
not	O
restricted	O
to	O
this	O
configuration	O
.	O
For	O
example	O
,	O
the	O
connection	O
state	O
may	O
be	O
determined	O
in	O
accordance	O
with	O
a	O
number	O
of	O
neuron	O
devices	O
or	O
a	O
learning	O
ability	O
of	O
each	O
layer	O
.	O

A	O
second	O
embodiment	O
according	O
to	O
the	O
present	O
invention	O
will	O
now	O
be	O
described	O
.	O

In	O
the	O
first	O
embodiment	O
,	O
the	O
spectral	O
data	O
analyzed	O
by	O
the	O
FFT	O
21	O
are	O
data	O
to	O
be	O
input	O
to	O
the	O
speech	O
input	O
layer	O
32	O
,	O
whereas	O
cepstrum	O
data	O
are	O
input	O
to	O
the	O
speech	O
input	O
layer	O
32	O
to	O
perform	O
speech	O
recognition	O
according	O
to	O
the	O
second	O
embodiment	O
.	O

FIG	O
.	O
9	O
shows	O
a	O
system	O
schematic	O
block	O
diagram	O
of	O
a	O
neural	O
network	O
according	O
to	O
the	O
second	O
embodiment	O
.	O
As	O
shown	O
in	O
the	O
drawing	O
,	O
a	O
cepstrum	O
unit	O
26	O
is	O
additionally	O
provided	O
to	O
the	O
phoneme	O
recognition	O
system	O
according	O
to	O
the	O
first	O
embodiment	O
in	O
this	O
neural	O
network	O
.	O

Since	O
other	O
parts	O
of	O
this	O
neural	O
network	O
are	O
similar	O
to	O
those	O
in	O
the	O
first	O
embodiment	O
,	O
like	O
reference	O
numerals	O
are	O
used	O
to	O
reference	O
these	O
parts	O
,	O
thereby	O
omitting	O
explanation	O
thereabout	O
.	O
Further	O
,	O
with	O
regard	O
to	O
the	O
neuron	O
device	O
network	O
22	O
,	O
it	O
may	O
be	O
possible	O
to	O
adopt	O
not	O
only	O
the	O
neuron	O
network	O
described	O
in	O
connection	O
with	O
FIG	O
.	O
2	O
of	O
the	O
first	O
embodiment	O
,	O
but	O
also	O
any	O
of	O
various	O
neuron	O
device	O
networks	O
22	O
explained	O
as	O
examples	O
and	O
variations	O
of	O
the	O
first	O
embodiment	O
.	O

Furthermore	O
,	O
when	O
explaining	O
the	O
second	O
and	O
third	O
embodiments	O
hereinafter	O
,	O
each	O
part	O
of	O
the	O
neuron	O
device	O
network	O
22	O
is	O
specified	O
with	O
the	O
same	O
reference	O
numerals	O
used	O
in	O
explanation	O
of	O
the	O
neuron	O
network	O
shown	O
in	O
FIGS	O
.	O
2	O
and	O
8	O
.	O
Note	O
that	O
in	O
the	O
case	O
of	O
the	O
speech	O
input	O
layer	O
32	O
,	O
for	O
example	O
,	O
the	O
reference	O
numeral	O
used	O
thereto	O
denotes	O
both	O
the	O
speech	O
input	O
layer	O
32	O
in	O
the	O
neuron	O
network	O
22	O
in	O
FIG	O
.	O
2	O
and	O
the	O
speech	O
input	O
layer	O
52	O
in	O
the	O
neuron	O
device	O
network	O
in	O
FIG	O
.	O
8	O
.	O

The	O
cepstrum	O
unit	O
26	O
obtains	O
cepstrum	O
data	O
by	O
subjecting	O
a	O
logarithm	O
of	O
a	O
short-time	O
amplitude	O
spectrum	O
of	O
a	O
waveform	O
which	O
has	O
been	O
spectral-analyzed	O
by	O
the	O
FFT	O
unit	O
21	O
to	O
inverse	O
Fourier	O
transformation	O
.	O
A	O
spectral	O
envelope	O
and	O
a	O
fine	O
structure	O
can	O
be	O
separated	O
and	O
extracted	O
by	O
the	O
cepstrum	O
unit	O
26	O
.	O

A	O
description	O
will	O
now	O
be	O
given	O
as	O
to	O
the	O
principle	O
of	O
the	O
cepstrum	O
.	O

Assuming	O
that	O
Fourier-transforms	O
of	O
impulse	O
responses	O
from	O
the	O
sound	O
source	O
and	O
path	O
are	O
represented	O
by	O
G	O
(	O
.	O
omega	O
.	O
)	O
and	O
H	O
(	O
.	O
omega	O
.	O
)	O
,	O
respectively	O
,	O
the	O
following	O
relation	O
can	O
be	O
obtained	O
by	O
a	O
linear	O
separate	O
transparent	O
circuit	O
model	O
:	O
X	O
(	O
.	O
omega	O
.	O
)	O
=	O
G	O
(	O
.	O
omega	O
.	O
)	O
.	O
times	O
.	O
H	O
(	O
.	O
omega	O
.	O
)	O
When	O
taking	O
logarithms	O
on	O
the	O
both	O
sides	O
of	O
this	O
equation	O
,	O
the	O
following	O
expression	O
(	O
1	O
)	O
can	O
be	O
obtained	O
:	O
log.vertline.X	O
(	O
.	O
omega	O
.	O
)	O
.vertline.=log.vertline.G	O
(	O
.	O
omega	O
.	O
)	O
.vertline.+log.ver	O
tline	O
.	O
H	O
(	O
.	O
omega	O
.	O
)	O
.	O
vertline	O
.	O
(	O
1	O
)	O
Further	O
,	O
when	O
taking	O
inverse	O
Fourier	O
transforms	O
of	O
the	O
both	O
sides	O
of	O
this	O
expression	O
(	O
1	O
)	O
,	O
the	O
following	O
expression	O
(	O
2	O
)	O
,	O
i	O
.	O
e	O
.	O
,	O
the	O
cepstrum	O
can	O
be	O
obtained	O
:	O
c	O
(	O
.	O
tau	O
.	O
)	O
=	O
F.	O
sup	O
.	O
-	O
1	O
log.vertline.X	O
(	O
.	O
omega	O
.	O
)	O
.	O
vertline	O
.	O

=	O
F.	O
sup	O
.	O
-	O
1	O
log.vertline.G	O
(	O
.	O
omega	O
.	O
)	O
.vertline.+F.sup.-1	O
log.vertline.H	O
(	O
.	O
omega	O
.	O
)	O
.	O
vertline	O
.	O
(	O
2	O
)	O
Here	O
,	O
a	O
dimension	O
of	O
.	O
tau	O
.	O
is	O
time	O
because	O
it	O
is	O
an	O
inverse	O
transform	O
obtained	O
from	O
the	O
frequency	O
domain	O
,	O
and	O
it	O
is	O
called	O
a	O
quefrency	O
.	O

Extraction	O
of	O
a	O
basis	O
cycle	O
and	O
an	O
envelope	O
will	O
now	O
be	O
explained	O
.	O

A	O
first	O
term	O
on	O
the	O
right	O
side	O
of	O
the	O
expression	O
(	O
1	O
)	O
represents	O
a	O
fine	O
structure	O
of	O
the	O
spectrum	O
,	O
while	O
a	O
second	O
term	O
on	O
the	O
right	O
side	O
is	O
a	O
spectral	O
envelope	O
.	O
The	O
inverse	O
Fourier	O
transforms	O
of	O
the	O
both	O
terms	O
largely	O
differ	O
from	O
each	O
other	O
,	O
and	O
the	O
first	O
term	O
represents	O
a	O
peak	O
of	O
the	O
high	O
quefrency	O
,	O
while	O
the	O
second	O
term	O
is	O
concentrated	O
on	O
a	O
low	O
quefrency	O
portion	O
of	O
approximately	O
0	O
to	O
4	O
ms	O
.	O

The	O
logarithmic	O
spectral	O
envelope	O
can	O
be	O
obtained	O
by	O
Fourier-transforming	O
all	O
parts	O
other	O
than	O
the	O
high	O
quefrency	O
part	O
,	O
and	O
the	O
spectral	O
envelope	O
can	O
be	O
obtained	O
by	O
exponential-transforming	O
that	O
result	O
.	O

The	O
degree	O
of	O
smoothness	O
of	O
the	O
obtained	O
spectral	O
envelope	O
varies	O
depending	O
on	O
the	O
quantity	O
of	O
components	O
of	O
the	O
low	O
quefrency	O
portion	O
to	O
be	O
used	O
.	O
The	O
operation	O
for	O
separating	O
quefrency	O
components	O
is	O
called	O
"	O
liftering	O
"	O
.	O

FIG	O
.	O
10	O
illustrates	O
the	O
schematic	O
structure	O
of	O
the	O
cepstrum	O
unit	O
26	O
.	O

The	O
cepstrum	O
unit	O
26	O
is	O
provided	O
with	O
a	O
logarithmic	O
transforming	O
unit	O
261	O
,	O
an	O
inverse	O
FFT	O
unit	O
262	O
,	O
a	O
cepstrum	O
window	O
263	O
,	O
a	O
peak	O
extracting	O
unit	O
264	O
and	O
an	O
FFT	O
unit	O
265	O
.	O

The	O
cepstrum	O
window	O
263	O
,	O
the	O
peak	O
extracting	O
unit	O
264	O
and	O
the	O
FFT	O
unit	O
265	O
are	O
not	O
required	O
when	O
cepstrum	O
data	O
obtained	O
by	O
the	O
inverse	O
FFT	O
unit	O
262	O
are	O
used	O
as	O
data	O
to	O
be	O
supplied	O
to	O
the	O
speech	O
input	O
layer	O
32	O
(	O
52	O
)	O
of	O
the	O
neuron	O
device	O
network	O
22	O
,	O
but	O
they	O
are	O
required	O
when	O
the	O
spectral	O
envelopes	O
are	O
used	O
as	O
input	O
data	O
of	O
the	O
neuron	O
device	O
network	O
22	O
.	O
Thus	O
,	O
there	O
are	O
several	O
possible	O
configurations	O
of	O
cepstrum	O
unit	O
26	O
.	O
The	O
configuration	O
of	O
the	O
cepstrum	O
unit	O
26	O
will	O
determine	O
the	O
connection	O
of	O
the	O
cepstrum	O
unit	O
26	O
to	O
the	O
speech	O
input	O
layer	O
32	O
(	O
52	O
)	O
.	O
The	O
various	O
connections	O
of	O
the	O
cepstrum	O
unit	O
26	O
to	O
the	O
speech	O
input	O
layer	O
32	O
(	O
52	O
)	O
are	O
shown	O
in	O
FIG	O
.	O
10	O
.	O

Further	O
,	O
the	O
FFT	O
unit	O
265	O
is	O
not	O
necessarily	O
required	O
and	O
it	O
may	O
be	O
substituted	O
by	O
the	O
FFT	O
unit	O
21	O
.	O

The	O
logarithmic	O
transformation	O
unit	O
261	O
performs	O
a	O
logarithmic	O
transformation	O
of	O
the	O
spectral	O
data	O
X	O
(	O
.	O
omega	O
.	O
)	O
supplied	O
from	O
the	O
FFT	O
21	O
in	O
accordance	O
with	O
the	O
expression	O
(	O
1	O
)	O
to	O
obtain	O
Log.vertline.X	O
(	O
.	O
omega	O
.	O
)	O
.	O
vertline	O
.	O
and	O
supplies	O
the	O
result	O
to	O
the	O
inverse	O
FFT	O
unit	O
262	O
.	O

The	O
inverse	O
FFT	O
unit	O
262	O
takes	O
the	O
inverse	O
FFT	O
from	O
the	O
supplied	O
value	O
and	O
calculates	O
c	O
(	O
.	O
tau	O
.	O
)	O
to	O
obtain	O
cepstrum	O
data	O
.	O
The	O
inverse	O
FFT	O
unit	O
262	O
outputs	O
the	O
obtained	O
cepstrum	O
data	O
as	O
input	O
data	O
In	O
to	O
the	O
speech	O
input	O
layer	O
32	O
of	O
the	O
neuron	O
device	O
network	O
22	O
,	O
as	O
described	O
in	O
the	O
first	O
embodiment	O
,	O
with	O
which	O
learning	O
about	O
the	O
speech	O
data	O
or	O
speech	O
recognition	O
is	O
carried	O
out	O
.	O
A	O
number	O
of	O
input	O
data	O
In	O
input	O
to	O
the	O
neuron	O
device	O
network	O
22	O
,	O
is	O
the	O
same	O
number	O
of	O
the	O
neuron	O
devices	O
of	O
the	O
speech	O
input	O
layer	O
32	O
,	O
which	O
has	O
been	O
arbitrarily	O
selected	O
in	O
accordance	O
with	O
speech	O
recognition	O
.	O
That	O
is	O
,	O
in	O
the	O
case	O
of	O
the	O
neuron	O
device	O
network	O
22	O
shown	O
in	O
FIG	O
.	O
2	O
,	O
since	O
a	O
number	O
of	O
neuron	O
devices	O
of	O
the	O
speech	O
input	O
layer	O
32	O
is	O
30	O
,	O
the	O
quefrency	O
(	O
.	O
tau	O
.	O
)	O
axis	O
is	O
divided	O
in	O
30	O
,	O
and	O
values	O
of	O
the	O
power	O
for	O
respective	O
quefrencies	O
are	O
supplied	O
as	O
input	O
data	O
of	O
the	O
neuron	O
devices	O
In1	O
to	O
In30	O
to	O
the	O
speech	O
input	O
layer	O
32	O
(	O
52	O
)	O
.	O

In	O
a	O
first	O
example	O
of	O
the	O
second	O
embodiment	O
,	O
the	O
cepstrum	O
data	O
obtained	O
by	O
the	O
inverse	O
FFT	O
portion	O
262	O
are	O
supplied	O
to	O
the	O
speech	O
input	O
layer	O
32	O
.	O

A	O
second	O
example	O
of	O
the	O
second	O
embodiment	O
will	O
now	O
be	O
described	O
.	O

In	O
the	O
second	O
example	O
,	O
quefrency	O
components	O
are	O
separated	O
into	O
the	O
high	O
quefrency	O
portions	O
and	O
the	O
low	O
quefrency	O
portions	O
by	O
littering	O
the	O
cepstrum	O
data	O
obtained	O
in	O
the	O
cepstrum	O
window	O
263	O
.	O

The	O
separated	O
low	O
quefrency	O
portion	O
is	O
subjected	O
to	O
Fourier	O
transformation	O
in	O
the	O
FFT	O
unit	O
265	O
to	O
obtain	O
the	O
logarithmic	O
spectral	O
envelope	O
.	O
Further	O
,	O
it	O
is	O
exponential-transformed	O
to	O
calculate	O
the	O
spectral	O
envelope	O
.	O
On	O
the	O
basis	O
of	O
the	O
spectral	O
envelope	O
data	O
,	O
the	O
frequency	O
axis	O
is	O
divided	O
into	O
a	O
number	O
equal	O
to	O
that	O
of	O
the	O
neuron	O
devices	O
,	O
and	O
a	O
value	O
of	O
the	O
power	O
for	O
each	O
frequency	O
is	O
input	O
to	O
the	O
speech	O
input	O
layer	O
32	O
(	O
52	O
)	O
.	O

Note	O
that	O
the	O
cepstrum	O
data	O
of	O
the	O
low	O
quefrency	O
portion	O
which	O
has	O
been	O
separated	O
in	O
the	O
cepstrum	O
window	O
263	O
may	O
be	O
supplied	O
as	O
the	O
input	O
data	O
to	O
the	O
speech	O
input	O
layer	O
32	O
.	O

The	O
basis	O
cycle	O
is	O
extracted	O
from	O
the	O
cepstrum	O
data	O
of	O
the	O
separated	O
high	O
quefrency	O
portion	O
by	O
the	O
peak	O
extracting	O
unit	O
264	O
,	O
and	O
the	O
extracted	O
cycle	O
may	O
be	O
used	O
as	O
one	O
of	O
the	O
input	O
data	O
together	O
with	O
the	O
data	O
of	O
the	O
spectral	O
envelope	O
obtained	O
by	O
the	O
FFT	O
unit	O
265	O
.	O
In	O
this	O
case	O
,	O
if	O
a	O
number	O
of	O
neuron	O
devices	O
in	O
the	O
speech	O
input	O
layer	O
32	O
is	O
N	O
,	O
(	O
N-1	O
)	O
input	O
data	O
In1	O
to	O
In	O
(	O
N-1	O
)	O
from	O
the	O
data	O
of	O
the	O
spectral	O
envelope	O
are	O
input	O
to	O
the	O
speech	O
input	O
layer	O
32	O
,	O
and	O
the	O
input	O
data	O
InN	O
from	O
data	O
of	O
the	O
basic	O
cycle	O
are	O
input	O
to	O
the	O
speech	O
input	O
layer	O
32	O
(	O
52	O
)	O
.	O

As	O
mentioned	O
above	O
,	O
according	O
to	O
the	O
second	O
embodiment	O
,	O
since	O
the	O
input	O
data	O
which	O
have	O
additional	O
speech	O
characteristics	O
than	O
the	O
power	O
spectrum	O
are	O
a	O
target	O
of	O
recognition	O
by	O
using	O
the	O
cepstrum	O
data	O
related	O
to	O
the	O
speech	O
data	O
,	O
the	O
rate	O
of	O
recognition	O
can	O
be	O
further	O
improved	O
.	O

Although	O
description	O
has	O
been	O
given	O
as	O
to	O
speech	O
recognition	O
in	O
the	O
second	O
embodiment	O
,	O
image	O
recognition	O
may	O
be	O
performed	O
by	O
using	O
cepstrum	O
data	O
of	O
image	O
data	O
.	O
In	O
this	O
case	O
,	O
either	O
the	O
image	O
data	O
read	O
by	O
the	O
graphic	O
reading	O
unit	O
24	O
or	O
the	O
image	O
data	O
received	O
by	O
the	O
communication	O
control	O
unit	O
15	O
may	O
be	O
used	O
as	O
the	O
image	O
data	O
.	O

A	O
third	O
embodiment	O
according	O
to	O
the	O
present	O
invention	O
will	O
now	O
be	O
explained	O
.	O

As	O
described	O
above	O
,	O
the	O
cepstrum	O
data	O
are	O
used	O
as	O
input	O
data	O
supplied	O
to	O
the	O
speech	O
input	O
layer	O
32	O
(	O
52	O
)	O
,	O
of	O
the	O
neuron	O
device	O
network	O
22	O
in	O
the	O
second	O
embodiment	O
,	O
whereas	O
data	O
of	O
the	O
hidden	O
layer	O
in	O
an	O
auto-associative	O
neural	O
network	O
are	O
used	O
as	O
the	O
input	O
data	O
in	O
the	O
third	O
embodiment	O
.	O

FIG	O
.	O
11	O
illustrates	O
,	O
in	O
block	O
diagram	O
fashion	O
,	O
the	O
system	O
structure	O
of	O
the	O
neural	O
network	O
using	O
the	O
auto-associative	O
NN	O
(	O
neural	O
network	O
)	O
27	O
in	O
the	O
third	O
embodiment	O
.	O
As	O
shown	O
in	O
the	O
drawing	O
,	O
an	O
auto-associative	O
neural	O
network	O
27	O
is	O
additionally	O
provided	O
to	O
the	O
system	O
of	O
the	O
first	O
embodiment	O
in	O
this	O
neural	O
network	O
.	O

An	O
area	O
for	O
storing	O
vector	O
rows	O
for	O
the	O
auto-associative	O
neural	O
network	O
,	O
as	O
well	O
as	O
the	O
vector	O
row	O
storing	O
area	O
for	O
storing	O
the	O
input	O
data	O
for	O
the	O
neuron	O
device	O
network	O
22	O
,	O
is	O
located	O
in	O
the	O
RAM	O
14	O
according	O
to	O
the	O
third	O
embodiment	O
.	O

Since	O
other	O
parts	O
of	O
this	O
network	O
are	O
similar	O
to	O
those	O
of	O
the	O
first	O
embodiment	O
,	O
like	O
reference	O
numerals	O
are	O
used	O
to	O
represent	O
these	O
parts	O
,	O
thereby	O
omitting	O
explanation	O
thereabout	O
.	O
Further	O
,	O
as	O
the	O
neuron	O
device	O
network	O
22	O
,	O
it	O
may	O
be	O
possible	O
to	O
adopt	O
not	O
only	O
the	O
neuron	O
device	O
network	O
22	O
in	O
accordance	O
with	O
the	O
first	O
embodiment	O
,	O
but	O
also	O
any	O
of	O
various	O
neuron	O
device	O
networks	O
22	O
which	O
have	O
been	O
described	O
as	O
examples	O
or	O
variations	O
of	O
the	O
first	O
embodiment	O
.	O

A	O
number	O
of	O
neuron	O
devices	O
In	O
of	O
the	O
speech	O
input	O
layer	O
32	O
in	O
the	O
neuron	O
device	O
network	O
22	O
according	O
to	O
the	O
third	O
embodiment	O
is	O
equal	O
to	O
the	O
number	O
neuron	O
devices	O
of	O
the	O
hidden	O
layer	O
AH	O
in	O
the	O
auto-associative	O
neural	O
network	O
27	O
.	O

FIG	O
.	O
12	O
illustrates	O
the	O
structure	O
of	O
the	O
auto-associative	O
neural	O
network	O
27	O
.	O

As	O
shown	O
in	O
FIG	O
.	O
12	O
,	O
the	O
auto-associative	O
neural	O
network	O
27	O
is	O
provided	O
with	O
three	O
layers	O
,	O
i	O
.	O
e	O
.	O
,	O
an	O
input	O
layer	O
AI	O
,	O
a	O
hidden	O
layer	O
AH	O
,	O
and	O
an	O
output	O
layer	O
AO	O
.	O

The	O
input	O
layer	O
AI	O
is	O
provided	O
with	O
p	O
neuron	O
devices	O
AI1	O
to	O
AIp	O
,	O
the	O
number	O
p	O
being	O
equal	O
to	O
p	O
input	O
data	O
which	O
are	O
arbitrarily	O
selected	O
in	O
accordance	O
with	O
various	O
types	O
of	O
processing	O
,	O
such	O
as	O
speech	O
recognition	O
or	O
graphic	O
recognition	O
.	O

The	O
hidden	O
layer	O
AH	O
is	O
provided	O
with	O
q	O
neuron	O
devices	O
AH1	O
to	O
AHq	O
,	O
the	O
number	O
q	O
being	O
smaller	O
than	O
the	O
number	O
p	O
(	O
i	O
.	O
e	O
.	O
,	O
q	O
<	O
p	O
)	O
of	O
neuron	O
devices	O
of	O
the	O
input	O
layer	O
AH	O
.	O

The	O
output	O
layer	O
AO	O
is	O
provided	O
with	O
p	O
neuron	O
devices	O
AO1	O
to	O
AOp	O
,	O
the	O
number	O
p	O
being	O
equal	O
to	O
the	O
number	O
p	O
of	O
neuron	O
devices	O
of	O
the	O
input	O
layer	O
AH	O
.	O

The	O
respective	O
neuron	O
devices	O
AH1	O
to	O
AHq	O
of	O
the	O
hidden	O
layer	O
AH	O
are	O
completely	O
connected	O
with	O
all	O
the	O
neuron	O
devices	O
of	O
the	O
input	O
layer	O
AI	O
by	O
connection	O
weights	O
AW11	O
to	O
AWpq	O
which	O
can	O
be	O
changed	O
during	O
learning	O
.	O

Further	O
,	O
the	O
respective	O
neuron	O
devices	O
AH1	O
to	O
AHq	O
of	O
the	O
hidden	O
layer	O
AH	O
have	O
threshold	O
values	O
which	O
can	O
be	O
changed	O
during	O
the	O
learning	O
process	O
.	O

The	O
neuron	O
devices	O
AH1	O
to	O
AHq	O
of	O
the	O
hidden	O
layer	O
AH	O
supply	O
output	O
values	O
by	O
forward-propagation	O
based	O
on	O
input	O
data	O
fed	O
to	O
the	O
input	O
layer	O
AI	O
,	O
the	O
connection	O
weights	O
AW	O
and	O
the	O
threshold	O
values	O
.	O
The	O
output	O
values	O
from	O
the	O
AH1	O
to	O
AHq	O
are	O
output	O
as	O
input	O
data	O
St	O
supplied	O
to	O
the	O
speech	O
input	O
layer	O
32	O
of	O
the	O
neuron	O
device	O
network	O
22	O
.	O

Furthermore	O
,	O
the	O
neuron	O
devices	O
AO1	O
to	O
AOp	O
of	O
the	O
output	O
layer	O
AO	O
are	O
completely	O
connected	O
with	O
all	O
the	O
neuron	O
devices	O
AH1	O
to	O
AHq	O
of	O
the	O
hidden	O
layer	O
AH	O
with	O
connection	O
weights	O
Aw11	O
to	O
Awq	O
which	O
can	O
be	O
changed	O
during	O
the	O
learning	O
process	O
as	O
described	O
above	O
.	O
Also	O
,	O
the	O
respective	O
neuron	O
devices	O
AO1	O
to	O
AOp	O
supply	O
output	O
values	O
of	O
the	O
auto-associative	O
neural	O
network	O
27	O
based	O
on	O
the	O
output	O
value	O
St	O
of	O
the	O
hidden	O
layer	O
AH	O
and	O
the	O
connection	O
weights	O
Aw	O
.	O

The	O
auto-associative	O
neural	O
network	O
27	O
is	O
provided	O
with	O
a	O
memory	O
(	O
not	O
shown	O
)	O
for	O
storing	O
the	O
connection	O
weights	O
AW	O
between	O
the	O
input	O
layer	O
AI	O
and	O
the	O
hidden	O
layer	O
AH	O
,	O
and	O
the	O
threshold	O
values	O
and	O
the	O
connection	O
weights	O
between	O
the	O
hidden	O
layer	O
AH	O
and	O
the	O
output	O
layer	O
AO	O
.	O

A	O
description	O
will	O
now	O
be	O
presented	O
regarding	O
generating	O
the	O
input	O
data	O
St	O
,	O
which	O
are	O
input	O
to	O
the	O
neuron	O
device	O
network	O
22	O
by	O
the	O
auto-associative	O
neural	O
network	O
27	O
when	O
performing	O
,	O
e.g.	O
,	O
speech	O
recognition	O
.	O

A	O
process	O
of	O
learning	O
about	O
a	O
phoneme	O
"	O
a	O
"	O
among	O
respective	O
phonemes	O
which	O
are	O
the	O
target	O
of	O
the	O
speech	O
recognition	O
process	O
will	O
be	O
explained	O
.	O

As	O
to	O
the	O
phoneme	O
"	O
a	O
"	O
which	O
is	O
the	O
target	O
of	O
learning	O
,	O
it	O
is	O
assumed	O
that	O
a	O
phoneme	O
which	O
is	O
uttered	O
at	O
the	O
beginning	O
of	O
a	O
word	O
is	O
represented	O
by	O
"	O
"	O
;	O
a	O
phoneme	O
which	O
is	O
uttered	O
at	O
the	O
end	O
of	O
the	O
word	O
is	O
represented	O
by	O
"	O
"	O
;	O
and	O
a	O
phoneme	O
which	O
is	O
uttered	O
in	O
the	O
middle	O
of	O
the	O
word	O
is	O
represented	O
by	O
"	O
A	O
"	O
.	O
For	O
example	O
,	O
"	O
"	O
is	O
taken	O
from	O
a	O
word	O
"	O
aki	O
"	O
(	O
autumn	O
)	O
;	O
"	O
"	O
is	O
taken	O
from	O
a	O
word	O
"	O
denwa	O
"	O
(	O
telephone	O
)	O
;	O
and	O
"	O
A	O
"	O
is	O
taken	O
from	O
a	O
word	O
"	O
tomari	O
"	O
(	O
stay	O
)	O
.	O
In	O
regard	O
to	O
the	O
phoneme	O
"	O
a	O
"	O
,	O
explanation	O
will	O
be	O
given	O
as	O
to	O
the	O
example	O
where	O
learning	O
about	O
three	O
patterns	O
of	O
the	O
phoneme	O
"	O
a	O
"	O
,	O
i	O
.	O
e	O
.	O
"	O
"	O
,	O
"	O
"	O
and	O
"	O
A	O
"	O
is	O
carried	O
out	O
.	O
However	O
,	O
the	O
present	O
invention	O
is	O
not	O
limited	O
to	O
this	O
number	O
of	O
patterns	O
.	O
Thus	O
,	O
learning	O
3	O
to	O
30	O
patterns	O
of	O
each	O
phoneme	O
,	O
or	O
more	O
preferably	O
,	O
approximately	O
100	O
patterns	O
of	O
each	O
phoneme	O
may	O
be	O
carried	O
out	O
.	O

FIGS	O
.	O
13A	O
,	O
13B	O
and	O
13C	O
represent	O
data	O
obtained	O
by	O
spectral-analyzing	O
the	O
three	O
patterns	O
"	O
"	O
,	O
"	O
"	O
and	O
"	O
A	O
"	O
in	O
FFT	O
process	O
by	O
the	O
FFT	O
unit	O
21	O
at	O
each	O
time	O
point	O
t	O
(	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
n	O
)	O
.	O

As	O
shown	O
in	O
FIGS	O
.	O
13A	O
,	O
13B	O
and	O
13C	O
,	O
the	O
FFT	O
unit	O
21	O
calculates	O
values	O
of	O
power	O
(	O
P	O
)	O
of	O
the	O
speech	O
data	O
with	O
respect	O
to	O
each	O
frequency	O
at	O
each	O
time	O
t	O
(	O
the	O
divided	O
number	O
of	O
frequencies	O
in	O
accordance	O
with	O
a	O
number	O
p	O
of	O
neuron	O
devices	O
of	O
the	O
input	O
layer	O
AI	O
which	O
corresponds	O
to	O
p	O
of	O
F1	O
to	O
Fp	O
)	O
.	O
In	O
this	O
manner	O
,	O
similar	O
to	O
the	O
first	O
embodiment	O
explained	O
above	O
in	O
connection	O
with	O
FIG	O
.	O
6	O
,	O
the	O
vector	O
rows	O
based	O
on	O
the	O
power	O
P	O
(	O
t	O
)	O
relative	O
to	O
the	O
respective	O
frequencies	O
are	O
stored	O
in	O
the	O
auto-associative	O
neural	O
network	O
27	O
vector	O
row	O
storing	O
area	O
of	O
the	O
RAM	O
14	O
at	O
each	O
of	O
the	O
respective	O
time	O
points	O
.	O

As	O
shown	O
in	O
FIG	O
.	O
13A	O
,	O
it	O
is	O
assumed	O
that	O
the	O
vector	O
row	O
of	O
the	O
power	O
P	O
(	O
1	O
)	O
at	O
time	O
t	O
=	O
1	O
,	O
which	O
is	O
obtained	O
by	O
spectral-analyzing	O
the	O
phoneme	O
"	O
"	O
is	O
represented	O
as	O
1	O
,	O
the	O
vector	O
row	O
of	O
the	O
power	O
P	O
(	O
2	O
)	O
at	O
time	O
t	O
=	O
2	O
is	O
represented	O
as	O
2	O
and	O
,	O
although	O
not	O
shown	O
,	O
the	O
vector	O
row	O
at	O
time	O
t	O
=	O
n	O
is	O
represented	O
as	O
n	O
.	O

Further	O
,	O
as	O
shown	O
in	O
FIG	O
.	O
13B	O
,	O
it	O
is	O
assumed	O
that	O
the	O
vector	O
row	O
of	O
the	O
power	O
P	O
(	O
1	O
)	O
at	O
time	O
t	O
=	O
1	O
which	O
is	O
obtained	O
by	O
spectral-analyzing	O
the	O
phoneme	O
"	O
"	O
is	O
represented	O
as	O
1	O
,	O
the	O
vector	O
row	O
of	O
the	O
power	O
P	O
(	O
2	O
)	O
at	O
time	O
t	O
=	O
2	O
is	O
represented	O
as	O
2	O
and	O
,	O
although	O
not	O
shown	O
,	O
the	O
vector	O
row	O
at	O
time	O
t	O
=	O
n	O
is	O
represented	O
as	O
n	O
.	O

Furthermore	O
,	O
as	O
shown	O
in	O
FIG	O
.	O
13C	O
,	O
it	O
is	O
assumed	O
that	O
the	O
vector	O
row	O
of	O
the	O
power	O
P	O
(	O
1	O
)	O
at	O
time	O
t	O
=	O
1	O
which	O
is	O
obtained	O
by	O
spectral-analyzing	O
the	O
phoneme	O
"	O
A	O
"	O
is	O
represented	O
as	O
A1	O
,	O
the	O
vector	O
row	O
of	O
the	O
power	O
P	O
(	O
2	O
)	O
at	O
time	O
t	O
=	O
2	O
is	O
represented	O
as	O
A2	O
and	O
,	O
although	O
not	O
shown	O
,	O
the	O
vector	O
row	O
at	O
time	O
t	O
=	O
n	O
is	O
represented	O
as	O
An	O
.	O

Learning	O
in	O
the	O
auto-associative	O
neural	O
network	O
27	O
and	O
generation	O
of	O
input	O
data	O
supplied	O
to	O
the	O
input	O
layer	O
IN	O
of	O
the	O
neuron	O
device	O
network	O
22	O
are	O
executed	O
at	O
each	O
time	O
point	O
of	O
the	O
power	O
P	O
(	O
t	O
)	O
and	O
are	O
obtained	O
by	O
spectral-analyzing	O
each	O
phoneme	O
.	O

In	O
other	O
words	O
,	O
learning	O
is	O
carried	O
out	O
in	O
accordance	O
with	O
each	O
vector	O
row	O
at	O
each	O
time	O
point	O
t	O
by	O
supplying	O
the	O
vector	O
rows	O
1	O
,	O
1	O
and	O
A1	O
of	O
the	O
respective	O
phonemes	O
at	O
the	O
same	O
time	O
,	O
e.g.	O
,	O
t	O
=	O
1	O
as	O
the	O
input	O
data	O
to	O
the	O
input	O
layer	O
AI	O
of	O
the	O
auto-associative	O
neural	O
network	O
27	O
,	O
and	O
using	O
the	O
vector	O
rows	O
as	O
instructor	O
signals	O
of	O
the	O
output	O
layer	O
AO	O
.	O
Once	O
the	O
output	O
value	O
St	O
from	O
the	O
hidden	O
layer	O
AH	O
is	O
obtained	O
when	O
learning	O
at	O
time	O
t	O
is	O
completed	O
,	O
it	O
is	O
treated	O
as	O
the	O
input	O
data	O
to	O
the	O
input	O
layer	O
IN	O
.	O

Note	O
that	O
various	O
kinds	O
of	O
learning	O
according	O
to	O
,	O
e.g.	O
,	O
back-propagation	O
are	O
adaptable	O
as	O
learning	O
in	O
the	O
auto-associative	O
neural	O
network	O
27	O
.	O

FIG	O
.	O
14	O
shows	O
input	O
data	O
and	O
instructor	O
signals	O
during	O
the	O
learning	O
process	O
in	O
the	O
auto-associative	O
neural	O
network	O
27	O
and	O
output	O
values	O
St	O
after	O
learning	O
in	O
the	O
same	O
.	O
FIG	O
.	O
14	O
illustrates	O
as	O
an	O
example	O
,	O
the	O
case	O
where	O
learning	O
is	O
carried	O
out	O
based	O
on	O
vector	O
rows	O
of	O
the	O
power	O
for	O
the	O
respective	O
phonemes	O
shown	O
in	O
FIG	O
.	O
13	O
.	O

As	O
shown	O
in	O
FIG	O
.	O
14	O
,	O
learning	O
is	O
performed	O
at	O
each	O
time	O
point	O
t	O
(	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
n	O
)	O
as	O
a	O
unit	O
,	O
and	O
the	O
input	O
data	O
St	O
are	O
generated	O
.	O
For	O
example	O
,	O
at	O
the	O
time	O
point	O
t1	O
,	O
learning	O
about	O
the	O
input	O
data	O
1	O
,	O
1	O
and	O
A1	O
is	O
performed	O
with	O
the	O
instructor	O
signal	O
as	O
1	O
,	O
and	O
learning	O
about	O
the	O
input	O
data	O
1	O
,	O
1	O
and	O
A1	O
is	O
then	O
performed	O
with	O
the	O
instructor	O
signal	O
1	O
.	O
Thereafter	O
,	O
learning	O
about	O
the	O
input	O
data	O
1	O
,	O
1	O
and	O
A1	O
is	O
carried	O
out	O
with	O
the	O
instructor	O
signal	O
A1	O
.	O

Upon	O
completion	O
of	O
the	O
learning	O
about	O
all	O
of	O
the	O
combinations	O
of	O
these	O
data	O
,	O
any	O
of	O
data	O
1	O
,	O
1	O
or	O
A1	O
is	O
input	O
to	O
the	O
input	O
layer	O
A1	O
,	O
and	O
the	O
input	O
data	O
S1	O
at	O
time	O
t	O
=	O
1	O
which	O
is	O
to	O
be	O
supplied	O
to	O
the	O
speech	O
input	O
layer	O
32	O
of	O
the	O
neuron	O
device	O
network	O
22	O
is	O
produced	O
from	O
the	O
current	O
output	O
value	O
of	O
the	O
hidden	O
layer	O
AH	O
.	O

Similarly	O
,	O
the	O
input	O
data	O
S2	O
at	O
time	O
t	O
=	O
1	O
which	O
are	O
to	O
be	O
input	O
to	O
the	O
speech	O
input	O
layer	O
32	O
are	O
generated	O
after	O
learning	O
about	O
all	O
the	O
combinations	O
of	O
the	O
input	O
data	O
based	O
on	O
2	O
,	O
2	O
and	O
A2	O
with	O
each	O
of	O
the	O
instructor	O
signals	O
at	O
time	O
t	O
=	O
2	O
.	O
Data	O
S3	O
,	O
S4	O
,	O
.	O
.	O
.	O
,	O
Sn	O
are	O
similarly	O
produced	O
.	O

The	O
learning	O
process	O
is	O
carried	O
out	O
by	O
the	O
neuron	O
device	O
network	O
22	O
in	O
accordance	O
with	O
the	O
input	O
data	O
St	O
(	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
n	O
)	O
generated	O
by	O
the	O
auto-associative	O
neural	O
network	O
27	O
.	O

In	O
the	O
case	O
of	O
the	O
neuron	O
device	O
network	O
22	O
according	O
to	O
the	O
first	O
embodiment	O
,	O
the	O
input	O
data	O
St	O
are	O
input	O
to	O
the	O
speech	O
input	O
layer	O
32	O
and	O
the	O
speech	O
output	O
layer	O
Ou	O
.	O
In	O
other	O
words	O
,	O
when	O
learning	O
about	O
the	O
spectral	O
data	O
at	O
time	O
t	O
=	O
i	O
is	O
performed	O
,	O
the	O
vector	O
row	O
of	O
the	O
input	O
data	O
Si	O
is	O
input	O
to	O
the	O
speech	O
input	O
layer	O
32	O
,	O
and	O
the	O
vector	O
row	O
of	O
the	O
input	O
data	O
S	O
(	O
i	O
+	O
1	O
)	O
is	O
input	O
as	O
the	O
instructor	O
signal	O
to	O
the	O
speech	O
output	O
layer	O
Ou	O
.	O

Input	O
of	O
the	O
instructor	O
signal	O
(	O
the	O
code	O
row	O
representing	O
the	O
phoneme	O
for	O
generating	O
the	O
input	O
data	O
St	O
)	O
to	O
the	O
hypothesis	O
layer	O
38	O
is	O
performed	O
in	O
a	O
manner	O
similar	O
to	O
that	O
in	O
the	O
first	O
embodiment	O
.	O

When	O
learning	O
in	O
the	O
auto-associative	O
neural	O
network	O
27	O
and	O
the	O
neuron	O
device	O
network	O
22	O
is	O
completed	O
according	O
to	O
this	O
manner	O
,	O
the	O
actual	O
speech	O
recognition	O
is	O
carried	O
out	O
as	O
follows	O
.	O

When	O
a	O
sound	O
,	O
which	O
is	O
a	O
target	O
of	O
recognition	O
,	O
is	O
first	O
input	O
from	O
the	O
speech	O
input	O
unit	O
23	O
,	O
spectral	O
analysis	O
is	O
carried	O
out	O
in	O
the	O
FFT	O
unit	O
21	O
,	O
and	O
the	O
vector	O
rows	O
of	O
the	O
powers	O
P	O
(	O
t	O
)	O
relative	O
to	O
respective	O
frequencies	O
at	O
the	O
respective	O
time	O
points	O
t	O
are	O
time-sequentially	O
obtained	O
.	O
The	O
vector	O
rows	O
are	O
stored	O
in	O
the	O
auto-associative	O
neural	O
network	O
27	O
vector	O
row	O
storing	O
area	O
in	O
the	O
RAM	O
14	O
at	O
predetermined	O
intervals	O
of	O
time	O
.	O

The	O
CPU	O
11	O
successively	O
inputs	O
the	O
vector	O
rows	O
P	O
(	O
t	O
)	O
obtained	O
,	O
after	O
spectral	O
analysis	O
of	O
the	O
sound	O
by	O
the	O
FFT	O
unit	O
21	O
is	O
completed	O
,	O
to	O
the	O
input	O
layer	O
A1	O
of	O
the	O
auto-associative	O
neural	O
network	O
27	O
.	O
The	O
auto-associative	O
neural	O
network	O
27	O
supplies	O
output	O
vectors	O
of	O
the	O
hidden	O
layer	O
AH	O
,	O
which	O
correspond	O
with	O
the	O
input	O
vector	O
rows	O
P	O
(	O
t	O
)	O
,	O
to	O
the	O
neuron	O
device	O
network	O
22	O
as	O
the	O
input	O
data	O
St	O
at	O
time	O
t	O
.	O

In	O
the	O
case	O
of	O
the	O
neuron	O
device	O
network	O
22	O
according	O
to	O
the	O
first	O
embodiment	O
,	O
the	O
input	O
data	O
S	O
(	O
t	O
)	O
at	O
each	O
time	O
t	O
(	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
n	O
)	O
are	O
sequentially	O
input	O
to	O
the	O
speech	O
input	O
layer	O
32	O
.	O
An	O
output	O
value	O
corresponding	O
to	O
each	O
input	O
data	O
is	O
input	O
from	O
each	O
neuron	O
device	O
of	O
the	O
hypothesis	O
layer	O
38	O
in	O
the	O
neuron	O
device	O
network	O
22	O
according	O
to	O
the	O
first	O
embodiment	O
.	O

Further	O
,	O
the	O
CPU	O
11	O
specifies	O
the	O
corresponding	O
phoneme	O
by	O
collating	O
output	O
values	O
from	O
the	O
respective	O
neuron	O
devices	O
with	O
the	O
code	O
rows	O
of	O
the	O
second	O
instructor	O
signals	O
stored	O
in	O
the	O
ROM	O
13	O
,	O
and	O
stores	O
the	O
phoneme	O
in	O
the	O
RAM	O
14	O
.	O

As	O
described	O
in	O
connection	O
with	O
the	O
first	O
embodiment	O
,	O
since	O
each	O
of	O
the	O
stored	O
phonemes	O
is	O
analyzed	O
into	O
a	O
plurality	O
of	O
vector	O
rows	O
P	O
(	O
tn	O
)	O
and	O
input	O
to	O
the	O
speech	O
input	O
layer	O
32	O
in	O
time	O
series	O
to	O
be	O
specified	O
,	O
a	O
plurality	O
of	O
phoneme	O
rows	O
are	O
obtained	O
.	O
That	O
is	O
,	O
if	O
a	O
phoneme	O
"	O
iro	O
"	O
is	O
input	O
,	O
"	O
iiiiirrroooo	O
"	O
is	O
obtained	O
,	O
for	O
example	O
.	O
The	O
CPU	O
11	O
therefore	O
recognizes	O
the	O
input	O
speech	O
as	O
"	O
iro	O
"	O
from	O
the	O
phoneme	O
rows	O
stored	O
in	O
the	O
RAM	O
14	O
.	O

The	O
CPU	O
11	O
then	O
transforms	O
the	O
recognized	O
speech	O
into	O
a	O
writing	O
represented	O
by	O
characters	O
in	O
accordance	O
with	O
the	O
,	O
e.g.	O
,	O
Japanese	O
transformation	O
system	O
,	O
and	O
transmits	O
data	O
to	O
various	O
communication	O
units	O
such	O
as	O
a	O
personal	O
computer	O
or	O
a	O
word	O
processor	O
through	O
the	O
communication	O
control	O
unit	O
5	O
and	O
the	O
communication	O
network	O
2	O
.	O

As	O
mentioned	O
above	O
,	O
the	O
vector	O
rows	O
input	O
to	O
the	O
neuron	O
device	O
network	O
22	O
are	O
reduced	O
by	O
using	O
the	O
auto-associative	O
neural	O
network	O
27	O
according	O
to	O
the	O
third	O
embodiment	O
,	O
and	O
a	O
number	O
of	O
neuron	O
devices	O
of	O
the	O
speech	O
input	O
layer	O
32	O
can	O
be	O
similarly	O
decreased	O
.	O
Thus	O
,	O
the	O
structure	O
of	O
the	O
neuron	O
device	O
network	O
22	O
can	O
be	O
made	O
small	O
in	O
scale	O
.	O

According	O
to	O
the	O
third	O
embodiment	O
mentioned	O
above	O
,	O
since	O
a	O
target	O
of	O
learning	O
in	O
the	O
auto-associative	O
type	O
neural	O
network	O
27	O
is	O
all	O
of	O
the	O
combinations	O
of	O
the	O
input	O
data	O
with	O
the	O
instructor	O
signals	O
for	O
each	O
pattern	O
of	O
the	O
phoneme	O
,	O
the	O
hidden	O
layer	O
AH	O
can	O
produce	O
the	O
generalized	O
vector	O
rows	O
St	O
(	O
t	O
=	O
1	O
to	O
n	O
)	O
of	O
that	O
phoneme	O
.	O

Additionally	O
,	O
instead	O
of	O
the	O
combinations	O
for	O
each	O
pattern	O
of	O
all	O
the	O
phonemes	O
,	O
the	O
same	O
patterns	O
may	O
be	O
used	O
for	O
the	O
input	O
data	O
of	O
the	O
input	O
layer	O
AI	O
and	O
the	O
instructor	O
signals	O
of	O
the	O
output	O
layer	O
AO	O
.	O

As	O
an	O
input	O
to	O
the	O
input	O
layer	O
AI	O
of	O
the	O
auto-associative	O
neural	O
network	O
27	O
during	O
learning	O
or	O
recognition	O
,	O
the	O
data	O
which	O
have	O
been	O
spectral-analyzed	O
by	O
the	O
FFT	O
unit	O
21	O
are	O
used	O
in	O
the	O
above-described	O
third	O
embodiment	O
.	O
On	O
the	O
other	O
hand	O
,	O
the	O
input	O
data	O
St	O
of	O
the	O
neuron	O
device	O
network	O
22	O
may	O
be	O
produced	O
by	O
inputting	O
the	O
cepstrum	O
data	O
to	O
the	O
input	O
layer	O
AI	O
of	O
the	O
auto-associative	O
neural	O
network	O
27	O
.	O

In	O
the	O
third	O
embodiment	O
mentioned	O
above	O
,	O
when	O
performing	O
speech	O
recognition	O
,	O
the	O
vector	O
rows	O
P	O
(	O
t	O
)	O
which	O
have	O
been	O
spectral-analyzed	O
by	O
the	O
FFT	O
unit	O
21	O
are	O
successively	O
input	O
to	O
the	O
input	O
layer	O
AI	O
of	O
the	O
auto-associative	O
neural	O
network	O
27	O
,	O
and	O
the	O
output	O
vectors	O
from	O
the	O
hidden	O
layer	O
AH	O
are	O
immediately	O
output	O
to	O
the	O
neuron	O
device	O
network	O
22	O
as	O
the	O
input	O
data	O
St	O
at	O
time	O
t	O
.	O

However	O
,	O
the	O
auto-associative	O
neural	O
network	O
27	O
may	O
be	O
used	O
as	O
a	O
filter	O
for	O
judging	O
whether	O
recognition	O
of	O
the	O
speech	O
uttered	O
by	O
a	O
speaker	O
is	O
possible	O
by	O
the	O
neuron	O
device	O
network	O
22	O
in	O
which	O
learning	O
has	O
been	O
carried	O
out	O
for	O
infinite	O
speakers	O
.	O
In	O
other	O
words	O
,	O
learning	O
for	O
speaker-independent	O
recognition	O
is	O
previously	O
carried	O
out	O
in	O
the	O
auto-associative	O
neural	O
network	O
27	O
with	O
respect	O
to	O
a	O
specific	O
keyword	O
using	O
data	O
for	O
infinite	O
or	O
generic	O
speaker	O
which	O
are	O
utilized	O
in	O
learning	O
process	O
of	O
the	O
neuron	O
device	O
network	O
22	O
.	O

The	O
speaker	O
then	O
utters	O
and	O
inputs	O
the	O
specific	O
keyword	O
to	O
the	O
speech	O
input	O
unit	O
23	O
when	O
performing	O
speech	O
recognition	O
.	O
The	O
input	O
keyword	O
is	O
spectral-analyzed	O
by	O
the	O
FFT	O
unit	O
21	O
and	O
input	O
to	O
the	O
input	O
layer	O
AI	O
of	O
the	O
auto-associative	O
neural	O
network	O
27	O
,	O
and	O
the	O
input	O
data	O
St	O
is	O
generated	O
from	O
the	O
output	O
values	O
of	O
the	O
hidden	O
layer	O
AH	O
.	O
The	O
input	O
data	O
St	O
of	O
the	O
speaker	O
are	O
compared	O
with	O
the	O
data	O
St	O
used	O
when	O
learning	O
was	O
initially	O
carried	O
out	O
for	O
infinite	O
speakers	O
,	O
and	O
if	O
both	O
data	O
are	O
significantly	O
different	O
from	O
each	O
other	O
,	O
it	O
can	O
be	O
judged	O
that	O
recognition	O
of	O
the	O
speech	O
of	O
the	O
speaker	O
by	O
the	O
input	O
neuron	O
device	O
network	O
22	O
may	O
not	O
be	O
able	O
to	O
be	O
performed	O
.	O

It	O
may	O
be	O
possible	O
to	O
judge	O
whether	O
recognition	O
of	O
the	O
speech	O
of	O
the	O
speaker	O
is	O
enabled	O
by	O
inputting	O
spectral	O
data	O
of	O
arbitrary	O
spectral	O
data	O
of	O
the	O
speaker	O
to	O
the	O
auto-associative	O
neural	O
network	O
27	O
in	O
which	O
learning	O
of	O
speech	O
of	O
infinite	O
speakers	O
has	O
been	O
already	O
been	O
completed	O
,	O
and	O
comparing	O
the	O
output	O
data	O
from	O
the	O
output	O
layer	O
AO	O
with	O
the	O
input	O
data	O
and	O
judging	O
whether	O
auto-association	O
has	O
been	O
substantially	O
made	O
.	O

Further	O
,	O
although	O
the	O
invention	O
has	O
been	O
described	O
herein	O
with	O
reference	O
to	O
particular	O
means	O
,	O
materials	O
and	O
embodiments	O
,	O
the	O
invention	O
is	O
not	O
intended	O
to	O
be	O
limited	O
to	O
the	O
particulars	O
disclosed	O
herein	O
;	O
rather	O
,	O
the	O
invention	O
extends	O
to	O
all	O
functionally	O
equivalent	O
structures	O
,	O
methods	O
and	O
uses	O
,	O
such	O
as	O
are	O
within	O
the	O
scope	O
of	O
the	O
appended	O
claims	O
.	O

The	O
present	O
disclosure	O
relates	O
to	O
subject	O
matter	O
contained	O
in	O
Japanese	O
Patent	O
Application	O
No.	O
336135/1994	O
,	O
filed	O
Dec.	O
22	O
,	O
1994	O
,	O
and	O
Japanese	O
Patent	O
Application	O
No.	O
236061/1995	O
,	O
filed	O
Aug.	O
22	O
,	O
1995	O
,	O
which	O
are	O
expressly	O
incorporated	O
herein	O
by	O
reference	O
in	O
their	O
entireties	O
.	O

1	O
.	O
A	O
learning	O
method	O
of	O
a	O
neural	O
network	O
comprising	O
:	O
inputting	O
first	O
vector	O
rows	O
representing	O
data	O
to	O
a	O
data	O
input	O
layer	O
;	O
inputting	O
second	O
vector	O
rows	O
as	O
a	O
first	O
instructor	O
signal	O
to	O
a	O
first	O
output	O
layer	O
,	O
the	O
first	O
instructor	O
signal	O
comprising	O
vector	O
rows	O
input	O
to	O
the	O
data	O
input	O
layer	O
at	O
a	O
time	O
after	O
inputting	O
the	O
first	O
vector	O
rows	O
;	O
and	O
inputting	O
a	O
definite	O
meaning	O
as	O
a	O
second	O
instructor	O
signal	O
to	O
a	O
second	O
output	O
layer	O
,	O
the	O
second	O
instructor	O
signal	O
being	O
determined	O
in	O
accordance	O
with	O
vector	O
rows	O
input	O
to	O
the	O
data	O
input	O
layer	O
at	O
times	O
before	O
and	O
after	O
the	O
first	O
vector	O
rows	O
are	O
input	O
to	O
the	O
data	O
input	O
layer	O
,	O
and	O
performing	O
learning	O
for	O
the	O
data	O
by	O
having	O
a	O
plurality	O
of	O
first	O
vector	O
rows	O
represent	O
the	O
definite	O
meaning	O
.	O

2	O
.	O
The	O
learning	O
method	O
of	O
a	O
neural	O
network	O
according	O
to	O
claim	O
1	O
wherein	O
learning	O
is	O
performed	O
in	O
accordance	O
with	O
error	O
back-propagation	O
.	O

3	O
.	O
A	O
learning	O
method	O
of	O
a	O
neural	O
network	O
comprising	O
:	O
inputting	O
first	O
vector	O
rows	O
representing	O
data	O
to	O
a	O
data	O
input	O
layer	O
;	O
inputting	O
second	O
vector	O
rows	O
as	O
a	O
first	O
instructor	O
signal	O
to	O
a	O
first	O
output	O
layer	O
;	O
and	O
inputting	O
a	O
definite	O
meaning	O
as	O
a	O
second	O
instructor	O
signal	O
to	O
a	O
second	O
output	O
layer	O
,	O
the	O
definite	O
meaning	O
is	O
phoneme	O
data	O
representing	O
a	O
speech	O
element	O
and	O
the	O
plurality	O
of	O
first	O
vector	O
rows	O
represent	O
characteristics	O
of	O
the	O
definite	O
meaning	O
analyzed	O
in	O
a	O
time	O
series	O
;	O
and	O
performing	O
learning	O
for	O
the	O
data	O
by	O
having	O
a	O
plurality	O
of	O
first	O
vector	O
rows	O
represent	O
the	O
definite	O
meaning	O
.	O

4	O
.	O
A	O
learning	O
method	O
of	O
a	O
neural	O
network	O
according	O
to	O
claim	O
3	O
,	O
wherein	O
one	O
of	O
spectral	O
data	O
,	O
cepstrum	O
data	O
of	O
the	O
speech	O
,	O
and	O
an	O
output	O
value	O
data	O
of	O
the	O
hidden	O
layer	O
of	O
an	O
auto-associative	O
neural	O
network	O
is	O
used	O
as	O
the	O
plurality	O
of	O
first	O
vectors	O
representing	O
the	O
characteristics	O
of	O
the	O
definite	O
meaning	O
.	O

5	O
.	O
A	O
learning	O
method	O
of	O
a	O
neural	O
network	O
comprising	O
:	O
inputting	O
,	O
to	O
a	O
feedback	O
input	O
layer	O
,	O
output	O
vector	O
values	O
of	O
a	O
hidden	O
layer	O
having	O
a	O
plurality	O
of	O
neuron	O
devices	O
,	O
the	O
plurality	O
of	O
neuron	O
devices	O
corresponding	O
to	O
first	O
vector	O
rows	O
of	O
the	O
feedback	O
input	O
layer	O
,	O
the	O
feedback	O
input	O
layer	O
connected	O
with	O
the	O
hidden	O
layer	O
and	O
having	O
a	O
number	O
of	O
neuron	O
devices	O
equal	O
to	O
a	O
number	O
of	O
neuron	O
devices	O
of	O
the	O
hidden	O
layer	O
;	O
inputting	O
second	O
vector	O
rows	O
representing	O
data	O
to	O
a	O
data	O
input	O
layer	O
;	O
inputting	O
third	O
vector	O
rows	O
as	O
a	O
first	O
instructor	O
signal	O
to	O
a	O
first	O
output	O
layer	O
,	O
the	O
first	O
instructor	O
signal	O
comprising	O
vector	O
rows	O
input	O
to	O
the	O
data	O
input	O
layer	O
at	O
a	O
time	O
after	O
inputting	O
the	O
first	O
vector	O
rows	O
;	O
and	O
inputting	O
a	O
definite	O
meaning	O
as	O
a	O
second	O
instructor	O
signal	O
to	O
a	O
second	O
output	O
layer	O
,	O
the	O
second	O
instructor	O
signal	O
being	O
determined	O
in	O
accordance	O
with	O
vector	O
rows	O
input	O
to	O
the	O
data	O
input	O
layer	O
at	O
times	O
before	O
and	O
after	O
the	O
first	O
vector	O
rows	O
are	O
input	O
to	O
the	O
data	O
input	O
layer	O
;	O
and	O
performing	O
learning	O
for	O
the	O
data	O
by	O
having	O
a	O
plurality	O
of	O
second	O
vector	O
rows	O
represent	O
the	O
definite	O
meaning	O
.	O

6	O
.	O
The	O
learning	O
method	O
of	O
a	O
neural	O
network	O
according	O
to	O
claim	O
5	O
,	O
wherein	O
learning	O
is	O
performed	O
in	O
accordance	O
with	O
error	O
back-propagation	O
.	O

7	O
.	O
A	O
learning	O
method	O
of	O
a	O
neural	O
network	O
comprising	O
:	O
inputting	O
to	O
a	O
feedback	O
input	O
layer	O
,	O
output	O
vector	O
values	O
of	O
a	O
hidden	O
layer	O
having	O
a	O
plurality	O
of	O
neuron	O
devices	O
,	O
the	O
plurality	O
of	O
neuron	O
devices	O
corresponding	O
to	O
first	O
vector	O
rows	O
of	O
the	O
feedback	O
input	O
layer	O
,	O
the	O
feedback	O
input	O
layer	O
connected	O
with	O
the	O
hidden	O
layer	O
and	O
having	O
a	O
number	O
of	O
neuron	O
devices	O
equal	O
to	O
a	O
number	O
of	O
neuron	O
devices	O
of	O
the	O
hidden	O
layer	O
;	O
inputting	O
second	O
vector	O
rows	O
representing	O
data	O
to	O
a	O
data	O
input	O
layer	O
;	O
inputting	O
third	O
vector	O
rows	O
as	O
a	O
first	O
instructor	O
signal	O
to	O
a	O
first	O
output	O
layer	O
,	O
and	O
inputting	O
a	O
definite	O
meaning	O
as	O
a	O
second	O
instructor	O
signal	O
to	O
a	O
second	O
output	O
layer	O
;	O
and	O
performing	O
learning	O
for	O
the	O
data	O
by	O
having	O
a	O
plurality	O
of	O
second	O
vector	O
rows	O
represent	O
the	O
definite	O
meaning	O
,	O
wherein	O
the	O
definite	O
meaning	O
is	O
phoneme	O
data	O
representing	O
a	O
speech	O
element	O
and	O
the	O
plurality	O
of	O
second	O
vector	O
rows	O
represent	O
characteristics	O
of	O
the	O
definite	O
meaning	O
analyzed	O
in	O
a	O
time	O
series	O
.	O

8	O
.	O
A	O
learning	O
method	O
of	O
a	O
neural	O
network	O
according	O
to	O
claim	O
7	O
,	O
wherein	O
one	O
of	O
spectral	O
data	O
,	O
cepstrum	O
data	O
of	O
the	O
speech	O
,	O
and	O
an	O
output	O
value	O
data	O
of	O
the	O
hidden	O
layer	O
of	O
an	O
auto-associative	O
neural	O
network	O
is	O
used	O
as	O
the	O
plurality	O
of	O
second	O
vector	O
rows	O
representing	O
the	O
characteristics	O
of	O
the	O
definite	O
meaning	O
.	O

9	O
.	O
A	O
learning	O
method	O
of	O
a	O
neural	O
network	O
comprising	O
:	O
inputting	O
output	O
vector	O
values	O
of	O
a	O
first	O
output	O
layer	O
corresponding	O
to	O
first	O
vector	O
rows	O
of	O
a	O
feedback	O
input	O
layer	O
,	O
the	O
feedback	O
input	O
layer	O
connected	O
to	O
a	O
hidden	O
layer	O
that	O
has	O
neuron	O
devices	O
whose	O
number	O
is	O
equal	O
to	O
that	O
of	O
neuron	O
devices	O
of	O
the	O
first	O
output	O
layer	O
;	O
inputting	O
second	O
vector	O
rows	O
to	O
a	O
data	O
input	O
layer	O
;	O
inputting	O
third	O
vector	O
tows	O
as	O
a	O
first	O
instructor	O
signal	O
to	O
the	O
first	O
output	O
layer	O
,	O
the	O
first	O
instructor	O
signal	O
comprising	O
vector	O
rows	O
input	O
to	O
the	O
data	O
input	O
layer	O
at	O
a	O
time	O
after	O
inputting	O
the	O
first	O
vector	O
rows	O
;	O
and	O
inputting	O
a	O
definite	O
meaning	O
as	O
a	O
second	O
instructor	O
signal	O
to	O
a	O
second	O
output	O
layer	O
,	O
the	O
second	O
instructor	O
signal	O
being	O
determined	O
in	O
accordance	O
with	O
vector	O
rows	O
input	O
to	O
the	O
data	O
input	O
layer	O
at	O
times	O
before	O
and	O
after	O
the	O
first	O
vector	O
rows	O
are	O
input	O
to	O
the	O
data	O
input	O
layer	O
,	O
wherein	O
learning	O
is	O
carried	O
out	O
for	O
data	O
having	O
a	O
plurality	O
of	O
the	O
second	O
vector	O
rows	O
representing	O
the	O
definite	O
meaning	O
.	O

10	O
.	O
A	O
learning	O
method	O
of	O
a	O
neural	O
network	O
according	O
to	O
claim	O
9	O
,	O
wherein	O
learning	O
is	O
performed	O
in	O
accordance	O
with	O
error	O
back-propagation	O
.	O

11	O
.	O
A	O
learning	O
method	O
of	O
a	O
neural	O
network	O
comprising	O
:	O
inputting	O
output	O
vector	O
values	O
of	O
a	O
first	O
output	O
layer	O
corresponding	O
to	O
first	O
vector	O
rows	O
of	O
a	O
feedback	O
input	O
layer	O
,	O
the	O
feedback	O
input	O
layer	O
connected	O
with	O
a	O
hidden	O
layer	O
and	O
having	O
neuron	O
devices	O
whose	O
number	O
is	O
equal	O
to	O
that	O
of	O
neuron	O
devices	O
of	O
the	O
first	O
output	O
layer	O
;	O
inputting	O
second	O
vector	O
rows	O
to	O
a	O
data	O
input	O
layer	O
;	O
inputting	O
third	O
vector	O
rows	O
as	O
a	O
first	O
instructor	O
signal	O
to	O
the	O
first	O
output	O
layer	O
;	O
and	O
inputting	O
a	O
definite	O
meaning	O
as	O
a	O
second	O
instructor	O
signal	O
to	O
a	O
second	O
output	O
layer	O
,	O
wherein	O
the	O
definite	O
meaning	O
is	O
phoneme	O
data	O
representing	O
a	O
speech	O
element	O
and	O
the	O
plurality	O
of	O
second	O
vector	O
rows	O
represent	O
characteristics	O
of	O
the	O
definite	O
meaning	O
analyzed	O
in	O
a	O
time	O
series	O
,	O
wherein	O
learning	O
is	O
carried	O
out	O
for	O
data	O
having	O
a	O
plurality	O
of	O
the	O
second	O
vector	O
rows	O
representing	O
the	O
definite	O
meaning	O
.	O

12	O
.	O
A	O
learning	O
method	O
of	O
a	O
neural	O
network	O
according	O
to	O
claim	O
11	O
,	O
wherein	O
one	O
of	O
spectral	O
data	O
,	O
cepstrum	O
data	O
of	O
the	O
speech	O
,	O
and	O
an	O
output	O
value	O
data	O
of	O
the	O
hidden	O
layer	O
of	O
an	O
auto-associative	O
neural	O
network	O
is	O
used	O
as	O
the	O
plurality	O
of	O
second	O
vector	O
rows	O
representing	O
the	O
characteristics	O
of	O
the	O
definite	O
meaning	O
.	O

13	O
.	O
A	O
neural	O
network	O
comprising	O
:	O
a	O
neuron	O
device	O
network	O
having	O
a	O
data	O
input	O
layer	O
,	O
a	O
hidden	O
layer	O
connected	O
to	O
said	O
data	O
input	O
layer	O
,	O
and	O
an	O
output	O
layer	O
connected	O
to	O
said	O
hidden	O
layer	O
,	O
said	O
output	O
layer	O
comprising	O
a	O
first	O
output	O
layer	O
and	O
a	O
second	O
output	O
layer	O
;	O
learning	O
means	O
in	O
said	O
neuron	O
device	O
network	O
for	O
learning	O
about	O
data	O
having	O
a	O
plurality	O
of	O
first	O
vector	O
rows	O
representing	O
a	O
definite	O
meaning	O
,	O
said	O
learning	O
means	O
inputting	O
said	O
plurality	O
of	O
first	O
vector	O
rows	O
to	O
said	O
data	O
input	O
layer	O
,	O
inputting	O
second	O
vector	O
rows	O
as	O
a	O
first	O
instructor	O
signal	O
to	O
said	O
first	O
output	O
layer	O
and	O
inputting	O
said	O
definite	O
meaning	O
as	O
a	O
second	O
instructor	O
signal	O
to	O
said	O
second	O
output	O
layer	O
;	O
inputting	O
means	O
for	O
inputting	O
said	O
plurality	O
of	O
first	O
vector	O
rows	O
to	O
said	O
data	O
input	O
layer	O
of	O
said	O
neuron	O
device	O
network	O
;	O
and	O
outputting	O
means	O
for	O
outputting	O
output	O
signals	O
of	O
said	O
second	O
output	O
layer	O
based	O
on	O
input	O
of	O
said	O
plurality	O
of	O
first	O
vector	O
rows	O
by	O
said	O
inputting	O
means	O
.	O

14	O
.	O
The	O
neural	O
network	O
according	O
to	O
claim	O
13	O
,	O
further	O
comprising	O
a	O
plurality	O
of	O
hidden	O
layers	O
at	O
least	O
equal	O
to	O
a	O
number	O
of	O
definite	O
meanings	O
;	O
wherein	O
said	O
data	O
input	O
layer	O
and	O
said	O
output	O
layer	O
are	O
connected	O
with	O
each	O
other	O
,	O
and	O
values	O
corresponding	O
with	O
said	O
output	O
signals	O
relative	O
to	O
a	O
plurality	O
of	O
third	O
vector	O
rows	O
are	O
fed	O
back	O
and	O
input	O
to	O
respective	O
hidden	O
layers	O
.	O

15	O
.	O
The	O
neural	O
network	O
according	O
to	O
claim	O
13	O
,	O
wherein	O
said	O
data	O
input	O
layer	O
,	O
said	O
hidden	O
layer	O
and	O
said	O
second	O
output	O
layer	O
each	O
have	O
connection	O
weights	O
between	O
respective	O
neuron	O
devices	O
.	O

16	O
.	O
A	O
neural	O
network	O
comprising	O
:	O
a	O
neuron	O
device	O
network	O
comprising	O
an	O
input	O
layer	O
having	O
a	O
data	O
input	O
layer	O
and	O
a	O
feedback	O
input	O
layer	O
;	O
a	O
hidden	O
layer	O
connected	O
to	O
said	O
input	O
layer	O
;	O
and	O
an	O
output	O
layer	O
connected	O
to	O
said	O
hidden	O
layer	O
,	O
said	O
output	O
layer	O
having	O
a	O
first	O
output	O
layer	O
and	O
a	O
second	O
output	O
layer	O
;	O
learning	O
means	O
in	O
said	O
neuron	O
device	O
network	O
for	O
learning	O
about	O
data	O
having	O
a	O
plurality	O
of	O
first	O
vector	O
rows	O
representing	O
a	O
definite	O
meaning	O
by	O
:	O
(	O
1	O
)	O
inputting	O
a	O
plurality	O
of	O
second	O
vector	O
values	O
of	O
said	O
hidden	O
layer	O
or	O
said	O
first	O
output	O
layer	O
,	O
to	O
said	O
input	O
layer	O
,	O
(	O
2	O
)	O
inputting	O
said	O
plurality	O
of	O
first	O
vector	O
rows	O
to	O
said	O
data	O
input	O
layer	O
of	O
the	O
input	O
layer	O
,	O
(	O
3	O
)	O
inputting	O
a	O
plurality	O
of	O
third	O
vector	O
rows	O
as	O
a	O
first	O
instructor	O
signal	O
to	O
said	O
first	O
output	O
layer	O
,	O
and	O
(	O
4	O
)	O
inputting	O
said	O
definite	O
meaning	O
as	O
a	O
second	O
instructor	O
signal	O
to	O
said	O
second	O
output	O
layer	O
;	O
inputting	O
means	O
for	O
inputting	O
said	O
plurality	O
of	O
first	O
vector	O
rows	O
to	O
said	O
data	O
input	O
layer	O
of	O
said	O
neuron	O
device	O
network	O
such	O
that	O
said	O
learning	O
means	O
performs	O
said	O
learning	O
;	O
and	O
outputting	O
means	O
for	O
outputting	O
output	O
signals	O
of	O
said	O
second	O
output	O
layer	O
based	O
on	O
input	O
of	O
said	O
plurality	O
of	O
first	O
vector	O
rows	O
by	O
said	O
inputting	O
means	O
,	O
said	O
input	O
layer	O
having	O
a	O
plurality	O
of	O
neuron	O
devices	O
equal	O
in	O
number	O
to	O
a	O
number	O
of	O
neuron	O
devices	O
of	O
said	O
hidden	O
layer	O
or	O
said	O
first	O
output	O
layer	O
.	O

17	O
.	O
The	O
neural	O
network	O
according	O
to	O
claim	O
16	O
,	O
wherein	O
said	O
data	O
input	O
layer	O
,	O
said	O
feedback	O
input	O
layer	O
,	O
said	O
hidden	O
layer	O
and	O
said	O
second	O
output	O
layer	O
each	O
have	O
connection	O
weights	O
between	O
respective	O
neuron	O
devices	O
.	O

18	O
.	O
A	O
speech	O
recognition	O
apparatus	O
comprising	O
:	O
a	O
neural	O
network	O
,	O
said	O
neural	O
network	O
including	O
an	O
input	O
layer	O
comprising	O
a	O
speech	O
input	O
layer	O
and	O
a	O
context	O
layer	O
,	O
a	O
hidden	O
layer	O
,	O
and	O
an	O
output	O
layer	O
comprising	O
a	O
speech	O
output	O
layer	O
and	O
a	O
hypothesis	O
layer	O
;	O
speech	O
inputting	O
means	O
for	O
inputting	O
speech	O
;	O
analyzing	O
means	O
for	O
analyzing	O
in	O
a	O
time-series	O
,	O
vector	O
rows	O
representing	O
characteristics	O
of	O
the	O
speech	O
input	O
by	O
said	O
speech	O
inputting	O
means	O
;	O
vector	O
row	O
inputting	O
means	O
for	O
successively	O
inputting	O
said	O
vector	O
rows	O
analyzed	O
by	O
said	O
analyzing	O
means	O
to	O
said	O
input	O
layer	O
of	O
said	O
neural	O
network	O
;	O
and	O
phoneme	O
specifying	O
means	O
for	O
specifying	O
a	O
phoneme	O
in	O
accordance	O
with	O
outputs	O
of	O
said	O
output	O
layer	O
of	O
said	O
neural	O
network	O
by	O
successively	O
inputting	O
said	O
vector	O
rows	O
to	O
said	O
data	O
input	O
layer	O
by	O
said	O
vector	O
row	O
inputting	O
means	O
.	O

19	O
.	O
The	O
neural	O
network	O
according	O
to	O
claim	O
18	O
,	O
wherein	O
said	O
analyzing	O
means	O
uses	O
one	O
of	O
spectral	O
data	O
,	O
cepstrum	O
data	O
of	O
the	O
speech	O
,	O
and	O
an	O
output	O
value	O
data	O
of	O
a	O
hidden	O
layer	O
of	O
an	O
auto-associative	O
neural	O
network	O
as	O
said	O
vector	O
rows	O
representing	O
said	O
characteristics	O
of	O
the	O
speech	O
.	O

