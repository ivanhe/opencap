US	O
6853972	O
B2	O
20050208	O

US	O
10206202	O
20020729	O

10	O
eng	O
eng	O

DE	O
10003550	O
20000727	O

20050208	O

20050208	O

7G	O
10L	O
15/00	O
A	O
7	O
G	O
10	O
L	O
15	O
00	O
A	O

G06F	O
3/00	O
20060101A	O
I20051110RMEP	O

20060101	O

A	O
G	O
06	O
F	O
3	O
00	O
I	O

20051110	O

EP	O

R	O
M	O

G06F	O
3/16	O
20060101A	O
I20051110RMEP	O

20060101	O

A	O
G	O
06	O
F	O
3	O
16	O
I	O

20051110	O

EP	O

R	O
M	O

G10L	O
11/00	O
20060101A	O
I20051110RMEP	O

20060101	O

A	O
G	O
10	O
L	O
11	O
00	O
I	O

20051110	O

EP	O

R	O
M	O

G10L	O
15/00	O
20060101A	O
I20051110RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
00	O
I	O

20051110	O

EP	O

R	O
M	O

G10L	O
15/26	O
20060101A	O
I20051110RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
26	O
I	O

20051110	O

EP	O

R	O
M	O

US	O

704/275	O
704	O
275	O

715/727	O
715	O
727	O

G10L	O
15/24	O
G	O
10	O
L	O
15	O
24	O

G06F	O
3/01B4	O
G	O
06	O
F	O
3	O
01	O

B	O
4	O

G06K	O
9/00S	O
G	O
06	O
K	O
9	O
00	O

S	O

S10L	O
15	O
:	O
22N2	O
S	O
10	O
L	O
15	O
22	O

N	O
2	O

US	O

704/275	O
704	O
275	O

US	O

704/270	O
704	O
270	O

US	O

704/246	O
704	O
246	O

US	O

704/251	O
704	O
251	O

US	O

704/272	O
704	O
272	O

US	O

345/727	O
345	O
727	O

US	O

345/728	O
345	O
728	O

US	O

351/209	O
351	O
209	O

19	O
System	O
and	O
method	O
for	O
eye	O
tracking	O
controlled	O
speech	O
processing	O

US	O
5668573	O
A	O
Favot	O
et_al.	O

19970916	O

19950606	O

US	O
5864815	O
A	O
Rozak	O
et_al.	O

19990126	O

19950731	O

US	O
5912721	O
A	O
Yamaguchi	O
et_al.	O

19990615	O

19970312	O

US	O
6076061	O
A	O
Kawasaki	O
et_al.	O

20000613	O

19950908	O

US	O
6111580	O
A	O
Kazama	O
et_al.	O

20000829	O

19960906	O

US	O
6243683	O
B1	O
Peters	O
20010605	O

19981229	O

DE	O
4306508	O
A1	O
19940908	O

19930303	O

DE	O
4307590	O
A1	O
19940915	O

19930310	O

DE	O
19731303	O
A1	O
19990204	O

19970713	O

EP	O
718823	O
A2	O
19960626	O

19951211	O

JP	O
04372012	O
A	O
19921225	O

19910620	O

JP	O
04372012	O
A	O
19921225	O

19910620	O

WO	O
93014454	O
A1	O
19930700	O

US	O
7548833	O
B2	O
20090616	O

20050819	O

US	O
7219062	O
B2	O
20070515	O

20020130	O

US	O
7082393	O
B2	O
20060725	O

20020327	O

US	O
7904300	O
B2	O
20110308	O

20050810	O

US	O
7920071	O
B2	O
20110405	O

20070308	O

US	O
PCTDE01/00137	O
20010115	O

PENDING	O

US	O
PCTDE01/00137	O
20010115	O

US	O
10206202	O

US	O
20030040914	O
A1	O
20030227	O

Siemens	O
Aktiengesellschaft	O
03	O

Munich	O
DE	O

Friedrich	O
Wolfgang	O

Bubenreuth	O
DE	O

DE	O

DE	O

Wohlgemuth	O
Wolfgang	O

Erlangen	O
DE	O

DE	O

DE	O

Ye	O
Xin	O

Erlangen	O
DE	O

DE	O

DE	O

Sughrue	O
Mion	O
,	O
PLLC	O

Abebe	O
Daniel	O
2655	O

WO	O
DE01000137	O
20010115	O

US	O
20030040914	O
A1	O
20030227	O

20020729	O

US	O
6853972	O
B2	O
20050208	O

20020729	O

US	O
20030040914	O
A1	O
20030227	O

20020729	O

US	O
6853972	O
B2	O
20050208	O

20020729	O

A	O
system	O
and	O
method	O
for	O
operating	O
and	O
monitoring	O
,	O
in	O
particular	O
,	O
an	O
automation	O
system	O
and/or	O
a	O
production	O
machine	O
and/or	O
machine	O
tool	O
.	O
The	O
visual	O
field	O
(	O
9	O
)	O
of	O
a	O
user	O
(	O
1	O
)	O
directed	O
onto	O
at	O
least	O
one	O
display	O
(	O
2	O
)	O
is	O
recorded	O
and	O
an	O
item	O
of	O
speech	O
information	O
(	O
8	O
)	O
of	O
the	O
user	O
(	O
1	O
)	O
is	O
evaluated	O
at	O
least	O
intermittently	O
,	O
such	O
that	O
specifically	O
predefined	O
information	O
data	O
,	O
which	O
is	O
linked	O
to	O
the	O
recorded	O
visual	O
field	O
(	O
9	O
)	O
and	O
to	O
the	O
recorded	O
speech	O
information	O
,	O
is	O
depicted	O
on	O
the	O
display	O
(	O
2	O
)	O
.	O
The	O
predefined	O
information	O
data	O
is	O
displayed	O
in	O
accordance	O
with	O
the	O
speech	O
information	O
(	O
8	O
)	O
that	O
is	O
given	O
by	O
the	O
user	O
(	O
1	O
)	O
and	O
recognized	O
by	O
a	O
speech	O
recognition	O
component	O
(	O
4	O
)	O
and	O
in	O
accordance	O
with	O
the	O
recorded	O
visual	O
field	O
(	O
9	O
)	O
.	O
This	O
provides	O
a	O
hands-free	O
operation	O
of	O
the	O
system	O
,	O
machine	O
and/or	O
tool	O
,	O
and	O
enables	O
the	O
user	O
to	O
navigate	O
in	O
an	O
environment	O
of	O
augmented	O
reality	O
applications	O
even	O
when	O
complex	O
technical	O
systems	O
are	O
involved	O
.	O

20021106	O

AS	O
ASSIGNMENT	O
N	O
US	O
6853972B2	O
SIEMENS	O
AKTIENGESELLSCHAFT	O
,	O
GERMANY	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNORS:FRIEDRICH	O
,	O
WOLFGANG;WOHLGEMUTH	O
,	O
WOLFGANG;YE	O
,	O
XIN;REEL/FRAME:013468/0485	O

20020906	O

20080715	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
6853972B2	O
4	O

This	O
is	O
a	O
Continuation	O
of	O
International	O
Application	O
PCT/DE01/00137	O
,	O
with	O
an	O
international	O
filing	O
date	O
of	O
Jan.	O
15	O
,	O
2001	O
,	O
which	O
was	O
published	O
under	O
PCT	O
Article	O
21	O
(	O
2	O
)	O
in	O
German	O
,	O
and	O
the	O
disclosure	O
of	O
which	O
is	O
incorporated	O
into	O
this	O
application	O
by	O
reference	O
.	O

FIELD	O
OF	O
AND	O
BACKGROUND	O
OF	O
THE	O
INVENTION	O
The	O
invention	O
relates	O
to	O
a	O
system	O
and	O
method	O
for	O
operating	O
and	O
monitoring	O
,	O
for	O
example	O
,	O
an	O
automation	O
system	O
and/or	O
a	O
production	O
machine	O
and/or	O
machine	O
tool	O
.	O

Such	O
systems	O
and	O
methods	O
are	O
used	O
,	O
for	O
example	O
,	O
in	O
the	O
field	O
of	O
automation	O
technology	O
,	O
in	O
production	O
machines	O
and	O
machine	O
tools	O
,	O
in	O
diagnostic/service	O
support	O
systems	O
,	O
and	O
for	O
complex	O
components	O
,	O
devices	O
and	O
systems	O
,	O
such	O
as	O
,	O
for	O
instance	O
,	O
vehicles	O
and	O
industrial	O
machinery	O
and	O
plants	O
.	O

OBJECTS	O
OF	O
THE	O
INVENTION	O
One	O
object	O
of	O
the	O
invention	O
is	O
to	O
provide	O
a	O
monitoring	O
and	O
operating	O
system	O
and	O
an	O
associated	O
method	O
,	O
in	O
particular	O
for	O
an	O
automation	O
system	O
and/or	O
a	O
production	O
machine	O
and/or	O
a	O
machine	O
tool	O
,	O
which	O
enable	O
hands-free	O
operation	O
by	O
persons	O
utilizing	O
the	O
system	O
.	O
It	O
is	O
another	O
object	O
to	O
provide	O
such	O
a	O
system	O
and	O
method	O
that	O
enable	O
users	O
to	O
“	O
navigate	O
”	O
in	O
an	O
environment	O
of	O
augmented	O
reality	O
applications	O
even	O
where	O
complex	O
technical	O
systems	O
are	O
involved	O
.	O

SUMMARY	O
OF	O
THE	O
INVENTION	O
These	O
and	O
other	O
objects	O
are	O
attained	O
,	O
according	O
to	O
one	O
formulation	O
of	O
the	O
invention	O
,	O
by	O
a	O
system	O
that	O
has	O
a	O
detection	O
means	O
for	O
detecting	O
a	O
visual	O
field	O
of	O
a	O
user	O
being	O
directed	O
onto	O
a	O
display	O
;	O
speech	O
recognition	O
means	O
for	O
recognizing	O
speech	O
information	O
of	O
the	O
user	O
;	O
and	O
control	O
means	O
for	O
controlling	O
the	O
system	O
;	O
wherein	O
,	O
as	O
a	O
function	O
of	O
the	O
speech	O
information	O
given	O
by	O
the	O
user	O
and	O
recognized	O
by	O
the	O
speech	O
recognition	O
means	O
,	O
and	O
as	O
a	O
function	O
of	O
the	O
visual	O
field	O
recorded	O
by	O
the	O
detection	O
means	O
,	O
the	O
control	O
means	O
displays	O
specifically	O
defined	O
information	O
data	O
linked	O
to	O
the	O
detected	O
visual	O
field	O
and	O
the	O
recognized	O
speech	O
information	O
on	O
the	O
display	O
;	O
and	O
wherein	O
the	O
control	O
means	O
activates	O
the	O
speech	O
recognition	O
means	O
only	O
if	O
the	O
detection	O
means	O
detects	O
that	O
the	O
user	O
's	O
visual	O
field	O
is	O
directed	O
to	O
the	O
display	O
.	O

A	O
detection	O
means	O
,	O
e.g.	O
,	O
a	O
camera	O
,	O
records	O
the	O
user	O
's	O
visual	O
field	O
on	O
the	O
display	O
means	O
,	O
e.g.	O
,	O
a	O
monitor	O
.	O
At	O
the	O
same	O
time	O
,	O
speech	O
recognition	O
means	O
evaluate	O
the	O
user	O
's	O
speech	O
.	O
The	O
detection	O
means	O
and	O
the	O
speech	O
recognition	O
means	O
together	O
form	O
an	O
eye	O
tracking	O
controlled	O
speech	O
recognition	O
system	O
.	O
As	O
a	O
function	O
of	O
the	O
recorded	O
speech	O
information	O
and	O
as	O
a	O
function	O
of	O
the	O
recorded	O
visual	O
field	O
on	O
the	O
display	O
means	O
,	O
certain	O
predefinable	O
information	O
data	O
is	O
displayed	O
.	O
If	O
,	O
for	O
instance	O
,	O
a	O
camera	O
integrated	O
with	O
the	O
display	O
means	O
,	O
e.g.	O
,	O
the	O
screen	O
,	O
detects	O
that	O
the	O
user	O
is	O
looking	O
directly	O
at	O
the	O
screen	O
and	O
consequently	O
intends	O
to	O
monitor	O
a	O
corresponding	O
process	O
,	O
the	O
camera	O
as	O
the	O
detection	O
means	O
activates	O
the	O
system	O
's	O
speech	O
recognition	O
means	O
,	O
resulting	O
in	O
a	O
specific	O
evaluation	O
of	O
the	O
speech	O
input	O
.	O
This	O
results	O
in	O
a	O
novel	O
form	O
of	O
interaction	O
for	O
process	O
visualization	O
.	O
If	O
the	O
user	O
looks	O
at	O
another	O
display	O
means	O
or	O
at	O
another	O
field	O
of	O
the	O
display	O
means	O
,	O
control	O
means	O
can	O
be	O
used	O
to	O
display	O
information	O
associated	O
with	O
this	O
visual	O
field	O
on	O
the	O
display	O
means	O
after	O
speech	O
recognition	O
has	O
taken	O
place	O
.	O
This	O
enables	O
the	O
user	O
intuitively	O
to	O
operate	O
and	O
monitor	O
a	O
desired	O
process	O
or	O
a	O
desired	O
production	O
machine	O
and/or	O
machine	O
tool	O
,	O
without	O
using	O
his	O
or	O
her	O
hands	O
,	O
by	O
changing	O
his	O
or	O
her	O
visual	O
field	O
in	O
combination	O
with	O
speech	O
control	O
.	O

Accidental	O
detection	O
and	O
interpretation	O
of	O
speech	O
information	O
that	O
is	O
not	O
intended	O
for	O
the	O
system	O
can	O
be	O
prevented	O
,	O
or	O
specific	O
activation	O
of	O
speech	O
recognition	O
can	O
be	O
achieved	O
by	O
controlling	O
the	O
control	O
means	O
of	O
the	O
system	O
in	O
such	O
a	O
way	O
that	O
the	O
speech	O
recognition	O
means	O
are	O
activated	O
only	O
if	O
the	O
detection	O
means	O
detect	O
that	O
the	O
user	O
's	O
visual	O
field	O
relates	O
to	O
a	O
specific	O
display	O
means	O
and/or	O
at	O
least	O
predefinable	O
areas	O
of	O
the	O
display	O
.	O

In	O
an	O
advantageous	O
embodiment	O
of	O
the	O
detection	O
means	O
the	O
detection	O
means	O
comprise	O
a	O
camera	O
to	O
detect	O
the	O
user	O
's	O
visual	O
field	O
.	O
However	O
,	O
the	O
detection	O
means	O
may	O
also	O
be	O
embodied	O
in	O
the	O
form	O
of	O
other	O
optical	O
tracking	O
devices	O
presently	O
known	O
or	O
hereafter	O
developed	O
.	O

Interactive	O
speech	O
communication	O
with	O
the	O
system	O
can	O
be	O
ensured	O
by	O
providing	O
the	O
system	O
with	O
acoustic	O
playback	O
means	O
.	O
These	O
acoustic	O
playback	O
means	O
provide	O
an	O
acoustic	O
rendering	O
of	O
acoustic	O
information	O
data	O
that	O
is	O
generated	O
in	O
response	O
to	O
speech	O
information	O
given	O
specifically	O
by	O
the	O
user	O
through	O
the	O
speech	O
recognition	O
means	O
and	O
is	O
then	O
transmitted	O
to	O
the	O
acoustic	O
playback	O
means	O
.	O

Specific	O
speech	O
processing	O
is	O
further	O
enhanced	O
by	O
providing	O
the	O
system	O
with	O
additional	O
display	O
means	O
,	O
which	O
are	O
provided	O
,	O
e.g.	O
,	O
for	O
signaling	O
the	O
location	O
of	O
information	O
data	O
that	O
is	O
linked	O
to	O
recognized	O
speech	O
information	O
.	O

The	O
user	O
is	O
preferably	O
provided	O
with	O
reliable	O
information	O
on	O
the	O
processing	O
status	O
of	O
the	O
system	O
with	O
respect	O
to	O
a	O
speech	O
signal	O
that	O
is	O
being	O
processed	O
by	O
including	O
in	O
the	O
system	O
a	O
means	O
for	O
generating	O
a	O
visual	O
feedback	O
signal	O
in	O
response	O
to	O
and/or	O
indicative	O
of	O
the	O
processing	O
status	O
of	O
the	O
recognized	O
speech	O
information	O
.	O

One	O
embodiment	O
of	O
the	O
feedback	O
signal	O
that	O
is	O
advantageous	O
and	O
easily	O
recognizable	O
by	O
the	O
user	O
is	O
a	O
visual	O
feedback	O
signal	O
configured	O
as	O
a	O
software	O
object	O
,	O
which	O
is	O
preferably	O
superimposed	O
in	O
the	O
area	O
of	O
the	O
user	O
's	O
detected	O
field	O
of	O
view	O
.	O

Another	O
means	O
for	O
readily	O
noticeable	O
feedback	O
to	O
support	O
the	O
speech	O
information	O
is	O
to	O
configure	O
the	O
visual	O
feedback	O
signal	O
as	O
a	O
color	O
signal	O
that	O
identifies	O
the	O
corresponding	O
processing	O
status	O
of	O
the	O
detected	O
speech	O
information	O
through	O
color-coding	O
.	O

BRIEF	O
DESCRIPTION	O
OF	O
THE	O
DRAWINGS	O
The	O
invention	O
will	O
now	O
be	O
described	O
and	O
explained	O
in	O
greater	O
detail	O
with	O
reference	O
to	O
the	O
exemplary	O
embodiments	O
depicted	O
in	O
the	O
figures	O
in	O
which	O

FIG	O
.	O
1	O
is	O
a	O
block	O
diagram	O
of	O
an	O
exemplary	O
embodiment	O
of	O
a	O
speech-controlled	O
system	O
for	O
operating	O
and	O
monitoring	O
an	O
automation	O
system	O
or	O
machine	O
tool	O
,	O

FIG	O
.	O
2	O
is	O
a	O
block	O
diagram	O
of	O
an	O
exemplary	O
embodiment	O
of	O
a	O
speech-controlled	O
system	O
for	O
operating	O
and	O
monitoring	O
an	O
automation	O
system	O
or	O
a	O
machine	O
tool	O
with	O
a	O
visual	O
feedback	O
signal	O
in	O
the	O
form	O
of	O
a	O
software	O
object	O
,	O
and	O

FIG	O
.	O
3	O
is	O
a	O
block	O
diagram	O
of	O
an	O
exemplary	O
embodiment	O
of	O
a	O
speech-controlled	O
system	O
for	O
operating	O
and	O
monitoring	O
an	O
automation	O
system	O
or	O
a	O
machine	O
tool	O
with	O
a	O
visual	O
feedback	O
signal	O
in	O
the	O
form	O
of	O
an	O
optical	O
device	O
.	O

DETAILED	O
DESCRIPTION	O
OF	O
THE	O
PREFERRED	O
EMBODIMENTS	O

FIG	O
.	O
1	O
shows	O
a	O
block	O
diagram	O
of	O
a	O
eye	O
tracking	O
controlled	O
speech	O
processing	O
system	O
for	O
operating	O
and	O
monitoring	O
an	O
automation	O
system	O
or	O
machine	O
tool	O
.	O
The	O
eye	O
tracking	O
controlled	O
speech	O
processing	O
system	O
includes	O
an	O
eye	O
tracking	O
controlled	O
speech	O
recognition	O
system	O
14	O
that	O
can	O
be	O
controlled	O
by	O
a	O
user	O
1	O
.	O
The	O
user	O
is	O
equipped	O
with	O
mobile	O
data	O
goggles	O
,	O
which	O
are	O
coupled	O
to	O
a	O
microphone	O
4	O
,	O
a	O
loudspeaker	O
3	O
and	O
a	O
data	O
transmission	O
device	O
15	O
.	O
The	O
eye	O
tracking	O
controlled	O
speech	O
recognition	O
system	O
14	O
includes	O
a	O
camera	O
5	O
as	O
detection	O
means	O
for	O
detecting	O
a	O
visual	O
field	O
9	O
,	O
i	O
.	O
e	O
.	O
,	O
the	O
eye	O
movements	O
of	O
user	O
1	O
.	O
The	O
signals	O
of	O
the	O
detection	O
means	O
5	O
are	O
supplied	O
to	O
an	O
evaluation	O
unit	O
7	O
.	O
The	O
eye	O
tracking	O
controlled	O
speech	O
recognition	O
system	O
14	O
further	O
includes	O
an	O
acoustic	O
evaluation	O
unit	O
6	O
,	O
e.g.	O
,	O
a	O
sound	O
card	O
of	O
a	O
computer	O
,	O
which	O
detects	O
a	O
speech	O
signal	O
8	O
of	O
user	O
1	O
.	O
If	O
both	O
the	O
evaluation	O
unit	O
7	O
and	O
the	O
acoustic	O
evaluation	O
unit	O
6	O
emit	O
a	O
positive	O
signal	O
17	O
,	O
18	O
,	O
i	O
.	O
e	O
.	O
,	O
if	O
on	O
the	O
one	O
hand	O
the	O
user	O
's	O
visual	O
field	O
9	O
is	O
directed	O
toward	O
at	O
least	O
predefinable	O
areas	O
of	O
e.g.	O
,	O
a	O
display	O
and	O
on	O
the	O
other	O
hand	O
a	O
corresponding	O
speech	O
signal	O
of	O
user	O
1	O
is	O
also	O
present	O
,	O
a	O
speech	O
recognition	O
device	O
19	O
,	O
in	O
case	O
of	O
a	O
recognized	O
speech	O
signal	O
8	O
,	O
issues	O
a	O
corresponding	O
command	O
20	O
to	O
the	O
display	O
,	O
e.g.	O
,	O
within	O
a	O
process	O
visualization	O
system	O
2	O
.	O

The	O
special	O
feature	O
of	O
the	O
eye	O
tracking	O
controlled	O
speech	O
processing	O
system	O
shown	O
in	O
FIG	O
.	O
1	O
is	O
the	O
combined	O
evaluation	O
of	O
both	O
the	O
visual	O
field	O
9	O
of	O
user	O
1	O
and	O
specific	O
speech	O
evaluation	O
.	O
This	O
results	O
in	O
specific	O
speech	O
recognition	O
,	O
so	O
that	O
the	O
user	O
does	O
not	O
first	O
have	O
to	O
look	O
for	O
the	O
desired	O
process	O
images	O
and	O
process	O
values	O
,	O
which	O
may	O
possibly	O
be	O
deeply	O
embedded	O
in	O
operating	O
hierarchies	O
.	O
Instead	O
,	O
the	O
user	O
can	O
directly	O
“	O
call	O
up	O
”	O
a	O
desired	O
object	O
,	O
a	O
desired	O
process	O
value	O
,	O
etc	O
.	O
by	O
focusing	O
his	O
or	O
her	O
eyes	O
on	O
a	O
certain	O
process	O
image	O
and	O
by	O
simultaneously	O
giving	O
a	O
corresponding	O
speech	O
signal	O
.	O
This	O
also	O
clearly	O
reduces	O
the	O
susceptibility	O
to	O
errors	O
in	O
speech	O
recognition	O
,	O
since	O
the	O
speech	O
recognition	O
unit	O
has	O
to	O
process	O
only	O
the	O
detected	O
speech	O
commands	O
that	O
are	O
linked	O
to	O
the	O
respectively	O
associated	O
visual	O
field	O
and	O
the	O
process	O
image	O
,	O
etc	O
.	O
which	O
is	O
related	O
thereto	O
.	O
This	O
increases	O
the	O
sensitivity	O
for	O
the	O
recognition	O
of	O
speech	O
commands	O
and	O
speech	O
signals	O
8	O
of	O
user	O
1	O
.	O
The	O
speech	O
signals	O
8	O
can	O
be	O
transmitted	O
from	O
microphone	O
4	O
to	O
sound	O
card	O
6	O
,	O
for	O
instance	O
,	O
by	O
a	O
wireless	O
unidirectional	O
or	O
bidirectional	O
air	O
interface	O
.	O
In	O
the	O
case	O
of	O
a	O
bidirectional	O
air	O
interface	O
between	O
transmission	O
system	O
15	O
and	O
the	O
eye	O
tracking	O
controlled	O
speech	O
recognition	O
system	O
14	O
,	O
it	O
is	O
also	O
possible	O
directly	O
to	O
issue	O
prompts	O
22	O
to	O
user	O
1	O
if	O
a	O
speech	O
signal	O
8	O
is	O
not	O
recognized	O
.	O
As	O
an	O
alternative	O
,	O
or	O
in	O
addition	O
thereto	O
,	O
such	O
prompts	O
22	O
may	O
also	O
be	O
output	O
via	O
a	O
loudspeaker	O
21	O
.	O

FIG	O
.	O
2	O
shows	O
,	O
in	O
a	O
second	O
embodiment	O
,	O
an	O
eye	O
tracking	O
controlled	O
speech	O
recognition	O
system	O
for	O
an	O
operating	O
and	O
monitoring	O
system	O
100	O
controlling	O
,	O
e.g.	O
,	O
an	O
automation	O
system	O
110	O
.	O
The	O
operating	O
and	O
monitoring	O
system	O
includes	O
a	O
process	O
visualization	O
system	O
102	O
,	O
which	O
enables	O
process	O
control	O
of	O
the	O
automation	O
system	O
through	O
,	O
e.g.	O
,	O
a	O
display	O
form	O
103	O
such	O
as	O
the	O
one	O
shown	O
in	O
FIG	O
.	O
2	O
.	O
The	O
automation	O
system	O
110	O
is	O
not	O
further	O
depicted	O
in	O
FIG	O
.	O
2	O
for	O
the	O
sake	O
of	O
clarity	O
.	O
The	O
operating	O
and	O
monitoring	O
system	O
further	O
includes	O
an	O
eye	O
tracking	O
controlled	O
speech	O
recognition	O
unit	O
101	O
,	O
which	O
receives	O
input	O
signals	O
104	O
,	O
105	O
and	O
supplies	O
output	O
signals	O
106	O
,	O
107	O
.	O
Input	O
signal	O
104	O
of	O
the	O
eye	O
tracking	O
controlled	O
speech	O
recognition	O
unit	O
is	O
determined	O
by	O
a	O
camera	O
5	O
as	O
the	O
detection	O
means	O
for	O
recording	O
the	O
visual	O
field	O
9	O
of	O
a	O
user	O
1	O
.	O
Input	O
signal	O
105	O
is	O
based	O
on	O
a	O
speech	O
signal	O
8	O
of	O
the	O
user	O
,	O
which	O
is	O
recorded	O
,	O
for	O
instance	O
,	O
through	O
a	O
microphone	O
4	O
of	O
a	O
headset	O
3	O
of	O
user	O
1	O
.	O
The	O
eye	O
tracking	O
controlled	O
speech	O
recognition	O
system	O
101	O
supplies	O
a	O
first	O
output	O
signal	O
106	O
that	O
represents	O
the	O
user	O
's	O
visual	O
field	O
9	O
and	O
a	O
second	O
output	O
signal	O
107	O
that	O
is	O
based	O
on	O
the	O
user	O
's	O
speech	O
signal	O
8	O
.	O
As	O
a	O
function	O
of	O
output	O
signals	O
106	O
,	O
107	O
of	O
the	O
eye	O
tracking	O
controlled	O
speech	O
recognition	O
unit	O
101	O
,	O
an	O
output	O
signal	O
108	O
is	O
generated	O
in	O
the	O
process	O
visualization	O
system	O
102	O
and	O
is	O
superimposed	O
on	O
the	O
display	O
form	O
103	O
as	O
a	O
software	O
object	O
associated	O
with	O
the	O
window	O
that	O
is	O
being	O
displayed	O
in	O
accordance	O
with	O
the	O
user	O
's	O
visual	O
field	O
9	O
.	O

The	O
exemplary	O
embodiment	O
depicted	O
in	O
FIG	O
.	O
2	O
is	O
based	O
,	O
e.g.	O
,	O
on	O
the	O
following	O
scenario	O
:	O
User	O
1	O
monitors	O
an	O
automation	O
system	O
110	O
with	O
the	O
aid	O
of	O
the	O
process	O
visualization	O
system	O
102	O
.	O
The	O
visual	O
field	O
9	O
of	O
user	O
1	O
is	O
simultaneously	O
tracked	O
by	O
the	O
eye	O
tracking	O
controlled	O
speech	O
recognition	O
system	O
101	O
.	O
User	O
1	O
,	O
through	O
speech	O
,	O
i	O
.	O
e	O
.	O
,	O
by	O
emitting	O
a	O
speech	O
signal	O
8	O
,	O
calls	O
up	O
a	O
desired	O
object	O
,	O
e.g.	O
,	O
a	O
certain	O
process	O
value	O
that	O
is	O
contained	O
in	O
visual	O
field	O
9	O
,	O
or	O
wishes	O
to	O
change	O
a	O
process	O
value	O
contained	O
in	O
visual	O
field	O
9	O
.	O
In	O
these	O
cases	O
,	O
user	O
1	O
calls	O
up	O
the	O
object	O
through	O
speech	O
or	O
calls	O
out	O
the	O
new	O
process	O
value	O
.	O
A	O
visual	O
object	O
109	O
,	O
which	O
contains	O
the	O
information	O
regarding	O
the	O
processing	O
of	O
the	O
user	O
's	O
speech	O
command	O
,	O
is	O
then	O
superimposed	O
directly	O
within	O
the	O
user	O
's	O
visual	O
field	O
9	O
.	O
If	O
the	O
control	O
by	O
means	O
of	O
the	O
eye	O
tracking	O
controlled	O
speech	O
recognition	O
system	O
101	O
is	O
successful	O
,	O
the	O
process	O
visualization	O
system	O
102	O
,	O
and	O
thus	O
the	O
operating	O
and	O
monitoring	O
system	O
100	O
incorporating	O
the	O
process	O
visualization	O
system	O
102	O
,	O
executes	O
the	O
corresponding	O
commands	O
and	O
actions	O
.	O
Using	O
the	O
eye	O
tracking	O
controlled	O
speech	O
recognition	O
system	O
coupled	O
with	O
a	O
visual	O
feedback	O
signal	O
results	O
in	O
a	O
particularly	O
simple	O
and	O
intuitive	O
man-machine	O
interaction	O
.	O
User	O
1	O
receives	O
direct	O
feedback	O
in	O
the	O
form	O
of	O
a	O
brief	O
feedback	O
signal	O
109	O
and	O
is	O
informed	O
as	O
to	O
whether	O
his	O
or	O
her	O
speech	O
command	O
105	O
,	O
107	O
was	O
registered	O
,	O
understood	O
and	O
executed	O
by	O
the	O
system	O
100	O
.	O

FIG	O
.	O
3	O
shows	O
a	O
further	O
exemplary	O
embodiment	O
of	O
an	O
operating	O
and	O
monitoring	O
system	O
with	O
speech	O
recognition	O
and	O
a	O
visual	O
feedback	O
signal	O
for	O
supporting	O
speech	O
interaction	O
in	O
process	O
visualization	O
and	O
in	O
production	O
machines	O
and	O
machine	O
tools	O
.	O
The	O
system	O
comprises	O
a	O
speech	O
interaction	O
system	O
200	O
that	O
evaluates	O
a	O
speech	O
signal	O
8	O
as	O
an	O
input	O
signal	O
of	O
a	O
user	O
1	O
.	O
As	O
a	O
function	O
of	O
speech	O
signal	O
8	O
,	O
the	O
speech	O
interaction	O
system	O
200	O
supplies	O
a	O
first	O
output	O
signal	O
203	O
and	O
a	O
second	O
output	O
signal	O
204	O
.	O
The	O
first	O
output	O
signal	O
203	O
is	O
supplied	O
to	O
a	O
display	O
205	O
that	O
displays	O
different	O
operating	O
states	O
,	O
e.g.	O
,	O
in	O
different	O
colors	O
.	O
The	O
second	O
output	O
signal	O
204	O
of	O
the	O
speech	O
interaction	O
system	O
200	O
is	O
supplied	O
to	O
a	O
process	O
visualization	O
system	O
201	O
of	O
a	O
plant	O
206	O
or	O
a	O
machine	O
tool	O
.	O
Visualization	O
and	O
operation	O
and	O
monitoring	O
of	O
plant	O
206	O
or	O
the	O
machine	O
tool	O
is	O
effected	O
via	O
an	O
operator	O
terminal	O
202	O
.	O

The	O
system	O
depicted	O
in	O
FIG	O
.	O
3	O
is	O
based	O
,	O
for	O
instance	O
,	O
on	O
the	O
following	O
scenario	O
.	O
The	O
user	O
,	O
e.g.	O
,	O
an	O
operator	O
of	O
a	O
machine	O
tool	O
or	O
a	O
plant	O
206	O
,	O
monitors	O
plant	O
206	O
by	O
means	O
of	O
the	O
process	O
visualization	O
system	O
201	O
via	O
the	O
operator	O
terminal	O
202	O
.	O
To	O
execute	O
an	O
action	O
of	O
the	O
plant/machine	O
206	O
,	O
user	O
1	O
uses	O
speech	O
to	O
call	O
out	O
commands	O
in	O
the	O
form	O
of	O
speech	O
signals	O
8	O
.	O
Through	O
a	O
separately	O
arranged	O
,	O
advantageously	O
conspicuously	O
positioned	O
display	O
device	O
,	O
e.g.	O
,	O
in	O
the	O
form	O
of	O
a	O
lamp	O
,	O
user	O
1	O
is	O
informed	O
of	O
the	O
processing	O
status	O
of	O
his	O
or	O
her	O
speech	O
command	O
.	O
For	O
instance	O
,	O
a	O
green	O
signal	O
of	O
lamp	O
205	O
indicates	O
that	O
a	O
command	O
has	O
been	O
recognized	O
and	O
the	O
desired	O
action	O
has	O
been	O
performed	O
.	O
A	O
red	O
signaling	O
lamp	O
205	O
indicates	O
that	O
a	O
command	O
was	O
not	O
recognized	O
.	O
An	O
orange	O
signaling	O
lamp	O
205	O
can	O
,	O
for	O
instance	O
,	O
indicate	O
that	O
a	O
command	O
has	O
been	O
recognized	O
but	O
that	O
the	O
corresponding	O
action	O
has	O
not	O
yet	O
been	O
executed	O
.	O
The	O
method	O
for	O
visualizing	O
feedback	O
in	O
response	O
to	O
speech	O
commands	O
for	O
process	O
monitoring	O
as	O
depicted	O
in	O
FIG	O
.	O
3	O
is	O
especially	O
suitable	O
in	O
the	O
field	O
of	O
production	O
machine	O
and	O
machine	O
tool	O
control	O
since	O
it	O
provides	O
the	O
user	O
,	O
i	O
.	O
e	O
.	O
,	O
the	O
operator	O
,	O
with	O
clear	O
and	O
unambiguous	O
signaling	O
of	O
the	O
processing	O
status	O
of	O
his	O
or	O
her	O
speech	O
command	O
8	O
even	O
across	O
,	O
if	O
necessary	O
,	O
relatively	O
large	O
distances	O
.	O

The	O
eye	O
tracking	O
controlled	O
speech	O
processing	O
for	O
speech-controlled	O
support	O
of	O
process	O
visualization	O
systems	O
and	O
machine	O
tools	O
should	O
be	O
understood	O
,	O
in	O
particular	O
,	O
in	O
the	O
special	O
context	O
of	O
the	O
application	O
fields	O
“	O
operation	O
and	O
monitoring	O
of	O
process	O
automation	O
systems	O
”	O
as	O
well	O
as	O
“	O
production	O
machines	O
and	O
machine	O
tools	O
.	O
”	O
The	O
use	O
of	O
standard	O
PCs	O
as	O
the	O
visualization	O
tools	O
both	O
on	O
the	O
management	O
level	O
and	O
on	O
the	O
field	O
level	O
is	O
a	O
current	O
trend	O
in	O
process	O
automation	O
.	O
This	O
form	O
of	O
interaction	O
is	O
not	O
limited	O
to	O
mouse	O
and	O
keyboard	O
,	O
however	O
.	O
The	O
increasing	O
complexity	O
of	O
technical	O
systems	O
is	O
making	O
navigation	O
among	O
the	O
process	O
images	O
more	O
difficult	O
.	O
It	O
is	O
often	O
necessary	O
to	O
go	O
deep	O
into	O
the	O
operating	O
hierarchies	O
in	O
order	O
to	O
find	O
a	O
process	O
image	O
or	O
a	O
process	O
value	O
.	O
In	O
the	O
field	O
,	O
hands-free	O
operation	O
,	O
which	O
is	O
not	O
possible	O
with	O
mouse	O
and	O
keyboard	O
,	O
can	O
be	O
advantageous	O
.	O

Speech	O
recognition	O
and	O
speech	O
output	O
systems	O
,	O
which	O
are	O
used	O
on	O
many	O
standard	O
PCs	O
for	O
dictation	O
today	O
,	O
make	O
it	O
easier	O
and	O
more	O
intuitive	O
to	O
navigate	O
in	O
process	O
visualization	O
.	O
The	O
user	O
does	O
not	O
need	O
to	O
look	O
for	O
the	O
desired	O
process	O
images	O
and	O
process	O
values	O
in	O
the	O
deep	O
operating	O
hierarchies	O
,	O
but	O
can	O
“	O
call	O
up	O
”	O
the	O
object	O
directly	O
.	O

It	O
is	O
advantageous	O
and	O
important	O
,	O
e.g.	O
,	O
in	O
the	O
context	O
of	O
a	O
control	O
room	O
,	O
where	O
several	O
operator	O
terminals	O
are	O
used	O
side	O
by	O
side	O
(	O
visualization	O
of	O
different	O
parts	O
of	O
the	O
plant	O
)	O
,	O
to	O
have	O
an	O
interaction	O
system	O
that	O
recognizes	O
where	O
the	O
user	O
is	O
focusing	O
his	O
or	O
her	O
attention	O
.	O
However	O
,	O
even	O
in	O
situations	O
where	O
only	O
a	O
single	O
terminal/monitor	O
is	O
used	O
,	O
it	O
is	O
very	O
useful	O
for	O
the	O
interaction	O
system	O
to	O
recognize	O
the	O
element	O
on	O
which	O
the	O
users	O
attention	O
is	O
focused	O
.	O

For	O
this	O
purpose	O
,	O
preferably	O
,	O
a	O
camera	O
integrated	O
with	O
the	O
display	O
is	O
used	O
to	O
record	O
whether	O
the	O
user	O
is	O
currently	O
looking	O
at	O
the	O
screen	O
and	O
wants	O
to	O
monitor	O
the	O
corresponding	O
process	O
.	O
According	O
to	O
the	O
invention	O
,	O
only	O
if	O
this	O
is	O
the	O
case	O
,	O
is	O
the	O
speech	O
recognition	O
activated	O
.	O
This	O
avoids	O
accidental	O
recognition/interpretation	O
of	O
speech	O
that	O
is	O
not	O
intended	O
as	O
an	O
instruction	O
directed	O
to	O
the	O
system	O
.	O
As	O
a	O
further	O
enhancement	O
,	O
the	O
system	O
can	O
be	O
designed	O
to	O
inform	O
the	O
user	O
whether	O
the	O
desired	O
action	O
is	O
being	O
executed	O
.	O

The	O
core	O
of	O
the	O
invention	O
lies	O
in	O
the	O
novel	O
form	O
of	O
interaction	O
for	O
process	O
visualization	O
and	O
recognition	O
of	O
the	O
element	O
on	O
which	O
the	O
user	O
's	O
attention	O
is	O
focused	O
in	O
order	O
to	O
process	O
speech	O
input	O
specifically	O
.	O

The	O
following	O
two	O
scenarios	O
describe	O
exemplary	O
embodiments	O
of	O
the	O
basic	O
idea	O
:	O
Scenario	O
A	O
:	O
(	O
on	O
the	O
Management	O
Level	O
)	O
A	O
plant	O
administrator	O
monitors	O
the	O
state	O
of	O
the	O
plant	O
through	O
a	O
visualization	O
system	O
.	O
A	O
portion	O
of	O
the	O
plant	O
is	O
in	O
a	O
critical	O
state	O
.	O
The	O
visualized	O
portion	O
is	O
blinking	O
red	O
in	O
the	O
overall	O
plant	O
image	O
.	O
The	O
administrator	O
looks	O
at	O
that	O
portion	O
of	O
the	O
screen	O
,	O
his	O
focus	O
is	O
localized	O
,	O
and	O
speech	O
recognition	O
is	O
activated	O
.	O
The	O
administrator	O
simply	O
calls	O
out	O
the	O
process	O
value	O
or	O
the	O
process	O
image	O
that	O
he	O
would	O
like	O
to	O
see	O
in	O
greater	O
detail	O
.	O
Acoustic	O
and/or	O
optical	O
feedback	O
through	O
a	O
signal	O
lamp	O
and	O
speech	O
output	O
confirms	O
to	O
the	O
administrator	O
that	O
his	O
words	O
were	O
recognized	O
by	O
the	O
system	O
and	O
that	O
the	O
desired	O
actions	O
have	O
been	O
executed	O
.	O

Scenario	O
B	O
:	O
(	O
in	O
the	O
Field	O
)	O
A	O
worker	O
looks	O
at	O
the	O
control	O
panel	O
and	O
would	O
like	O
to	O
check	O
certain	O
process	O
values	O
or	O
perform	O
certain	O
actions	O
.	O
His	O
hands	O
are	O
not	O
free	O
at	O
the	O
moment	O
,	O
however	O
,	O
and	O
the	O
control	O
panel	O
is	O
located	O
at	O
a	O
considerable	O
distance	O
.	O
The	O
worker	O
calls	O
up	O
the	O
desired	O
process	O
state	O
through	O
speech	O
.	O
Optical	O
and	O
acoustic	O
feedback	O
confirms	O
that	O
the	O
command	O
has	O
been	O
recognized	O
and	O
executed	O
.	O
If	O
required	O
,	O
process	O
values	O
can	O
also	O
be	O
announced	O
.	O

In	O
summary	O
,	O
the	O
invention	O
thus	O
relates	O
to	O
a	O
system	O
and	O
method	O
for	O
operating	O
and	O
monitoring	O
,	O
in	O
particular	O
,	O
an	O
automation	O
system	O
and/or	O
a	O
production	O
machine	O
and/or	O
a	O
machine	O
tool	O
.	O
The	O
system	O
recognizes	O
when	O
the	O
visual	O
field	O
9	O
of	O
a	O
user	O
1	O
is	O
directed	O
at	O
at	O
least	O
one	O
display	O
.	O
Speech	O
information	O
8	O
of	O
user	O
1	O
is	O
evaluated	O
at	O
least	O
intermittently	O
in	O
such	O
a	O
way	O
that	O
,	O
as	O
a	O
function	O
of	O
the	O
speech	O
information	O
8	O
given	O
by	O
user	O
1	O
and	O
recognized	O
by	O
speech	O
recognition	O
means	O
4	O
and	O
as	O
a	O
function	O
of	O
the	O
detected	O
visual	O
field	O
9	O
,	O
specifically	O
predefinable	O
information	O
data	O
that	O
is	O
linked	O
to	O
the	O
detected	O
visual	O
field	O
9	O
and	O
the	O
recognized	O
speech	O
information	O
can	O
be	O
displayed	O
on	O
the	O
display	O
.	O
This	O
provides	O
hands-free	O
operation	O
and	O
a	O
means	O
for	O
navigation	O
in	O
the	O
environment	O
of	O
augmented	O
reality	O
applications	O
even	O
when	O
complex	O
technical	O
systems	O
are	O
involved	O
.	O

The	O
above	O
description	O
of	O
the	O
preferred	O
embodiments	O
has	O
been	O
given	O
by	O
way	O
of	O
example	O
.	O
From	O
the	O
disclosure	O
given	O
,	O
those	O
skilled	O
in	O
the	O
art	O
will	O
not	O
only	O
understand	O
the	O
present	O
invention	O
and	O
its	O
attendant	O
advantages	O
,	O
but	O
will	O
also	O
find	O
apparent	O
various	O
changes	O
and	O
modifications	O
to	O
the	O
structures	O
disclosed	O
.	O
It	O
is	O
sought	O
,	O
therefore	O
,	O
to	O
cover	O
all	O
such	O
changes	O
and	O
modifications	O
as	O
fall	O
within	O
the	O
spirit	O
and	O
scope	O
of	O
the	O
invention	O
,	O
as	O
defined	O
by	O
the	O
appended	O
claims	O
,	O
and	O
equivalents	O
thereof	O
.	O

1	O
.	O
System	O
,	O
comprising	O
:	O
detection	O
means	O
for	O
detecting	O
a	O
visual	O
field	O
of	O
a	O
user	O
being	O
directed	O
onto	O
a	O
display	O
;	O
speech	O
recognition	O
means	O
for	O
recognizing	O
speech	O
information	O
of	O
the	O
user	O
;	O
and	O
control	O
means	O
for	O
controlling	O
the	O
system	O
;	O
wherein	O
,	O
as	O
a	O
function	O
of	O
the	O
speech	O
information	O
given	O
by	O
the	O
user	O
and	O
recognized	O
by	O
said	O
speech	O
recognition	O
means	O
,	O
and	O
as	O
a	O
function	O
of	O
the	O
visual	O
field	O
recorded	O
by	O
said	O
detection	O
means	O
,	O
said	O
control	O
means	O
displays	O
specifically	O
defined	O
information	O
data	O
linked	O
to	O
the	O
detected	O
visual	O
field	O
and	O
the	O
recognized	O
speech	O
information	O
on	O
the	O
display	O
;	O
and	O
wherein	O
said	O
control	O
means	O
activates	O
said	O
speech	O
recognition	O
means	O
only	O
if	O
said	O
detection	O
means	O
detects	O
that	O
the	O
user	O
's	O
visual	O
field	O
is	O
directed	O
to	O
a	O
predefined	O
area	O
of	O
the	O
display	O
.	O

2	O
.	O
System	O
as	O
claimed	O
in	O
claim	O
1	O
,	O
further	O
comprising	O
at	O
least	O
one	O
signal	O
path	O
for	O
signals	O
that	O
operate	O
and	O
monitor	O
at	O
least	O
one	O
of	O
an	O
automation	O
system	O
,	O
a	O
production	O
machine	O
,	O
and	O
a	O
machine	O
tool	O
.	O

3	O
.	O
System	O
as	O
claimed	O
in	O
claim	O
1	O
,	O
wherein	O
said	O
detection	O
means	O
comprise	O
a	O
camera	O
for	O
recording	O
the	O
visual	O
field	O
of	O
the	O
user	O
.	O

4	O
.	O
System	O
as	O
claimed	O
in	O
claim	O
1	O
,	O
further	O
comprising	O
acoustic	O
playback	O
means	O
for	O
outputting	O
acoustic	O
information	O
data	O
generated	O
in	O
response	O
to	O
the	O
speech	O
information	O
of	O
the	O
user	O
recognized	O
by	O
said	O
speech	O
recognition	O
means	O
.	O

5	O
.	O
System	O
as	O
claimed	O
in	O
claim	O
1	O
,	O
further	O
comprising	O
additional	O
display	O
means	O
,	O
which	O
signal	O
a	O
location	O
of	O
the	O
information	O
data	O
that	O
is	O
linked	O
with	O
recognized	O
speech	O
information	O
.	O

6	O
.	O
System	O
as	O
claimed	O
in	O
claim	O
1	O
,	O
further	O
comprising	O
means	O
for	O
generating	O
a	O
visual	O
feedback	O
signal	O
relating	O
to	O
a	O
processing	O
status	O
of	O
the	O
speech	O
information	O
of	O
the	O
user	O
.	O

7	O
.	O
System	O
as	O
claimed	O
in	O
claim	O
6	O
,	O
wherein	O
the	O
visual	O
feedback	O
signal	O
comprises	O
a	O
software	O
object	O
that	O
is	O
superimposed	O
onto	O
the	O
display	O
in	O
the	O
detected	O
visual	O
field	O
of	O
the	O
user	O
.	O

8	O
.	O
System	O
as	O
claimed	O
in	O
claim	O
6	O
,	O
wherein	O
the	O
visual	O
feedback	O
signal	O
comprises	O
a	O
color	O
signal	O
that	O
identifies	O
the	O
processing	O
status	O
of	O
the	O
speech	O
information	O
of	O
the	O
user	O
.	O

9	O
.	O
System	O
as	O
claimed	O
in	O
claim	O
1	O
,	O
wherein	O
the	O
predefined	O
area	O
of	O
the	O
display	O
is	O
a	O
discrete	O
portion	O
of	O
the	O
display	O
which	O
is	O
smaller	O
than	O
the	O
whole	O
of	O
the	O
display	O
.	O

10	O
.	O
A	O
method	O
comprising	O
:	O
detecting	O
that	O
a	O
user	O
's	O
view	O
is	O
directed	O
onto	O
a	O
display	O
;	O
at	O
least	O
intermittently	O
recognizing	O
speech	O
information	O
spoken	O
by	O
the	O
user	O
;	O
as	O
a	O
function	O
of	O
the	O
speech	O
information	O
spoken	O
by	O
the	O
user	O
and	O
recognized	O
by	O
the	O
speech	O
recognition	O
means	O
,	O
and	O
as	O
a	O
function	O
of	O
the	O
detected	O
view	O
,	O
displaying	O
predefined	O
information	O
data	O
that	O
is	O
linked	O
to	O
the	O
user	O
's	O
view	O
and	O
to	O
the	O
recognized	O
speech	O
information	O
on	O
the	O
display	O
;	O
and	O
activating	O
the	O
recognition	O
of	O
the	O
user	O
's	O
speech	O
information	O
only	O
once	O
the	O
user	O
's	O
vision	O
is	O
directed	O
onto	O
a	O
predefined	O
area	O
of	O
the	O
display	O
.	O

11	O
.	O
Method	O
as	O
claimed	O
in	O
claim	O
10	O
,	O
further	O
comprising	O
:	O
operating	O
and	O
monitoring	O
at	O
least	O
one	O
of	O
an	O
automation	O
system	O
,	O
a	O
production	O
machine	O
,	O
and	O
a	O
machine	O
tool	O
.	O

12	O
.	O
Method	O
as	O
claimed	O
in	O
claim	O
10	O
,	O
wherein	O
the	O
user	O
's	O
vision	O
is	O
detected	O
with	O
a	O
camera	O
.	O

13	O
.	O
Method	O
as	O
claimed	O
in	O
claim	O
10	O
,	O
further	O
comprising	O
transmitting	O
acoustic	O
information	O
data	O
in	O
response	O
to	O
the	O
speech	O
information	O
spoken	O
by	O
the	O
user	O
.	O

14	O
.	O
Method	O
as	O
claimed	O
in	O
claim	O
10	O
,	O
further	O
comprising	O
signaling	O
a	O
location	O
of	O
the	O
display	O
of	O
the	O
predefined	O
information	O
data	O
that	O
is	O
linked	O
to	O
the	O
user	O
's	O
view	O
and	O
to	O
the	O
recognized	O
speech	O
information	O
.	O

15	O
.	O
Method	O
as	O
claimed	O
in	O
claim	O
10	O
,	O
further	O
comprising	O
generating	O
a	O
visual	O
feedback	O
signal	O
that	O
indicates	O
a	O
processing	O
status	O
regarding	O
the	O
recognized	O
speech	O
information	O
.	O

16	O
.	O
Method	O
as	O
claimed	O
in	O
claim	O
15	O
,	O
wherein	O
the	O
visual	O
feedback	O
signal	O
comprises	O
a	O
software	O
object	O
that	O
is	O
superimposed	O
into	O
the	O
detected	O
user	O
's	O
view	O
.	O

17	O
.	O
Method	O
as	O
claimed	O
in	O
claim	O
15	O
,	O
wherein	O
the	O
visual	O
feedback	O
signal	O
comprises	O
a	O
color	O
signal	O
identifying	O
the	O
processing	O
status	O
of	O
the	O
speech	O
information	O
.	O

18	O
.	O
Method	O
as	O
claimed	O
in	O
claim	O
10	O
,	O
wherein	O
the	O
recognition	O
of	O
the	O
user	O
's	O
speech	O
information	O
is	O
activated	O
only	O
once	O
the	O
user	O
's	O
vision	O
is	O
detected	O
as	O
being	O
directed	O
onto	O
the	O
predefined	O
area	O
of	O
the	O
display	O
.	O

19	O
.	O
Method	O
as	O
claimed	O
in	O
claim	O
18	O
,	O
wherein	O
the	O
predefined	O
area	O
of	O
the	O
display	O
is	O
a	O
discrete	O
portion	O
of	O
the	O
display	O
which	O
is	O
smaller	O
than	O
the	O
whole	O
of	O
the	O
display	O
.	O

