US	O
6324510	O
B1	O
20011127	O

US	O
09187902	O
19981106	O

09	O
eng	O
eng	O

US	O
09187902	O
19981106	O

20011127	O

20011127	O

7G	O
10L	O
15/14	O
A	O
7	O
G	O
10	O
L	O
15	O
14	O
A	O

G06F	O
17/18	O
20060101CFI20060310RMJP	O

20060101	O

C	O
G	O
06	O
F	O
17	O
18	O
F	O
I	O

20060310	O

JP	O

R	O
M	O

G06F	O
17/18	O
20060101AFI20060310RMJP	O

20060101	O

A	O
G	O
06	O
F	O
17	O
18	O
F	O
I	O

20060310	O

JP	O

R	O
M	O

G06N	O
3/00	O
20060101CLI20060310RMJP	O

20060101	O

C	O
G	O
06	O
N	O
3	O
00	O
L	O
I	O

20060310	O

JP	O

R	O
M	O

G06N	O
3/00	O
20060101ALI20060310RMJP	O

20060101	O

A	O
G	O
06	O
N	O
3	O
00	O
L	O
I	O

20060310	O

JP	O

R	O
M	O

G10L	O
15/00	O
20060101C	O
I20051008RMEP	O

20060101	O

C	O
G	O
10	O
L	O
15	O
00	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/02	O
20060101A	O
I20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
02	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/06	O
20060101A	O
I20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
06	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/14	O
20060101ALI20060310RMJP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
14	O
L	O
I	O

20060310	O

JP	O

R	O
M	O

G10L	O
15/16	O
20060101ALI20060310RMJP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
16	O
L	O
I	O

20060310	O

JP	O

R	O
M	O

US	O

704/256	O
.	O
7	O
704	O
256	O
.	O
7	O

704/232	O
704	O
232	O

704/242	O
704	O
242	O

704/254	O
704	O
254	O

704/E15.004	O
704	O
E15	O
.	O
004	O

704/E15.009	O
704	O
E15	O
.	O
009	O

G10L	O
15/02	O
G	O
10	O
L	O
15	O
02	O

G10L	O
15/06A	O
G	O
10	O
L	O
15	O
06	O

A	O

S10L	O
15	O
:	O
14M4	O
S	O
10	O
L	O
15	O
14	O

M	O
4	O

S10L101:00N	O
S	O
10	O
L	O
101	O
00	O

N	O

US	O

704/232	O
704	O
232	O

US	O

704/242	O
704	O
242	O

US	O

704/245	O
704	O
245	O

US	O

704/254	O
704	O
254	O

US	O

704/256	O
704	O
256	O

US	O

704/259	O
704	O
259	O

36	O
Method	O
and	O
apparatus	O
of	O
hierarchically	O
organizing	O
an	O
acoustic	O
model	O
for	O
speech	O
recognition	O
and	O
adaptation	O
of	O
the	O
model	O
to	O
unseen	O
domains	O

US	O
4803729	O
A	O
Baker	O
19890207	O

19870403	O

US	O
5303299	O
A	O
Hunt	O
et_al.	O

19940412	O

19920622	O

US	O
5345535	O
A	O
Doddington	O
19940906	O

19930714	O

US	O
5715367	O
A	O
Gillick	O
et_al.	O

19980203	O

19950123	O

US	O
5745649	O
A	O
Lubensky	O
19980428	O

19970319	O

US	O
5799277	O
A	O
Takami	O
19980825	O

19951025	O

US	O
5806030	O
A	O
Junqua	O
19980908	O

19960506	O

US	O
5983180	O
A	O
Robinson	O
19991109	O

19980227	O

US	O
6067517	O
A	O
Bahl	O
et_al.	O

20000523	O

19960202	O

Jurgen	B-Citation
Fritsch	I-Citation
,	I-Citation
Michael	I-Citation
Finke	I-Citation
,	I-Citation
“	I-Citation
Acid/HNN	I-Citation
:	I-Citation
Clustering	I-Citation
Hierarchies	I-Citation
of	I-Citation
Neural	I-Citation
Networks	I-Citation
for	I-Citation
Context	I-Citation
—	I-Citation
Dependent	I-Citation
Connectionist	I-Citation
Acoustic	I-Citation
Modeling	I-Citation
,	I-Citation
”	I-Citation
IEEE	I-Citation
International	I-Citation
conference	I-Citation
on	I-Citation
Acoustics	I-Citation
,	I-Citation
Speech	I-Citation
and	I-Citation
Signal	I-Citation
Processing	I-Citation
,	I-Citation
Conference	I-Citation
23	I-Citation
(	I-Citation
New	I-Citation
York	I-Citation
,	I-Citation
New	I-Citation
York	I-Citation
)	I-Citation
,	I-Citation
p.	I-Citation
505	I-Citation
-	I-Citation
508	I-Citation
,	I-Citation
(	I-Citation
1998	I-Citation
)	I-Citation
.	O

J.	B-Citation
Fritsch	I-Citation
,	I-Citation
M.	I-Citation
Finke	I-Citation
,	I-Citation
A.	I-Citation
Waibel	I-Citation
,	I-Citation
“	I-Citation
Effective	I-Citation
Structural	I-Citation
Adaptation	I-Citation
of	I-Citation
LVCSR	I-Citation
Systems	I-Citation
to	I-Citation
Unsen	I-Citation
domains	I-Citation
using	I-Citation
hierarchical	I-Citation
connectionist	I-Citation
acoustic	I-Citation
models	I-Citation
,	I-Citation
”	I-Citation
Proceedings	I-Citation
of	I-Citation
the	I-Citation
International	I-Citation
Conference	I-Citation
on	I-Citation
Spoken	I-Citation
Language	I-Citation
Processing	I-Citation
,	I-Citation
p.	I-Citation
2919	I-Citation
-	I-Citation
2922	I-Citation
,	I-Citation
(	I-Citation
Nov.	I-Citation
30-Dec	I-Citation
.	I-Citation
4	I-Citation
,	I-Citation
1998	I-Citation
)	I-Citation
.	O

Paul	B-Citation
,	I-Citation
D.	I-Citation
B.	I-Citation
,	I-Citation
“	I-Citation
Extensions	I-Citation
to	I-Citation
Phone-State	I-Citation
Decision-Tree	I-Citation
Clustering	I-Citation
:	I-Citation
Single	I-Citation
Tree	I-Citation
and	I-Citation
Tagged	I-Citation
Clustering	I-Citation
,	I-Citation
”	I-Citation
IEEE	I-Citation
Comp	I-Citation
.	I-Citation
Soc	I-Citation
.	I-Citation
Press	I-Citation
,	I-Citation
IEEE	I-Citation
International	I-Citation
Conference	I-Citation
on	I-Citation
Acoustic	I-Citation
,	I-Citation
Speech	I-Citation
,	I-Citation
and	I-Citation
Signal	I-Citation
Processing	I-Citation
(	I-Citation
Los	I-Citation
Alamitos	I-Citation
,	I-Citation
US	I-Citation
)	I-Citation
,	I-Citation
p.	I-Citation
1487	I-Citation
-	I-Citation
1490	I-Citation
,	I-Citation
(	I-Citation
1997	I-Citation
)	I-Citation
.	O

H.	B-Citation
Franco	I-Citation
,	I-Citation
“	I-Citation
Context-Dependent	I-Citation
Connectionist	I-Citation
Probability	I-Citation
Estimation	I-Citation
in	I-Citation
a	I-Citation
Hybrid	I-Citation
Markov	I-Citation
Model-Neural	I-Citation
Net	I-Citation
Speech	I-Citation
Recognition	I-Citation
System	I-Citation
,	I-Citation
”	I-Citation
Computer	I-Citation
Speech	I-Citation
and	I-Citation
Language	I-Citation
,	I-Citation
vol.	I-Citation
8	I-Citation
(	I-Citation
No.	I-Citation
3	I-Citation
)	I-Citation
,	I-Citation
(	I-Citation
Feb.	I-Citation
22	I-Citation
,	I-Citation
1994	I-Citation
)	I-Citation
.	O

J	B-Citation
Fritsch	I-Citation
,	I-Citation
et_al.	I-Citation
,	I-Citation
“	I-Citation
Context-Dependent	I-Citation
Hybrid	I-Citation
HME/HMM	I-Citation
Speech	I-Citation
Recognition	I-Citation
Using	I-Citation
Polyphone	I-Citation
Clustering	I-Citation
Decision	I-Citation
Trees	I-Citation
,	I-Citation
”	I-Citation
Proc.	I-Citation
of	I-Citation
ICASSP	I-Citation
'97	I-Citation
.	O

D.	B-Citation
J.	I-Citation
Kershaw	I-Citation
,	I-Citation
et_al.	I-Citation
,	I-Citation
“	I-Citation
Context-Dependent	I-Citation
Classes	I-Citation
in	I-Citation
a	I-Citation
Hybrid	I-Citation
Recurrent	I-Citation
Network	I-Citation
HMM	I-Citation
Speech	I-Citation
Recognition	I-Citation
System	I-Citation
,	I-Citation
”	I-Citation
Tech.	I-Citation
Rep.	I-Citation
CUED/F-INFENG/TR217	I-Citation
,	I-Citation
CUED	I-Citation
,	I-Citation
Cambridge	I-Citation
England	I-Citation
1995	I-Citation
.	O

D.	B-Citation
L.	I-Citation
Thomson	I-Citation
,	I-Citation
“	I-Citation
Ten	I-Citation
Case	I-Citation
Studies	I-Citation
of	I-Citation
the	I-Citation
Effect	I-Citation
of	I-Citation
Field	I-Citation
Conditions	I-Citation
on	I-Citation
Speech	I-Citation
Recognition	I-Citation
Errors	I-Citation
,	I-Citation
”	I-Citation
Proceedings	I-Citation
of	I-Citation
the	I-Citation
IEEE	I-Citation
ASRU	I-Citation
Workshop	I-Citation
,	I-Citation
(	I-Citation
Feb.	I-Citation
22	I-Citation
,	I-Citation
1997	I-Citation
)	I-Citation
.	O

J.	B-Citation
Schurmann	I-Citation
and	I-Citation
W.	I-Citation
Doster	I-Citation
,	I-Citation
“	I-Citation
A	I-Citation
Decision	I-Citation
Theoretic	I-Citation
Approach	I-Citation
to	I-Citation
Hierarchical	I-Citation
Classifier	I-Citation
Design	I-Citation
,	I-Citation
”	I-Citation
Pattern	I-Citation
Recognition	I-Citation
17	I-Citation
(	I-Citation
3	I-Citation
)	I-Citation
,	I-Citation
(	I-Citation
Feb.	I-Citation
22	I-Citation
,	I-Citation
1994	I-Citation
)	I-Citation
.	O

J.	B-Citation
Fritsch	I-Citation
,	I-Citation
“	I-Citation
Acid/HNN	I-Citation
;	I-Citation
A	I-Citation
Framework	I-Citation
for	I-Citation
Hierarchical	I-Citation
Connectionist	I-Citation
Acoustic	I-Citation
Modeling	I-Citation
,	I-Citation
”	I-Citation
Proceedsing	I-Citation
of	I-Citation
IEEE	I-Citation
ASRU	I-Citation
Workshop	I-Citation
,	I-Citation
(	I-Citation
Feb.	I-Citation
22	I-Citation
,	I-Citation
1997	I-Citation
)	I-Citation
.	O

C.	B-Citation
J.	I-Citation
Leggetter	I-Citation
and	I-Citation
P.	I-Citation
C.	I-Citation
Woodland	I-Citation
,	I-Citation
“	I-Citation
Speaker	I-Citation
Adaptation	I-Citation
of	I-Citation
HMMs	I-Citation
using	I-Citation
Linear	I-Citation
Regression	I-Citation
,	I-Citation
”	I-Citation
Tech.	I-Citation
Rep.	I-Citation
CUED/F-INFENG/TR181	I-Citation
,	I-Citation
CUED	I-Citation
,	I-Citation
(	I-Citation
Feb.	I-Citation
22	I-Citation
,	I-Citation
1994	I-Citation
)	I-Citation
.	O

Franco	B-Citation
,	I-Citation
H.	I-Citation
,	I-Citation
“	I-Citation
Context-Dependent	I-Citation
Connectionist	I-Citation
Probability	I-Citation
Estimation	I-Citation
in	I-Citation
a	I-Citation
Hybrid	I-Citation
Markov	I-Citation
Model-Neural	I-Citation
Net	I-Citation
Speech	I-Citation
Recognition	I-Citation
System	I-Citation
”	I-Citation
,	I-Citation
Computer	I-Citation
Speech	I-Citation
and	I-Citation
Language,vol	I-Citation
.	I-Citation
8	I-Citation
,	I-Citation
No.	I-Citation
3	I-Citation
,	I-Citation
Jul.	I-Citation
1994	I-Citation
.	O

Fritsch	B-Citation
,	I-Citation
J.	I-Citation
,	I-Citation
et	I-Citation
al	I-Citation
,	I-Citation
“	I-Citation
Context-Dependent	I-Citation
Hybrid	I-Citation
MHE/HMM	I-Citation
Speech	I-Citation
Recognition	I-Citation
Using	I-Citation
Polyphone	I-Citation
Clustering	I-Citation
Decision	I-Citation
Trees	I-Citation
”	I-Citation
,	I-Citation
Proc.	I-Citation
Of	I-Citation
ICASS	I-Citation
'97	I-Citation
,	I-Citation
Apr.	I-Citation
21	I-Citation
-	I-Citation
24	I-Citation
,	I-Citation
1997	I-Citation
.	O

Kershaw	B-Citation
,	I-Citation
D.	I-Citation
J.	I-Citation
,	I-Citation
et	I-Citation
al	I-Citation
,	I-Citation
“	I-Citation
Contest-Dependent	I-Citation
Classes	I-Citation
in	I-Citation
a	I-Citation
Hybrid	I-Citation
Recurrent	I-Citation
Network	I-Citation
HMM	I-Citation
Speech	I-Citation
Recognition	I-Citation
System	I-Citation
”	I-Citation
,	I-Citation
Tech.	I-Citation
Rep	I-Citation
CUED/F-INFENG/TR217	I-Citation
,	I-Citation
CUED,Cambridge	I-Citation
,	I-Citation
England	I-Citation
,	I-Citation
Jul.	I-Citation
1995	I-Citation
.	O

Thomson	B-Citation
,	I-Citation
D.	I-Citation
L.	I-Citation
,	I-Citation
“	I-Citation
Ten	I-Citation
Case	I-Citation
Studies	I-Citation
of	I-Citation
the	I-Citation
Effect	I-Citation
of	I-Citation
Field	I-Citation
Conditions	I-Citation
on	I-Citation
Speech	I-Citation
Recognition	I-Citation
Errors	I-Citation
”	I-Citation
,Proceedings	I-Citation
of	I-Citation
the	I-Citation
IEEE	I-Citation
ASRU	I-Citation
Workshop,Dec	I-Citation
.	I-Citation
17	I-Citation
,	I-Citation
1997	I-Citation
.	O

Schurmann	B-Citation
,	I-Citation
J.	I-Citation
,	I-Citation
et_al.	I-Citation
“	I-Citation
A	I-Citation
Decision	I-Citation
Theoretic	I-Citation
Approach	I-Citation
to	I-Citation
Hierarchical	I-Citation
Classifier	I-Citation
Design	I-Citation
”	I-Citation
,	I-Citation
Pattern	I-Citation
Recognition,17	I-Citation
(	I-Citation
3	I-Citation
)	I-Citation
,	I-Citation
1984	I-Citation
.	O

Fritsch	B-Citation
.	I-Citation
,	I-Citation
J.	I-Citation
,	I-Citation
“	I-Citation
ACIDHNN	I-Citation
;	I-Citation
A	I-Citation
Framcwork	I-Citation
for	I-Citation
Hierarchical	I-Citation
Connectionist	I-Citation
Acoustic	I-Citation
Modeling	I-Citation
”	I-Citation
,Proceedings	I-Citation
of	I-Citation
IEEE	I-Citation
ASRU	I-Citation
Workshop,Dec	I-Citation
.	I-Citation
14	I-Citation
-	I-Citation
17	I-Citation
,	I-Citation
1997	I-Citation
.	O

Leggetter	B-Citation
,	I-Citation
C.	I-Citation
J.	I-Citation
,	I-Citation
et	I-Citation
al	I-Citation
,	I-Citation
“	I-Citation
Speaker	I-Citation
Adaptation	I-Citation
of	I-Citation
HMM	I-Citation
's	I-Citation
Using	I-Citation
Linear	I-Citation
Regression	I-Citation
”	I-Citation
,	I-Citation
Tech.	I-Citation
Rep.	I-Citation
CUED/F-INFENG/TR181	I-Citation
,	I-Citation
CUED	I-Citation
,	I-Citation
Jun.	I-Citation
1994	I-Citation
.	O

US	O
7542949	O
B2	O
20090602	O

20040512	O

US	O
7587321	O
B2	O
20090908	O

20010508	O

US	O
7526431	O
B2	O
20090428	O

20040924	O

US	O
7505911	O
B2	O
20090317	O

20041205	O

US	O
7467089	O
B2	O
20081216	O

20041205	O

US	O
7472064	O
B1	O
20081230	O

20000930	O

US	O
7444286	O
B2	O
20081028	O

20041205	O

US	O
7295979	O
B2	O
20071113	O

20010222	O

US	O
7313526	O
B2	O
20071225	O

20040924	O

US	O
7225130	O
B2	O
20070529	O

20020906	O

US	O
7050668	O
B2	O
20060523	O

20030619	O

US	O
6999925	O
B2	O
20060214	O

20011113	O

US	O
7016887	O
B2	O
20060321	O

20010629	O

US	O
6947891	O
B2	O
20050920	O

20010122	O

US	O
6865531	O
B1	O
20050308	O

20010301	O

US	O
7805301	O
B2	O
20100928	O

20050701	O

US	O
7809574	O
B2	O
20101005	O

20040924	O

Lernout	O
&	O
Hauspie	O
Speech	O
Products	O
N.	O
V.	O

03	O

BE	O

Waibel	O
,	O
Alex	O

Pittsburgh	O
PA	O

Fritsch	O
,	O
Juergen	O

Karlsruhe	O
DE	O

Bromberg	O
&	O
Sunstein	O

Dorvil	O
Richemond	O
2741	O

Abebe	O
Daniel	O

JP	O
2002529800	O
A	O
20020910	O

19991105	O

DE	O
19982503	O
T1	O
20010308	O

19991105	O

WO	O
0028526	O
A1	O
20000518	O

19991105	O

JP	O
2002529800	O
T	O
20020910	O

19991105	O

US	O
6324510	O
B1	O
20011127	O

19981106	O

US	O
6324510	O
B1	O
20011127	O

19981106	O

WO	O
0028526	O
A1	O
20000518	O

19991105	O

JP	O
2002529800	O
T	O
20020910	O

19991105	O

DE	O
19982503	O
T1	O
20010308	O

19991105	O

JP	O
2002529800	O
A	O
20020910	O

19991105	O

DE	O
19982503	O
T0	O
20010308	O

19991105	O

A	O
method	O
of	O
organizing	O
an	O
acoustic	O
model	O
for	O
speech	O
recognition	O
is	O
comprised	O
of	O
the	O
steps	O
of	O
calculating	O
a	O
measure	O
of	O
acoustic	O
dissimilarity	O
of	O
subphonetic	O
units	O
.	O
A	O
clustering	O
technique	O
is	O
recursively	O
applied	O
to	O
the	O
subphonetic	O
units	O
based	O
on	O
the	O
calculated	O
measure	O
of	O
acoustic	O
dissimilarity	O
to	O
automatically	O
generate	O
a	O
hierarchically	O
arranged	O
model	O
.	O
Each	O
application	O
of	O
the	O
clustering	O
technique	O
produces	O
another	O
level	O
of	O
the	O
hierarchy	O
with	O
the	O
levels	O
progressing	O
from	O
the	O
least	O
specific	O
to	O
the	O
most	O
specific	O
.	O
A	O
technique	O
for	O
adapting	O
the	O
structure	O
and	O
size	O
of	O
a	O
trained	O
acoustic	O
model	O
to	O
an	O
unseen	O
domain	O
using	O
only	O
a	O
small	O
amount	O
of	O
adaptation	O
data	O
is	O
also	O
disclosed	O
.	O

19990208	O

AS	O
ASSIGNMENT	O
N	O
US	O
6324510B1	O
INTERACTIVE	O
SYSTEMS	O
,	O
INC.	O
,	O
PENNSYLVANIA	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNORS:WAIBEL	O
,	O
ALEX;FRITSCH	O
,	O
JUERGEN;REEL/FRAME:009747/0830;SIGNING	O
DATES	O
FROM	O
19990102	O
TO	O
19990119	O

20000918	O

AS	O
ASSIGNMENT	O
N	O
US	O
6324510B1	O
LERNOUT	O
&	O
HAUSPIE	O
SPEECH	O
PRODUCTS	O
N.	O
V.	O
,	O
BELGIUM	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNOR:INTERACTIVE	O
SYSTEMS	O
,	O
INC.;REEL/FRAME:011096/0707	O

20000912	O

20020205	O

AS	O
ASSIGNMENT	O
N	O
US	O
6324510B1	O
MULTIMODAL	O
TECHNOLOGIES	O
,	O
INC.	O
,	O
PENNSYLVANIA	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNOR:INTERACTIVE	O
SYSTEMS	O
,	O
INC.;REEL/FRAME:012581/0006	O

20011221	O

20020509	O

AS	O
ASSIGNMENT	O
N	O
US	O
6324510B1	O
MULTIMODAL	O
TECHNOLOGIES	O
,	O
INC.	O
,	O
PENNSYLVANIA	O
CORRECTIVE	O
ASSIGNMENT	O
TO	O
CORRECT	O
THE	O
ASSIGNEE	O
'S	O
ADDRESS	O
,	O
PREVIOUSLY	O
RECORDED	O
AT	O
REEL	O
012581	O
FRAME	O
0006;ASSIGNOR:INTERACTIVE	O
SYSTEMS	O
,	O
INC.;REEL/FRAME:012884/0497	O

20011221	O

20031105	O

AS	O
ASSIGNMENT	O
N	O
US	O
6324510B1	O
MULTIMODAL	O
TECHNOLOGIES	O
,	O
INC.	O
,	O
PENNSYLVANIA	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNORS:INTERACTIVE	O
SYSTEMS	O
,	O
INC.;LERNOUT	O
&	O
HAUSPIE	O
SPEECH	O
PRODUCTS	O
N.V.;REEL/FRAME:014653/0931	O

20031009	O

20050325	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
6324510B1	O
4	O

20090306	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
6324510B1	O
8	O

BACKGROUND	O
OF	O
THE	O
INVENTION	O
1	O
.	O
Field	O
of	O
the	O
Invention	O
The	O
present	O
invention	O
is	O
directed	O
generally	O
to	O
speech	O
recognition	O
systems	O
and	O
,	O
more	O
particularly	O
,	O
to	O
speech	O
recognition	O
systems	O
utilizing	O
hierarchical	O
connectionist	O
acoustic	O
models	O
.	O

2	O
.	O
Description	O
of	O
the	O
Background	O
Statistical	O
speech	O
recognition	O
based	O
on	O
hidden	O
Markov	O
models	O
(	O
HMM	O
)	O
currently	O
is	O
the	O
dominating	O
paradigm	O
in	O
the	O
research	O
community	O
even	O
though	O
several	O
limitations	O
of	O
that	O
technique	O
are	O
repeatedly	O
being	O
discussed	O
.	O
Connectionist	O
acoustic	O
models	O
have	O
proven	O
to	O
be	O
able	O
to	O
overcome	O
some	O
of	O
the	O
drawbacks	O
of	O
HMMs	O
.	O
H.	B-Citation
Bourlard	I-Citation
et_al.	I-Citation
,	I-Citation
“	I-Citation
Connectionist	I-Citation
Speech	I-Citation
Recognition	I-Citation
—	I-Citation
A	I-Citation
Hybrid	I-Citation
Approach	I-Citation
”	I-Citation
,	I-Citation
Kluwer	I-Citation
Academic	I-Citation
Press	I-Citation
,	I-Citation
1994	I-Citation
.	O
In	O
particular	O
,	O
connectionist	O
acoustic	O
models	O
were	O
shown	O
to	O
outperform	O
traditional	O
mixtures	O
of	O
Gaussians	O
based	O
acoustic	O
models	O
on	O
small	O
,	O
controlled	O
tasks	O
using	O
context-independent	O
HMMs	O
.	O

However	O
,	O
wide-spread	O
use	O
of	O
connectionist	O
acoustic	O
models	O
is	O
hindered	O
by	O
at	O
least	O
two	O
issues	O
:	O
(	O
1	O
)	O
training	O
of	O
connectionist	O
acoustic	O
models	O
is	O
much	O
slower	O
,	O
leading	O
to	O
training	O
times	O
of	O
several	O
days	O
,	O
if	O
not	O
weeks	O
,	O
and	O
(	O
2	O
)	O
poor	O
scalability	O
of	O
connectionist	O
acoustic	O
models	O
to	O
larger	O
systems	O
.	O
Refinement	O
of	O
traditional	O
mixtures	O
of	O
Gaussians	O
based	O
acoustic	O
modeling	O
using	O
phonetic	O
decision	O
trees	O
for	O
polyphonic	O
context	O
modeling	O
has	O
led	O
to	O
systems	O
consisting	O
of	O
thousands	O
of	O
HMM	O
states	O
.	O
Significant	O
gains	O
in	O
recognition	O
accuracy	O
have	O
been	O
observed	O
in	O
such	O
systems	O
.	O
Nevertheless	O
,	O
research	O
in	O
context-dependent	O
connectionist	O
acoustic	O
models	O
has	O
long	O
concentrated	O
on	O
comparably	O
small	O
systems	O
because	O
it	O
was	O
not	O
clear	O
how	O
to	O
reliably	O
estimate	O
posterior	O
probabilities	O
for	O
thousands	O
of	O
states	O
.	O
Application	O
of	O
a	O
single	O
artificial	O
neural	O
network	O
as	O
in	O
context-independent	O
modeling	O
leads	O
to	O
an	O
unfeasibly	O
large	O
number	O
of	O
output	O
nodes	O
.	O
Factoring	O
posteriors	O
based	O
on	O
context	O
,	O
monophone	O
or	O
HMM	O
state	O
identity	O
was	O
shown	O
to	O
be	O
capable	O
of	O
breaking	O
down	O
the	O
global	O
estimation	O
problem	O
into	O
subproblems	O
of	O
small	O
enough	O
size	O
to	O
allow	O
the	O
application	O
of	O
multiple	O
artificial	O
neural	O
networks	O
.	O
H.	B-Citation
Franco	I-Citation
,	I-Citation
“	I-Citation
Context-dependent	I-Citation
connectionist	I-Citation
probability	I-Citation
estimation	I-Citation
in	I-Citation
a	I-Citation
hybrid	I-Citation
Hidden	I-Citation
Markov	I-Citation
Model	I-Citation
—	I-Citation
Neural	I-Citation
Net	I-Citation
speech	I-Citation
recognition	I-Citation
system	I-Citation
”	I-Citation
,	I-Citation
Computer	I-Citation
Speech	I-Citation
and	I-Citation
Language,Vol	I-Citation
.	I-Citation
8	I-Citation
,	I-Citation
No.	I-Citation
3	I-Citation
,	I-Citation
1994	I-Citation
;	O
J.	B-Citation
Fritsch	I-Citation
,	I-Citation
et_al.	I-Citation
,	I-Citation
“	I-Citation
Context-Dependent	I-Citation
Hybrid	I-Citation
HME/HMM	I-Citation
Speech	I-Citation
Recognition	I-Citation
using	I-Citation
Polyphone	I-Citation
Clustering	I-Citation
Decision	I-Citation
Trees	I-Citation
”	I-Citation
,	I-Citation
Proc.	I-Citation
Of	I-Citation
ICASSP	I-Citation
'97	I-Citation
,	I-Citation
Munich	I-Citation
1997	I-Citation
;	O
D.	B-Citation
J.	I-Citation
Kershaw	I-Citation
,	I-Citation
et	I-Citation
al	I-Citation
,	I-Citation
“	I-Citation
Context-Dependent	I-Citation
Classes	I-Citation
in	I-Citation
a	I-Citation
Hybrid	I-Citation
Recurrent	I-Citation
Network	I-Citation
HMM	I-Citation
Speech	I-Citation
Recognition	I-Citation
System	I-Citation
”	I-Citation
,	I-Citation
Tech.	I-Citation
Rep.	I-Citation
CUED/F-INFENG/TR217	I-Citation
,	I-Citation
CUED	I-Citation
,	I-Citation
Cambridge	I-Citation
,	I-Citation
England	I-Citation
1995	I-Citation
.	O

Comparable	O
gains	O
in	O
performance	O
were	O
achieved	O
with	O
context-dependent	O
connectionist	O
acoustic	O
models	O
based	O
on	O
that	O
technique	O
.	O
However	O
,	O
factoring	O
posteriors	O
in	O
terms	O
of	O
monophone	O
and	O
context	O
identity	O
seems	O
to	O
be	O
limited	O
to	O
medium	O
size	O
systems	O
.	O
In	O
large	O
systems	O
,	O
non-uniform	O
distribution	O
of	O
the	O
number	O
of	O
context	O
classes	O
again	O
leads	O
to	O
unfeasibly	O
large	O
numbers	O
of	O
output	O
nodes	O
for	O
some	O
of	O
the	O
context	O
networks	O
.	O

Another	O
problem	O
with	O
current	O
HMM-based	O
speech	O
recognition	O
technology	O
is	O
that	O
it	O
suffers	O
from	O
domain	O
dependence	O
.	O
Over	O
the	O
years	O
,	O
the	O
community	O
has	O
validated	O
and	O
commercialized	O
the	O
technology	O
based	O
on	O
standardized	O
training	O
and	O
test	O
sets	O
in	O
restricted	O
domains	O
,	O
such	O
as	O
the	O
Wall	O
Street	O
Journal	O
(	O
WSJ	O
)	O
(	O
business	O
newspaper	O
texts	O
)	O
,	O
Switchboard	O
(	O
SWB	O
)	O
(	O
spontaneous	O
telephone	O
conversations	O
)	O
and	O
Broadcast	O
News	O
(	O
BN	O
)	O
(	O
radio/tv	O
news	O
shows	O
)	O
.	O
Performance	O
of	O
systems	O
trained	O
on	O
such	O
domains	O
typically	O
drops	O
significantly	O
when	O
applied	O
to	O
a	O
different	O
domain	O
,	O
especially	O
with	O
changing	O
speaking	O
style	O
,	O
e.g.	O
when	O
moving	O
from	O
read	O
speech	O
to	O
spontaneous	O
speech	O
.	O
D.	B-Citation
L.	I-Citation
Thomson	I-Citation
,	I-Citation
“	I-Citation
Ten	I-Citation
Case	I-Citation
Studies	I-Citation
of	I-Citation
the	I-Citation
Effect	I-Citation
of	I-Citation
Field	I-Citation
Conditions	I-Citation
on	I-Citation
Speech	I-Citation
Recognition	I-Citation
Errors	I-Citation
”	I-Citation
,Proceedings	I-Citation
of	I-Citation
the	I-Citation
IEEE	I-Citation
ASRU	I-Citation
Workshop,Santa	I-Citation
Barbara	I-Citation
,	I-Citation
1997	I-Citation
.	O
For	O
instance	O
,	O
performance	O
of	O
a	O
recognizer	O
trained	O
on	O
WSJ	O
typically	O
decreases	O
severely	O
when	O
decoding	O
SWB	O
data	O
.	O
Several	O
factors	O
can	O
be	O
held	O
responsible	O
for	O
the	O
strong	O
domain	O
dependence	O
of	O
current	O
statistical	O
speech	O
recognition	O
systems	O
.	O
One	O
is	O
constrained	O
quality	O
,	O
type	O
or	O
recording	O
conditions	O
of	O
domain	O
specific	O
speech	O
data	O
(	O
read	O
,	O
conversational	O
,	O
spontaneous	O
speech/noisy	O
,	O
clean	O
recordings/presence	O
of	O
acoustic	O
background	O
sources	O
,	O
etc	O
.	O
)	O
.	O
Another	O
is	O
vocabulary	O
and	O
language	O
model	O
dependence	O
of	O
phonetic	O
context	O
modeling	O
based	O
on	O
phonetic	O
decision	O
trees	O
.	O
That	O
implies	O
a	O
strong	O
dependence	O
of	O
allophonic	O
models	O
on	O
the	O
specific	O
domain	O
.	O
Another	O
factor	O
is	O
domain	O
dependent	O
optimization	O
of	O
size	O
of	O
acoustic	O
model	O
based	O
on	O
amount	O
of	O
available	O
training	O
data	O
and/or	O
size	O
of	O
vocabulary	O
.	O
While	O
the	O
first	O
of	O
the	O
above-mentioned	O
factors	O
is	O
typically	O
addressed	O
by	O
some	O
sort	O
of	O
speaker	O
and/or	O
environment	O
adaptation	O
technique	O
,	O
the	O
latter	O
two	O
factors	O
are	O
usually	O
not	O
adequately	O
addressed	O
in	O
cross-domain	O
applications	O
.	O

Consider	O
the	O
scenario	O
of	O
porting	O
a	O
trained	O
recognizer	O
to	O
a	O
different	O
domain	O
within	O
the	O
same	O
language	O
.	O
Usually	O
,	O
a	O
phonetic	O
dictionary	O
for	O
the	O
new	O
domain	O
based	O
on	O
the	O
set	O
of	O
phones	O
modeled	O
by	O
the	O
recognizer	O
can	O
be	O
constructed	O
relatively	O
easily	O
using	O
a	O
large	O
background	O
dictionary	O
and	O
,	O
if	O
necessary	O
,	O
applying	O
a	O
set	O
of	O
phone	O
mapping	O
rules	O
.	O
Also	O
,	O
we	O
consider	O
it	O
justifiable	O
to	O
assume	O
that	O
enough	O
text	O
data	O
is	O
available	O
,	O
such	O
that	O
we	O
can	O
train	O
a	O
statistical	O
language	O
model	O
for	O
the	O
new	O
domain	O
.	O
What	O
typically	O
makes	O
porting	O
efforts	O
expensive	O
and	O
time	O
consuming	O
is	O
the	O
adaptation	O
of	O
the	O
acoustic	O
model	O
.	O
The	O
most	O
common	O
approach	O
of	O
applying	O
supervised	O
acoustic	O
adaptation	O
techniques	O
requires	O
large	O
amounts	O
of	O
transcribed	O
speech	O
data	O
from	O
the	O
new	O
domain	O
to	O
capture	O
the	O
differing	O
statistics	O
reasonably	O
well	O
.	O

Thus	O
,	O
the	O
need	O
exists	O
for	O
an	O
acoustic	O
model	O
which	O
exhibits	O
full	O
scalability	O
,	O
avoids	O
stability	O
problems	O
due	O
to	O
non-uniform	O
prior	O
distributions	O
and	O
is	O
easily	O
integrated	O
into	O
existing	O
large	O
vocabulary	O
conversational	O
speech	O
recognition	O
(	O
LVCSR	O
)	O
systems	O
.	O
The	O
need	O
also	O
exists	O
for	O
a	O
trained	O
acoustic	O
model	O
to	O
be	O
easily	O
adapted	O
in	O
structure	O
and	O
size	O
to	O
unseen	O
domains	O
using	O
only	O
small	O
amounts	O
of	O
adaptation	O
data	O
.	O

SUMMARY	O
OF	O
THE	O
INVENTION	O
The	O
present	O
invention	O
is	O
directed	O
to	O
a	O
method	O
of	O
organizing	O
an	O
acoustic	O
model	O
for	O
speech	O
recognition	O
comprised	O
of	O
the	O
steps	O
of	O
calculating	O
a	O
measure	O
of	O
acoustic	O
dissimilarity	O
of	O
subphonetic	O
units	O
.	O
Recursively	O
clustering	O
the	O
subphonetic	O
units	O
based	O
on	O
the	O
calculated	O
measure	O
automatically	O
generates	O
a	O
hierarchically	O
arranged	O
model	O
.	O
An	O
apparatus	O
for	O
performing	O
the	O
method	O
is	O
also	O
disclosed	O
.	O

Starting	O
from	O
an	O
initial	O
set	O
of	O
decision	O
tree	O
clustered	O
,	O
context-dependent	O
,	O
subphonetic	O
units	O
,	O
the	O
present	O
invention	O
uses	O
an	O
aglommerative	O
clustering	O
algorithm	O
across	O
monophones	O
to	O
automatically	O
design	O
a	O
tree-structured	O
decomposition	O
of	O
posterior	O
probabilities	O
which	O
is	O
instantiated	O
with	O
thousands	O
of	O
small	O
neural	O
network	O
estimators	O
at	O
each	O
of	O
the	O
nodes	O
of	O
the	O
tree	O
.	O

The	O
present	O
invention	O
is	O
also	O
directed	O
to	O
a	O
method	O
of	O
structurally	O
adapting	O
a	O
hierarchical	O
acoustic	O
model	O
having	O
both	O
nodes	O
and	O
leaves	O
to	O
a	O
new	O
domain	O
.	O
The	O
method	O
is	O
comprised	O
of	O
the	O
steps	O
of	O
identifying	O
nodes	O
that	O
receive	O
more	O
than	O
a	O
predetermined	O
amount	O
of	O
adaptation	O
data	O
and	O
adapting	O
the	O
local	O
estimators	O
of	O
conditional	O
posteriors	O
and	O
priors	O
of	O
the	O
identified	O
nodes	O
using	O
data	O
from	O
the	O
new	O
domain	O
.	O
A	O
user-specified	O
quantity	O
of	O
the	O
non-identified	O
nodes	O
are	O
removed	O
and	O
leaves	O
are	O
created	O
,	O
where	O
needed	O
,	O
to	O
replace	O
the	O
removed	O
nodes	O
.	O
All	O
of	O
the	O
HMM	O
states	O
are	O
related	O
to	O
the	O
new	O
leaves	O
such	O
that	O
they	O
share	O
a	O
single	O
model	O
represented	O
by	O
the	O
new	O
leaves	O
.	O

The	O
disclosed	O
method	O
allows	O
effective	O
adaptation	O
of	O
the	O
structure	O
and	O
size	O
of	O
a	O
trained	O
acoustic	O
model	O
to	O
an	O
unseen	O
domain	O
using	O
only	O
a	O
small	O
amount	O
of	O
adaptation	O
data	O
.	O
The	O
present	O
invention	O
benefits	O
from	O
the	O
multi-level	O
,	O
hierarchical	O
representation	O
of	O
the	O
context-dependent	O
acoustic	O
model	O
.	O
In	O
contrast	O
to	O
approaches	O
based	O
on	O
acoustic	O
adaptation	O
only	O
,	O
the	O
present	O
invention	O
uses	O
an	O
estimate	O
of	O
the	O
a-priori	O
distribution	O
of	O
modeled	O
HMM	O
states	O
on	O
the	O
new	O
domain	O
to	O
dynamically	O
downsize	O
or	O
prune	O
the	O
tree-structured	O
acoustic	O
model	O
.	O
In	O
that	O
manner	O
,	O
the	O
present	O
invention	O
accounts	O
for	O
differences	O
in	O
vocabulary	O
size	O
and	O
adjusts	O
to	O
the	O
specificity	O
of	O
phonetic	O
context	O
observed	O
in	O
the	O
new	O
domain	O
.	O

By	O
adapting	O
the	O
specificity	O
of	O
the	O
acoustic	O
model	O
,	O
improved	O
performance	O
can	O
be	O
obtained	O
with	O
very	O
little	O
requirements	O
for	O
adaptation	O
data	O
.	O
Furthermore	O
,	O
the	O
present	O
invention	O
compensates	O
over	O
fitting	O
effects	O
particularly	O
when	O
targeting	O
a	O
domain	O
with	O
a	O
much	O
smaller	O
vocabulary	O
.	O
The	O
present	O
invention	O
may	O
also	O
be	O
applied	O
to	O
downsize/prune	O
an	O
acoustic	O
model	O
to	O
any	O
desired	O
size	O
to	O
accommodate	O
computing	O
and/or	O
memory	O
resource	O
limitations	O
.	O
Those	O
,	O
and	O
other	O
advantages	O
and	O
benefits	O
of	O
the	O
present	O
invention	O
,	O
will	O
become	O
apparent	O
from	O
reading	O
the	O
Description	O
Of	O
The	O
Preferred	O
Embodiment	O
hereinbelow	O
.	O

BRIEF	O
DESCRIPTION	O
OF	O
THE	O
DRAWINGS	O
For	O
the	O
present	O
invention	O
to	O
be	O
easily	O
understood	O
and	O
readily	O
practiced	O
,	O
the	O
present	O
invention	O
will	O
now	O
be	O
described	O
for	O
purposes	O
of	O
illustration	O
and	O
not	O
limitation	O
in	O
connection	O
with	O
the	O
following	O
figures	O
wherein	O
:	O

FIG	O
.	O
1illustrates	O
a	O
computer	O
on	O
which	O
the	O
present	O
invention	O
may	O
be	O
practiced	O
;	O

FIG	O
.	O
2illustrates	O
the	O
tree	O
structure	O
of	O
a	O
hierarchical	O
decomposition	O
of	O
posteriors	O
;	O

FIG	O
.	O
3illustrates	O
a	O
partial	O
dendrogram	O
resulting	O
from	O
the	O
clustering	O
technique	O
of	O
the	O
present	O
invention	O
;	O

FIGS	O
.	O
4A	O
,	O
4B	O
,	O
4C	O
and4D	O
illustrate	O
the	O
integration	O
of	O
the	O
present	O
invention	O
's	O
hierarchical	O
neural	O
network	O
architecture	O
into	O
a	O
large	O
vocabulary	O
conversational	O
speech	O
recognition	O
system	O
;	O
and	O

FIGS	O
.	O
5A	O
,	O
5B	O
,	O
5C	O
and5D	O
illustrate	O
the	O
steps	O
in	O
adapting	O
a	O
baseline	O
tree	O
structure	O
to	O
a	O
new	O
,	O
smaller	O
domain	O
.	O

DESCRIPTION	O
OF	O
PREFERRED	O
EMBODIMENT	O
The	O
methods	O
of	O
the	O
present	O
invention	O
may	O
be	O
carried	O
out	O
on	O
a	O
computer10of	O
the	O
type	O
illustrated	O
in	O
FIG	O
.	O
1	O
.	O
It	O
is	O
anticipated	O
that	O
the	O
methods	O
of	O
the	O
present	O
invention	O
will	O
be	O
embodied	O
in	O
software	O
and	O
conventionally	O
stored	O
such	O
as	O
on	O
the	O
computer	O
's	O
hard	O
drive12	O
,	O
a	O
floppy	O
disk14	O
,	O
or	O
other	O
storage	O
medium	O
.	O
When	O
the	O
computer10executes	O
software	O
which	O
embodies	O
the	O
methods	O
of	O
the	O
present	O
invention	O
,	O
the	O
computer10becomes	O
the	O
means	O
necessary	O
for	O
performing	O
the	O
various	O
steps	O
of	O
the	O
method	O
.	O

Using	O
Bayes	O
rule	O
,	O
HMM	O
emission	O
probabilities	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
posterior	O
state	O
probabilities	O
.	O
This	O
is	O
attractive	O
,	O
because	O
it	O
leads	O
to	O
maximum	O
a-posteriori	O
(	O
MAP	O
)	O
instead	O
of	O
standard	O
maximum	O
likelihood	O
(	O
ML	O
)	O
training	O
.	O
According	O
to	O
this	O
setting	O
,	O
scaled	O
likelihoods	O
can	O
be	O
computed	O
from	O
posterior	O
state	O
probabilities	O
by	O
dividing	O
by	O
priors	O
,	O
which	O
are	O
estimated	O
by	O
relative	O
frequencies	O
.	O

Let	O
S	O
be	O
a	O
set	O
of	O
HMM	O
states	O
skHere	O
,	O
HMM	O
states	O
means	O
a	O
set	O
of	O
tied	O
or	O
related	O
HMM	O
states	O
,	O
typically	O
clustered	O
by	O
means	O
of	O
phonetic	O
decision	O
trees	O
.	O
If	O
we	O
have	O
a	O
method	O
which	O
gives	O
us	O
a	O
reasonable	O
partition	O
of	O
such	O
a	O
set	O
S	O
into	O
M	O
disjoint	O
and	O
non-empty	O
subsets	O
Si	O
.	O
then	O
a	O
particular	O
state	O
skwill	O
now	O
be	O
a	O
member	O
of	O
S	O
and	O
exactly	O
one	O
of	O
the	O
subsets	O
Si	O
.	O
Therefore	O
,	O
we	O
can	O
rewrite	O
the	O
posterior	O
probability	O
of	O
state	O
skas	O
a	O
joint	O
probability	O
of	O
state	O
and	O
appropriate	O
subset	O
Siand	O
factor	O
it	O
according	O
toP	O
[	O
!	O
af	O
!	O
]	O
(	O
sk	O
[	O
!VerticalSeparator	O
!	O
]	O
x	O
)	O
=	O
p	O
[	O
!	O
af	O
!	O
]	O
(	O
sk	O
,	O
Si	O
[	O
!VerticalSeparator	O
!	O
]	O
x	O
)	O
[	O
!	O
it	O
!	O
]	O
[	O
!	O
it	O
!	O
]	O
with	O
[	O
!	O
it	O
!	O
]	O
[	O
!	O
it	O
!	O
]	O
sk	O
[	O
!	O
Element	O
!	O
]	O
Si	O
=	O
p	O
[	O
!	O
af	O
!	O
]	O
(	O
Si	O
[	O
!VerticalSeparator	O
!	O
]	O
x	O
)	O
[	O
!	O
it	O
!	O
]	O
p	O
[	O
!	O
af	O
!	O
]	O
(	O
sk	O
[	O
!VerticalSeparator	O
!	O
]	O
Si	O
,	O
x	O
)	O
Thus	O
,	O
the	O
global	O
task	O
of	O
discriminating	O
between	O
all	O
the	O
states	O
in	O
S	O
has	O
been	O
converted	O
into	O
(	O
1	O
)	O
discriminating	O
between	O
subsets	O
Siand	O
(	O
2	O
)	O
independently	O
discriminating	O
between	O
the	O
states	O
skcontained	O
within	O
each	O
of	O
the	O
subsets	O
Si	O
.	O
Automatically	O
repeating	O
this	O
process	O
yields	O
a	O
hierarchical	O
tree-organized	O
structure	O
of	O
the	O
type	O
shown	O
in	O
FIG	O
.	O
2	O
.	O
Each	O
iteration	O
of	O
the	O
clustering	O
generates	O
another	O
level	O
,	O
e.g.	O
levels	O
16	O
and	O
18	O
,	O
of	O
the	O
tree	O
.	O
The	O
clustering	O
may	O
be	O
carried	O
out	O
until	O
a	O
desired	O
level	O
of	O
particularity	O
is	O
obtained	O
.	O
Each	O
level	O
of	O
the	O
model	O
thus	O
contains	O
information	O
about	O
similarity	O
of	O
acoustic	O
units	O
moving	O
from	O
a	O
course	O
to	O
a	O
fine	O
scale	O
.	O
That	O
,	O
for	O
example	O
,	O
can	O
be	O
exploited	O
in	O
efforts	O
to	O
speed	O
up	O
the	O
recognition	O
process	O
.	O
The	O
structure	O
ofFIG	O
.	O
2may	O
be	O
interpreted	O
as	O
a	O
probability	O
mass	O
distribution	O
device	O
.	O
J.	B-Citation
Schürmann	I-Citation
and	I-Citation
W.	I-Citation
Doster	I-Citation
,	I-Citation
“	I-Citation
A	I-Citation
Decision	I-Citation
Theoretic	I-Citation
Approach	I-Citation
to	I-Citation
Hierarchical	I-Citation
Classifier	I-Citation
Design	I-Citation
”	I-Citation
,	I-Citation
Pattern	I-Citation
Recognition17	I-Citation
(	I-Citation
3	I-Citation
)	I-Citation
,	I-Citation
1984	I-Citation
.	O

At	O
the	O
root	O
node	O
,	O
an	O
initial	O
probability	O
mass	O
of	O
1	O
is	O
fed	O
into	O
the	O
architecture	O
.	O
At	O
each	O
node	O
,	O
the	O
incoming	O
probability	O
mass	O
is	O
multiplied	O
by	O
the	O
conditional	O
posterior	O
probabilities	O
and	O
fed	O
into	O
the	O
children	O
nodes	O
.	O
Eventually	O
,	O
the	O
probability	O
mass	O
is	O
distributed	O
among	O
all	O
the	O
leaves	O
(	O
states	O
)	O
rendering	O
their	O
posterior	O
probabilities	O
.	O
In	O
contrast	O
,	O
typical	O
hierarchical	O
classifiers	O
such	O
as	O
classification	O
trees	O
operate	O
as	O
hard	O
switching	O
devices	O
,	O
allowing	O
only	O
a	O
single	O
path	O
from	O
root	O
node	O
to	O
one	O
of	O
the	O
leaves	O
,	O
depending	O
on	O
the	O
outcome	O
of	O
categorical	O
questions	O
in	O
internal	O
nodes	O
.	O

Because	O
perfect	O
estimation	O
of	O
(	O
conditional	O
)	O
posterior	O
probabilities	O
cannot	O
be	O
achieved	O
in	O
practice	O
,	O
the	O
proposed	O
hierarchical	O
decomposition	O
depends	O
on	O
the	O
method	O
used	O
to	O
design	O
the	O
tree	O
structure	O
.	O
In	O
our	O
preferred	O
embodiment	O
,	O
we	O
prefer	O
not	O
to	O
adopt	O
phonetic	O
decision	O
trees	O
for	O
several	O
reasons	O
:	O
(	O
1	O
)	O
In	O
most	O
cases	O
,	O
separate	O
decision	O
trees	O
are	O
used	O
to	O
independently	O
cluster	O
context	O
classes	O
for	O
each	O
monophone	O
,	O
and	O
(	O
2	O
)	O
phonetic	O
decision	O
trees	O
often	O
are	O
highly	O
unbalanced	O
.	O
Therefore	O
,	O
an	O
unconstrained	O
clustering	O
algorithm	O
that	O
allows	O
formation	O
of	O
tree	O
structured	O
hierarchies	O
across	O
phone	O
identities	O
is	O
applied	O
.	O
Furthermore	O
,	O
our	O
approach	O
implicitly	O
pursues	O
uniform	O
prior	O
distributions	O
in	O
each	O
node	O
and	O
therefore	O
avoids	O
unbalanced	O
splits	O
which	O
could	O
lead	O
to	O
poorly	O
approximated	O
conditional	O
posteriors	O
.	O

When	O
dealing	O
with	O
a	O
rather	O
large	O
number	O
of	O
classes	O
,	O
several	O
thousands	O
in	O
the	O
case	O
of	O
an	O
acoustic	O
model	O
,	O
evaluation	O
of	O
all	O
possible	O
configurations	O
for	O
a	O
hierarchical	O
decomposition	O
of	O
the	O
posterior	O
class	O
probabilities	O
becomes	O
intractable	O
.	O
Also	O
,	O
common	O
heuristic	O
top-down	O
approaches	O
based	O
on	O
examination	O
of	O
the	O
class	O
confusion	O
matrix	O
of	O
pre-trained	O
monolithic	O
classifiers	O
are	O
problematic	O
.	O
An	O
agglomerative	O
(	O
bottom-up	O
)	O
clustering	O
scheme	O
using	O
the	O
symmetric	O
information	O
divergence	O
as	O
a	O
measure	O
of	O
acoustic	O
dissimilarity	O
of	O
subphonetic	O
units	O
is	O
applied	O
.	O
Based	O
on	O
this	O
rather	O
inexpensive	O
distance	O
measure	O
,	O
subphonetic	O
units	O
can	O
be	O
clustered	O
efficiently	O
yielding	O
a	O
suitable	O
hierarchical	O
decomposition	O
of	O
posteriors	O
.	O

Consider	O
the	O
case	O
of	O
two	O
acoustic	O
classes	O
,	O
siand	O
sjwhich	O
are	O
to	O
be	O
discriminated	O
.	O
Let	O
p	O
(	O
x	O
|	O
si	O
)	O
and	O
p	O
(	O
x	O
|	O
sj	O
)	O
be	O
the	O
class	O
conditional	O
likelihoods	O
for	O
siand	O
sj	O
,	O
respectively	O
.	O
The	O
average	O
symmetric	O
discriminating	O
information	O
or	O
symmetric	O
information	O
divergence	O
between	O
siand	O
sjcan	O
then	O
be	O
defined	O
as	O
:	O
d	O
[	O
!	O
af	O
!	O
]	O
(	O
si	O
,	O
sj	O
)	O
=	O
[	O
!	O
Integral	O
!	O
]	O
x	O
[	O
!	O
it	O
!	O
]	O
(	O
p	O
[	O
!	O
af	O
!	O
]	O
(	O
x	O
[	O
!VerticalSeparator	O
!	O
]	O
si	O
)	O
-	O
p	O
[	O
!	O
af	O
!	O
]	O
(	O
x	O
[	O
!VerticalSeparator	O
!	O
]	O
sj	O
)	O
)	O
[	O
!	O
it	O
!	O
]	O
log	O
[	O
!	O
it	O
!	O
]	O
[	O
!	O
it	O
!	O
]	O
p	O
[	O
!	O
af	O
!	O
]	O
(	O
x	O
[	O
!VerticalSeparator	O
!	O
]	O
si	O
)	O
p	O
[	O
!	O
af	O
!	O
]	O
(	O
x	O
[	O
!VerticalSeparator	O
!	O
]	O
sj	O
)	O
[	O
!	O
it	O
!	O
]	O
[	O
!	O
it	O
!	O
]	O
[	O
!	O
dd	O
!	O
]	O
x	O
The	O
class-conditional	O
likelihoods	O
may	O
be	O
modeled	O
using	O
single	O
full	O
covariance	O
multivariate	O
Gaussians	O
with	O
mean	O
vectors	O
μiand	O
covariance	O
matrices	O
Σias	O
described	O
in	O
J.	B-Citation
Fritsch	I-Citation
,	I-Citation
“	I-Citation
ACID/HNN	I-Citation
;	I-Citation
A	I-Citation
Framework	I-Citation
for	I-Citation
Hierarchical	I-Citation
Connectionist	I-Citation
Acoustic	I-Citation
Modeling	I-Citation
”	I-Citation
,	I-Citation
Proceedings	I-Citation
of	I-Citation
IEEE	I-Citation
ASRU	I-Citation
Workshop	I-Citation
,	I-Citation
Santa	I-Citation
Barbara	I-Citation
,	I-Citation
1997	I-Citation
,	O
which	O
is	O
hereby	O
incorporated	O
by	O
reference	O
.	O
The	O
resulting	O
distance	O
measure	O
between	O
clusters	O
of	O
Gaussians	O
Skand	O
SlisD	O
[	O
!	O
af	O
!	O
]	O
(	O
Sk	O
,	O
Sl	O
)	O
=	O
[	O
!	O
Sum	O
!	O
]	O
si	O
[	O
!	O
Element	O
!	O
]	O
Sk	O
[	O
!	O
it	O
!	O
]	O
p	O
[	O
!	O
af	O
!	O
]	O
(	O
si	O
[	O
!VerticalSeparator	O
!	O
]	O
Sk	O
)	O
[	O
!	O
it	O
!	O
]	O
[	O
!	O
Sum	O
!	O
]	O
sj	O
[	O
!	O
Element	O
!	O
]	O
Sl	O
[	O
!	O
it	O
!	O
]	O
p	O
[	O
!	O
af	O
!	O
]	O
(	O
sj	O
[	O
!VerticalSeparator	O
!	O
]	O
Sl	O
)	O
[	O
!	O
it	O
!	O
]	O
d	O
[	O
!	O
af	O
!	O
]	O
(	O
si	O
,	O
sj	O
)	O
This	O
distance	O
measure	O
is	O
used	O
in	O
the	O
following	O
clustering	O
algorithm	O
:	O
1	O
.	O
Initialize	O
algorithm	O
with	O
n	O
clusters	O
Si	O
,	O
each	O
containing	O
(	O
1	O
)	O
a	O
parametric	O
model	O
of	O
the	O
class-conditional	O
likelihood	O
and	O
(	O
2	O
)	O
a	O
count	O
Ci	O
,	O
indicating	O
the	O
frequency	O
of	O
class	O
siin	O
the	O
training	O
set	O
.	O

2	O
.	O
Compute	O
within	O
cluster	O
priors	O
p	O
(	O
si	O
|	O
Sk	O
)	O
for	O
each	O
cluster	O
Sk	O
,	O
using	O
the	O
counts	O
Ci	O
3	O
.	O
Compute	O
the	O
symmetric	O
divergence	O
measure	O
D	O
(	O
Sk	O
,	O
Si	O
)	O
between	O
all	O
pairs	O
of	O
clusters	O
Skand	O
Si	O
.	O

4	O
.	O
Find	O
the	O
pair	O
of	O
clusters	O
with	O
minimum	O
divergence	O
,	O
Sk	O
*	O
and	O
Si	O
*	O
5	O
.	O
Create	O
a	O
new	O
cluster	O
S	O
=	O
Sk	O
*	O
∪	O
Sl	O
*	O
containing	O
all	O
Gaussians	O
of	O
Sk	O
*	O
and	O
Si	O
*	O
plus	O
their	O
respective	O
class	O
counts	O
.	O
The	O
resulting	O
parametric	O
model	O
is	O
a	O
mixture	O
of	O
Gaussians	O
where	O
the	O
mixture	O
coefficients	O
are	O
the	O
class	O
priors	O
6	O
.	O
Delete	O
clusters	O
Sk	O
*	O
and	O
Si	O
*	O
7	O
.	O
While	O
there	O
are	O
at	O
least	O
2	O
clusters	O
remaining	O
,	O
continue	O
with	O
step	O
2	O
.	O

Note	O
that	O
this	O
algorithm	O
clusters	O
HMM	O
sates	O
without	O
knowledge	O
of	O
their	O
phonetic	O
identity	O
but	O
rather	O
solely	O
on	O
acoustic	O
dissimilarity.FIG	O
.	O
3illustrates	O
the	O
resulting	O
clustering	O
on	O
a	O
very	O
small	O
subset	O
of	O
initial	O
clusters	O
.	O
The	O
ordinate	O
of	O
the	O
dendrogram	O
plot	O
shows	O
the	O
information	O
divergence	O
at	O
which	O
the	O
merger	O
occurred	O
.	O
Names	O
encode	O
monophone	O
,	O
state	O
(	O
begin	O
,	O
middle	O
,	O
end	O
)	O
and	O
context	O
if	O
(	O
numeric	O
)	O
.	O

Each	O
node	O
in	O
a	O
tree	O
structure	O
produced	O
by	O
such	O
a	O
clustering	O
algorithm	O
represents	O
conditional	O
posteriors	O
when	O
interpreted	O
as	O
a	O
hierarchical	O
decomposition	O
.	O
Estimators	O
such	O
as	O
polynomial	O
regressors	O
,	O
radial	O
basis	O
functions	O
,	O
feed-forward	O
networks	O
,	O
or	O
neural	O
networks	O
may	O
be	O
trained	O
to	O
estimate	O
such	O
posteriors	O
.	O
The	O
complete	O
connectionist	O
acoustic	O
model	O
is	O
called	O
a	O
Hierarchy	O
of	O
Neural	O
Networks	O
(	O
HNN	O
)	O
.	O
It	O
may	O
be	O
advantageous	O
to	O
reduce	O
the	O
number	O
of	O
networks	O
in	O
an	O
HNN	O
by	O
applying	O
a	O
greedy	O
,	O
bottom-up	O
,	O
node-merging	O
algorithm	O
as	O
a	O
second	O
step	O
of	O
the	O
clustering	O
process	O
.	O
Using	O
that	O
strategy	O
,	O
the	O
average	O
arity	O
of	O
the	O
HNN	O
tree	O
has	O
been	O
increased	O
from	O
2	O
to	O
about	O
8	O
.	O

Experiments	O
using	O
such	O
a	O
hierarchical	O
acoustic	O
model	O
are	O
detailed	O
in	O
J.	B-Citation
Fritsch	I-Citation
,	I-Citation
“	I-Citation
ACID/HNN	I-Citation
:	I-Citation
A	I-Citation
Framework	I-Citation
for	I-Citation
Hierarchical	I-Citation
Connectionist	I-Citation
Acoustic	I-Citation
Modeling	I-Citation
”	I-Citation
,	I-Citation
Proceedings	I-Citation
of	I-Citation
IEEE	I-Citation
ASUR	I-Citation
Workshop	I-Citation
,	I-Citation
Santa	I-Citation
Barbara	I-Citation
,	I-Citation
1997	I-Citation
.	O
The	O
method	O
of	O
the	O
present	O
invention	O
has	O
enable	O
the	O
construction	O
of	O
competitive	O
connectionist	O
acoustic	O
models	O
for	O
as	O
many	O
as	O
24	O
,	O
000	O
allophonic	O
HMM	O
states	O
.	O
Furthermore	O
,	O
the	O
hierarchical	O
structure	O
allows	O
dynamic	O
pruning	O
of	O
the	O
model	O
and	O
supports	O
acoustic	O
adaptation	O
as	O
will	O
be	O
discussed	O
.	O
For	O
a	O
given	O
acoustic	O
feature	O
vector	O
,	O
posterior	O
,	O
prior	O
,	O
and	O
scaled	O
likelihood	O
of	O
an	O
HNN	O
leaf	O
modeling	O
state	O
can	O
be	O
computed	O
incrementally	O
in	O
log	O
space	O
as	O
is	O
demonstrated	O
in	O
Fritsch	B-Citation
et_al.	I-Citation
,	I-Citation
“	I-Citation
Effective	I-Citation
Structural	I-Citation
Adaptation	I-Citation
of	I-Citation
LVCSR	I-Citation
Systems	I-Citation
to	I-Citation
Unseen	I-Citation
Domains	I-Citation
Using	I-Citation
Hierarchical	I-Citation
Connectionist	I-Citation
Acoustic	I-Citation
Models	I-Citation
”	I-Citation
,	I-Citation
In	I-Citation
Proceedings	I-Citation
of	I-Citation
ICSLP	I-Citation
'98	I-Citation
,	I-Citation
Sydney	I-Citation
,	I-Citation
Australia	I-Citation
,	I-Citation
December	I-Citation
1998	I-Citation
,	O
which	O
is	O
hereby	O
incorporated	O
by	O
reference	O
.	O

Because	O
the	O
conditional	O
log	O
posteriors	O
and	O
log	O
priors	O
are	O
all	O
negative	O
,	O
partial	O
posteriors	O
and	O
priors	O
of	O
leaf	O
nodes	O
decrease	O
monotonically	O
when	O
traversing	O
the	O
tree	O
and	O
computing	O
the	O
above	O
sums	O
.	O
This	O
property	O
can	O
,	O
for	O
instance	O
,	O
be	O
exploited	O
in	O
posterior	O
pruning	O
which	O
typically	O
yields	O
significant	O
savings	O
in	O
computational	O
load	O
.	O

FIGS	O
.	O
4A	O
,	O
4B	O
and4C	O
provide	O
an	O
overview	O
of	O
how	O
the	O
HNN	O
architecture	O
is	O
applied	O
to	O
the	O
estimation	O
of	O
HMM	O
emission	O
probabilities	O
using	O
phonetic	O
decision	O
trees	O
to	O
assign	O
scaled	O
likelihoods	O
at	O
HNN	O
leaves	O
to	O
actual	O
HMM	O
states	O
.	O
FIG	O
.	O
4Dillustrates	O
the	O
instantiation	O
of	O
a	O
node	O
with	O
a	O
neural	O
network	O
.	O

An	O
interesting	O
property	O
of	O
HNNs	O
that	O
can	O
be	O
exploited	O
for	O
structural	O
adaptation	O
is	O
that	O
partially	O
computed	O
posterior	O
probabilities	O
at	O
all	O
crossed	O
paths	O
in	O
every	O
horizontal	O
cross	O
section	O
of	O
the	O
tree	O
constitute	O
a	O
legal	O
posterior	O
probability	O
distribution	O
over	O
a	O
reduced	O
(	O
merged	O
)	O
set	O
of	O
leaves	O
.	O
A	O
starting	O
point	O
for	O
structural	O
adaptation	O
is	O
an	O
HNN	O
constructed	O
and	O
trained	O
on	O
a	O
domain	O
exhibiting	O
sufficiently	O
rich	O
diversity	O
in	O
phonetic	O
context	O
to	O
provide	O
a	O
basis	O
for	O
any	O
new	O
,	O
unseen	O
domain	O
.	O
To	O
adapt	O
this	O
baseline	O
for	O
any	O
new	O
,	O
smaller	O
domain	O
typically	O
exhibiting	O
very	O
different	O
specificity	O
of	O
phonetic	O
context	O
,	O
the	O
following	O
steps	O
are	O
performed	O
:	O
1	O
.	O
Take	O
the	O
baseline	O
HNN	O
tree	O
(	O
circles=nodes	O
,	O
squares=leaves	O
)	O
(	O
FIG	O
.	O
5A	O
)	O
2	O
.	O
Select	O
nodes	O
that	O
receive	O
more	O
than	O
a	O
predetermined	O
,	O
sufficiently	O
large	O
amount	O
of	O
adaptation	O
data	O
(	O
mincount	O
)	O
and	O
adapt	O
their	O
local	O
estimators	O
of	O
conditional	O
posteriors	O
and	O
priors	O
using	O
adaptation	O
data	O
from	O
the	O
new	O
domain	O
.	O
(	O
FIG	O
.	O
5B	O
)	O
3	O
.	O
Remove	O
all	O
nodes	O
that	O
receive	O
less	O
than	O
a	O
predetermined	O
amount	O
of	O
adaptation	O
data	O
.	O
Create	O
new	O
leaf	O
nodes	O
(	O
squares	O
)	O
in	O
place	O
of	O
the	O
root	O
nodes	O
of	O
pruned	O
subtrees	O
.	O
(	O
FIG	O
.	O
5C	O
)	O
4	O
.	O
Finally	O
,	O
merge	O
leaf	O
nodes	O
of	O
pruned	O
subtrees	O
.	O
(	O
FIG	O
.	O
5D	O
)	O
Tie	O
all	O
HMM	O
states	O
corresponding	O
to	O
the	O
leaves	O
of	O
pruned	O
subtrees	O
in	O
the	O
original	O
tree	O
such	O
that	O
they	O
share	O
a	O
single	O
model	O
,	O
represented	O
by	O
the	O
newly	O
created	O
leaves	O
.	O

Although	O
step	O
2	O
appears	O
to	O
operate	O
similar	O
to	O
adaptation	O
techniques	O
such	O
as	O
regression	O
tree	O
based	O
MLLR	O
,	O
its	O
effects	O
are	O
actually	O
quite	O
different	O
due	O
to	O
the	O
possibility	O
and	O
necessity	O
of	O
adapting	O
the	O
priors	O
too	O
,	O
a	O
feature	O
that	O
is	O
unique	O
to	O
connectionist	O
architectures	O
.	O
By	O
adapting	O
the	O
local	O
conditional	O
priors	O
,	O
step	O
2	O
already	O
modifies	O
the	O
structure	O
of	O
HNNs	O
implicitly	O
by	O
,	O
for	O
instance	O
,	O
cutting	O
off	O
subtrees	O
whose	O
models	O
could	O
not	O
be	O
observed	O
in	O
the	O
adaptation	O
data	O
.	O
In	O
addition	O
,	O
steps	O
3	O
and	O
4	O
are	O
used	O
to	O
control	O
the	O
size	O
of	O
the	O
resulting	O
HNN	O
by	O
merging	O
the	O
models	O
with	O
the	O
smallest	O
prior	O
probability	O
in	O
the	O
target	O
domain	O
.	O
Furthermore	O
,	O
computational	O
complexity	O
of	O
model	O
evaluation	O
can	O
be	O
traded	O
off	O
against	O
recognition	O
accuracy	O
.	O
In	O
fact	O
,	O
it	O
turns	O
out	O
that	O
in	O
many	O
cases	O
,	O
one	O
can	O
heavily	O
downsize	O
the	O
HNN	O
tree	O
without	O
losing	O
recognition	O
accuracy	O
.	O

Experimental	O
results	O
achieved	O
using	O
the	O
disclosed	O
structural	O
adaptation	O
method	O
can	O
be	O
found	O
in	O
Fritsch	B-Citation
,	I-Citation
“	I-Citation
Effective	I-Citation
Structural	I-Citation
Adaptation	I-Citation
of	I-Citation
LVCSR	I-Citation
Systems	I-Citation
to	I-Citation
Unseen	I-Citation
Domains	I-Citation
Using	I-Citation
Hierarchical	I-Citation
Connectionist	I-Citation
Acoustic	I-Citation
Models	I-Citation
”	I-Citation
,	I-Citation
supra	I-Citation
.	O

In	O
contrast	O
to	O
conventional	O
mixtures	O
of	O
Gaussians	O
based	O
acoustic	O
models	O
,	O
the	O
HNN	O
framework	O
of	O
the	O
present	O
invention	O
does	O
not	O
require	O
additional	O
structures	O
to	O
reduce	O
the	O
complexity	O
of	O
model	O
evaluation	O
.	O
The	O
tree	O
structure	O
itself	O
can	O
be	O
exploited	O
to	O
control	O
the	O
speed-accuracy	O
trade-off	O
.	O
The	O
size	O
of	O
the	O
tree	O
,	O
and	O
hence	O
the	O
degree	O
of	O
accuracy	O
,	O
may	O
be	O
dynamically	O
adapted	O
based	O
on	O
the	O
requirements	O
and	O
data	O
available	O
for	O
a	O
given	O
task	O
.	O
The	O
evaluation	O
of	O
posterior	O
state	O
probabilities	O
follows	O
a	O
path	O
from	O
root	O
node	O
to	O
a	O
specific	O
leaf	O
in	O
the	O
HNN	O
,	O
multiplying	O
all	O
estimates	O
of	O
conditional	O
posteriors	O
along	O
the	O
way	O
.	O
Subtrees	O
can	O
be	O
pruned	O
by	O
closing	O
paths	O
whenever	O
the	O
partial	O
probability	O
falls	O
below	O
a	O
suitable	O
threshold	O
.	O
This	O
can	O
be	O
performed	O
dynamically	O
during	O
speech	O
recognition	O
.	O
This	O
way	O
the	O
evaluation	O
of	O
a	O
significant	O
amount	O
of	O
networks	O
at	O
the	O
bottom	O
of	O
the	O
HNN	O
can	O
be	O
avoided	O
,	O
possibly	O
at	O
the	O
cost	O
of	O
increased	O
error	O
rate	O
.	O

To	O
achieve	O
robust	O
adaptation	O
to	O
specific	O
speakers	O
on	O
limited	O
data	O
,	O
conventional	O
acoustic	O
models	O
usually	O
require	O
additional	O
structure	O
in	O
the	O
form	O
of	O
regression	O
trees	O
to	O
assign	O
a	O
small	O
set	O
of	O
adaptation	O
transformations	O
to	O
parameters	O
of	O
HMMs	O
as	O
in	O
an	O
MLLR	O
framework	O
.	O
C.	B-Citation
J.	I-Citation
Leggetter	I-Citation
and	I-Citation
P.	I-Citation
C.	I-Citation
Woodland	I-Citation
,	I-Citation
“	I-Citation
Speaker	I-Citation
Adaptation	I-Citation
of	I-Citation
HMMs	I-Citation
using	I-Citation
Linear	I-Citation
Regression	I-Citation
”	I-Citation
,	I-Citation
Tech.	I-Citation
Rep.	I-Citation
CUED/F-INFENG/TR181	I-Citation
,	I-Citation
CUED	I-Citation
,	I-Citation
Cambridge	I-Citation
,	I-Citation
England	I-Citation
1994	I-Citation
.	O
Such	O
information	O
is	O
readily	O
available	O
in	O
the	O
HNN	O
structure	O
and	O
robust	O
speaker	O
adaptation	O
can	O
be	O
accomplished	O
by	O
simply	O
adapting	O
those	O
networks	O
in	O
the	O
HNN	O
tree	O
that	O
receive	O
enough	O
adaptation	O
data	O
.	O
Individual	O
networks	O
can	O
be	O
adapted	O
by	O
updating	O
weights	O
of	O
either	O
all	O
or	O
some	O
of	O
the	O
layers	O
using	O
error	O
back	O
propagation	O
on	O
Viterbi	O
state	O
alignments	O
.	O
This	O
scheme	O
automatically	O
adjusts	O
to	O
the	O
amount	O
of	O
available	O
adaptation	O
data	O
.	O
In	O
case	O
of	O
very	O
little	O
data	O
,	O
only	O
a	O
few	O
networks	O
in	O
the	O
vicinity	O
of	O
the	O
root	O
node	O
will	O
get	O
updated	O
.	O
The	O
more	O
data	O
that	O
becomes	O
available	O
,	O
the	O
more	O
networks	O
receive	O
enough	O
samples	O
,	O
until	O
eventually	O
all	O
of	O
the	O
networks	O
in	O
the	O
HNN	O
become	O
subject	O
to	O
an	O
update	O
.	O

The	O
present	O
invention	O
maintains	O
the	O
advantages	O
of	O
discriminative	O
training	O
while	O
circumventing	O
the	O
limitations	O
of	O
standard	O
connectionist	O
acoustic	O
models	O
.	O
Furthermore	O
,	O
HNN	O
acoustic	O
models	O
incorporate	O
the	O
structure	O
for	O
speaker	O
adaptation	O
and	O
scoring	O
speed-up	O
algorithms	O
that	O
usually	O
require	O
additional	O
effort	O
in	O
traditional	O
mixture	O
densities	O
acoustic	O
models	O
.	O
The	O
present	O
invention	O
enables	O
effective	O
adaptation	O
of	O
the	O
structure	O
of	O
a	O
tree-structured	O
hierarchical	O
connectionist	O
acoustic	O
model	O
to	O
unseen	O
new	O
domains	O
.	O
In	O
contrast	O
to	O
existing	O
architectures	O
and	O
adaptation	O
techniques	O
,	O
the	O
present	O
invention	O
not	O
only	O
compensates	O
for	O
mismatches	O
in	O
acoustic	O
space	O
,	O
but	O
adapts	O
to	O
differing	O
specificity	O
of	O
phonetic	O
context	O
in	O
unseen	O
domains	O
by	O
adapting	O
node	O
priors	O
and	O
pruning	O
defective	O
parts	O
of	O
the	O
modeling	O
hierarchy	O
.	O

While	O
the	O
present	O
invention	O
has	O
been	O
described	O
in	O
connection	O
with	O
a	O
preferred	O
embodiment	O
thereof	O
,	O
those	O
of	O
ordinary	O
skill	O
in	O
the	O
art	O
will	O
recognize	O
that	O
many	O
modifications	O
and	O
variations	O
thereof	O
are	O
possible	O
.	O
For	O
example	O
,	O
the	O
present	O
invention	O
is	O
not	O
limited	O
to	O
the	O
disclosed	O
distance	O
measure	O
.	O
Other	O
measures	O
,	O
as	O
well	O
as	O
other	O
distance	O
measures	O
,	O
may	O
be	O
used	O
.	O
The	O
foregoing	O
disclosure	O
and	O
the	O
following	O
claims	O
are	O
intended	O
to	O
encompass	O
all	O
such	O
modifications	O
and	O
variations	O
.	O

1	O
.	O
A	O
memory	O
device	O
encoded	O
with	O
a	O
computer	O
program	O
for	O
enabling	O
a	O
computer	O
program	O
to	O
execute	O
a	O
method	O
comprised	O
of	O
the	O
steps	O
of:initializing	O
a	O
system	O
of	O
clusters	O
,	O
each	O
cluster	O
containing	O
a	O
model	O
of	O
the	O
class-conditional	O
likelihood	O
and	O
a	O
count	O
indicating	O
the	O
frequency	O
of	O
that	O
class	O
in	O
the	O
training	O
set;computing	O
within	O
cluster	O
priors	O
for	O
each	O
cluster	O
using	O
the	O
counts;computing	O
a	O
divergence	O
measure	O
between	O
all	O
pairs	O
of	O
clusters;determining	O
the	O
pair	O
of	O
clusters	O
with	O
a	O
minimum	O
divergence	O
measure;creating	O
a	O
new	O
cluster	O
based	O
on	O
the	O
determined	O
pair	O
and	O
deleting	O
the	O
pair	O
having	O
a	O
minimum	O
divergence	O
measure	O
;	O
andrepeating	O
the	O
process	O
until	O
some	O
predetermined	O
criteria	O
is	O
met	O
.	O

2	O
.	O
A	O
memory	O
device	O
according	O
to	O
claim1	O
,	O
wherein	O
each	O
cluster	O
contains	O
at	O
least	O
one	O
hidden	O
Markov	O
model	O
state	O
.	O

3	O
.	O
A	O
memory	O
device	O
according	O
to	O
claim1	O
,	O
wherein	O
the	O
divergence	O
measure	O
includes	O
a	O
symmetric	O
information	O
divergence	O
calculation	O
.	O

4	O
.	O
A	O
memory	O
device	O
according	O
to	O
claim1	O
,	O
wherein	O
the	O
class	O
conditional	O
likelihood	O
includes	O
a	O
parametric	O
model	O
.	O

5	O
.	O
A	O
memory	O
device	O
according	O
to	O
claim4	O
,	O
wherein	O
the	O
parametric	O
model	O
includes	O
a	O
multivariate	O
Gaussian	O
.	O

6	O
.	O
An	O
apparatus	O
,	O
comprising:means	O
for	O
initializing	O
a	O
system	O
of	O
clusters	O
,	O
each	O
cluster	O
containing	O
a	O
model	O
of	O
the	O
class-conditional	O
likelihood	O
and	O
a	O
count	O
indicating	O
the	O
frequency	O
of	O
that	O
class	O
in	O
the	O
training	O
set	O
;	O
means	O
for	O
computing	O
within	O
cluster	O
priors	O
for	O
each	O
cluster	O
using	O
the	O
counts;means	O
for	O
computing	O
a	O
divergence	O
measure	O
between	O
all	O
pairs	O
of	O
clusters;means	O
for	O
determining	O
the	O
pair	O
of	O
clusters	O
with	O
a	O
minimum	O
divergence	O
measure	O
;	O
andmeans	O
for	O
creating	O
a	O
new	O
cluster	O
based	O
on	O
the	O
determined	O
pair	O
and	O
deleting	O
the	O
pair	O
having	O
a	O
minimum	O
divergence	O
measure	O
.	O

7	O
.	O
An	O
apparatus	O
according	O
to	O
claim6	O
,	O
wherein	O
each	O
cluster	O
contains	O
at	O
least	O
one	O
hidden	O
Markov	O
model	O
state	O
.	O

8	O
.	O
An	O
apparatus	O
according	O
to	O
claim6	O
,	O
wherein	O
the	O
divergence	O
measure	O
includes	O
a	O
symmetric	O
information	O
divergence	O
calculation	O
.	O

9	O
.	O
An	O
apparatus	O
according	O
to	O
claim6	O
,	O
wherein	O
the	O
class	O
conditional	O
likelihood	O
includes	O
a	O
parametric	O
model	O
.	O

10	O
.	O
An	O
apparatus	O
according	O
to	O
claim9	O
,	O
wherein	O
the	O
parametric	O
model	O
includes	O
a	O
multivariate	O
Gaussian	O
.	O

11	O
.	O
A	O
method	O
of	O
organizing	O
an	O
acoustic	O
model	O
for	O
speech	O
recognition	O
,	O
comprising:initializing	O
a	O
system	O
of	O
clusters	O
,	O
each	O
cluster	O
containing	O
a	O
model	O
of	O
the	O
class-conditional	O
likelihood	O
and	O
a	O
count	O
indicating	O
the	O
frequency	O
of	O
that	O
class	O
in	O
the	O
training	O
set;computing	O
within	O
cluster	O
priors	O
for	O
each	O
cluster	O
using	O
the	O
counts;computing	O
a	O
divergence	O
measure	O
between	O
all	O
pairs	O
of	O
clusters;determining	O
the	O
pair	O
of	O
clusters	O
with	O
a	O
minimum	O
divergence	O
measure;creating	O
a	O
new	O
cluster	O
based	O
on	O
the	O
determined	O
pair	O
and	O
deleting	O
the	O
pair	O
having	O
a	O
minimum	O
divergence	O
measure	O
;	O
andrepeating	O
the	O
process	O
until	O
some	O
predetermined	O
criteria	O
is	O
met	O
.	O

12	O
.	O
A	O
method	O
according	O
to	O
claim11	O
,	O
wherein	O
each	O
cluster	O
contains	O
at	O
least	O
one	O
hidden	O
Markov	O
model	O
state	O
.	O

13	O
.	O
A	O
method	O
according	O
to	O
claim11	O
,	O
wherein	O
the	O
divergence	O
measure	O
includes	O
a	O
symmetric	O
information	O
divergence	O
calculation	O
.	O

14	O
.	O
A	O
method	O
according	O
to	O
claim11	O
,	O
wherein	O
the	O
class	O
conditional	O
likelihood	O
includes	O
a	O
parametric	O
model	O
.	O

15	O
.	O
A	O
method	O
according	O
to	O
claim14	O
,	O
wherein	O
the	O
parametric	O
model	O
includes	O
a	O
multivariate	O
Gaussian	O
.	O

16	O
.	O
A	O
method	O
of	O
structurally	O
adapting	O
a	O
hierarchical	O
acoustic	O
model	O
having	O
nodes	O
and	O
leaves	O
to	O
a	O
new	O
domain	O
,	O
comprising:identifying	O
nodes	O
that	O
receive	O
more	O
than	O
a	O
predetermined	O
amount	O
of	O
adaptation	O
data;adapting	O
the	O
local	O
estimators	O
of	O
conditional	O
posteriors	O
and	O
priors	O
of	O
the	O
identified	O
nodes	O
using	O
data	O
from	O
the	O
new	O
domain;removing	O
a	O
predetermined	O
number	O
of	O
the	O
non-identified	O
nodes;creating	O
new	O
leaves	O
where	O
needed	O
to	O
replace	O
the	O
removed	O
nodes	O
;	O
andrelating	O
all	O
HMM	O
states	O
corresponding	O
to	O
the	O
new	O
leaves	O
such	O
that	O
they	O
share	O
a	O
single	O
model	O
represented	O
by	O
the	O
new	O
leaves	O
.	O

17	O
.	O
A	O
method	O
according	O
to	O
claim16	O
,	O
wherein	O
the	O
hierarchical	O
acoustic	O
model	O
is	O
a	O
connectionist	O
acoustic	O
model	O
.	O

18	O
.	O
A	O
method	O
according	O
to	O
claim17	O
,	O
wherein	O
the	O
connectionist	O
acoustic	O
model	O
is	O
a	O
hierarchy	O
of	O
neural	O
networks	O
(	O
HNN	O
)	O
.	O

19	O
.	O
A	O
method	O
according	O
to	O
claim17	O
,	O
wherein	O
the	O
HNN	O
is	O
based	O
on	O
an	O
agglomerative	O
clustering	O
algorithm	O
.	O

20	O
.	O
A	O
method	O
according	O
to	O
claim16	O
,	O
wherein	O
the	O
hierarchical	O
acoustic	O
model	O
is	O
context-dependent	O
.	O

21	O
.	O
A	O
method	O
according	O
to	O
claim16	O
,	O
wherein	O
the	O
leaves	O
include	O
HMM	O
states	O
.	O

22	O
.	O
A	O
method	O
according	O
to	O
claim16	O
,	O
wherein	O
the	O
nodes	O
include	O
HMM	O
states	O
.	O

23	O
.	O
A	O
memory	O
device	O
encoded	O
with	O
a	O
computer	O
program	O
for	O
enabling	O
a	O
computer	O
program	O
to	O
execute	O
a	O
method	O
for	O
structurally	O
adapting	O
a	O
hierarchical	O
acoustic	O
model	O
having	O
nodes	O
and	O
leaves	O
to	O
a	O
new	O
domain	O
comprised	O
of	O
the	O
steps	O
of:identifying	O
nodes	O
that	O
receive	O
more	O
than	O
a	O
predetermined	O
amount	O
of	O
adaptation	O
data;adapting	O
the	O
local	O
estimators	O
of	O
conditional	O
posteriors	O
and	O
priors	O
of	O
the	O
identified	O
nodes	O
using	O
data	O
from	O
the	O
new	O
domain;removing	O
a	O
predetermined	O
number	O
of	O
the	O
non-identified	O
nodes;creating	O
new	O
leaves	O
where	O
needed	O
to	O
replace	O
the	O
removed	O
nodes	O
;	O
andrelating	O
all	O
HMM	O
states	O
corresponding	O
to	O
the	O
new	O
leaves	O
such	O
that	O
they	O
share	O
a	O
single	O
model	O
represented	O
by	O
the	O
new	O
leaves	O
.	O

24	O
.	O
A	O
memory	O
device	O
according	O
to	O
claim23	O
,	O
wherein	O
the	O
hierarchical	O
acoustic	O
model	O
is	O
a	O
connectionist	O
acoustic	O
model	O
.	O

25	O
.	O
A	O
memory	O
device	O
according	O
to	O
claim24	O
,	O
wherein	O
the	O
connectionist	O
acoustic	O
model	O
is	O
a	O
hierarchy	O
of	O
neural	O
networks	O
(	O
HNN	O
)	O
.	O

26	O
.	O
A	O
memory	O
device	O
according	O
to	O
claim25	O
,	O
wherein	O
the	O
HNN	O
is	O
based	O
on	O
an	O
agglomerative	O
clustering	O
algorithm	O
.	O

27	O
.	O
A	O
memory	O
device	O
according	O
to	O
claim23	O
,	O
wherein	O
the	O
hierarchical	O
acoustic	O
model	O
is	O
context-dependent	O
.	O

28	O
.	O
A	O
memory	O
device	O
according	O
to	O
claim23	O
,	O
wherein	O
the	O
leaves	O
include	O
HMM	O
states	O
.	O

29	O
.	O
A	O
memory	O
device	O
according	O
to	O
claim23	O
,	O
wherein	O
the	O
nodes	O
include	O
HMM	O
states	O
.	O

30	O
.	O
An	O
apparatus	O
for	O
structurally	O
adapting	O
a	O
hierarchical	O
acoustic	O
model	O
having	O
nodes	O
and	O
leaves	O
to	O
a	O
new	O
domain	O
,	O
comprising:means	O
for	O
identifying	O
nodes	O
that	O
receive	O
more	O
than	O
a	O
predetermined	O
amount	O
of	O
adaptation	O
data	O
;	O
means	O
for	O
adapting	O
the	O
local	O
estimators	O
of	O
conditional	O
posteriors	O
and	O
priors	O
of	O
the	O
identified	O
nodes	O
using	O
data	O
from	O
the	O
new	O
domain;means	O
for	O
removing	O
a	O
predetermined	O
number	O
of	O
the	O
non-identified	O
nodes;means	O
for	O
creating	O
new	O
leaves	O
where	O
needed	O
to	O
replace	O
the	O
removed	O
nodes	O
;	O
andmeans	O
for	O
relating	O
all	O
HMM	O
states	O
corresponding	O
to	O
the	O
new	O
leaves	O
such	O
that	O
they	O
share	O
a	O
single	O
model	O
represented	O
by	O
the	O
new	O
leaves	O
.	O

31	O
.	O
An	O
apparatus	O
according	O
to	O
claim30	O
,	O
wherein	O
the	O
hierarchical	O
acoustic	O
model	O
is	O
a	O
connectionist	O
acoustic	O
model	O
.	O

32	O
.	O
An	O
apparatus	O
according	O
to	O
claim31	O
,	O
wherein	O
the	O
connectionist	O
acoustic	O
model	O
is	O
a	O
hierarchy	O
of	O
neural	O
networks	O
(	O
HNN	O
)	O
.	O

33	O
.	O
An	O
apparatus	O
according	O
to	O
claim32	O
,	O
wherein	O
the	O
HNN	O
is	O
based	O
on	O
an	O
agglomerative	O
clustering	O
algorithm	O
.	O

34	O
.	O
An	O
apparatus	O
according	O
to	O
claim30	O
,	O
wherein	O
the	O
hierarchical	O
acoustic	O
model	O
is	O
context-dependent	O
.	O

35	O
.	O
An	O
apparatus	O
according	O
to	O
claim30	O
,	O
wherein	O
the	O
leaves	O
include	O
HMM	O
states	O
.	O

36	O
.	O
An	O
apparatus	O
according	O
to	O
claim30	O
,	O
wherein	O
the	O
nodes	O
include	O
HMM	O
states	O
.	O

