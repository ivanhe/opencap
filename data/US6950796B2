US	O
6950796	O
B2	O
20050927	O

US	O
10007886	O
20011105	O

10	O
eng	O
eng	O

US	O
09007886	O
A	O
20011105	O

US20010007886	O

20050927	O

20050927	O

614	O

7G	O
10L	O
15/06	O
A	O
7	O
G	O
10	O
L	O
15	O
06	O
A	O

7G	O
10L	O
15/14	O
B	O
7	O
G	O
10	O
L	O
15	O
14	O
B	O

7G	O
10L	O
15/20	O
B	O
7	O
G	O
10	O
L	O
15	O
20	O
B	O

7G	O
10L	O
21/02	O
B	O
7	O
G	O
10	O
L	O
21	O
02	O
B	O

G10L	O
15/00	O
20060101C	O
I20051110RMEP	O

20060101	O

C	O
G	O
10	O
L	O
15	O
00	O
I	O

20051110	O

EP	O

R	O
M	O

G10L	O
15/04	O
20060101A	O
I20051110RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
04	O
I	O

20051110	O

EP	O

R	O
M	O

G10L	O
15/06	O
20060101A	O
I20051110RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
06	O
I	O

20051110	O

EP	O

R	O
M	O

G10L	O
15/12	O
20060101A	O
I20051110RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
12	O
I	O

20051110	O

EP	O

R	O
M	O

G10L	O
15/14	O
20060101A	O
I20051110RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
14	O
I	O

20051110	O

EP	O

R	O
M	O

G10L	O
15/20	O
20060101A	O
I20051110RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
20	O
I	O

20051110	O

EP	O

R	O
M	O

G10L	O
21/00	O
20060101C	O
I20051110RMEP	O

20060101	O

C	O
G	O
10	O
L	O
21	O
00	O
I	O

20051110	O

EP	O

R	O
M	O

G10L	O
21/02	O
20060101A	O
I20051110RMEP	O

20060101	O

A	O
G	O
10	O
L	O
21	O
02	O
I	O

20051110	O

EP	O

R	O
M	O

US	O

704/244	O
704	O
244	O

704/228	O
704	O
228	O

704/233	O
704	O
233	O

704/256	O
704	O
256	O

704/E15.039	O
704	O
E15	O
.	O
039	O

G10L	O
15/20	O
G	O
10	O
L	O
15	O
20	O

S10L	O
21	O
:	O
02M1P	O
S	O
10	O
L	O
21	O
02	O

M	O
1	O
P	O

US	O

704/244	O
704	O
244	O

US	O

704/233	O
704	O
233	O

US	O

704/256	O
704	O
256	O

US	O

704/228	O
704	O
228	O

18	O
Speech	O
recognition	O
by	O
dynamical	O
noise	O
model	O
adaptation	O

US	O
5594834	O
A	O
Wang	O
19970114	O

19940930	O

US	O
5649057	O
A	O
Lee	O
et_al.	O

19970715	O

19960116	O

US	O
5664059	O
A	O
Zhao	O
19970902	O

19960916	O

US	O
5721808	O
A	O
Minami	O
et_al.	O

19980224	O

19960304	O

US	O
5956679	O
A	O
Komori	O
et_al.	O

19990921	O

19971202	O

US	O
5960395	O
A	O
Tzirkel-Hancock	O
19990928	O

19970206	O

US	O
5970446	O
A	O
Goldberg	O
et_al.	O

19991019	O

19971125	O

US	O
6026359	O
A	O
Yamaguchi	O
et_al.	O

20000215	O

19970915	O

US	O
6076057	O
A	O
Narayanan	O
et_al.	O

20000613	O

19970521	O

US	O
6078884	O
A	O
Downey	O
20000620	O

19980326	O

US	O
6108610	O
A	O
Winn	O
20000822	O

19981013	O

US	O
6131089	O
A	O
Campbell	O
et_al.	O

20001010	O

19980504	O

US	O
6188982	O
B1	O
Chiang	O
20010213	O

19971201	O

US	O
6418411	O
B1	O
Gong	O
20020709	O

20000210	O

US	O
6772117	O
B1	O
Laurila	O
et_al.	O

20040803	O

19980409	O

US	O
6778959	O
B1	O
Wu	O
et_al.	O

20040817	O

20001018	O

US	O
20010025276	O
A1	O
Pao	O
20010927	O

20001226	O

US	O
20020062212	O
A1	O
Nakatsuka	O
20020523	O

20010830	O

Reichl	B-Citation
et_al.	I-Citation
,	I-Citation
“	I-Citation
Discriminative	I-Citation
Training	I-Citation
for	I-Citation
Continuous	I-Citation
Speech	I-Citation
Recognition	I-Citation
”	I-Citation
,	I-Citation
Eurospeech-95	I-Citation
,	I-Citation
Madrid	I-Citation
,	I-Citation
Spain	I-Citation
,	I-Citation
Sep.	I-Citation
1995	I-Citation
,	I-Citation
pp.	I-Citation
537	I-Citation
-	I-Citation
540	I-Citation
.	O

US	O
7203635	O
B2	O
20070410	O

20020627	O

US	O
7660717	O
B2	O
20100209	O

20080109	O

US	O
7664643	O
B2	O
20100216	O

20060825	O

US	O
7487091	O
B2	O
20090203	O

20041109	O

US	O
7403896	O
B2	O
20080722	O

20030314	O

US	O
7797156	O
B2	O
20100914	O

20060215	O

US	O
20030088411	O
A1	O
20030508	O

Motorola	O
,	O
Inc.	O

02	O

Schaumburg	O
IL	O
US	O

Ma	O
Changxue	O

Barrington	O
IL	O
US	O

US	O

Wei	O
Yuan-Jun	O

Hoffman	O
Estates	O
IL	O
US	O

US	O

Ometz	O
David	O
L.	O

2655	O

Albertalli	O
Brian	O
L	O

WO	O
03041052	O
A1	O
20030515	O

20021028	O

US	O
6950796	O
B2	O
20050927	O

20011105	O

US	O
20030088411	O
A1	O
20030508	O

20011105	O

WO	O
03041052	O
A1	O
20030515	O

20021028	O

US	O
6950796	O
B2	O
20050927	O

20011105	O

US	O
20030088411	O
A1	O
20030508	O

20011105	O

The	O
invention	O
provides	O
a	O
Hidden	O
Markov	O
Model	O
(	O
132	O
)	O
based	O
automated	O
speech	O
recognition	O
system	O
(	O
100	O
)	O
that	O
dynamically	O
adapts	O
to	O
changing	O
background	O
noise	O
by	O
detecting	O
long	O
pauses	O
in	O
speech	O
,	O
and	O
for	O
each	O
pause	O
processing	O
background	O
noise	O
during	O
the	O
pause	O
to	O
extract	O
a	O
feature	O
vector	O
that	O
characterizes	O
the	O
background	O
noise	O
,	O
identifying	O
a	O
Gaussian	O
mixture	O
component	O
of	O
noise	O
states	O
that	O
most	O
closely	O
matches	O
the	O
extracted	O
feature	O
vector	O
,	O
and	O
updating	O
the	O
mean	O
of	O
the	O
identified	O
Gaussian	O
mixture	O
component	O
so	O
that	O
it	O
more	O
closely	O
matches	O
the	O
extracted	O
feature	O
vector	O
,	O
and	O
consequently	O
more	O
closely	O
matches	O
the	O
current	O
noise	O
environment	O
.	O
Alternatively	O
,	O
the	O
process	O
is	O
also	O
applied	O
to	O
refine	O
the	O
Gaussian	O
mixtures	O
associated	O
with	O
other	O
emitting	O
states	O
of	O
the	O
Hidden	O
Markov	O
Model	O
.	O

20011105	O

AS	O
ASSIGNMENT	O
N	O
US	O
6950796B2	O
MOTOROLA	O
,	O
INC.	O
,	O
ILLINOIS	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNORS:MA	O
,	O
CHANGXUE;WEI	O
,	O
YUAN-JUN;REEL/FRAME:012369/0404	O

20011105	O

20090224	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
6950796B2	O
4	O

20101213	O

AS	O
ASSIGNMENT	O
N	O
US	O
6950796B2	O
MOTOROLA	O
MOBILITY	O
,	O
INC	O
,	O
ILLINOIS	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNOR:MOTOROLA	O
,	O
INC;REEL/FRAME:025673/0558	O

20100731	O

FIELD	O
OF	O
THE	O
INVENTION	O
This	O
invention	O
pertains	O
to	O
automated	O
speech	O
recognition	O
.	O
More	O
particularly	O
this	O
invention	O
pertains	O
to	O
speaker	O
independent	O
speech	O
recognition	O
suitable	O
for	O
varied	O
background	O
noise	O
environments	O
.	O

BACKGROUND	O
OF	O
THE	O
INVENTION	O
Recently	O
as	O
the	O
processing	O
power	O
of	O
portable	O
electronic	O
devices	O
has	O
increased	O
there	O
has	O
been	O
an	O
increased	O
interest	O
in	O
adding	O
speech	O
recognition	O
capabilities	O
to	O
such	O
devices	O
.	O
Wireless	O
telephones	O
that	O
are	O
capable	O
of	O
operating	O
under	O
the	O
control	O
of	O
voice	O
commands	O
have	O
been	O
introduced	O
into	O
the	O
market	O
.	O
Speech	O
recognition	O
has	O
the	O
potential	O
to	O
decrease	O
the	O
effort	O
and	O
attention	O
required	O
of	O
users	O
operating	O
wireless	O
phones	O
.	O
This	O
is	O
especially	O
advantageous	O
for	O
users	O
that	O
are	O
frequently	O
engaged	O
in	O
other	O
critical	O
activities	O
(	O
e.g.	O
,	O
driving	O
)	O
while	O
operating	O
their	O
wireless	O
phones	O
.	O

The	O
most	O
widely	O
used	O
algorithms	O
for	O
performing	O
automated	O
speech	O
recognition	O
(	O
ASR	O
)	O
are	O
based	O
on	O
Hidden	O
Markov	O
Models	O
(	O
HMM	O
)	O
.	O
In	O
a	O
HMM	O
ASR	O
speech	O
is	O
modeled	O
as	O
a	O
sequence	O
of	O
states	O
.	O
These	O
states	O
are	O
assumed	O
to	O
be	O
hidden	O
and	O
only	O
output	O
based	O
on	O
the	O
states	O
,	O
i	O
.	O
e	O
.	O
speech	O
is	O
observed	O
.	O
According	O
to	O
the	O
model	O
,	O
transitions	O
between	O
these	O
states	O
are	O
governed	O
by	O
a	O
matrix	O
of	O
transition	O
probabilities	O
.	O
For	O
each	O
state	O
there	O
is	O
an	O
output	O
function	O
,	O
specifically	O
a	O
probability	O
density	O
function	O
that	O
determines	O
an	O
a	O
posteriori	O
probability	O
that	O
the	O
HMM	O
was	O
in	O
the	O
state	O
,	O
given	O
measured	O
features	O
of	O
an	O
acoustic	O
signal	O
.	O
The	O
matrix	O
of	O
transition	O
probabilities	O
,	O
and	O
parameters	O
of	O
the	O
output	O
functions	O
are	O
determined	O
during	O
a	O
training	O
procedure	O
which	O
involves	O
feeding	O
known	O
words	O
,	O
and	O
or	O
sentences	O
into	O
the	O
HMM	O
ASR	O
and	O
fine	O
tuning	O
the	O
transition	O
probabilities	O
and	O
output	O
function	O
parameters	O
to	O
achieve	O
optimized	O
recognition	O
performance	O
.	O

In	O
order	O
to	O
accommodate	O
the	O
variety	O
of	O
accents	O
and	O
other	O
variations	O
in	O
the	O
way	O
words	O
are	O
pronounced	O
,	O
spoken	O
messages	O
to	O
be	O
identified	O
using	O
a	O
HMM	O
ASR	O
system	O
are	O
processed	O
in	O
such	O
a	O
manner	O
as	O
to	O
extract	O
feature	O
vectors	O
that	O
characterize	O
successive	O
periods	O
of	O
the	O
spoken	O
message	O
.	O

In	O
performing	O
ASR	O
a	O
most	O
likely	O
sequence	O
of	O
the	O
states	O
of	O
the	O
HMM	O
is	O
determined	O
in	O
view	O
of	O
the	O
transition	O
probability	O
for	O
each	O
transition	O
in	O
the	O
sequence	O
,	O
the	O
extracted	O
feature	O
vectors	O
,	O
and	O
the	O
a	O
posteriori	O
probabilities	O
associated	O
with	O
the	O
states	O
.	O

Background	O
noise	O
,	O
which	O
predominates	O
during	O
pauses	O
in	O
speech	O
,	O
is	O
also	O
modeled	O
by	O
one	O
or	O
more	O
states	O
of	O
the	O
HMM	O
model	O
so	O
that	O
the	O
ASR	O
will	O
properly	O
identify	O
pauses	O
and	O
not	O
try	O
to	O
construe	O
background	O
noise	O
as	O
speech	O
.	O

One	O
problem	O
for	O
ASR	O
systems	O
,	O
particularly	O
those	O
used	O
in	O
portable	O
devices	O
,	O
is	O
that	O
the	O
characteristics	O
of	O
the	O
background	O
noise	O
in	O
the	O
environment	O
of	O
the	O
ASR	O
system	O
is	O
not	O
fixed	O
.	O
If	O
an	O
ASR	O
system	O
is	O
trained	O
in	O
an	O
acoustic	O
environment	O
where	O
there	O
is	O
no	O
background	O
noise	O
,	O
or	O
in	O
an	O
acoustic	O
environment	O
with	O
one	O
particular	O
type	O
of	O
background	O
noise	O
,	O
the	O
system	O
will	O
be	O
prone	O
to	O
making	O
errors	O
when	O
operated	O
in	O
an	O
environment	O
with	O
background	O
noise	O
of	O
different	O
type	O
.	O
Different	O
background	O
noise	O
that	O
is	O
unfamiliar	O
to	O
the	O
ASR	O
system	O
may	O
be	O
construed	O
as	O
parts	O
of	O
speech	O
.	O

What	O
is	O
needed	O
is	O
a	O
ASR	O
system	O
that	O
can	O
achieve	O
high	O
rates	O
of	O
speech	O
recognition	O
when	O
operated	O
in	O
environments	O
with	O
different	O
types	O
of	O
background	O
noise	O
.	O

What	O
is	O
needed	O
is	O
a	O
ASR	O
system	O
that	O
can	O
adapt	O
to	O
different	O
types	O
of	O
background	O
noise	O
.	O

BRIEF	O
DESCRIPTION	O
OF	O
THE	O
DRAWINGS	O
The	O
features	O
of	O
the	O
invention	O
believed	O
to	O
be	O
novel	O
are	O
set	O
forth	O
in	O
the	O
claims	O
.	O
The	O
invention	O
itself	O
,	O
however	O
,	O
may	O
be	O
best	O
understood	O
by	O
reference	O
to	O
the	O
following	O
detailed	O
description	O
of	O
certain	O
exemplary	O
embodiments	O
of	O
the	O
invention	O
,	O
taken	O
in	O
conjunction	O
with	O
the	O
accompanying	O
drawings	O
in	O
which	O
:	O

FIG	O
.	O
1	O
is	O
a	O
functional	O
block	O
diagram	O
of	O
a	O
system	O
for	O
performing	O
automated	O
speech	O
recognition	O
according	O
to	O
the	O
preferred	O
embodiment	O
of	O
the	O
invention	O
.	O

FIG	O
.	O
2	O
is	O
a	O
flow	O
chart	O
of	O
a	O
process	O
for	O
updating	O
a	O
model	O
of	O
background	O
noise	O
according	O
to	O
the	O
preferred	O
embodiment	O
of	O
the	O
invention	O
.	O

FIG	O
.	O
3	O
is	O
a	O
high	O
level	O
flow	O
chart	O
of	O
a	O
process	O
of	O
performing	O
automated	O
speech	O
recognition	O
using	O
a	O
Hidden	O
Markov	O
Model	O
.	O

FIG	O
.	O
4	O
is	O
a	O
first	O
part	O
of	O
flow	O
chart	O
of	O
a	O
process	O
for	O
extracting	O
feature	O
vectors	O
from	O
an	O
audio	O
signal	O
according	O
to	O
the	O
preferred	O
embodiment	O
of	O
the	O
invention	O
.	O

FIG	O
.	O
5	O
is	O
a	O
second	O
part	O
of	O
the	O
flow	O
chart	O
begun	O
in	O
FIG	O
.	O
4	O
.	O

FIG	O
.	O
6	O
is	O
a	O
hardware	O
block	O
diagram	O
of	O
the	O
system	O
for	O
performing	O
automated	O
speech	O
recognition	O
according	O
to	O
the	O
preferred	O
embodiment	O
of	O
the	O
invention	O
.	O

DETAILED	O
DESCRIPTION	O
OF	O
THE	O
PREFERRED	O
EMBODIMENT	O
While	O
this	O
invention	O
is	O
susceptible	O
of	O
embodiment	O
in	O
many	O
different	O
forms	O
,	O
there	O
are	O
shown	O
in	O
the	O
drawings	O
and	O
will	O
herein	O
be	O
described	O
in	O
detail	O
specific	O
embodiments	O
,	O
with	O
the	O
understanding	O
that	O
the	O
present	O
disclosure	O
is	O
to	O
be	O
considered	O
as	O
an	O
example	O
of	O
the	O
principles	O
of	O
the	O
invention	O
and	O
not	O
intended	O
to	O
limit	O
the	O
invention	O
to	O
the	O
specific	O
embodiments	O
shown	O
and	O
described	O
.	O
Further	O
,	O
the	O
terms	O
and	O
words	O
used	O
herein	O
are	O
not	O
to	O
be	O
considered	O
limiting	O
,	O
but	O
rather	O
merely	O
descriptive	O
.	O
In	O
the	O
description	O
below	O
,	O
like	O
reference	O
numbers	O
are	O
used	O
to	O
describe	O
the	O
same	O
,	O
similar	O
,	O
or	O
corresponding	O
parts	O
in	O
the	O
several	O
views	O
of	O
the	O
drawings	O
.	O

FIG	O
.	O
1	O
is	O
a	O
functional	O
block	O
diagram	O
of	O
a	O
system	O
100	O
for	O
performing	O
automated	O
speech	O
recognition	O
according	O
to	O
the	O
preferred	O
embodiment	O
of	O
the	O
invention	O
.	O
Audio	O
signals	O
from	O
a	O
transducer	O
(	O
e.g.	O
,	O
microphone	O
,	O
not	O
shown	O
)	O
are	O
input	O
at	O
and	O
an	O
input	O
102	O
of	O
an	O
audio	O
signal	O
sampler	O
104	O
.	O
The	O
audio	O
signal	O
sampler	O
104	O
preferably	O
samples	O
the	O
audio	O
signal	O
at	O
a	O
sampling	O
rate	O
of	O
about	O
8	O
,	O
000	O
to	O
16	O
,	O
000	O
samples	O
per	O
second	O
and	O
at	O
8	O
to	O
16	O
bit	O
resolution	O
and	O
outputs	O
a	O
representation	O
of	O
the	O
input	O
audio	O
signal	O
that	O
is	O
discretized	O
in	O
time	O
and	O
amplitude	O
.	O
The	O
audio	O
signals	O
may	O
be	O
represented	O
as	O
a	O
sequence	O
of	O
binary	O
numbers	O
:	O
[	O
in-line-formulae	O
]	O
Xn	O
,	O
n	O
=	O
0	O
.	O
.	O
.	O
N	O
,	O
[	O
/in-line-formulae	O
]	O
where	O
Xn	O
is	O
an	O
nth	O
indexed	O
digitized	O
sample	O
,	O
and	O
the	O
index	O
n	O
ranges	O
up	O
to	O
a	O
limit	O
N	O
determined	O
by	O
the	O
length	O
of	O
the	O
audio	O
signal	O
.	O

A	O
Finite	O
Impulse	O
Response	O
(	O
FIR	O
)	O
time	O
domain	O
filter	O
106	O
is	O
coupled	O
to	O
the	O
audio	O
signal	O
sampler	O
104	O
for	O
receiving	O
the	O
discretized	O
audio	O
signal	O
.	O
The	O
FIR	O
filter	O
106	O
serves	O
to	O
increase	O
the	O
magnitude	O
of	O
high	O
frequency	O
components	O
compared	O
to	O
low	O
frequency	O
components	O
of	O
the	O
discretized	O
audio	O
signal	O
.	O
The	O
FIR	O
time	O
domain	O
filter	O
106	O
processes	O
the	O
discretized	O
audio	O
signal	O
and	O
outputs	O
a	O
sequence	O
of	O
filtered	O
discretized	O
samples	O
at	O
the	O
sampling	O
rate	O
.	O
The	O
each	O
nth	O
filter	O
output	O
may	O
be	O
expressed	O
as	O
:	O
Xnl	O
=	O
∑	O
k	O
=	O
0M	O
⁢	O
Ck	O
⁢	O
Xn-kwhere	O
Xnl	O
is	O
an	O
nth	O
time	O
domain	O
filtered	O
output	O
,	O
Ck	O
is	O
a	O
kth	O
FIR	O
time	O
domain	O
filter	O
coefficient,M	O
is	O
one	O
less	O
than	O
the	O
number	O
of	O
FIR	O
time	O
domain	O
coefficients	O
;	O
andXn	O
−	O
k	O
is	O
an	O
indexed	O
digitized	O
sample	O
received	O
from	O
the	O
audio	O
signal	O
sampler	O
104	O
.	O

Preferably	O
,	O
M	O
is	O
equal	O
to	O
1	O
,	O
C0	O
is	O
about	O
equal	O
to	O
unity	O
and	O
C1	O
is	O
about	O
equal	O
to	O
negative	O
0	O
.	O
95	O
.	O
Other	O
suitable	O
filter	O
functions	O
may	O
be	O
used	O
for	O
pre-emphasizing	O
high	O
frequency	O
components	O
of	O
the	O
discretized	O
audio	O
signal	O
.	O

A	O
windower	O
108	O
is	O
coupled	O
to	O
the	O
FIR	O
filter	O
106	O
for	O
receiving	O
the	O
filtered	O
discretized	O
samples	O
.	O
The	O
windower	O
108	O
multiplies	O
successive	O
subsets	O
of	O
filtered	O
discretized	O
samples	O
by	O
a	O
discretized	O
representation	O
of	O
a	O
window	O
function	O
.	O
For	O
example	O
each	O
subset	O
that	O
is	O
termed	O
a	O
frame	O
may	O
comprise	O
about	O
25	O
to	O
30	O
ms	O
of	O
speech	O
.	O
(	O
about	O
200	O
to	O
480	O
samples	O
)	O
.	O
Preferably	O
,	O
there	O
is	O
about	O
a	O
15	O
-	O
20	O
ms	O
overlaps	O
between	O
the	O
two	O
successive	O
blocks	O
.	O
Each	O
filtered	O
discretized	O
sample	O
in	O
each	O
frame	O
is	O
multiplied	O
by	O
a	O
specific	O
coefficient	O
of	O
the	O
window	O
function	O
that	O
is	O
determined	O
by	O
the	O
position	O
of	O
the	O
filtered	O
discretized	O
sample	O
in	O
the	O
window	O
.	O
The	O
windower	O
108	O
preferably	O
outputs	O
windowed	O
filtered	O
speech	O
samples	O
at	O
an	O
average	O
rate	O
equal	O
to	O
the	O
inverse	O
of	O
the	O
difference	O
between	O
length	O
of	O
each	O
frame	O
and	O
the	O
overlap	O
between	O
frames	O
.	O
Each	O
windowed	O
filtered	O
sample	O
within	O
a	O
frame	O
may	O
be	O
denoted	O
:	O
[	O
in-line-formulae	O
]	O
XnF	O
=	O
XnlWn	O
[	O
/in-line-formulae	O
]	O
where	O
the	O
index	O
n	O
now	O
denotes	O
position	O
within	O
a	O
frame	O
;	O
the	O
index	O
F	O
denotes	O
a	O
frame	O
number	O
;	O
XnF	O
is	O
a	O
nth	O
windowed	O
filtered	O
sample	O
;	O
andWn	O
is	O
a	O
window	O
coefficient	O
corresponding	O
to	O
the	O
nth	O
position	O
within	O
each	O
frame	O
.	O

Applying	O
the	O
windowing	O
function	O
to	O
the	O
discretized	O
audio	O
signal	O
,	O
aids	O
in	O
reducing	O
spectral	O
overlap	O
between	O
adjacent	O
frequency	O
components	O
that	O
are	O
output	O
by	O
a	O
Fast	O
Fourier	O
Transform	O
FFT	O
110	O
.	O
A	O
Hamming	O
window	O
function	O
is	O
preferred	O
.	O

The	O
FFT	O
110	O
is	O
coupled	O
to	O
the	O
windower	O
108	O
for	O
receiving	O
the	O
successive	O
frames	O
of	O
windowed	O
filtered	O
samples	O
.	O
The	O
FFT	O
projects	O
successive	O
frames	O
of	O
windowed	O
filtered	O
discretized	O
audio	O
signal	O
samples	O
onto	O
a	O
Fourier	O
frequency	O
domain	O
basis	O
to	O
obtain	O
and	O
outputs	O
a	O
plurality	O
of	O
audio	O
signal	O
Fourier	O
frequency	O
components	O
,	O
and	O
processes	O
the	O
Fourier	O
frequency	O
components	O
to	O
determine	O
a	O
set	O
of	O
power	O
Fourier	O
frequency	O
component	O
for	O
each	O
frame	O
.	O
The	O
FFT	O
110	O
outputs	O
a	O
sequence	O
of	O
power	O
Fourier	O
components	O
.	O
The	O
power	O
FFT	O
components	O
are	O
given	O
by	O
the	O
following	O
relations	O
:	O
P	O
⁢	O
(	O
0	O
)	O
=	O
1N2	O
⁢	O
	O
C0	O
	O
2P	O
⁡	O
(	O
fk	O
)	O
=	O
1N2	O
⁡	O
[	O
|	O
Ck	O
⁢	O
|	O
2	O
⁢	O
+	O
|	O
CN-k	O
⁢	O
|	O
2	O
]	O
⁢	O
⁢	O
P	O
⁡	O
(	O
fN/2	O
)	O
=	O
1N2	O
⁢	O
	O
CN/2	O
	O
2where	O
,	O
P	O
(	O
0	O
)	O
is	O
a	O
zero	O
order	O
power	O
Fourier	O
frequency	O
component	O
(	O
equal	O
to	O
an	O
average	O
of	O
power	O
of	O
a	O
frame	O
)	O
;	O
P	O
(	O
fl	O
)	O
is	O
an	O
lth	O
power	O
Fourier	O
frequency	O
component	O
of	O
the	O
frame	O
;	O
N	O
is	O
the	O
number	O
of	O
samples	O
per	O
frame	O
;	O
and	O
Ck	O
=	O
∑	O
n	O
=	O
0N-1	O
⁢	O
XnF	O
⁢	O
ⅇ2	O
⁢	O
π	O
⁢	O
⁢	O
i	O
⁢	O
⁢	O
n	O
⁢	O
⁢	O
k/N	O
⁢	O
⁢	O
k	O
=	O
0	O
,	O
…	O
⁢	O
,	O
N-1where	O
CK	O
is	O
a	O
kth	O
Fourier	O
frequency	O
component	O
;	O
i	O
is	O
the	O
square	O
root	O
of	O
negative	O
one	O
;	O
n	O
is	O
a	O
summation	O
index	O
;	O
N	O
−	O
1	O
is	O
the	O
number	O
of	O
samples	O
per	O
frame	O
A	O
MEL	O
scale	O
filter	O
bank	O
112	O
is	O
coupled	O
to	O
the	O
FFT	O
110	O
for	O
receiving	O
the	O
power	O
Fourier	O
frequency	O
components	O
.	O
The	O
MEL	O
scale	O
filter	O
bank	O
includes	O
a	O
plurality	O
of	O
MEL	O
scale	O
band	O
pass	O
filters	O
112A	O
,	O
112B	O
,	O
112C	O
,	O
112D	O
(	O
four	O
of	O
which	O
are	O
shown	O
)	O
.	O
Each	O
MEL	O
scale	O
band	O
pass	O
filter	O
preferably	O
is	O
a	O
weighted	O
sum	O
of	O
a	O
plurality	O
of	O
power	O
Fourier	O
frequency	O
components	O
.	O
The	O
MEL	O
scale	O
band	O
pass	O
filters	O
112A-112D	O
preferably	O
have	O
a	O
triangular	O
profile	O
in	O
the	O
frequency	O
domain	O
.	O
Alternatively	O
,	O
the	O
MEL	O
scale	O
bandpass	O
filters	O
112A-112D	O
have	O
Hamming	O
or	O
Hanning	O
frequency	O
domain	O
profile	O
.	O
Each	O
MEL	O
bandpass	O
filter	O
112A-112D	O
preferably	O
integrates	O
a	O
plurality	O
of	O
power	O
Fourier	O
frequency	O
components	O
into	O
a	O
MEL	O
scale	O
frequency	O
component	O
.	O
By	O
integrating	O
plural	O
power	O
Fourier	O
frequency	O
components	O
with	O
the	O
MEL	O
bandpass	O
filters	O
112A-112D	O
the	O
dimensionality	O
of	O
the	O
audio	O
signal	O
information	O
is	O
reduced	O
.	O
The	O
MEL	O
scale	O
bands	O
are	O
chosen	O
in	O
view	O
of	O
understood	O
characteristics	O
of	O
human	O
acoustic	O
perception	O
.	O
There	O
are	O
preferably	O
about	O
10	O
evenly	O
spaced	O
MEL	O
scale	O
bandpass	O
filters	O
below	O
1	O
KHz	O
.	O
Beyond	O
1	O
KHz	O
the	O
bandwidth	O
of	O
successive	O
MEL	O
frequency	O
bandpass	O
filters	O
preferably	O
increase	O
by	O
a	O
factor	O
of	O
about	O
1	O
.	O
2	O
.	O
There	O
are	O
preferably	O
about	O
10	O
to	O
20	O
MEL	O
scale	O
bandpass	O
filters	O
above	O
1	O
KHz	O
,	O
and	O
more	O
preferably	O
about	O
14	O
.	O
The	O
MEL	O
scale	O
filter	O
bank	O
112	O
outputs	O
a	O
plurality	O
of	O
MEL	O
scale	O
frequency	O
components	O
.	O
An	O
mth	O
MEL	O
scale	O
frequency	O
component	O
of	O
the	O
MEL	O
scale	O
filter	O
bank	O
112	O
corresponding	O
to	O
an	O
mth	O
MEL	O
bandpass	O
filter	O
is	O
denoted	O
Z	O
(	O
m	O
)	O
.	O

A	O
log-magnitude	O
evaluator	O
114	O
is	O
coupled	O
to	O
the	O
MEL	O
scale	O
frequency	O
filter	O
bank	O
112	O
for	O
applying	O
a	O
composite	O
function	O
to	O
each	O
MEL	O
scale	O
frequency	O
component	O
.	O
The	O
composite	O
function	O
comprises	O
taking	O
the	O
magnitude	O
of	O
each	O
MEL	O
scale	O
frequency	O
component	O
,	O
and	O
taking	O
the	O
log	O
of	O
the	O
result	O
.	O
By	O
taking	O
the	O
magnitude	O
of	O
each	O
MEL	O
scale	O
frequency	O
component	O
,	O
phase	O
information	O
,	O
which	O
does	O
not	O
encode	O
speech	O
information	O
,	O
is	O
discarded	O
.	O
By	O
discarding	O
phase	O
information	O
,	O
the	O
dimensionality	O
of	O
acoustic	O
signal	O
information	O
is	O
further	O
reduced	O
.	O
By	O
taking	O
the	O
log	O
of	O
the	O
resulting	O
magnitude	O
the	O
magnitudes	O
of	O
the	O
MEL	O
scale	O
frequency	O
components	O
are	O
put	O
on	O
a	O
scale	O
which	O
more	O
accurately	O
models	O
the	O
response	O
of	O
the	O
human	O
hearing	O
to	O
changes	O
in	O
sound	O
intensity	O
.	O
The	O
log-magnitude	O
evaluator	O
114	O
outputs	O
a	O
plurality	O
of	O
rescaled	O
magnitudes	O
of	O
the	O
MEL	O
scale	O
frequency	O
components	O
of	O
the	O
form	O
log	O
(	O
|	O
Z	O
(	O
m	O
)	O
|	O
)	O
.	O

A	O
discrete	O
cosine	O
transform	O
block	O
(	O
DCT	O
)	O
116	O
is	O
coupled	O
to	O
the	O
log	O
absolute	O
value	O
taker	O
114	O
for	O
receiving	O
the	O
rescaled	O
magnitudes	O
.	O
The	O
DCT	O
116	O
transforms	O
the	O
rescaled	O
magnitudes	O
to	O
the	O
time	O
domain	O
.	O
The	O
output	O
of	O
the	O
DCT	O
116	O
comprises	O
a	O
set	O
of	O
DCT	O
components	O
values	O
(	O
cepstral	O
coefficients	O
)	O
for	O
each	O
frame	O
.	O
The	O
zero	O
order	O
component	O
output	O
by	O
the	O
DCT	O
is	O
proportional	O
to	O
the	O
log	O
energy	O
of	O
the	O
acoustic	O
signal	O
during	O
the	O
frame	O
from	O
which	O
the	O
component	O
was	O
generated	O
.	O
The	O
DCT	O
components	O
output	O
by	O
the	O
DCT	O
116	O
are	O
preferably	O
of	O
the	O
following	O
form	O
:	O
yP	O
⁢	O
(	O
k	O
)	O
=	O
∑	O
m	O
=	O
1M	O
⁢	O
log	O
⁢	O
(	O
|	O
Z	O
⁢	O
(	O
m	O
)	O
|	O
)	O
⁢	O
cos	O
⁢	O
(	O
k	O
⁢	O
(	O
m-12	O
)	O
⁢	O
πM	O
)	O
where	O
yP	O
(	O
k	O
)	O
is	O
a	O
kth	O
order	O
DCT	O
component	O
output	O
by	O
the	O
DCT	O
116	O
for	O
a	O
pth	O
frame	O
;	O
and	O
M	O
in	O
this	O
case	O
is	O
the	O
number	O
of	O
MEL	O
scale	O
frequency	O
components	O
.	O

The	O
summation	O
on	O
the	O
left	O
hand	O
side	O
of	O
the	O
above	O
equation	O
effects	O
the	O
DCT	O
transformation	O
.	O
The	O
DCT	O
components	O
are	O
also	O
termed	O
cepstrum	O
coefficients	O
.	O

The	O
windower	O
108	O
,	O
FFT	O
110	O
,	O
MEL	O
scale	O
filter	O
bank	O
112	O
,	O
log-magnitude	O
evaluator	O
114	O
,	O
and	O
DCT	O
116	O
operate	O
in	O
synchronism	O
.	O
The	O
DCT	O
116	O
sequentially	O
outputs	O
sets	O
of	O
DCT	O
components	O
corresponding	O
to	O
frames	O
of	O
discretized	O
samples	O
output	O
by	O
the	O
windower	O
108	O
.	O

A	O
first	O
buffer	O
118	O
is	O
coupled	O
to	O
the	O
DCT	O
116	O
for	O
receiving	O
successive	O
sets	O
of	O
DCT	O
component	O
values	O
.	O
A	O
differencer	O
120	O
is	O
coupled	O
to	O
the	O
first	O
buffer	O
118	O
for	O
receiving	O
successive	O
sets	O
of	O
DCT	O
component	O
values	O
.	O
The	O
differencer	O
120	O
operates	O
on	O
two	O
or	O
more	O
successive	O
sets	O
of	O
component	O
values	O
by	O
taking	O
the	O
difference	O
between	O
corresponding	O
DCT	O
component	O
values	O
from	O
different	O
sets	O
and	O
outputting	O
sets	O
of	O
discrete	O
differences	O
(	O
including	O
one	O
difference	O
for	O
each	O
DCT	O
component	O
)	O
of	O
first	O
and/or	O
higher	O
order	O
,	O
for	O
each	O
frame	O
.	O
The	O
discrete	O
differences	O
characterize	O
the	O
time-wise	O
variation	O
of	O
the	O
DCT	O
component	O
values	O
.	O
The	O
lth	O
order	O
discrete	O
time	O
difference	O
for	O
the	O
pth	O
frame	O
Δl	O
(	O
yP	O
(	O
k	O
)	O
)	O
applied	O
to	O
the	O
sequence	O
of	O
DCT	O
components	O
is	O
given	O
by	O
the	O
following	O
recursion	O
relations	O
:	O
[	O
in-line-formulae	O
]	O
Δl	O
(	O
yP	O
(	O
k	O
)	O
)	O
=	O
Δl	O
−	O
1	O
(	O
yP	O
+	O
1	O
(	O
k	O
)	O
)	O
−	O
Δl	O
−	O
1	O
(	O
yP	O
−	O
1	O
(	O
k	O
)	O
)	O
[	O
/in-line-formulae	O
]	O
[	O
in-line-formulae	O
]	O
Δ0	O
(	O
yP	O
(	O
k	O
)	O
)	O
=	O
yP	O
(	O
k	O
)	O
[	O
/in-line-formulae	O
]	O
The	O
DCT	O
component	O
values	O
output	O
for	O
each	O
frame	O
by	O
the	O
DCT	O
116	O
,	O
along	O
with	O
discrete	O
differences	O
of	O
one	O
or	O
more	O
orders	O
serve	O
to	O
characterize	O
the	O
audio	O
signal	O
during	O
each	O
frame	O
.	O
(	O
The	O
DCT	O
component	O
values	O
and	O
the	O
discrete	O
differences	O
are	O
numbers	O
.	O
)	O
The	O
DCT	O
component	O
values	O
and	O
discrete	O
differences	O
of	O
one	O
or	O
more	O
orders	O
are	O
preferably	O
stored	O
in	O
arrays	O
(	O
one	O
for	O
each	O
frame	O
)	O
and	O
treated	O
as	O
vectors	O
,	O
hereinafter	O
termed	O
feature	O
vectors	O
.	O
Preferably	O
,	O
DCT	O
components	O
and	O
the	O
first	O
two	O
orders	O
of	O
differences	O
are	O
used	O
in	O
the	O
feature	O
vectors	O
.	O
The	O
feature	O
vectors	O
for	O
a	O
given	O
frame	O
P	O
are	O
denoted	O
:	O
[	O
in-line-formulae	O
]	O
YP	O
=	O
[	O
Y1P,Y2P,Y3P	O
,	O
.	O
.	O
.	O
YKP	O
.	O
.	O
.	O
YDP	O
]	O
[	O
/in-line-formulae	O
]	O
where	O
the	O
first	O
k	O
vector	O
elements	O
are	O
DCT	O
components	O
,	O
and	O
the	O
(	O
k	O
+	O
1	O
)	O
th	O
through	O
Dth	O
vector	O
elements	O
are	O
discrete	O
differences	O
of	O
the	O
DCT	O
components	O
.	O

According	O
to	O
an	O
alternative	O
embodiment	O
the	O
differencer	O
120	O
is	O
eliminated	O
,	O
and	O
only	O
the	O
DCT	O
components	O
are	O
used	O
to	O
characterize	O
the	O
audio	O
signal	O
during	O
each	O
frame	O
.	O

The	O
first	O
buffer	O
118	O
,	O
and	O
the	O
differencer	O
120	O
are	O
coupled	O
to	O
a	O
second	O
buffer	O
122	O
.	O
The	O
feature	O
vectors	O
are	O
assembled	O
and	O
stored	O
in	O
the	O
second	O
buffer	O
122	O
.	O

The	O
above	O
described	O
functional	O
blocks	O
including	O
the	O
audio	O
signal	O
sampler	O
104	O
,	O
FIR	O
time	O
domain	O
filter	O
106	O
,	O
windower	O
108	O
,	O
FFT	O
110	O
,	O
MEL	O
scale	O
filter	O
bank	O
112	O
,	O
log-magnitude	O
evaluator	O
114	O
,	O
DCT	O
116	O
,	O
first	O
buffer	O
118	O
,	O
differencer	O
120	O
,	O
and	O
second	O
buffer	O
122	O
,	O
are	O
parts	O
of	O
a	O
feature	O
extractor	O
124	O
.	O
The	O
function	O
of	O
the	O
feature	O
extractor	O
124	O
is	O
to	O
eliminate	O
extraneous	O
,	O
and	O
redundant	O
information	O
from	O
audio	O
signals	O
that	O
include	O
speech	O
sounds	O
,	O
and	O
produce	O
feature	O
vectors	O
each	O
of	O
which	O
is	O
highly	O
correlated	O
to	O
a	O
particular	O
sound	O
that	O
is	O
one	O
variation	O
of	O
a	O
component	O
of	O
spoken	O
language	O
.	O
Although	O
a	O
preferred	O
structure	O
and	O
operation	O
of	O
the	O
feature	O
extractor	O
124	O
has	O
been	O
described	O
above	O
,	O
other	O
types	O
of	O
feature	O
extractor	O
that	O
have	O
different	O
internal	O
structures	O
,	O
and/or	O
operate	O
differently	O
to	O
process	O
audio	O
signals	O
that	O
include	O
speech	O
sounds	O
,	O
and	O
produce	O
by	O
such	O
processing	O
characterizations	O
of	O
different	O
sub	O
parts	O
(	O
e.g.	O
,	O
frames	O
)	O
of	O
the	O
audio	O
signal	O
may	O
be	O
used	O
in	O
practicing	O
the	O
invention	O
.	O

The	O
second	O
buffer	O
122	O
supplies	O
feature	O
vectors	O
for	O
each	O
frame	O
to	O
a	O
Hidden	O
Markov	O
Model	O
(	O
HMM	O
)	O
132	O
.	O
The	O
HMM	O
132	O
models	O
spoken	O
language	O
.	O
The	O
HMM	O
132	O
comprises	O
a	O
hierarchy	O
of	O
three	O
interconnected	O
layers	O
of	O
states	O
including	O
an	O
acoustic	O
layer	O
134	O
,	O
a	O
phoneme	O
layer	O
136	O
,	O
and	O
a	O
word	O
layer	O
138	O
.	O
The	O
word	O
layers	O
138	O
includes	O
a	O
plurality	O
of	O
states	O
corresponding	O
to	O
a	O
plurality	O
of	O
words	O
in	O
a	O
vocabulary	O
of	O
the	O
HMM	O
.	O
Transitions	O
between	O
states	O
in	O
the	O
word	O
layer	O
are	O
governed	O
by	O
a	O
word	O
layer	O
transition	O
matrix	O
.	O
The	O
word	O
layer	O
transition	O
matrix	O
includes	O
a	O
probability	O
for	O
each	O
possible	O
transition	O
between	O
word	O
states	O
.	O
Some	O
transition	O
probabilities	O
may	O
be	O
zero	O
.	O

The	O
phoneme	O
layer	O
136	O
includes	O
a	O
word	O
HMM	O
for	O
each	O
word	O
in	O
the	O
word	O
layer	O
138	O
.	O
Each	O
word	O
HMM	O
includes	O
a	O
sequence	O
of	O
states	O
corresponding	O
to	O
a	O
sequence	O
of	O
phonemes	O
that	O
comprise	O
the	O
word	O
.	O
Transitions	O
between	O
phoneme	O
states	O
in	O
the	O
word	O
layer	O
are	O
also	O
governed	O
by	O
a	O
matrix	O
of	O
transition	O
probabilities	O
.	O
There	O
may	O
be	O
more	O
than	O
one	O
word	O
HMM	O
for	O
each	O
word	O
in	O
the	O
word	O
layer	O
138	O
.	O

Finally	O
,	O
the	O
acoustic	O
layer	O
134	O
includes	O
a	O
phoneme	O
HMM	O
model	O
of	O
each	O
phoneme	O
in	O
the	O
language	O
that	O
the	O
HMM	O
132	O
is	O
capable	O
of	O
recognizing	O
.	O
Each	O
phoneme	O
HMM	O
includes	O
beginning	O
states	O
and	O
ending	O
states	O
.	O
A	O
first	O
phoneme	O
HMM	O
model	O
140	O
and	O
second	O
phoneme	O
HMM	O
model	O
142	O
are	O
illustrated	O
.	O
In	O
actuality	O
,	O
there	O
are	O
many	O
phoneme	O
HMM	O
models	O
in	O
the	O
acoustic	O
layer	O
134	O
.	O
The	O
details	O
of	O
phoneme	O
HMM	O
models	O
will	O
be	O
discussed	O
with	O
reference	O
to	O
the	O
first	O
phoneme	O
HMM	O
model	O
140	O
.	O
A	O
beginning	O
state	O
140A	O
and	O
an	O
ending	O
states	O
140D	O
are	O
non-emitting	O
which	O
is	O
to	O
say	O
that	O
these	O
states	O
140A	O
,	O
140D	O
are	O
not	O
associated	O
with	O
acoustic	O
features	O
.	O
Between	O
the	O
beginning	O
and	O
ending	O
states	O
of	O
each	O
phoneme	O
HMM	O
are	O
a	O
number	O
of	O
acoustic	O
emitting	O
states	O
(	O
e.g.	O
,	O
140B	O
,	O
140C	O
)	O
.	O
Although	O
two	O
are	O
shown	O
for	O
the	O
purpose	O
of	O
illustration	O
,	O
in	O
practice	O
there	O
may	O
be	O
more	O
than	O
two	O
emitting	O
states	O
in	O
each	O
phoneme	O
model	O
.	O
Each	O
emitting	O
state	O
of	O
each	O
phoneme	O
HMM	O
model	O
(	O
e.g.	O
,	O
140	O
)	O
is	O
intended	O
to	O
correspond	O
to	O
an	O
acoustically	O
quasi	O
stationary	O
frame	O
of	O
a	O
phoneme	O
.	O
Transitions	O
between	O
the	O
states	O
in	O
each	O
phoneme	O
model	O
are	O
also	O
governed	O
by	O
a	O
transition	O
probability	O
matrix	O
.	O

The	O
acoustic	O
layer	O
also	O
includes	O
an	O
HMM	O
model	O
156	O
for	O
the	O
absence	O
of	O
speech	O
sounds	O
that	O
occur	O
between	O
speech	O
sounds	O
(	O
e.g.	O
,	O
between	O
words	O
,	O
and	O
between	O
sentences	O
)	O
.	O
The	O
model	O
for	O
the	O
absence	O
of	O
speech	O
sounds	O
156	O
(	O
background	O
sound	O
model	O
)	O
156	O
is	O
intended	O
to	O
correspond	O
to	O
background	O
noise	O
which	O
predominates	O
in	O
the	O
absence	O
of	O
speech	O
sounds	O
.	O
The	O
background	O
sound	O
model	O
156	O
includes	O
a	O
first	O
state	O
158	O
that	O
is	O
non-emitting	O
,	O
and	O
a	O
final	O
state	O
160	O
that	O
is	O
non-emitting	O
.	O
An	O
emitting	O
state	O
146	O
is	O
located	O
between	O
the	O
first	O
158	O
and	O
final	O
160	O
states	O
.	O
The	O
emitting	O
state	O
146	O
represents	O
background	O
sounds	O
.	O
As	O
mentioned	O
above	O
a	O
difficulty	O
arises	O
in	O
ASR	O
due	O
to	O
the	O
fact	O
that	O
the	O
background	O
noise	O
varies	O
.	O

Feature	O
vectors	O
that	O
characterizes	O
the	O
audio	O
signal	O
that	O
are	O
output	O
by	O
the	O
feature	O
extractor	O
124	O
are	O
input	O
into	O
the	O
HMM	O
132	O
and	O
used	O
within	O
the	O
acoustic	O
layer	O
134	O
.	O
Each	O
emitting	O
state	O
in	O
the	O
acoustic	O
layer	O
134	O
has	O
associated	O
with	O
it	O
a	O
probability	O
density	O
function	O
(	O
PDF	O
)	O
which	O
determines	O
the	O
a	O
posteriori	O
probability	O
that	O
the	O
acoustic	O
state	O
occurred	O
given	O
the	O
feature	O
vector	O
.	O
The	O
emitting	O
states	O
140B	O
and	O
140C	O
of	O
the	O
first	O
phoneme	O
HMM	O
have	O
associated	O
probability	O
density	O
functions	O
144	O
and	O
162	O
respectively	O
.	O
Likewise	O
,	O
the	O
emitting	O
state	O
146	O
of	O
the	O
background	O
sound	O
model	O
156	O
has	O
a	O
background	O
sound	O
PDF	O
148	O
.	O
Gaussian	O
mixture	O
component	O
means	O
for	O
the	O
background	O
sound	O
model	O
156	O
,	O
that	O
uses	O
Gaussian	O
mixture	O
component	O
means	O
150	O
that	O
are	O
described	O
below	O
.	O

The	O
a	O
posteriori	O
probability	O
for	O
each	O
emitting	O
state	O
(	O
including	O
the	O
emitting	O
state	O
146	O
in	O
the	O
background	O
sound	O
model	O
150	O
)	O
is	O
preferably	O
a	O
multi	O
component	O
Gaussian	O
mixture	O
of	O
the	O
form	O
:	O
bj	O
⁢	O
(	O
YP	O
)	O
=	O
∑	O
n	O
=	O
1M	O
⁢	O
cjn	O
⁢	O
bjn	O
⁢	O
(	O
YP	O
)	O
where	O
,	O
bj	O
(	O
YP	O
)	O
is	O
the	O
a	O
posteriori	O
probability	O
that	O
the	O
HMM	O
model	O
132	O
was	O
in	O
a	O
jth	O
state	O
during	O
frame	O
P	O
given	O
the	O
fact	O
that	O
the	O
audio	O
signals	O
during	O
frame	O
P	O
was	O
characterized	O
by	O
a	O
feature	O
vector	O
YP	O
;	O
Cjn	O
is	O
a	O
mixture	O
component	O
weight	O
;	O
andbjn	O
(	O
YP	O
)	O
is	O
an	O
nth	O
mixture	O
component	O
for	O
the	O
jth	O
state	O
that	O
is	O
given	O
by	O
:	O
bjn	O
⁢	O
(	O
YP	O
)	O
=	O
1	O
(	O
2	O
⁢	O
π	O
)	O
D	O
⁢	O
∏	O
i	O
=	O
1D	O
⁢	O
σi	O
⁢	O
⁢	O
j	O
⁢	O
⁢	O
n2	O
⁢	O
exp	O
⁢	O
{	O
-	O
12	O
⁢	O
∑	O
i	O
=	O
1D	O
⁢	O
(	O
YiP-μijn	O
)	O
2σij	O
⁢	O
⁢	O
n	O
}	O
where	O
,	O
μijn	O
is	O
a	O
mean	O
of	O
an	O
ith	O
parameter	O
(	O
corresponding	O
to	O
an	O
ith	O
elements	O
of	O
the	O
feature	O
vectors	O
)	O
,	O
of	O
the	O
nth	O
mixture	O
component	O
of	O
the	O
jth	O
acoustic	O
state	O
132	O
(	O
for	O
a	O
phoneme	O
or	O
for	O
background	O
sounds	O
)	O
of	O
the	O
HMM	O
model	O
.	O
σijn	O
is	O
a	O
variance	O
associated	O
with	O
the	O
ith	O
parameter	O
of	O
the	O
nth	O
mixture	O
component	O
of	O
the	O
jth	O
acoustic	O
state	O
of	O
the	O
acoustic	O
layer	O
.	O

The	O
means	O
μijn	O
serve	O
as	O
reference	O
characterizations	O
of	O
a	O
sound	O
modeled	O
by	O
the	O
a	O
posteriori	O
probability	O
.	O

In	O
the	O
operation	O
a	O
seach	O
engine	O
164	O
searches	O
the	O
HMM	O
132	O
,	O
for	O
one	O
or	O
more	O
sequences	O
of	O
states	O
that	O
are	O
characterized	O
by	O
high	O
probabilities	O
,	O
and	O
outputs	O
one	O
or	O
more	O
sequences	O
of	O
words	O
that	O
correspond	O
to	O
the	O
high	O
probability	O
sequences	O
of	O
states	O
.	O
The	O
probability	O
of	O
sequences	O
of	O
states	O
are	O
determined	O
by	O
the	O
product	O
of	O
transition	O
probabilities	O
for	O
the	O
sequence	O
of	O
states	O
multiplied	O
by	O
the	O
a	O
posteriori	O
probabilities	O
that	O
the	O
sequence	O
of	O
states	O
occurred	O
based	O
on	O
their	O
associated	O
a	O
posteriori	O
probabilities	O
in	O
view	O
of	O
a	O
sequence	O
of	O
feature	O
vectors	O
extracted	O
from	O
the	O
audio	O
signal	O
to	O
be	O
recognized	O
.	O
The	O
a	O
posteriori	O
probabilities	O
evaluating	O
the	O
a	O
posteriori	O
probabilities	O
associated	O
with	O
a	O
sequence	O
of	O
postulated	O
states	O
with	O
an	O
extracted	O
sequence	O
of	O
feature	O
vectors	O
.	O
Expressed	O
mathematically	O
the	O
probability	O
of	O
a	O
sequence	O
of	O
states	O
S1	O
.	O
.	O
.	O
T	O
given	O
the	O
fact	O
that	O
a	O
sequence	O
of	O
feature	O
vectors	O
Y1	O
.	O
.	O
.	O
T	O
was	O
extracted	O
from	O
the	O
audio	O
signal	O
is	O
given	O
by	O
:	O
P	O
⁢	O
(	O
S1	O
⁢	O
⁢	O
…	O
⁢	O
⁢	O
T	O
,	O
Y1	O
⁢	O
⁢	O
…	O
⁢	O
⁢	O
T	O
,	O
Θ	O
)	O
=	O
πs1	O
⁢	O
bs1	O
⁢	O
(	O
Y1	O
)	O
⁢	O
∏	O
t	O
=	O
2T	O
⁢	O
aSt-1	O
⁢	O
St	O
⁢	O
bst	O
⁢	O
(	O
Yt	O
)	O
where	O
Θ	O
specifies	O
the	O
underlying	O
HMM	O
model	O
;	O
πs1	O
specifies	O
the	O
probability	O
of	O
a	O
first	O
postulated	O
state	O
in	O
the	O
sequence	O
of	O
states.;aSt	O
−	O
1St	O
specifies	O
the	O
probability	O
of	O
a	O
transition	O
between	O
a	O
first	O
state	O
postulated	O
for	O
a	O
first	O
time	O
t	O
−	O
1	O
and	O
second	O
state	O
postulated	O
for	O
the	O
successive	O
time	O
t	O
;	O
andother	O
quantities	O
are	O
defined	O
above	O
.	O

Various	O
methods	O
are	O
know	O
to	O
persons	O
of	O
ordinary	O
skill	O
in	O
the	O
ASR	O
art	O
for	O
finding	O
a	O
likely	O
sequence	O
of	O
states	O
without	O
having	O
to	O
exhaustively	O
evaluate	O
the	O
above	O
equation	O
for	O
each	O
possible	O
sequence	O
of	O
states	O
.	O
One	O
known	O
method	O
is	O
the	O
Viterbi	O
search	O
method	O
.	O

In	O
the	O
HMM	O
132	O
,	O
transitions	O
from	O
various	O
phoneme	O
states	O
to	O
the	O
model	O
for	O
the	O
absence	O
of	O
speech	O
sounds	O
are	O
allowed	O
.	O
Such	O
transitions	O
often	O
occur	O
at	O
the	O
end	O
of	O
postulated	O
words	O
.	O
Thus	O
,	O
in	O
order	O
to	O
be	O
able	O
to	O
determine	O
the	O
ending	O
of	O
words	O
,	O
and	O
in	O
order	O
to	O
be	O
able	O
to	O
discriminate	O
between	O
short	O
words	O
that	O
sound	O
like	O
the	O
beginning	O
of	O
longer	O
words	O
and	O
the	O
longer	O
words	O
,	O
it	O
is	O
important	O
to	O
be	O
able	O
to	O
recognize	O
background	O
sounds	O
.	O

In	O
training	O
an	O
HMM	O
based	O
ASR	O
system	O
that	O
includes	O
a	O
model	O
of	O
non-speech	O
sounds	O
,	O
certain	O
parameters	O
that	O
described	O
the	O
non	O
speech	O
background	O
sounds	O
must	O
be	O
set	O
.	O
For	O
example	O
if	O
an	O
a	O
posterior	O
probability	O
of	O
the	O
form	O
shown	O
above	O
is	O
used	O
then	O
the	O
mixture	O
component	O
weights	O
,	O
the	O
means	O
μijn	O
and	O
the	O
variances	O
σijn	O
that	O
characterize	O
background	O
sound	O
must	O
be	O
set	O
during	O
training	O
.	O
As	O
discussed	O
in	O
the	O
background	O
section	O
characteristics	O
of	O
the	O
background	O
sound	O
are	O
not	O
fixed	O
.	O
If	O
a	O
portable	O
device	O
that	O
includes	O
an	O
HMM	O
ASR	O
system	O
is	O
taken	O
to	O
different	O
locations	O
the	O
characteristics	O
of	O
the	O
background	O
sound	O
is	O
likely	O
to	O
change	O
.	O
When	O
the	O
background	O
sound	O
in	O
use	O
differs	O
from	O
that	O
present	O
during	O
training	O
,	O
the	O
HMM	O
ASR	O
is	O
more	O
likely	O
to	O
make	O
errors	O
.	O

According	O
to	O
the	O
present	O
invention	O
a	O
model	O
used	O
in	O
the	O
ASR	O
,	O
preferably	O
the	O
model	O
of	O
non-speech	O
background	O
sounds	O
is	O
updated	O
frequently	O
while	O
the	O
ASR	O
is	O
in	O
regular	O
use	O
.	O
The	O
model	O
of	O
non-speech	O
background	O
sounds	O
is	O
updated	O
so	O
as	O
to	O
better	O
model	O
current	O
background	O
sounds	O
.	O
According	O
to	O
the	O
present	O
invention	O
,	O
the	O
background	O
sound	O
is	O
preferably	O
measured	O
in	O
the	O
absence	O
of	O
speech	O
sounds	O
,	O
e.g.	O
,	O
between	O
words	O
or	O
sentences	O
.	O
According	O
to	O
the	O
preferred	O
embodiment	O
of	O
the	O
invention	O
the	O
updating	O
takes	O
place	O
during	O
breaks	O
of	O
at	O
least	O
600	O
milliseconds	O
,	O
e.g.	O
breaks	O
that	O
occur	O
between	O
sentences	O
.	O

According	O
to	O
the	O
preferred	O
embodiment	O
of	O
the	O
invention	O
,	O
the	O
detection	O
of	O
the	O
absence	O
of	O
voiced	O
sounds	O
is	O
premised	O
on	O
the	O
assumption	O
that	O
speech	O
sounds	O
reaching	O
the	O
input	O
102	O
of	O
the	O
ASR	O
system	O
100	O
have	O
greater	O
power	O
than	O
background	O
sounds	O
.	O
According	O
to	O
the	O
preferred	O
embodiment	O
of	O
the	O
invention	O
the	O
interruptions	O
in	O
speech	O
sounds	O
between	O
sentences	O
are	O
detected	O
by	O
comparing	O
the	O
zero	O
order	O
DCT	O
coefficient	O
of	O
each	O
frame	O
which	O
represents	O
the	O
log	O
energy	O
of	O
each	O
frame	O
to	O
a	O
threshold	O
,	O
and	O
requiring	O
that	O
the	O
zero	O
order	O
DCT	O
coefficient	O
remain	O
below	O
the	O
threshold	O
for	O
a	O
predetermined	O
period	O
.	O
By	O
requiring	O
that	O
the	O
zero	O
order	O
DCT	O
coefficient	O
remain	O
below	O
the	O
threshold	O
it	O
is	O
possible	O
to	O
distinguish	O
longer	O
inter	O
sentence	O
breaks	O
in	O
speech	O
sound	O
from	O
shorter	O
intra	O
sentence	O
breaks	O
.	O
According	O
to	O
an	O
alternative	O
embodiment	O
of	O
the	O
invention	O
an	O
absence	O
of	O
speech	O
sounds	O
is	O
detected	O
by	O
comparing	O
a	O
weighted	O
sum	O
of	O
DCT	O
coefficients	O
to	O
a	O
threshold	O
value	O
.	O
The	O
threshold	O
may	O
be	O
set	O
dynamically	O
based	O
on	O
a	O
running	O
average	O
of	O
the	O
power	O
of	O
the	O
audio	O
signal	O
.	O

An	O
inter	O
sentence	O
pause	O
detector	O
152	O
is	O
coupled	O
to	O
the	O
DCT	O
116	O
for	O
receiving	O
one	O
or	O
more	O
of	O
the	O
coefficients	O
output	O
by	O
the	O
DCT	O
for	O
each	O
frame	O
.	O
Preferably	O
,	O
the	O
inter-sentence	O
pause	O
detector	O
receives	O
the	O
zero	O
order	O
DCT	O
coefficient	O
(	O
log	O
energy	O
value	O
)	O
for	O
each	O
frame	O
.	O
If	O
the	O
zero	O
order	O
DCT	O
,	O
(	O
Alternatively	O
,	O
a	O
sum	O
of	O
DCT	O
coefficients	O
,	O
or	O
a	O
weighted	O
sum	O
of	O
the	O
DCT	O
coefficients	O
)	O
remains	O
below	O
a	O
predetermined	O
threshold	O
value	O
for	O
a	O
predetermined	O
time	O
and	O
then	O
goes	O
above	O
the	O
threshold	O
,	O
the	O
inter	O
sentence	O
pause	O
detector	O
152	O
outputs	O
a	O
trigger	O
signal	O
.	O
The	O
predetermined	O
time	O
is	O
set	O
to	O
be	O
longer	O
than	O
the	O
average	O
of	O
intra	O
sentence	O
pauses	O
.	O
The	O
trigger	O
signal	O
is	O
output	O
at	O
the	O
end	O
of	O
long	O
(	O
inter	O
sentence	O
)	O
pauses	O
.	O
According	O
to	O
the	O
preferred	O
embodiment	O
of	O
the	O
invention	O
adjustment	O
of	O
the	O
non	O
speech	O
sound	O
model	O
is	O
based	O
on	O
background	O
sounds	O
that	O
occur	O
near	O
the	O
end	O
of	O
inter	O
sentence	O
breaks	O
in	O
speech	O
sound	O
.	O
Note	O
that	O
inter	O
sentence	O
pause	O
detector	O
152	O
may	O
be	O
triggered	O
after	O
long	O
breaks	O
(	O
e.g.	O
,	O
15	O
minutes	O
)	O
in	O
speech	O
sounds	O
A	O
comparer	O
and	O
updater	O
154	O
is	O
coupled	O
to	O
the	O
inter-sentence	O
pause	O
detector	O
for	O
receiving	O
the	O
trigger	O
signal	O
.	O
The	O
comparer	O
and	O
updater	O
154	O
also	O
coupled	O
to	O
the	O
second	O
buffer	O
122	O
for	O
receiving	O
feature	O
vectors	O
.	O
In	O
response	O
to	O
receiving	O
the	O
trigger	O
signal	O
the	O
comparer	O
and	O
updater	O
154	O
reads	O
one	O
or	O
more	O
feature	O
vectors	O
that	O
were	O
extracted	O
from	O
the	O
end	O
of	O
the	O
inter	O
sentence	O
pause	O
from	O
the	O
second	O
buffer	O
122	O
.	O
Preferably	O
,	O
more	O
than	O
one	O
feature	O
vector	O
is	O
read	O
from	O
the	O
second	O
buffer	O
122	O
and	O
averaged	O
together	O
element	O
by	O
element	O
to	O
obtain	O
a	O
characteristic	O
feature	O
vector	O
(	O
CRV	O
)	O
that	O
corresponds	O
to	O
at	O
least	O
a	O
portion	O
of	O
the	O
inter	O
sentence	O
pause	O
.	O
Alternatively	O
a	O
weighted	O
sum	O
of	O
feature	O
vectors	O
from	O
the	O
inter	O
sentence	O
pause	O
is	O
used	O
.	O
Weights	O
used	O
in	O
the	O
weighted	O
sum	O
may	O
be	O
coefficients	O
of	O
a	O
FIR	O
low	O
pass	O
filter	O
.	O
According	O
to	O
another	O
alternative	O
embodiment	O
of	O
the	O
invention	O
the	O
weighted	O
sum	O
may	O
sum	O
feature	O
vectors	O
extracted	O
from	O
multiple	O
inter	O
sentence	O
pauses	O
(	O
excluding	O
speech	O
sounds	O
between	O
them	O
)	O
.	O
Alternatively	O
,	O
one	O
feature	O
vector	O
extracted	O
from	O
the	O
vicinity	O
of	O
the	O
end	O
of	O
the	O
inter	O
sentence	O
pause	O
is	O
used	O
as	O
the	O
characteristic	O
feature	O
vector	O
.	O
Once	O
the	O
characteristic	O
feature	O
vector	O
has	O
been	O
obtained	O
,	O
a	O
mean	O
vector	O
,	O
from	O
among	O
a	O
plurality	O
mean	O
vectors	O
of	O
one	O
or	O
more	O
emitting	O
states	O
of	O
the	O
background	O
sound	O
model	O
,	O
that	O
is	O
closest	O
to	O
the	O
characteristic	O
feature	O
vector	O
is	O
determined	O
.	O
The	O
closest	O
mean	O
is	O
denoted	O
[	O
in-line-formulae	O
]	O
μjn	O
*	O
=	O
[	O
μ1jn	O
,	O
μ1jn	O
,	O
μ1jn	O
,	O
.	O
.	O
.	O
μijn	O
,	O
.	O
.	O
.	O
μDjn	O
,	O
]	O
[	O
/in-line-formulae	O
]	O
The	O
closest	O
mean	O
belongs	O
to	O
an	O
nth	O
mixture	O
component	O
of	O
a	O
jth	O
state	O
.	O

Closeness	O
is	O
preferably	O
judged	O
by	O
determining	O
which	O
mixture	O
component	O
assumes	O
the	O
highest	O
value	O
when	O
evaluated	O
using	O
the	O
characteristic	O
feature	O
vector	O
.	O
Alternatively	O
,	O
closeness	O
is	O
judged	O
by	O
determining	O
which	O
mean	O
vector	O
μjn	O
yields	O
the	O
highest	O
dot	O
product	O
with	O
the	O
characteristic	O
feature	O
vector	O
.	O
According	O
to	O
another	O
alternative	O
,	O
closeness	O
is	O
judged	O
by	O
evaluating	O
the	O
Euclidean	O
vector	O
norm	O
distance	O
between	O
the	O
characteristic	O
feature	O
vector	O
and	O
each	O
mean	O
vector	O
μjn	O
and	O
determining	O
which	O
distance	O
is	O
smallest	O
.	O
The	O
invention	O
is	O
not	O
limited	O
to	O
any	O
particular	O
way	O
of	O
determining	O
the	O
closeness	O
of	O
the	O
characteristic	O
feature	O
vector	O
to	O
the	O
mean	O
vectors	O
μjn	O
of	O
the	O
Gaussian	O
mixture	O
components	O
.	O
Once	O
the	O
closest	O
mean	O
vector	O
is	O
identified	O
,	O
the	O
mixture	O
component	O
with	O
which	O
it	O
is	O
associated	O
is	O
altered	O
so	O
that	O
it	O
yields	O
a	O
higher	O
a	O
posteriori	O
probability	O
when	O
evaluated	O
with	O
the	O
characteristic	O
feature	O
vector	O
.	O
Preferably	O
,	O
the	O
latter	O
is	O
accomplished	O
by	O
altering	O
the	O
identified	O
closest	O
mean	O
vector	O
so	O
that	O
it	O
is	O
closer	O
to	O
the	O
characteristic	O
feature	O
vector	O
.	O
More	O
preferably	O
the	O
alteration	O
of	O
the	O
identified	O
closest	O
mean	O
vector	O
μjn	O
*	O
is	O
performed	O
using	O
the	O
following	O
transformation	O
equation	O
:	O
[	O
in-line-formulae	O
]	O
μjnnew	O
=	O
(	O
1	O
−	O
α	O
)	O
μjn	O
*	O
α	O
*	O
CRV	O
[	O
/in-line-formulae	O
]	O
where	O
μjnnew	O
is	O
a	O
new	O
mean	O
vector	O
to	O
replace	O
the	O
identified	O
closest	O
mean	O
vector	O
μjn	O
*	O
α	O
is	O
a	O
weighting	O
parameter	O
that	O
is	O
preferably	O
at	O
least	O
about	O
0	O
.	O
7	O
and	O
more	O
preferably	O
at	O
least	O
about	O
0	O
.	O
9	O
;	O
and	O

CRV	O
is	O
the	O
characteristic	O
feature	O
vector	O
for	O
non	O
speech	O
background	O
sounds	O
as	O
measured	O
during	O
the	O
inter	O
sentence	O
pause	O
.	O

Thus	O
as	O
a	O
user	O
continues	O
to	O
use	O
the	O
ASR	O
system	O
100	O
as	O
the	O
background	O
sounds	O
in	O
the	O
environment	O
of	O
the	O
ASR	O
system	O
100	O
change	O
,	O
the	O
system	O
100	O
will	O
continue	O
to	O
update	O
one	O
or	O
more	O
of	O
the	O
means	O
of	O
the	O
Gaussian	O
mixtures	O
of	O
the	O
non	O
speech	O
sound	O
emitting	O
state	O
,	O
so	O
that	O
the	O
at	O
least	O
one	O
component	O
of	O
the	O
Gaussian	O
mixtures	O
better	O
match	O
the	O
ambient	O
noise	O
.	O
Thus	O
the	O
ASR	O
system	O
100	O
will	O
be	O
better	O
able	O
to	O
identify	O
background	O
noise	O
,	O
and	O
the	O
likelihood	O
of	O
the	O
ASR	O
system	O
100	O
construing	O
background	O
noise	O
100	O
as	O
a	O
speech	O
phoneme	O
will	O
be	O
reduced	O
.	O
Ultimately	O
,	O
the	O
recognition	O
performance	O
of	O
the	O
ASR	O
system	O
is	O
improved	O
.	O

The	O
ASR	O
system	O
100	O
may	O
be	O
implemented	O
in	O
hardware	O
or	O
software	O
or	O
a	O
combination	O
of	O
the	O
two	O
.	O

FIG	O
.	O
2	O
is	O
a	O
flow	O
chart	O
of	O
a	O
process	O
200	O
for	O
updating	O
a	O
model	O
of	O
background	O
noise	O
according	O
to	O
the	O
preferred	O
embodiment	O
of	O
the	O
invention	O
.	O
Referring	O
to	O
FIG	O
.	O
2	O
,	O
in	O
process	O
block	O
202	O
an	O
HMM	O
ASR	O
process	O
is	O
run	O
on	O
an	O
audio	O
signal	O
that	O
includes	O
speech	O
and	O
non	O
speech	O
background	O
sounds	O
.	O
Block	O
202	O
is	O
decision	O
block	O
that	O
depends	O
on	O
whether	O
a	O
long	O
pause	O
in	O
the	O
speech	O
component	O
of	O
the	O
audio	O
signal	O
is	O
detected	O
.	O
If	O
a	O
long	O
pause	O
is	O
not	O
detected	O
then	O
the	O
process	O
200	O
loops	O
back	O
to	O
block	O
204	O
and	O
continues	O
to	O
run	O
the	O
HMM	O
ASR	O
process	O
.	O
If	O
a	O
long	O
pause	O
is	O
detected	O
,	O
the	O
process	O
continues	O
with	O
process	O
block	O
206	O
in	O
which	O
a	O
characteristic	O
feature	O
vector	O
that	O
characterizes	O
the	O
audio	O
signal	O
during	O
the	O
long	O
pause	O
(	O
i	O
.	O
e	O
.	O
,	O
characterizes	O
the	O
background	O
sound	O
)	O
is	O
extracted	O
from	O
the	O
audio	O
signal	O
.	O
After	O
process	O
block	O
206	O
,	O
in	O
process	O
block	O
208	O
a	O
particular	O
mean	O
of	O
a	O
multi-component	O
Gaussian	O
mixture	O
that	O
is	O
used	O
to	O
model	O
non	O
speech	O
background	O
sounds	O
that	O
is	O
closest	O
to	O
the	O
characteristic	O
feature	O
vector	O
extracted	O
in	O
block	O
206	O
is	O
found	O
.	O
In	O
process	O
block	O
210	O
the	O
particular	O
mean	O
found	O
in	O
process	O
block	O
208	O
is	O
updated	O
so	O
that	O
it	O
is	O
closer	O
to	O
the	O
characteristic	O
feature	O
vector	O
extracted	O
in	O
block	O
206	O
.	O
From	O
block	O
210	O
the	O
process	O
200	O
loops	O
back	O
to	O
block	O
202	O
.	O

FIG	O
.	O
3	O
is	O
a	O
high	O
level	O
flow	O
chart	O
of	O
a	O
process	O
300	O
of	O
performing	O
automated	O
speech	O
recognition	O
using	O
an	O
HMM	O
.	O
FIG	O
.	O
3	O
is	O
a	O
preferred	O
form	O
of	O
block	O
202	O
of	O
FIG	O
.	O
2	O
.	O
In	O
process	O
block	O
302	O
for	O
each	O
successive	O
increment	O
of	O
time	O
(	O
frame	O
)	O
a	O
feature	O
vector	O
that	O
characterizes	O
an	O
audio	O
signal	O
is	O
extracted	O
.	O
In	O
process	O
block	O
304	O
for	O
each	O
successive	O
increment	O
of	O
time	O
,	O
the	O
feature	O
vector	O
is	O
used	O
to	O
evaluate	O
Gaussian	O
mixtures	O
that	O
give	O
the	O
a	O
posteriori	O
probabilities	O
that	O
various	O
states	O
of	O
the	O
HMM	O
result	O
in	O
audio	O
signal	O
characterized	O
by	O
the	O
feature	O
vector	O
.	O
In	O
process	O
block	O
306	O
the	O
most	O
probable	O
sequence	O
of	O
HMM	O
states	O
is	O
determined	O
in	O
view	O
of	O
the	O
a	O
posteriori	O
probabilities	O
and	O
transition	O
probabilities	O
that	O
govern	O
transitions	O
between	O
the	O
HMM	O
states	O
.	O
For	O
each	O
subsequent	O
frame	O
i	O
.	O
e	O
.	O
,	O
as	O
speech	O
continues	O
to	O
be	O
processed	O
,	O
the	O
most	O
probable	O
sequence	O
of	O
HMM	O
states	O
is	O
updated	O
.	O
A	O
variety	O
of	O
methods	O
of	O
varying	O
computational	O
complexity	O
are	O
known	O
to	O
persons	O
of	O
ordinary	O
skill	O
in	O
the	O
ASR	O
art	O
for	O
finding	O
the	O
most	O
probable	O
sequence	O
of	O
HMM	O
states	O
.	O

FIG	O
.	O
4	O
is	O
a	O
first	O
part	O
of	O
flow	O
chart	O
of	O
a	O
process	O
400	O
for	O
extracting	O
feature	O
vectors	O
from	O
an	O
audio	O
signal	O
according	O
to	O
the	O
preferred	O
embodiment	O
of	O
the	O
invention	O
.	O
FIGS	O
.	O
4	O
and	O
5	O
show	O
a	O
preferred	O
form	O
of	O
block	O
302	O
of	O
FIG	O
.	O
3	O
.	O
In	O
step	O
402	O
an	O
audio	O
signal	O
is	O
sampled	O
in	O
the	O
time	O
domain	O
to	O
obtain	O
a	O
discretized	O
representation	O
of	O
the	O
audio	O
signal	O
that	O
includes	O
a	O
sequence	O
of	O
samples	O
.	O
In	O
step	O
404	O
a	O
FIR	O
filter	O
is	O
applied	O
to	O
the	O
sequence	O
of	O
samples	O
to	O
emphasize	O
high	O
frequency	O
components	O
.	O
In	O
step	O
406	O
a	O
window	O
function	O
is	O
applied	O
to	O
successive	O
subsets	O
(	O
frames	O
)	O
of	O
the	O
sequence	O
of	O
samples	O
.	O
In	O
step	O
408	O
a	O
FFT	O
is	O
applied	O
to	O
successive	O
frames	O
of	O
samples	O
to	O
obtain	O
a	O
plurality	O
of	O
frequency	O
components	O
.	O
In	O
step	O
410	O
the	O
plurality	O
of	O
frequency	O
components	O
are	O
run	O
through	O
a	O
MEL	O
scale	O
filter	O
bank	O
to	O
obtain	O
a	O
plurality	O
of	O
MEL	O
scale	O
frequency	O
components	O
.	O
In	O
step	O
412	O
the	O
magnitude	O
of	O
each	O
MEL	O
scale	O
frequency	O
component	O
is	O
taken	O
to	O
obtain	O
a	O
plurality	O
of	O
MEL	O
frequency	O
component	O
magnitudes	O
.	O
In	O
step	O
414	O
the	O
log	O
of	O
each	O
MEL	O
frequency	O
component	O
magnitude	O
is	O
taken	O
to	O
obtain	O
a	O
plurality	O
of	O
log	O
magnitude	O
MEL	O
scale	O
frequency	O
components	O
.	O
Referring	O
to	O
FIG	O
.	O
5	O
which	O
is	O
a	O
second	O
part	O
of	O
the	O
flow	O
chart	O
begun	O
in	O
FIG	O
.	O
4	O
,	O
in	O
step	O
502	O
a	O
DCT	O
is	O
applied	O
to	O
the	O
log	O
magnitude	O
MEL	O
scale	O
frequency	O
components	O
for	O
each	O
frame	O
to	O
obtain	O
a	O
cepstral	O
coefficient	O
vector	O
for	O
each	O
frame	O
.	O
In	O
step	O
504	O
first	O
or	O
higher	O
order	O
differences	O
are	O
taken	O
between	O
corresponding	O
cepstral	O
coefficients	O
for	O
two	O
or	O
more	O
frames	O
to	O
obtain	O
at	O
least	O
first	O
order	O
inter	O
frame	O
cepstral	O
coefficient	O
differences	O
(	O
deltas	O
)	O
.	O
In	O
step	O
506	O
for	O
each	O
frame	O
the	O
cepstral	O
coefficients	O
and	O
the	O
inter	O
frame	O
cepstral	O
coefficient	O
differences	O
are	O
output	O
as	O
a	O
feature	O
vector	O
.	O

FIG	O
.	O
6	O
is	O
a	O
hardware	O
block	O
diagram	O
of	O
the	O
system	O
100	O
for	O
performing	O
automated	O
speech	O
recognition	O
according	O
to	O
the	O
preferred	O
embodiment	O
of	O
the	O
invention	O
.	O
As	O
illustrated	O
in	O
FIG	O
.	O
6	O
,	O
the	O
system	O
100	O
is	O
a	O
processor	O
602	O
based	O
system	O
that	O
executes	O
programs	O
200	O
,	O
300	O
,	O
400	O
that	O
are	O
stored	O
in	O
a	O
program	O
memory	O
606	O
.	O
The	O
program	O
memory	O
606	O
is	O
a	O
form	O
of	O
computer	O
readable	O
medium	O
.	O
The	O
processor	O
602	O
,	O
program	O
memory	O
606	O
,	O
a	O
workspace	O
memory	O
604	O
,	O
e.g.	O
Random	O
Access	O
Memory	O
(	O
RAM	O
)	O
,	O
and	O
input/output	O
(	O
I/O	O
)	O
interface	O
610	O
are	O
coupled	O
together	O
through	O
a	O
digital	O
signal	O
bus	O
608	O
.	O
The	O
I/O	O
interface	O
610	O
is	O
also	O
coupled	O
to	O
an	O
analog	O
to	O
digital	O
converter	O
(	O
A/D	O
)	O
612	O
and	O
to	O
a	O
transcribed	O
language	O
output	O
614	O
.	O
The	O
A/D	O
612	O
is	O
coupled	O
to	O
the	O
audio	O
signal	O
input	O
102	O
that	O
preferably	O
comprises	O
a	O
microphone	O
.	O
In	O
operation	O
the	O
audio	O
signal	O
is	O
input	O
at	O
the	O
audio	O
signal	O
input	O
102	O
converted	O
to	O
the	O
above	O
mentioned	O
discretized	O
representation	O
of	O
the	O
audio	O
signal	O
by	O
the	O
A/D	O
612	O
which	O
operates	O
under	O
the	O
control	O
of	O
the	O
processor	O
602	O
.	O
The	O
processor	O
executes	O
the	O
programs	O
described	O
with	O
reference	O
to	O
FIGS	O
.	O
2	O
-	O
5	O
and	O
outputs	O
a	O
stream	O
of	O
recognized	O
sentences	O
through	O
the	O
transcribed	O
language	O
output	O
614	O
.	O
Alternatively	O
the	O
recognized	O
words	O
or	O
sentences	O
are	O
used	O
to	O
control	O
the	O
operation	O
of	O
other	O
programs	O
executed	O
by	O
the	O
processor	O
.	O
For	O
example	O
the	O
system	O
100	O
may	O
comprise	O
other	O
peripheral	O
devices	O
such	O
as	O
wireless	O
phone	O
transceiver	O
(	O
not	O
shown	O
)	O
,	O
in	O
which	O
case	O
the	O
recognized	O
words	O
may	O
be	O
used	O
to	O
select	O
a	O
telephone	O
number	O
to	O
be	O
dialed	O
automatically	O
.	O
The	O
processor	O
602	O
preferably	O
comprises	O
a	O
digital	O
signal	O
processor	O
(	O
DST	O
)	O
.	O
Digital	O
signal	O
processors	O
have	O
instruction	O
sets	O
and	O
architectures	O
that	O
are	O
suitable	O
for	O
processing	O
audio	O
signal	O
.	O

As	O
will	O
be	O
apparent	O
to	O
those	O
of	O
ordinary	O
skill	O
in	O
the	O
pertinent	O
arts	O
,	O
the	O
invention	O
may	O
be	O
implemented	O
in	O
hardware	O
or	O
software	O
or	O
a	O
combination	O
thereof	O
.	O
Programs	O
embodying	O
the	O
invention	O
or	O
portions	O
thereof	O
may	O
be	O
stored	O
on	O
a	O
variety	O
of	O
types	O
of	O
computer	O
readable	O
media	O
including	O
optical	O
disks	O
,	O
hard	O
disk	O
drives	O
,	O
tapes	O
,	O
programmable	O
read	O
only	O
memory	O
chips	O
.	O
Network	O
circuits	O
may	O
also	O
serve	O
temporarily	O
as	O
computer	O
readable	O
media	O
from	O
which	O
programs	O
taught	O
by	O
the	O
present	O
invention	O
are	O
read	O
.	O

While	O
the	O
preferred	O
and	O
other	O
embodiments	O
of	O
the	O
invention	O
have	O
been	O
illustrated	O
and	O
described	O
,	O
it	O
will	O
be	O
clear	O
that	O
the	O
invention	O
is	O
not	O
so	O
limited	O
.	O
Numerous	O
modifications	O
,	O
changes	O
,	O
variations	O
,	O
substitutions	O
,	O
and	O
equivalents	O
will	O
occur	O
to	O
those	O
of	O
ordinary	O
skill	O
in	O
the	O
art	O
without	O
departing	O
from	O
the	O
spirit	O
and	O
scope	O
of	O
the	O
present	O
invention	O
as	O
defined	O
by	O
the	O
following	O
claims	O
.	O

1	O
.	O
A	O
method	O
of	O
performing	O
automatic	O
speech	O
recognition	O
in	O
a	O
variable	O
background	O
noise	O
environment	O
,	O
the	O
method	O
comprising	O
the	O
steps	O
of	O
:	O
processing	O
a	O
first	O
portion	O
of	O
an	O
inter-sentence	O
pause	O
to	O
obtain	O
a	O
first	O
characterization	O
of	O
the	O
first	O
portion	O
of	O
the	O
inter-sentence	O
pause	O
;	O
comparing	O
the	O
first	O
characterization	O
to	O
a	O
set	O
of	O
non-speech	O
audio	O
characterizations	O
to	O
determine	O
a	O
particular	O
non-speech	O
audio	O
characterization	O
among	O
the	O
set	O
of	O
non-speech	O
audio	O
characterizations	O
that	O
most	O
closely	O
matches	O
the	O
first	O
characterization	O
;	O
generating	O
an	O
updated	O
set	O
of	O
non-speech	O
characterizations	O
by	O
updating	O
the	O
particular	O
non-speech	O
audio	O
characterization	O
so	O
that	O
the	O
particular	O
non-speech	O
audio	O
characterization	O
more	O
closely	O
resembles	O
the	O
first	O
characterization	O
.	O

2	O
.	O
The	O
method	O
according	O
to	O
claim	O
1	O
further	O
comprising	O
the	O
step	O
of	O
:	O
detecting	O
the	O
inter-sentence	O
pause	O
.	O

3	O
.	O
The	O
method	O
according	O
to	O
claim	O
1	O
wherein	O
:	O
the	O
step	O
of	O
processing	O
the	O
first	O
portion	O
of	O
the	O
inter-sentence	O
pause	O
to	O
obtain	O
a	O
first	O
characterization	O
includes	O
a	O
sub-step	O
of	O
:	O
processing	O
the	O
first	O
portion	O
of	O
the	O
inter-sentence	O
pause	O
to	O
obtain	O
a	O
first	O
set	O
of	O
numbers	O
that	O
characterize	O
the	O
first	O
portion	O
of	O
the	O
inter-sentence	O
pause	O
;	O
and	O
the	O
step	O
of	O
comparing	O
the	O
first	O
characterization	O
to	O
a	O
set	O
of	O
non-speech	O
audio	O
characterizations	O
comprises	O
the	O
sub-steps	O
of	O
:	O
comparing	O
the	O
first	O
set	O
numbers	O
to	O
a	O
plurality	O
of	O
non-speech	O
audio	O
sets	O
of	O
numbers	O
to	O
determining	O
a	O
particular	O
set	O
of	O
non-speech	O
audio	O
numbers	O
that	O
most	O
closely	O
matches	O
the	O
first	O
set	O
of	O
numbers	O
.	O

4	O
.	O
The	O
method	O
according	O
to	O
claim	O
3	O
wherein	O
the	O
step	O
of	O
updating	O
the	O
non-speech	O
audio	O
characterization	O
comprises	O
the	O
sub-steps	O
of	O
:	O
replacing	O
each	O
number	O
in	O
the	O
particular	O
set	O
of	O
numbers	O
with	O
a	O
weighted	O
average	O
of	O
the	O
number	O
and	O
a	O
corresponding	O
number	O
in	O
the	O
first	O
set	O
of	O
numbers	O
.	O

5	O
.	O
The	O
method	O
according	O
to	O
claim	O
4	O
wherein	O
the	O
step	O
of	O
comparing	O
the	O
first	O
characterization	O
to	O
a	O
set	O
of	O
non-speech	O
audio	O
characterizations	O
comprises	O
the	O
sub-steps	O
of	O
:	O
taking	O
a	O
dot	O
product	O
between	O
the	O
first	O
set	O
of	O
numbers	O
and	O
each	O
of	O
the	O
plurality	O
of	O
non-speech	O
audio	O
sets	O
of	O
numbers	O
.	O

6	O
.	O
The	O
method	O
according	O
to	O
claim	O
5	O
wherein	O
the	O
plurality	O
of	O
non-speech	O
audio	O
sets	O
of	O
numbers	O
are	O
means	O
of	O
components	O
of	O
Gaussian	O
mixtures	O
that	O
characterize	O
the	O
probability	O
of	O
an	O
underlying	O
state	O
of	O
a	O
hidden	O
Markov	O
model	O
of	O
the	O
audio	O
signal	O
,	O
given	O
the	O
first	O
set	O
of	O
numbers	O
.	O

7	O
.	O
The	O
method	O
according	O
to	O
claim	O
6	O
wherein	O
the	O
step	O
of	O
processing	O
the	O
first	O
portion	O
of	O
the	O
inter-sentence	O
pause	O
to	O
obtain	O
the	O
first	O
characterization	O
of	O
the	O
first	O
portion	O
of	O
the	O
inter-sentence	O
pause	O
comprises	O
the	O
sub-steps	O
of	O
:	O
a	O
)	O
time	O
domain	O
sampling	O
the	O
inter-sentence	O
pause	O
to	O
obtain	O
a	O
discretized	O
representation	O
of	O
the	O
inter-sentence	O
pause	O
that	O
includes	O
a	O
sequence	O
of	O
samples	O
;	O
b	O
)	O
time	O
domain	O
filtering	O
the	O
sequence	O
of	O
samples	O
to	O
obtain	O
a	O
filtered	O
sequence	O
of	O
samples	O
;	O
c	O
)	O
applying	O
a	O
window	O
function	O
to	O
successive	O
subsets	O
of	O
the	O
filtered	O
sequence	O
of	O
samples	O
to	O
obtain	O
a	O
sequence	O
of	O
frames	O
of	O
windowed	O
filtered	O
samples	O
;	O
d	O
)	O
transforming	O
each	O
of	O
the	O
frames	O
of	O
windowed	O
filtered	O
samples	O
to	O
a	O
frequency	O
domain	O
to	O
obtain	O
a	O
plurality	O
of	O
frequency	O
components	O
;	O
e	O
)	O
taking	O
a	O
plurality	O
of	O
weighted	O
sums	O
of	O
the	O
plurality	O
of	O
frequency	O
components	O
to	O
obtain	O
a	O
plurality	O
of	O
bandpass	O
filtered	O
outputs	O
;	O
f	O
)	O
taking	O
the	O
log	O
of	O
the	O
magnitude	O
of	O
each	O
of	O
the	O
bandpass	O
filtered	O
outputs	O
to	O
obtain	O
a	O
plurality	O
of	O
log	O
magnitude	O
band	O
pass	O
filtered	O
outputs	O
;	O
and	O
g	O
)	O
transforming	O
the	O
plurality	O
of	O
log	O
magnitude	O
bandpass	O
filtered	O
outputs	O
to	O
a	O
time	O
domain	O
to	O
obtain	O
at	O
least	O
a	O
subset	O
of	O
the	O
first	O
set	O
of	O
numbers	O
.	O

8	O
.	O
The	O
method	O
according	O
to	O
claim	O
7	O
wherein	O
the	O
step	O
of	O
processing	O
the	O
first	O
portion	O
of	O
the	O
inter-sentence	O
pause	O
to	O
obtain	O
the	O
first	O
characterization	O
of	O
the	O
first	O
portion	O
of	O
the	O
inter-sentence	O
pause	O
further	O
comprises	O
the	O
sub-steps	O
of	O
:	O
repeating	O
sub-steps	O
(	O
a	O
)	O
through	O
(	O
g	O
)	O
for	O
two	O
portions	O
obtained	O
from	O
at	O
least	O
one	O
inter-sentence	O
pause	O
to	O
obtain	O
two	O
sets	O
of	O
numbers	O
;	O
and	O
taking	O
the	O
difference	O
between	O
corresponding	O
numbers	O
in	O
the	O
two	O
sets	O
of	O
numbers	O
to	O
obtain	O
at	O
least	O
a	O
subset	O
of	O
the	O
first	O
set	O
of	O
numbers	O
.	O

9	O
.	O
An	O
automated	O
speech	O
recognition	O
system	O
comprising	O
:	O
an	O
audio	O
signal	O
input	O
for	O
inputting	O
an	O
audio	O
signal	O
that	O
includes	O
speech	O
and	O
background	O
sounds	O
;	O
a	O
feature	O
extractor	O
coupled	O
to	O
the	O
audio	O
signal	O
input	O
for	O
receiving	O
the	O
audio	O
signal	O
and	O
outputting	O
characterizations	O
of	O
a	O
sequence	O
of	O
segments	O
of	O
the	O
audio	O
signal	O
;	O
a	O
model	O
coupled	O
to	O
the	O
feature	O
extractor	O
,	O
wherein	O
the	O
model	O
includes	O
a	O
plurality	O
of	O
states	O
to	O
which	O
characterization	O
of	O
the	O
sequence	O
of	O
segments	O
are	O
applied	O
for	O
evaluating	O
a	O
posteriori	O
probabilities	O
that	O
one	O
or	O
more	O
of	O
the	O
plurality	O
of	O
states	O
occurred	O
;	O
a	O
search	O
engine	O
coupled	O
to	O
model	O
for	O
finding	O
one	O
or	O
more	O
high	O
probability	O
sequences	O
of	O
the	O
plurality	O
of	O
states	O
of	O
the	O
model	O
;	O
a	O
detector	O
for	O
detecting	O
an	O
absence	O
of	O
speech	O
sounds	O
of	O
the	O
audio	O
signal	O
and	O
outputting	O
a	O
predetermined	O
signal	O
when	O
the	O
absence	O
of	O
speech	O
sounds	O
is	O
detected	O
;	O
and	O
a	O
comparer	O
and	O
updater	O
coupled	O
to	O
the	O
detector	O
for	O
receiving	O
the	O
predetermined	O
signal	O
and	O
in	O
response	O
thereto	O
determines	O
a	O
mean	O
of	O
a	O
multi	O
component	O
Gaussian	O
mixture	O
associated	O
with	O
background	O
sounds	O
that	O
is	O
closest	O
to	O
a	O
feature	O
vector	O
that	O
characterizes	O
the	O
audio	O
signal	O
during	O
the	O
absence	O
of	O
speech	O
sounds	O
,	O
and	O
updates	O
the	O
mean	O
so	O
that	O
the	O
mean	O
is	O
closer	O
to	O
the	O
feature	O
vector	O
that	O
characterizes	O
the	O
audio	O
signal	O
during	O
the	O
absence	O
of	O
speech	O
sounds	O
.	O

10	O
.	O
The	O
automated	O
speech	O
recognition	O
system	O
according	O
to	O
claim	O
9	O
wherein	O
:	O
the	O
feature	O
extractor	O
outputs	O
characterizations	O
for	O
each	O
of	O
a	O
succession	O
of	O
frames	O
that	O
include	O
feature	O
vectors	O
that	O
include	O
cepstral	O
coefficients	O
;	O
the	O
model	O
comprises	O
a	O
hidden	O
marcov	O
model	O
that	O
includes	O
a	O
plurality	O
of	O
emitting	O
states	O
and	O
multi	O
component	O
Gaussian	O
mixtures	O
that	O
give	O
the	O
a	O
posteriori	O
probability	O
that	O
a	O
given	O
feature	O
vector	O
is	O
attributable	O
to	O
a	O
given	O
emitting	O
state	O
;	O
the	O
detector	O
detects	O
the	O
absence	O
of	O
speech	O
sounds	O
by	O
comparing	O
a	O
function	O
of	O
one	O
or	O
more	O
cepstral	O
coefficients	O
to	O
a	O
threshold	O
:	O
and	O
.	O

11	O
.	O
An	O
automated	O
speech	O
recognition	O
system	O
comprising	O
:	O
an	O
audio	O
input	O
for	O
inputting	O
an	O
audio	O
signal	O
;	O
an	O
analog	O
to	O
digital	O
converter	O
coupled	O
to	O
the	O
audio	O
input	O
for	O
sampling	O
the	O
audio	O
signal	O
and	O
outputting	O
a	O
discretized	O
audio	O
signal	O
;	O
and	O
a	O
microprocessor	O
coupled	O
to	O
the	O
analog	O
to	O
digital	O
converter	O
for	O
receiving	O
the	O
discretized	O
audio	O
signal	O
and	O
executing	O
a	O
program	O
for	O
performing	O
automated	O
speech	O
recognition	O
,	O
the	O
program	O
comprising	O
programming	O
instructions	O
for	O
:	O
detecting	O
an	O
inter-sentence	O
pause	O
of	O
an	O
audio	O
signal	O
;	O
processing	O
a	O
first	O
portion	O
of	O
the	O
inter-sentence	O
pause	O
to	O
obtain	O
a	O
first	O
characterization	O
of	O
the	O
first	O
portion	O
of	O
the	O
inter-sentence	O
pause	O
;	O
comparing	O
the	O
first	O
characterization	O
to	O
a	O
set	O
of	O
non-speech	O
audio	O
characterization	O
to	O
determine	O
a	O
particular	O
non-speech	O
audio	O
characterization	O
among	O
the	O
set	O
of	O
non-speech	O
audio	O
characterizations	O
that	O
most	O
closely	O
matches	O
the	O
first	O
characterization	O
;	O
and	O
updating	O
the	O
particular	O
non-speech	O
audio	O
characterization	O
so	O
that	O
the	O
particular	O
non-speech	O
audio	O
characterization	O
more	O
closely	O
resembles	O
the	O
first	O
characterization	O
.	O

12	O
.	O
A	O
computer	O
readable	O
medium	O
storing	O
programming	O
instructions	O
for	O
performing	O
automatic	O
speech	O
recognition	O
in	O
a	O
variable	O
background	O
noise	O
environment	O
,	O
including	O
programming	O
instructions	O
for	O
:	O
detecting	O
a	O
plurality	O
of	O
inter-sentence	O
pauses	O
of	O
an	O
audio	O
signal	O
;	O
processing	O
a	O
first	O
portion	O
of	O
a	O
first	O
inter-sentence	O
pause	O
to	O
obtain	O
a	O
first	O
characterization	O
of	O
the	O
first	O
portion	O
of	O
the	O
first	O
inter-sentence	O
pause	O
;	O
comparing	O
the	O
first	O
characterization	O
to	O
a	O
set	O
of	O
non-speech	O
audio	O
characterizations	O
to	O
determine	O
a	O
particular	O
non-speech	O
audio	O
characterization	O
among	O
the	O
set	O
of	O
non-speech	O
audio	O
characterizations	O
that	O
most	O
closely	O
matches	O
the	O
first	O
characterization	O
;	O
and	O
updating	O
the	O
particular	O
non-speech	O
audio	O
so	O
that	O
the	O
particular	O
non-speech	O
audio	O
characterization	O
more	O
closely	O
resembles	O
the	O
first	O
characterization	O
;	O
processing	O
one	O
or	O
more	O
additional	O
portions	O
of	O
the	O
plurality	O
of	O
inter-sentence	O
pauses	O
to	O
obtain	O
one	O
or	O
more	O
additional	O
characterizations	O
that	O
characterize	O
the	O
one	O
or	O
more	O
additional	O
portions	O
of	O
the	O
plurality	O
of	O
inter-sentence	O
pauses	O
;	O
comparing	O
the	O
one	O
or	O
more	O
additional	O
characterizations	O
to	O
the	O
set	O
of	O
reference	O
characterization	O
to	O
find	O
reference	O
characterizations	O
among	O
the	O
set	O
of	O
non-speech	O
audio	O
characterizations	O
that	O
most	O
closely	O
matches	O
the	O
one	O
or	O
more	O
additional	O
characterizations	O
.	O

13	O
.	O
The	O
computer	O
readable	O
medium	O
according	O
to	O
claim	O
12	O
wherein	O
:	O
the	O
programming	O
instructions	O
for	O
processing	O
the	O
first	O
portion	O
of	O
the	O
first	O
inter-sentence	O
pause	O
to	O
obtain	O
a	O
first	O
characterization	O
include	O
programming	O
instructions	O
for	O
:	O
processing	O
the	O
first	O
portion	O
of	O
the	O
first	O
inter-sentence	O
pause	O
to	O
obtain	O
a	O
first	O
set	O
of	O
numbers	O
that	O
characterize	O
the	O
first	O
portion	O
of	O
the	O
first	O
inter-sentence	O
pause	O
;	O
and	O
the	O
programming	O
instructions	O
for	O
comparing	O
the	O
first	O
characterization	O
to	O
a	O
set	O
of	O
non-speech	O
audio	O
characterizations	O
comprises	O
the	O
programming	O
instructions	O
for	O
:	O
comparing	O
the	O
first	O
set	O
numbers	O
to	O
a	O
plurality	O
of	O
non-speech	O
audio	O
sets	O
of	O
numbers	O
to	O
determining	O
a	O
particular	O
set	O
of	O
non-speech	O
audio	O
numbers	O
that	O
most	O
closely	O
matches	O
the	O
first	O
set	O
of	O
numbers	O
.	O

14	O
.	O
The	O
computer	O
readable	O
medium	O
according	O
to	O
claim	O
13	O
wherein	O
the	O
programming	O
instructions	O
for	O
updating	O
the	O
non-speech	O
audio	O
characterization	O
comprising	O
programming	O
instructions	O
for	O
:	O
replacing	O
each	O
number	O
in	O
the	O
particular	O
set	O
of	O
numbers	O
with	O
a	O
weighted	O
average	O
of	O
the	O
number	O
and	O
a	O
corresponding	O
number	O
in	O
the	O
first	O
set	O
of	O
numbers	O
.	O

15	O
.	O
The	O
computer	O
readable	O
medium	O
according	O
to	O
claim	O
14	O
wherein	O
the	O
programming	O
instructions	O
for	O
comparing	O
the	O
first	O
characterization	O
to	O
a	O
set	O
of	O
non-speech	O
audio	O
characterizations	O
comprise	O
programming	O
instructions	O
for	O
:	O
taking	O
a	O
dot	O
product	O
between	O
the	O
first	O
set	O
of	O
numbers	O
and	O
each	O
of	O
the	O
plurality	O
of	O
non-speech	O
audio	O
sets	O
of	O
numbers	O
.	O

16	O
.	O
The	O
computer	O
readable	O
medium	O
according	O
to	O
claim	O
15	O
wherein	O
:	O
the	O
plurality	O
of	O
non-speech	O
audio	O
sets	O
of	O
numbers	O
are	O
means	O
of	O
components	O
of	O
Gaussian	O
mixtures	O
that	O
characterize	O
the	O
probability	O
of	O
an	O
underlying	O
state	O
of	O
a	O
hidden	O
markov	O
model	O
of	O
the	O
audio	O
signal	O
,	O
given	O
the	O
first	O
set	O
of	O
numbers	O
.	O

17	O
.	O
The	O
computer	O
readable	O
medium	O
according	O
to	O
claim	O
16	O
wherein	O
the	O
programming	O
instructions	O
for	O
processing	O
the	O
first	O
portion	O
of	O
the	O
first	O
inter-sentence	O
pause	O
to	O
obtain	O
the	O
first	O
characterization	O
of	O
the	O
first	O
portion	O
of	O
the	O
first	O
inter-sentence	O
pause	O
comprise	O
the	O
programming	O
instruction	O
,	O
for	O
:	O
a	O
)	O
time	O
domain	O
sampling	O
the	O
first	O
inter-sentence	O
pause	O
to	O
obtain	O
a	O
discretized	O
representation	O
of	O
the	O
audio	O
signal	O
that	O
includes	O
a	O
sequence	O
of	O
samples	O
;	O
b	O
)	O
time	O
domain	O
filtering	O
the	O
sequence	O
of	O
samples	O
to	O
obtain	O
a	O
filtered	O
sequence	O
of	O
samples	O
;	O
c	O
)	O
applying	O
a	O
window	O
function	O
to	O
successive	O
subsets	O
of	O
the	O
filtered	O
sequence	O
of	O
samples	O
to	O
obtain	O
a	O
sequence	O
of	O
frames	O
of	O
windowed	O
filtered	O
samples	O
;	O
d	O
)	O
transforming	O
each	O
of	O
the	O
frames	O
of	O
windowed	O
filtered	O
samples	O
to	O
a	O
frequency	O
domain	O
to	O
obtain	O
a	O
plurality	O
of	O
frequency	O
components	O
;	O
e	O
)	O
taking	O
a	O
plurality	O
of	O
weighted	O
sums	O
of	O
the	O
plurality	O
of	O
frequency	O
components	O
to	O
obtain	O
a	O
plurality	O
of	O
bandpass	O
filtered	O
outputs	O
;	O
f	O
)	O
taking	O
the	O
log	O
of	O
the	O
magnitude	O
of	O
each	O
of	O
the	O
bandpass	O
filtered	O
outputs	O
to	O
obtain	O
a	O
plurality	O
of	O
log	O
magnitude	O
bandpass	O
filtered	O
outputs	O
;	O
and	O
g	O
)	O
transforming	O
the	O
plurality	O
of	O
log	O
magnitude	O
bandpass	O
filtered	O
outputs	O
to	O
a	O
time	O
domain	O
to	O
obtain	O
at	O
least	O
a	O
subset	O
of	O
the	O
first	O
set	O
of	O
numbers	O
.	O

18	O
.	O
The	O
computer	O
readable	O
medium	O
according	O
to	O
claim	O
17	O
wherein	O
the	O
programming	O
instructions	O
for	O
processing	O
the	O
first	O
portion	O
of	O
the	O
first	O
inter-sentence	O
pause	O
to	O
obtain	O
the	O
first	O
characterization	O
of	O
the	O
first	O
portion	O
of	O
the	O
first	O
inter-sentence	O
pause	O
further	O
comprises	O
programming	O
instructions	O
for	O
:	O
applying	O
programming	O
instructions	O
(	O
a	O
)	O
through	O
(	O
g	O
)	O
to	O
two	O
portions	O
of	O
the	O
first	O
inter-sentence	O
pause	O
to	O
obtain	O
two	O
sets	O
of	O
numbers	O
;	O
and	O
taking	O
the	O
difference	O
between	O
corresponding	O
numbers	O
in	O
the	O
two	O
sets	O
of	O
numbers	O
to	O
obtain	O
at	O
least	O
a	O
subset	O
of	O
the	O
first	O
set	O
of	O
numbers	O
.	O

