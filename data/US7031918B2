US	O
7031918	O
B2	O
20060418	O

US	O
10103184	O
20020320	O

10	O
eng	O
eng	O

US	O
10103184	O
20020320	O

20060418	O

20060418	O

705	O

G10L	O
15/00	O
20060101CFI20060418BHUS	O

20060101	O

C	O
G	O
10	O
L	O
15	O
00	O
F	O
I	O

20060418	O

US	O

B	O
H	O

G10L	O
15/06	O
20060101AFI20060418BHUS	O

20060101	O

A	O
G	O
10	O
L	O
15	O
06	O
F	O
I	O

20060418	O

US	O

B	O
H	O

G10L	O
15/18	O
20060101A	O
N20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
18	O
N	O

20051008	O

EP	O

R	O
M	O

US	O

704/243	O
704	O
243	O

704/254	O
704	O
254	O

704/E15.008	O
704	O
E15	O
.	O
008	O

704/E15.009	O
704	O
E15	O
.	O
009	O

G10L	O
15/06A	O
G	O
10	O
L	O
15	O
06	O

A	O

G10L	O
15/06T	O
G	O
10	O
L	O
15	O
06	O

T	O

S10L	O
15	O
:	O
18L	O
S	O
10	O
L	O
15	O
18	O

L	O

22	O
Generating	O
a	O
task-adapted	O
acoustic	O
model	O
from	O
one	O
or	O
more	O
supervised	O
and/or	O
unsupervised	O
corpora	O

US	O
4994983	O
A	O
Landell	O
et_al.	O

19910219	O

19890502	O

US	O
5675707	O
A	O
Gorin	O
et_al.	O

19971007	O

19950915	O

US	O
5675710	O
A	O
Lewis	O
19971007	O

19950607	O

US	O
6021384	O
A	O
Gorin	O
et_al.	O

20000201	O

19971029	O

US	O
6061653	O
A	O
Fisher	O
et_al.	O

20000509	O

19980714	O

US	O
6173261	O
B1	O
Arai	O
et_al.	O

20010109	O

19981221	O

US	O
6205426	O
B1	O
Nguyen	O
et_al.	O

20010320	O

19990125	O

US	O
6260014	O
B1	O
Bahl	O
et_al.	O

20010710	O

19980914	O

US	O
6430551	O
B1	O
Thelen	O
et_al.	O

20020806	O

19981006	O

US	O
6681206	O
B1	O
Gorin	O
et_al.	O

20040120	O

20001018	O

US	O
6728674	O
B1	O
Griniasty	O
20040427	O

20000731	O

US	O
6799162	O
B1	O
Goronzy	O
et_al.	O

20040928	O

19991215	O

US	O
6879956	O
B1	O
Honda	O
et_al.	O

20050412	O

20000929	O

US	O
20020087314	O
A1	O
Fischer	O
et_al.	O

20020704	O

20011113	O

Fabrice	B-Citation
Lefevre	I-Citation
,	I-Citation
Jean-Luc	I-Citation
Gauvain	I-Citation
,	I-Citation
and	I-Citation
Lori	I-Citation
Lamel	I-Citation
,	I-Citation
“	I-Citation
Towards	I-Citation
Task-Independent	I-Citation
Speech	I-Citation
Recognition	I-Citation
,	I-Citation
”	I-Citation
Proc.	I-Citation
2001	I-Citation
IEEE	I-Citation
Int.	I-Citation
Conf.	I-Citation
Acoust	I-Citation
.	I-Citation
Speech	I-Citation
,	I-Citation
Sig	I-Citation
.	I-Citation
Proc.	I-Citation
(	I-Citation
ICASSP	I-Citation
'01	I-Citation
)	I-Citation
,	I-Citation
May	I-Citation
7	I-Citation
,	I-Citation
2001-May	I-Citation
11	I-Citation
,	I-Citation
2001	I-Citation
,	I-Citation
vol.	I-Citation
1	I-Citation
,	I-Citation
pp.	I-Citation
521	I-Citation
-	I-Citation
524	I-Citation
.	O

Investigating	B-Citation
Lightly	I-Citation
Supervised	I-Citation
Acoustic	I-Citation
Model	I-Citation
Training	I-Citation
,	I-Citation
by	I-Citation
Lori	I-Citation
Lamel	I-Citation
et_al.	I-Citation
2001	I-Citation
IEEE	I-Citation
International	I-Citation
Conference	I-Citation
on	I-Citation
Acoustics	I-Citation
,	I-Citation
Speech	I-Citation
,	I-Citation
and	I-Citation
Signal	I-Citation
Processing	I-Citation
May	I-Citation
7	I-Citation
-	I-Citation
11	I-Citation
,	I-Citation
2001	I-Citation
,	I-Citation
Salt	I-Citation
Lake	I-Citation
City	I-Citation
,	I-Citation
UT	I-Citation
,	I-Citation
USA	I-Citation
.	O

Confidence-Measure-Driven	B-Citation
Unsupervised	I-Citation
Incremental	I-Citation
Adaptation	I-Citation
For	I-Citation
HMM-Based	I-Citation
Speech	I-Citation
Recognition	I-Citation
,	I-Citation
by	I-Citation
Delphine	I-Citation
Charlet	I-Citation
.	I-Citation
2001	I-Citation
IEEE	I-Citation
International	I-Citation
Conference	I-Citation
on	I-Citation
Acoustics	I-Citation
,	I-Citation
Speech	I-Citation
,	I-Citation
and	I-Citation
Signal	I-Citation
Processing	I-Citation
May	I-Citation
7	I-Citation
-	I-Citation
11	I-Citation
,	I-Citation
2001	I-Citation
,	I-Citation
Salt	I-Citation
Lake	I-Citation
City	I-Citation
,	I-Citation
UT	I-Citation
,	I-Citation
USA	I-Citation
.	O

WO	O
2006015169	O
A3	O
20061005	O

20050728	O

WO	O
2006015169	O
A2	O
20060209	O

20050728	O

GB	O
2432704	O
A	O
20070530	O

20050728	O

GB	O
2432704	O
B	O
20091209	O

20050728	O

US	O
7818175	O
B2	O
20101019	O

20050728	O

US	O
7827032	O
B2	O
20101102	O

20061006	O

US	O
7860716	O
B2	O
20101228	O

20070424	O

US	O
7865362	O
B2	O
20110104	O

20050204	O

US	O
7895039	O
B2	O
20110222	O

20070321	O

US	O
7949533	O
B2	O
20110524	O

20070321	O

US	O
20030182120	O
A1	O
20030925	O

Microsoft	O
Corporation	O
02	O

Redmond	O
WA	O
US	O

Hwang	O
Mei	O
Yuh	O

Sammamish	O
WA	O
US	O

US	O

Kelly	O
Joseph	O
R.	O

Westman	O
,	O
Champlin	O
&	O
Kelly	O
,	O
P.	O
A.	O

Storm	O
Donald	O
L.	O

2654	O

US	O
7031918	O
B2	O
20060418	O

20020320	O

US	O
20030182120	O
A1	O
20030925	O

20020320	O

US	O
20030182120	O
A1	O
20030925	O

20020320	O

US	O
7031918	O
B2	O
20060418	O

20020320	O

Unsupervised	O
speech	O
data	O
is	O
provided	O
to	O
a	O
speech	O
recognizer	O
that	O
recognizes	O
the	O
speech	O
data	O
and	O
outputs	O
a	O
recognition	O
result	O
along	O
with	O
a	O
confidence	O
measure	O
for	O
each	O
recognized	O
word	O
.	O
A	O
task-related	O
acoustic	O
model	O
is	O
generated	O
based	O
on	O
the	O
recognition	O
result	O
,	O
the	O
speech	O
data	O
and	O
the	O
confidence	O
measure	O
.	O
Additional	O
task	O
independent	O
model	O
can	O
be	O
used	O
.	O
The	O
speech	O
data	O
can	O
be	O
weighted	O
by	O
the	O
confidence	O
measure	O
in	O
generating	O
the	O
acoustic	O
model	O
so	O
that	O
only	O
data	O
that	O
has	O
been	O
recognized	O
with	O
a	O
high	O
degree	O
of	O
confidence	O
will	O
weigh	O
heavily	O
in	O
generation	O
of	O
the	O
acoustic	O
model	O
.	O
The	O
acoustic	O
model	O
can	O
be	O
formed	O
from	O
a	O
Gaussian	O
mean	O
and	O
variance	O
of	O
the	O
data	O
.	O

20020320	O

AS	O
ASSIGNMENT	O
N	O
US	O
7031918B2	O
MICROSOFT	O
CORPORATION	O
,	O
WASHINGTON	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNOR:HWANG	O
,	O
MEI	O
YUH;REEL/FRAME:012723/0628	O

20020320	O

20090916	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
7031918B2	O
4	O

BACKGROUND	O
OF	O
THE	O
INVENTION	O
The	O
present	O
invention	O
relates	O
to	O
speech	O
recognition	O
.	O
More	O
specifically	O
,	O
the	O
present	O
invention	O
relates	O
to	O
generating	O
an	O
acoustic	O
model	O
for	O
a	O
speech	O
recognizer	O
from	O
one	O
or	O
more	O
different	O
corpora	O
,	O
such	O
as	O
supervised	O
and/or	O
unsupervised	O
training	O
corpora	O
.	O
Conventional	O
speech	O
recognition	O
engines	O
compare	O
an	O
input	O
signal	O
,	O
representative	O
of	O
an	O
utterance	O
of	O
speech	O
to	O
be	O
recognized	O
,	O
against	O
speech	O
and	O
language	O
related	O
models	O
.	O
The	O
speech	O
recognizers	O
then	O
output	O
a	O
recognition	O
result	O
indicative	O
of	O
recognized	O
speech	O
(	O
recognized	O
from	O
the	O
input	O
signal	O
)	O
based	O
on	O
the	O
comparison	O
against	O
the	O
models	O
.	O

Most	O
state-of-the-art	O
speech	O
recognition	O
systems	O
include	O
two	O
major	O
components	O
in	O
their	O
modeling	O
techniques	O
.	O
Those	O
components	O
include	O
a	O
language	O
model	O
and	O
an	O
acoustic	O
model	O
.	O

The	O
language	O
model	O
models	O
the	O
linguistic	O
context	O
of	O
lexical	O
units	O
,	O
which	O
are	O
usually	O
words	O
.	O
A	O
popular	O
language	O
model	O
for	O
dictation	O
is	O
an	O
n-gram	O
model	O
.	O
In	O
the	O
n-gram	O
model	O
,	O
the	O
likelihood	O
of	O
the	O
next	O
word	O
,	O
given	O
a	O
history	O
of	O
n	O
−	O
1	O
previous	O
words	O
,	O
is	O
predicted	O
.	O
Another	O
type	O
of	O
language	O
model	O
is	O
typically	O
used	O
on	O
limited	O
domain	O
applications	O
.	O
That	O
model	O
is	O
a	O
context-free	O
grammar	O
,	O
and	O
is	O
used	O
where	O
the	O
input	O
utterance	O
is	O
expected	O
to	O
follow	O
a	O
more	O
strict	O
sequence	O
of	O
words	O
than	O
is	O
required	O
for	O
a	O
general	O
dictation	O
system	O
.	O

For	O
example	O
,	O
in	O
a	O
system	O
where	O
a	O
user	O
is	O
expected	O
to	O
answer	O
the	O
question	O
“	O
how	O
old	O
are	O
you	O
?	O
”	O
,	O
the	O
system	O
may	O
use	O
a	O
context-free	O
grammar	O
which	O
begins	O
with	O
optional	O
words	O
“	O
I	O
am	O
”	O
followed	O
by	O
a	O
number	O
,	O
and	O
then	O
followed	O
by	O
optional	O
words	O
“	O
years	O
old	O
”	O
.	O
Such	O
a	O
stricter	O
model	O
constrains	O
the	O
search	O
space	O
and	O
makes	O
the	O
recognition	O
task	O
both	O
easier	O
and	O
faster	O
.	O

An	O
acoustic	O
model	O
models	O
the	O
sound	O
produced	O
by	O
a	O
human	O
speaker	O
.	O
The	O
acoustics	O
vary	O
partly	O
based	O
on	O
the	O
characteristics	O
of	O
the	O
speaker	O
.	O
For	O
example	O
,	O
the	O
acoustics	O
can	O
vary	O
based	O
on	O
different	O
speakers	O
,	O
the	O
accents	O
of	O
the	O
speaker	O
,	O
or	O
the	O
speaking	O
style	O
,	O
etc	O
.	O
However	O
,	O
the	O
acoustics	O
can	O
vary	O
based	O
on	O
other	O
criteria	O
as	O
well	O
,	O
such	O
as	O
the	O
particular	O
microphone	O
being	O
used	O
on	O
the	O
input	O
end	O
to	O
the	O
speech	O
recognizer	O
,	O
the	O
environment	O
in	O
which	O
the	O
speech	O
recognizer	O
is	O
being	O
used	O
,	O
the	O
application	O
domain	O
in	O
which	O
the	O
speech	O
recognizer	O
is	O
operating	O
,	O
etc	O
.	O

In	O
order	O
to	O
generate	O
a	O
general	O
acoustic	O
model	O
which	O
is	O
to	O
be	O
used	O
in	O
an	O
application	O
that	O
is	O
both	O
speaker-independent	O
and	O
task-independent	O
,	O
a	O
wide	O
variety	O
of	O
data	O
is	O
used	O
.	O
For	O
example	O
,	O
speech	O
training	O
data	O
gathered	O
from	O
different	O
speakers	O
,	O
different	O
tasks	O
,	O
different	O
microphones	O
,	O
etc	O
.	O
,	O
is	O
simply	O
pooled	O
together	O
and	O
the	O
parameters	O
of	O
the	O
acoustic	O
model	O
are	O
estimated	O
without	O
bias	O
.	O
The	O
training	O
corpus	O
typically	O
includes	O
a	O
plurality	O
of	O
different	O
utterances	O
represented	O
by	O
WAV	O
files	O
.	O
Corresponding	O
to	O
each	O
WAV	O
file	O
is	O
a	O
manual	O
transcription	O
of	O
the	O
words	O
represented	O
by	O
the	O
WAV	O
file	O
.	O
Such	O
a	O
training	O
corpus	O
is	O
referred	O
to	O
as	O
supervised	O
data	O
,	O
in	O
that	O
a	O
laborious	O
manual	O
transcription	O
has	O
been	O
preformed	O
which	O
corresponds	O
exactly	O
to	O
the	O
words	O
spoken	O
in	O
the	O
WAV	O
file	O
.	O

However	O
,	O
it	O
is	O
well	O
known	O
that	O
a	O
speaker-dependent	O
acoustic	O
model	O
(	O
one	O
in	O
which	O
the	O
acoustic	O
model	O
is	O
trained	O
on	O
a	O
single	O
speaker	O
and	O
used	O
by	O
the	O
same	O
speaker	O
only	O
)	O
produces	O
two-three	O
times	O
lower	O
word	O
error	O
rate	O
than	O
a	O
speaker-independent	O
acoustic	O
model	O
.	O
Therefore	O
,	O
conventional	O
dictation	O
systems	O
usually	O
encourage	O
the	O
user	O
to	O
spend	O
varying	O
amounts	O
of	O
time	O
“	O
enrolling	O
”	O
himself	O
or	O
herself	O
in	O
the	O
system	O
.	O
This	O
often	O
entails	O
reading	O
some	O
pre-selected	O
texts	O
to	O
the	O
system	O
for	O
at	O
least	O
several	O
minutes	O
,	O
and	O
in	O
many	O
cases	O
much	O
longer	O
.	O

Similarly	O
,	O
a	O
task-dependent	O
acoustic	O
model	O
(	O
one	O
in	O
which	O
the	O
acoustic	O
model	O
is	O
trained	O
on	O
only	O
those	O
utterances	O
that	O
are	O
related	O
to	O
the	O
task	O
for	O
which	O
the	O
acoustic	O
model	O
will	O
be	O
used	O
)	O
performs	O
significantly	O
better	O
than	O
a	O
task-independent	O
acoustic	O
model	O
.	O
Such	O
a	O
system	O
is	O
discussed	O
in	O
F.	B-Citation
Lefevre	I-Citation
,	I-Citation
J	I-Citation
—	I-Citation
L	I-Citation
Gauvain	I-Citation
and	I-Citation
L.	I-Citation
Lamel	I-Citation
,	I-Citation
Towards	I-Citation
Task	I-Citation
Independent	I-Citation
Speech	I-Citation
Recognition	I-Citation
,	I-Citation
ICASSP-2001	I-Citation
.	O

In	O
order	O
to	O
adapt	O
a	O
task-independent	O
acoustic	O
model	O
to	O
become	O
a	O
task-dependent	O
acoustic	O
model	O
,	O
one	O
proposed	O
solution	O
has	O
been	O
to	O
collect	O
a	O
task-dependent	O
acoustic	O
corpus	O
and	O
transcribe	O
the	O
acoustic	O
corpus	O
manually	O
.	O
However	O
,	O
sparse	O
data	O
presents	O
a	O
problem	O
,	O
in	O
that	O
collecting	O
a	O
sufficient	O
amount	O
of	O
task-dependent	O
data	O
and	O
manually	O
transcribing	O
it	O
is	O
a	O
tedious	O
and	O
costly	O
process	O
.	O

Another	O
way	O
to	O
adapt	O
an	O
acoustic	O
model	O
,	O
which	O
has	O
been	O
proposed	O
in	O
the	O
past	O
,	O
is	O
to	O
use	O
an	O
existing	O
body	O
of	O
close-captioned	O
data	O
.	O
Such	O
data	O
is	O
referred	O
to	O
as	O
“	O
lightly	O
supervised	O
data	O
”	O
in	O
L.	B-Citation
Lamel	I-Citation
,	I-Citation
J-L	I-Citation
Gauvain	I-Citation
and	I-Citation
G.	I-Citation
Adda	I-Citation
,	I-Citation
Investigating	I-Citation
Lightly	I-Citation
Supervised	I-Citation
Acoustic	I-Citation
Model	I-Citation
Training	I-Citation
,	I-Citation
ICASSP-2001	I-Citation
,	O
because	O
transcription	O
generated	O
during	O
close-captioning	O
is	O
error	O
prone	O
and	O
is	O
generally	O
not	O
of	O
good	O
quality	O
.	O
In	O
addition	O
,	O
the	O
close-captioned	O
data	O
must	O
be	O
sorted	O
through	O
to	O
obtain	O
data	O
that	O
is	O
task-dependent	O
as	O
well	O
.	O
A	O
further	O
problem	O
with	O
using	O
lightly	O
supervised	O
data	O
is	O
that	O
during	O
close-captioning	O
,	O
phrase	O
segmentation	O
information	O
may	O
not	O
be	O
available	O
.	O

Yet	O
another	O
proposed	O
solution	O
is	O
to	O
simply	O
collect	O
a	O
huge	O
amount	O
of	O
task-independent	O
data	O
,	O
and	O
simply	O
hope	O
that	O
enough	O
of	O
the	O
data	O
is	O
relevant	O
to	O
the	O
task	O
at	O
hand	O
that	O
the	O
acoustic	O
model	O
can	O
be	O
adequately	O
trained	O
.	O
Of	O
course	O
,	O
this	O
is	O
uncertain	O
and	O
can	O
be	O
costly	O
and	O
time	O
consuming	O
as	O
well	O
.	O

Still	O
a	O
further	O
proposed	O
solution	O
is	O
to	O
use	O
unsupervised	O
training	O
data	O
,	O
(	O
data	O
which	O
has	O
no	O
manual	O
transcription	O
)	O
and	O
feed	O
that	O
data	O
into	O
a	O
speech	O
recognizer	O
to	O
obtain	O
the	O
associated	O
transcription	O
.	O
However	O
,	O
a	O
primary	O
problem	O
with	O
using	O
unsupervised	O
training	O
data	O
is	O
that	O
it	O
is	O
unsupervised	O
.	O
Therefore	O
errors	O
in	O
the	O
first-pass	O
speech	O
recognition	O
update	O
incorrect	O
parameters	O
in	O
the	O
acoustic	O
model	O
and	O
render	O
this	O
proposed	O
solution	O
inefficient	O
.	O

The	O
present	O
invention	O
addresses	O
one	O
or	O
more	O
of	O
the	O
problems	O
discussed	O
above	O
.	O

SUMMARY	O
OF	O
THE	O
INVENTION	O
This	O
invention	O
involves	O
three	O
major	O
components	O
.	O
First	O
of	O
all	O
,	O
we	O
propose	O
a	O
method	O
of	O
combining	O
task	O
independent	O
supervised	O
or	O
unsupervised	O
training	O
corpora	O
to	O
better	O
suit	O
the	O
task	O
in	O
interest	O
by	O
defining	O
“	O
task	O
relevance	O
”	O
for	O
each	O
unit	O
of	O
speech	O
in	O
the	O
training	O
data	O
.	O
Usually	O
the	O
unit	O
is	O
a	O
word	O
.	O
Training	O
data	O
is	O
weighted	O
by	O
the	O
task	O
relevance	O
to	O
generate	O
a	O
task-related	O
acoustic	O
model	O
.	O
Our	O
lab	O
experiments	O
showed	O
3	O
%	O
error	O
rate	O
reduction	O
compared	O
with	O
blindly	O
combining	O
all	O
available	O
data	O
together	O
.	O

Unsupervised	O
task	O
dependent	O
speech	O
data	O
is	O
provided	O
to	O
a	O
speech	O
recognizer	O
that	O
recognizes	O
the	O
speech	O
data	O
and	O
outputs	O
a	O
recognition	O
result	O
along	O
with	O
a	O
confidence	O
measure	O
for	O
each	O
recognized	O
word	O
.	O
A	O
task-dependent	O
acoustic	O
model	O
is	O
generated	O
based	O
on	O
the	O
recognition	O
result	O
,	O
the	O
speech	O
data	O
and	O
the	O
confidence	O
measure	O
.	O
The	O
speech	O
data	O
is	O
weighted	O
by	O
the	O
confidence	O
measure	O
so	O
that	O
only	O
data	O
that	O
has	O
been	O
recognized	O
with	O
a	O
high	O
degree	O
of	O
confidence	O
will	O
weigh	O
heavily	O
in	O
generation	O
of	O
the	O
acoustic	O
model	O
.	O

Finally	O
the	O
task	O
dependent	O
acoustic	O
model	O
is	O
smoothed	O
with	O
the	O
task	O
related	O
acoustic	O
model	O
,	O
depending	O
on	O
the	O
occupancy	O
count	O
of	O
each	O
parameter	O
(	O
such	O
as	O
senone	O
)	O
from	O
the	O
task	O
dependent	O
data	O
.	O
Our	O
lab	O
experiments	O
showed	O
a	O
11	O
%	O
error	O
rate	O
reduction	O
after	O
unsupervised	O
confidence-based	O
training	O
compared	O
with	O
task	O
independent	O
acoustic	O
model	O
.	O

BRIEF	O
DESCRIPTION	O
OF	O
THE	O
DRAWINGS	O

FIG	O
.	O
1	O
is	O
a	O
block	O
diagram	O
of	O
an	O
environment	O
in	O
which	O
the	O
present	O
invention	O
can	O
be	O
used	O
.	O

FIG	O
.	O
2	O
is	O
a	O
block	O
diagram	O
illustrating	O
the	O
general	O
method	O
for	O
training	O
a	O
task-related	O
(	O
or	O
task-adapted	O
)	O
acoustic	O
model	O
.	O

FIGS	O
.	O
2	O
,	O
2	O
-	O
1	O
and	O
2	O
-	O
2	O
are	O
block	O
diagrams	O
briefly	O
showing	O
embodiments	O
for	O
generation	O
of	O
a	O
task-related	O
model	O
.	O

FIG	O
.	O
3	O
is	O
a	O
flow	O
diagram	O
which	O
better	O
illustrates	O
the	O
process	O
of	O
generating	O
a	O
task-dependent	O
acoustic	O
model	O
in	O
accordance	O
with	O
one	O
embodiment	O
of	O
the	O
present	O
invention	O
.	O

FIG	O
.	O
4	O
is	O
a	O
block	O
diagram	O
illustrating	O
a	O
system	O
used	O
to	O
generate	O
the	O
task-dependent	O
acoustic	O
model	O
in	O
accordance	O
with	O
one	O
embodiment	O
of	O
the	O
present	O
invention	O
.	O

FIG	O
.	O
5	O
is	O
a	O
flow	O
diagram	O
illustrating	O
a	O
method	O
of	O
generating	O
a	O
task-dependent	O
acoustic	O
model	O
in	O
accordance	O
with	O
another	O
embodiment	O
of	O
the	O
present	O
invention	O
.	O

FIG	O
.	O
6	O
is	O
a	O
block	O
diagram	O
illustrating	O
a	O
system	O
for	O
generating	O
the	O
task-dependent	O
acoustic	O
model	O
using	O
the	O
method	O
shown	O
in	O
FIG	O
.	O
5	O
.	O

FIGS	O
.	O
7	O
and	O
8	O
are	O
flow	O
diagrams	O
illustrating	O
two	O
exemplary	O
ways	O
in	O
which	O
a	O
relevance	O
measure	O
can	O
be	O
obtained	O
in	O
accordance	O
with	O
two	O
exemplary	O
embodiments	O
of	O
the	O
present	O
invention	O
.	O

FIG	O
.	O
9	O
is	O
a	O
block	O
diagram	O
of	O
a	O
system	O
for	O
generating	O
a	O
task-dependent	O
acoustic	O
model	O
from	O
unsupervised	O
task-dependent	O
speech	O
data	O
.	O

FIG	O
.	O
10	O
is	O
a	O
flow	O
diagram	O
illustrating	O
the	O
operation	O
of	O
the	O
system	O
shown	O
in	O
FIG	O
.	O
9	O
.	O

FIG	O
.	O
11	O
is	O
a	O
block	O
diagram	O
similar	O
to	O
the	O
system	O
shown	O
in	O
FIG	O
.	O
9	O
,	O
but	O
combined	O
with	O
supervised	O
task-independent	O
training	O
data	O
.	O

FIG	O
.	O
11	O
-	O
1	O
is	O
a	O
block	O
diagram	O
of	O
one	O
embodiment	O
of	O
a	O
system	O
for	O
generating	O
a	O
task-adapted	O
acoustic	O
model	O
.	O

FIG	O
.	O
11	O
-	O
2	O
is	O
a	O
plot	O
of	O
weight	O
versus	O
occupancy	O
count	O
.	O

FIG	O
.	O
12	O
is	O
a	O
flow	O
diagram	O
illustrating	O
the	O
operation	O
of	O
the	O
system	O
shown	O
in	O
FIG	O
.	O
11	O
.	O

FIG	O
.	O
12	O
-	O
1	O
is	O
a	O
flow	O
diagram	O
better	O
illustrating	O
the	O
generation	O
of	O
a	O
composite	O
acoustic	O
model	O
based	O
on	O
weighted	O
contributions	O
of	O
other	O
acoustic	O
models	O
.	O

FIG	O
.	O
13	O
is	O
a	O
block	O
diagram	O
of	O
a	O
system	O
for	O
generating	O
a	O
confidence	O
and	O
relevance	O
weighted	O
acoustic	O
model	O
from	O
unsupervised	O
,	O
task-independent	O
speech	O
data	O
.	O

FIG	O
.	O
14	O
is	O
a	O
flow	O
diagram	O
illustrating	O
the	O
operation	O
of	O
the	O
system	O
shown	O
in	O
FIG	O
.	O
13	O
.	O

DETAILED	O
DESCRIPTION	O
OF	O
ILLUSTRATIVE	O
EMBODIMENTS	O

FIG	O
.	O
1	O
illustrates	O
an	O
example	O
of	O
a	O
suitable	O
computing	O
system	O
environment	O
100	O
on	O
which	O
the	O
invention	O
may	O
be	O
implemented	O
.	O
The	O
computing	O
system	O
environment	O
100	O
is	O
only	O
one	O
example	O
of	O
a	O
suitable	O
computing	O
environment	O
and	O
is	O
not	O
intended	O
to	O
suggest	O
any	O
limitation	O
as	O
to	O
the	O
scope	O
of	O
use	O
or	O
functionality	O
of	O
the	O
invention	O
.	O
Neither	O
should	O
the	O
computing	O
environment	O
100	O
be	O
interpreted	O
as	O
having	O
any	O
dependency	O
or	O
requirement	O
relating	O
to	O
any	O
one	O
or	O
combination	O
of	O
components	O
illustrated	O
in	O
the	O
exemplary	O
operating	O
environment	O
100	O
.	O

The	O
invention	O
is	O
operational	O
with	O
numerous	O
other	O
general	O
purpose	O
or	O
special	O
purpose	O
computing	O
system	O
environments	O
or	O
configurations	O
.	O
Examples	O
of	O
well	O
known	O
computing	O
systems	O
,	O
environments	O
,	O
and/or	O
configurations	O
that	O
may	O
be	O
suitable	O
for	O
use	O
with	O
the	O
invention	O
include	O
,	O
but	O
are	O
not	O
limited	O
to	O
,	O
personal	O
computers	O
,	O
server	O
computers	O
,	O
hand-held	O
or	O
laptop	O
devices	O
,	O
multiprocessor	O
systems	O
,	O
microprocessor-based	O
systems	O
,	O
set	O
top	O
boxes	O
,	O
programmable	O
consumer	O
electronics	O
,	O
network	O
PCs	O
,	O
minicomputers	O
,	O
mainframe	O
computers	O
,	O
distributed	O
computing	O
environments	O
that	O
include	O
any	O
of	O
the	O
above	O
systems	O
or	O
devices	O
,	O
and	O
the	O
like	O
.	O

The	O
invention	O
may	O
be	O
described	O
in	O
the	O
general	O
context	O
of	O
computer-executable	O
instructions	O
,	O
such	O
as	O
program	O
modules	O
,	O
being	O
executed	O
by	O
a	O
computer	O
.	O
Generally	O
,	O
program	O
modules	O
include	O
routines	O
,	O
programs	O
,	O
objects	O
,	O
components	O
,	O
data	O
structures	O
,	O
etc	O
.	O
that	O
perform	O
particular	O
tasks	O
or	O
implement	O
particular	O
abstract	O
data	O
types	O
.	O
The	O
invention	O
may	O
also	O
be	O
practiced	O
in	O
distributed	O
computing	O
environments	O
where	O
tasks	O
are	O
performed	O
by	O
remote	O
processing	O
devices	O
that	O
are	O
linked	O
through	O
a	O
communications	O
network	O
.	O
In	O
a	O
distributed	O
computing	O
environment	O
,	O
program	O
modules	O
may	O
be	O
located	O
in	O
both	O
local	O
and	O
remote	O
computer	O
storage	O
media	O
including	O
memory	O
storage	O
devices	O
.	O

With	O
reference	O
to	O
FIG	O
.	O
1	O
,	O
an	O
exemplary	O
system	O
for	O
implementing	O
the	O
invention	O
includes	O
a	O
general	O
purpose	O
computing	O
device	O
in	O
the	O
form	O
of	O
a	O
computer	O
110	O
.	O
Components	O
of	O
computer	O
110	O
may	O
include	O
,	O
but	O
are	O
not	O
limited	O
to	O
,	O
a	O
processing	O
unit	O
120	O
,	O
a	O
system	O
memory	O
130	O
,	O
and	O
a	O
system	O
bus	O
121	O
that	O
couples	O
various	O
system	O
components	O
including	O
the	O
system	O
memory	O
to	O
the	O
processing	O
unit	O
120	O
.	O
The	O
system	O
bus	O
121	O
may	O
be	O
any	O
of	O
several	O
types	O
of	O
bus	O
structures	O
including	O
a	O
memory	O
bus	O
or	O
memory	O
controller	O
,	O
a	O
peripheral	O
bus	O
,	O
and	O
a	O
local	O
bus	O
using	O
any	O
of	O
a	O
variety	O
of	O
bus	O
architectures	O
.	O
By	O
way	O
of	O
example	O
,	O
and	O
not	O
limitation	O
,	O
such	O
architectures	O
include	O
Industry	O
Standard	O
Architecture	O
(	O
ISA	O
)	O
bus	O
,	O
Micro	O
Channel	O
Architecture	O
(	O
MCA	O
)	O
bus	O
,	O
Enhanced	O
ISA	O
(	O
EISA	O
)	O
bus	O
,	O
Video	O
Electronics	O
Standards	O
Association	O
(	O
VESA	O
)	O
local	O
bus	O
,	O
and	O
Peripheral	O
Component	O
Interconnect	O
(	O
PCI	O
)	O
bus	O
also	O
known	O
as	O
Mezzanine	O
bus	O
.	O

Computer	O
110	O
typically	O
includes	O
a	O
variety	O
of	O
computer	O
readable	O
media	O
.	O
Computer	O
readable	O
media	O
can	O
be	O
any	O
available	O
media	O
that	O
can	O
be	O
accessed	O
by	O
computer	O
110	O
and	O
includes	O
both	O
volatile	O
and	O
nonvolatile	O
media	O
,	O
removable	O
and	O
non-removable	O
media	O
.	O
By	O
way	O
of	O
example	O
,	O
and	O
not	O
limitation	O
,	O
computer	O
readable	O
media	O
may	O
comprise	O
computer	O
storage	O
media	O
and	O
communication	O
media	O
.	O
Computer	O
storage	O
media	O
includes	O
both	O
volatile	O
and	O
nonvolatile	O
,	O
removable	O
and	O
non-removable	O
media	O
implemented	O
in	O
any	O
method	O
or	O
technology	O
for	O
storage	O
of	O
information	O
such	O
as	O
computer	O
readable	O
instructions	O
,	O
data	O
structures	O
,	O
program	O
modules	O
or	O
other	O
data	O
.	O
Computer	O
storage	O
media	O
includes	O
,	O
but	O
is	O
not	O
limited	O
to	O
,	O
RAM	O
,	O
ROM	O
,	O
EEPROM	O
,	O
flash	O
memory	O
or	O
other	O
memory	O
technology	O
,	O
CD-ROM	O
,	O
digital	O
versatile	O
disks	O
(	O
DVD	O
)	O
or	O
other	O
optical	O
disk	O
storage	O
,	O
magnetic	O
cassettes	O
,	O
magnetic	O
tape	O
,	O
magnetic	O
disk	O
storage	O
or	O
other	O
magnetic	O
storage	O
devices	O
,	O
or	O
any	O
other	O
medium	O
which	O
can	O
be	O
used	O
to	O
store	O
the	O
desired	O
information	O
and	O
which	O
can	O
be	O
accessed	O
by	O
computer	O
110	O
.	O
Communication	O
media	O
typically	O
embodies	O
computer	O
readable	O
instructions	O
,	O
data	O
structures	O
,	O
program	O
modules	O
or	O
other	O
data	O
in	O
a	O
modulated	O
data	O
signal	O
such	O
as	O
a	O
carrier	O
wave	O
or	O
other	O
transport	O
mechanism	O
and	O
includes	O
any	O
information	O
delivery	O
media	O
.	O
The	O
term	O
“	O
modulated	O
data	O
signal	O
”	O
means	O
a	O
signal	O
that	O
has	O
one	O
or	O
more	O
of	O
its	O
characteristics	O
set	O
or	O
changed	O
in	O
such	O
a	O
manner	O
as	O
to	O
encode	O
information	O
in	O
the	O
signal	O
.	O
By	O
way	O
of	O
example	O
,	O
and	O
not	O
limitation	O
,	O
communication	O
media	O
includes	O
wired	O
media	O
such	O
as	O
a	O
wired	O
network	O
or	O
direct-wired	O
connection	O
,	O
and	O
wireless	O
media	O
such	O
as	O
acoustic	O
,	O
RF	O
,	O
infrared	O
and	O
other	O
wireless	O
media	O
.	O
Combinations	O
of	O
any	O
of	O
the	O
above	O
should	O
also	O
be	O
included	O
within	O
the	O
scope	O
of	O
computer	O
readable	O
media	O
.	O

The	O
system	O
memory	O
130	O
includes	O
computer	O
storage	O
media	O
in	O
the	O
form	O
of	O
volatile	O
and/or	O
nonvolatile	O
memory	O
such	O
as	O
read	O
only	O
memory	O
(	O
ROM	O
)	O
131	O
and	O
random	O
access	O
memory	O
(	O
RAM	O
)	O
132	O
.	O
A	O
basic	O
input/output	O
system	O
133	O
(	O
BIOS	O
)	O
,	O
containing	O
the	O
basic	O
routines	O
that	O
help	O
to	O
transfer	O
information	O
between	O
elements	O
within	O
computer	O
110	O
,	O
such	O
as	O
during	O
start-up	O
,	O
is	O
typically	O
stored	O
in	O
ROM	O
131	O
.	O
RAM	O
132	O
typically	O
contains	O
data	O
and/or	O
program	O
modules	O
that	O
are	O
immediately	O
accessible	O
to	O
and/or	O
presently	O
being	O
operated	O
on	O
by	O
processing	O
unit	O
120	O
.	O
By	O
way	O
of	O
example	O
,	O
and	O
not	O
limitation	O
,	O
FIG	O
.	O
1	O
illustrates	O
operating	O
system	O
134	O
,	O
application	O
programs	O
135	O
,	O
other	O
program	O
modules	O
136	O
,	O
and	O
program	O
data	O
137	O
.	O

The	O
computer	O
110	O
may	O
also	O
include	O
other	O
removable/non-removable	O
volatile/nonvolatile	O
computer	O
storage	O
media	O
.	O
By	O
way	O
of	O
example	O
only	O
,	O
FIG	O
.	O
1	O
illustrates	O
a	O
hard	O
disk	O
drive	O
141	O
that	O
reads	O
from	O
or	O
writes	O
to	O
non-removable	O
,	O
nonvolatile	O
magnetic	O
media	O
,	O
a	O
magnetic	O
disk	O
drive	O
151	O
that	O
reads	O
from	O
or	O
writes	O
to	O
a	O
removable	O
,	O
nonvolatile	O
magnetic	O
disk	O
152	O
,	O
and	O
an	O
optical	O
disk	O
drive	O
155	O
that	O
reads	O
from	O
or	O
writes	O
to	O
a	O
removable	O
,	O
nonvolatile	O
optical	O
disk	O
156	O
such	O
as	O
a	O
CD	O
ROM	O
or	O
other	O
optical	O
media	O
.	O
Other	O
removable/non-removable	O
,	O
volatile/nonvolatile	O
computer	O
storage	O
media	O
that	O
can	O
be	O
used	O
in	O
the	O
exemplary	O
operating	O
environment	O
include	O
,	O
but	O
are	O
not	O
limited	O
to	O
,	O
magnetic	O
tape	O
cassettes	O
,	O
flash	O
memory	O
cards	O
,	O
digital	O
versatile	O
disks	O
,	O
digital	O
video	O
tape	O
,	O
solid	O
state	O
RAM	O
,	O
solid	O
state	O
ROM	O
,	O
and	O
the	O
like	O
.	O
The	O
hard	O
disk	O
drive	O
141	O
is	O
typically	O
connected	O
to	O
the	O
system	O
bus	O
121	O
through	O
a	O
non-removable	O
memory	O
interface	O
such	O
as	O
interface	O
140	O
,	O
and	O
magnetic	O
disk	O
drive	O
151	O
and	O
optical	O
disk	O
drive	O
155	O
are	O
typically	O
connected	O
to	O
the	O
system	O
bus	O
121	O
by	O
a	O
removable	O
memory	O
interface	O
,	O
such	O
as	O
interface	O
150	O
.	O

The	O
drives	O
and	O
their	O
associated	O
computer	O
storage	O
media	O
discussed	O
above	O
and	O
illustrated	O
in	O
FIG	O
.	O
1	O
,	O
provide	O
storage	O
of	O
computer	O
readable	O
instructions	O
,	O
data	O
structures	O
,	O
program	O
modules	O
and	O
other	O
data	O
for	O
the	O
computer	O
110	O
.	O
In	O
FIG	O
.	O
1	O
,	O
for	O
example	O
,	O
hard	O
disk	O
drive	O
141	O
is	O
illustrated	O
as	O
storing	O
operating	O
system	O
144	O
,	O
application	O
programs	O
145	O
,	O
other	O
program	O
modules	O
146	O
,	O
and	O
program	O
data	O
147	O
.	O
Note	O
that	O
these	O
components	O
can	O
either	O
be	O
the	O
same	O
as	O
or	O
different	O
from	O
operating	O
system	O
134	O
,	O
application	O
programs	O
135	O
,	O
other	O
program	O
modules	O
136	O
,	O
and	O
program	O
data	O
137	O
.	O
Operating	O
system	O
144	O
,	O
application	O
programs	O
145	O
,	O
other	O
program	O
modules	O
146	O
,	O
and	O
program	O
data	O
147	O
are	O
given	O
different	O
numbers	O
here	O
to	O
illustrate	O
that	O
,	O
at	O
a	O
minimum	O
,	O
they	O
are	O
different	O
copies	O
.	O

A	O
user	O
may	O
enter	O
commands	O
and	O
information	O
into	O
the	O
computer	O
110	O
through	O
input	O
devices	O
such	O
as	O
a	O
keyboard	O
162	O
,	O
a	O
microphone	O
163	O
,	O
and	O
a	O
pointing	O
device	O
161	O
,	O
such	O
as	O
a	O
mouse	O
,	O
trackball	O
or	O
touch	O
pad	O
.	O
Other	O
input	O
devices	O
(	O
not	O
shown	O
)	O
may	O
include	O
a	O
joystick	O
,	O
game	O
pad	O
,	O
satellite	O
dish	O
,	O
scanner	O
,	O
or	O
the	O
like	O
.	O
These	O
and	O
other	O
input	O
devices	O
are	O
often	O
connected	O
to	O
the	O
processing	O
unit	O
120	O
through	O
a	O
user	O
input	O
interface	O
160	O
that	O
is	O
coupled	O
to	O
the	O
system	O
bus	O
,	O
but	O
may	O
be	O
connected	O
by	O
other	O
interface	O
and	O
bus	O
structures	O
,	O
such	O
as	O
a	O
parallel	O
port	O
,	O
game	O
port	O
or	O
a	O
universal	O
serial	O
bus	O
(	O
USB	O
)	O
.	O
A	O
monitor	O
191	O
or	O
other	O
type	O
of	O
display	O
device	O
is	O
also	O
connected	O
to	O
the	O
system	O
bus	O
121	O
via	O
an	O
interface	O
,	O
such	O
as	O
a	O
video	O
interface	O
190	O
.	O
In	O
addition	O
to	O
the	O
monitor	O
,	O
computers	O
may	O
also	O
include	O
other	O
peripheral	O
output	O
devices	O
such	O
as	O
speakers	O
197	O
and	O
printer	O
196	O
,	O
which	O
may	O
be	O
connected	O
through	O
an	O
output	O
peripheral	O
interface	O
195	O
.	O

The	O
computer	O
110	O
may	O
operate	O
in	O
a	O
networked	O
environment	O
using	O
logical	O
connections	O
to	O
one	O
or	O
more	O
remote	O
computers	O
,	O
such	O
as	O
a	O
remote	O
computer	O
180	O
.	O
The	O
remote	O
computer	O
180	O
may	O
be	O
a	O
personal	O
computer	O
,	O
a	O
hand-held	O
device	O
,	O
a	O
server	O
,	O
a	O
router	O
,	O
a	O
network	O
PC	O
,	O
a	O
peer	O
device	O
or	O
other	O
common	O
network	O
node	O
,	O
and	O
typically	O
includes	O
many	O
or	O
all	O
of	O
the	O
elements	O
described	O
above	O
relative	O
to	O
the	O
computer	O
110	O
.	O
The	O
logical	O
connections	O
depicted	O
in	O
FIG	O
.	O
1	O
include	O
a	O
local	O
area	O
network	O
(	O
LAN	O
)	O
171	O
and	O
a	O
wide	O
area	O
network	O
(	O
WAN	O
)	O
173	O
,	O
but	O
may	O
also	O
include	O
other	O
networks	O
.	O
Such	O
networking	O
environments	O
are	O
commonplace	O
in	O
offices	O
,	O
enterprise-wide	O
computer	O
networks	O
,	O
intranets	O
and	O
the	O
Internet	O
.	O

When	O
used	O
in	O
a	O
LAN	O
networking	O
environment	O
,	O
the	O
computer	O
110	O
is	O
connected	O
to	O
the	O
LAN	O
171	O
through	O
a	O
network	O
interface	O
or	O
adapter	O
170	O
.	O
When	O
used	O
in	O
a	O
WAN	O
networking	O
environment	O
,	O
the	O
computer	O
110	O
typically	O
includes	O
a	O
modem	O
172	O
or	O
other	O
means	O
for	O
establishing	O
communications	O
over	O
the	O
WAN	O
173	O
,	O
such	O
as	O
the	O
Internet	O
.	O
The	O
modem	O
172	O
,	O
which	O
may	O
be	O
internal	O
or	O
external	O
,	O
may	O
be	O
connected	O
to	O
the	O
system	O
bus	O
121	O
via	O
the	O
user	O
input	O
interface	O
160	O
,	O
or	O
other	O
appropriate	O
mechanism	O
.	O
In	O
a	O
networked	O
environment	O
,	O
program	O
modules	O
depicted	O
relative	O
to	O
the	O
computer	O
110	O
,	O
or	O
portions	O
thereof	O
,	O
may	O
be	O
stored	O
in	O
the	O
remote	O
memory	O
storage	O
device	O
.	O
By	O
way	O
of	O
example	O
,	O
and	O
not	O
limitation	O
,	O
FIG	O
.	O
1	O
illustrates	O
remote	O
application	O
programs	O
185	O
as	O
residing	O
on	O
remote	O
computer	O
180	O
.	O
It	O
will	O
be	O
appreciated	O
that	O
the	O
network	O
connections	O
shown	O
are	O
exemplary	O
and	O
other	O
means	O
of	O
establishing	O
a	O
communications	O
link	O
between	O
the	O
computers	O
may	O
be	O
used	O
.	O

Prior	O
to	O
discussing	O
the	O
present	O
invention	O
in	O
greater	O
detail	O
,	O
a	O
number	O
of	O
abbreviations	O
used	O
herein	O
are	O
identified	O
as	O
follows	O
:	O
TI	O
:	O
Task	O
Independent;TD	O
:	O
Task	O
Dependent;TA	O
:	O
Task	O
Adapted;HMM	O
:	O
Hidden	O
Markov	O
Model	O
;	O
ML	O
:	O
Maximum	O
Likelihood;AM	O
:	O
Acoustic	O
Model	O
;	O
LM	O
:	O
Language	O
Model	O
;	O
andMMIE	O
:	O
Maximum	O
Mutual	O
Information	O
Estimation	O
.	O

FIG	O
.	O
2	O
is	O
a	O
diagram	O
that	O
generally	O
illustrates	O
a	O
high	O
level	O
view	O
of	O
a	O
system	O
200	O
for	O
generating	O
a	O
task-related	O
acoustic	O
model	O
202	O
from	O
a	O
task-independent	O
supervised	O
training	O
corpus	O
204	O
.	O
Task-independent	O
training	O
corpus	O
204	O
can	O
be	O
a	O
known	O
training	O
corpus	O
.	O
For	O
example	O
,	O
popular	O
data	O
collection	O
agencies	O
include	O
Linguistic	O
Data	O
Consortium	O
(	O
LDC	O
)	O
,	O
Texas	O
Instruments	O
,	O
and	O
Oregon	O
Graduate	O
Institute	O
,	O
(	O
OGI	O
)	O
,	O
as	O
examples	O
.	O
These	O
agencies	O
provide	O
detailed	O
orthographic	O
transcriptions	O
for	O
a	O
fairly	O
large	O
number	O
of	O
sizable	O
corpora	O
,	O
such	O
as	O
Macrophone	O
,	O
the	O
Wall	O
Street	O
Journal	O
,	O
North	O
America	O
Broadcast	O
News	O
,	O
Switch	O
Boards	O
,	O
TI	O
Digits	O
,	O
etc	O
.	O
In	O
addition	O
,	O
these	O
transcriptions	O
are	O
typically	O
provided	O
in	O
a	O
number	O
of	O
different	O
languages	O
.	O
Such	O
transcriptions	O
are	O
created	O
and	O
verified	O
manually	O
(	O
and	O
thus	O
called	O
supervised	O
data	O
)	O
and	O
are	O
of	O
high	O
quality	O
.	O
Thus	O
,	O
task-independent	O
training	O
corpus	O
204	O
can	O
be	O
one	O
of	O
these	O
,	O
or	O
any	O
other	O
task-independent	O
training	O
corpus	O
.	O

As	O
discussed	O
in	O
the	O
background	O
portion	O
of	O
the	O
specification	O
,	O
it	O
is	O
difficult	O
to	O
categorize	O
data	O
in	O
a	O
task	O
independent	O
training	O
corpus	O
204	O
into	O
different	O
tasks	O
.	O
For	O
example	O
,	O
assuming	O
the	O
task	O
in	O
which	O
acoustic	O
model	O
202	O
is	O
to	O
be	O
used	O
is	O
a	O
stock	O
quote	O
application	O
,	O
a	O
weather	O
forecasting	O
application	O
or	O
a	O
travel	O
web	O
site	O
application	O
,	O
it	O
would	O
be	O
highly	O
advantageous	O
to	O
train	O
acoustic	O
model	O
202	O
based	O
on	O
task-dependent	O
terms	O
which	O
would	O
be	O
found	O
in	O
the	O
particular	O
task	O
in	O
which	O
the	O
acoustic	O
model	O
202	O
is	O
to	O
be	O
used	O
.	O
However	O
,	O
though	O
task-independent	O
training	O
corpus	O
204	O
certainly	O
contains	O
such	O
task-dependent	O
data	O
206	O
,	O
it	O
is	O
difficult	O
to	O
extract	O
task-dependent	O
utterances	O
206	O
and	O
to	O
train	O
acoustic	O
model	O
202	O
based	O
on	O
that	O
data	O
.	O

Some	O
of	O
the	O
problems	O
are	O
discussed	O
in	O
the	O
background	O
portion	O
of	O
the	O
specification	O
.	O
For	O
example	O
,	O
sparse	O
data	O
presents	O
a	O
problem	O
in	O
that	O
the	O
amount	O
of	O
specifically	O
task-dependent	O
data	O
206	O
in	O
corpus	O
204	O
may	O
be	O
so	O
small	O
that	O
it	O
is	O
difficult	O
to	O
train	O
an	O
acoustic	O
model	O
202	O
based	O
on	O
that	O
small	O
amount	O
of	O
data	O
.	O

Therefore	O
,	O
in	O
accordance	O
with	O
one	O
embodiment	O
of	O
the	O
present	O
invention	O
,	O
a	O
relevance	O
measure	O
for	O
each	O
word	O
(	O
relative	O
to	O
the	O
task	O
in	O
which	O
acoustic	O
model	O
202	O
is	O
to	O
be	O
used	O
)	O
in	O
task-independent	O
training	O
corpus	O
204	O
is	O
defined	O
.	O
The	O
relevance	O
measure	O
is	O
applied	O
to	O
the	O
data	O
associated	O
with	O
each	O
word	O
as	O
a	O
weighting	O
factor	O
.	O
The	O
words	O
which	O
are	O
more	O
relevant	O
to	O
the	O
task	O
at	O
hand	O
are	O
weighted	O
higher	O
than	O
those	O
that	O
are	O
not	O
as	O
relevant	O
.	O
Thus	O
,	O
in	O
one	O
embodiment	O
,	O
all	O
of	O
the	O
data	O
in	O
corpus	O
204	O
can	O
be	O
used	O
in	O
training	O
acoustic	O
model	O
202	O
,	O
but	O
the	O
task-dependent	O
information	O
206	O
will	O
simply	O
be	O
weighted	O
more	O
heavily	O
.	O
In	O
that	O
embodiment	O
,	O
both	O
the	O
task-independent	O
data	O
and	O
the	O
task-dependent	O
data	O
206	O
are	O
combined	O
in	O
a	O
training	O
process	O
illustrated	O
by	O
block	O
208	O
to	O
generate	O
task-related	O
or	O
task-adapted	O
acoustic	O
model	O
202	O
.	O

For	O
simplicity	O
of	O
discussion	O
,	O
the	O
present	O
invention	O
is	O
described	O
in	O
a	O
word-based	O
task	O
relevance	O
paradigm	O
from	O
now	O
on	O
.	O
That	O
is	O
,	O
relevance	O
is	O
assigned	O
on	O
a	O
word	O
basis	O
.	O
Readers	O
should	O
keep	O
in	O
mind	O
,	O
however	O
,	O
that	O
the	O
same	O
principle	O
applies	O
to	O
other	O
smaller	O
(	O
or	O
even	O
bigger	O
)	O
units	O
as	O
is	O
described	O
with	O
respect	O
to	O
FIGS	O
.	O
7	O
and	O
8	O
.	O
We	O
will	O
also	O
use	O
TR	O
(	O
w	O
)	O
or	O
ρ	O
(	O
w	O
)	O
interchangeably	O
to	O
represent	O
a	O
task	O
relevance	O
coefficient	O
.	O
Finally	O
for	O
ease	O
of	O
reference	O
,	O
the	O
TD	O
model	O
trained	O
with	O
this	O
task	O
relevance	O
approach	O
is	O
called	O
a	O
TR	O
model	O
.	O

TR	O
Model	O
202	O
can	O
be	O
generated	O
from	O
TI	O
corpus	O
204	O
in	O
a	O
variety	O
of	O
ways	O
,	O
and	O
two	O
exemplary	O
ways	O
will	O
be	O
described	O
.	O
First	O
,	O
(	O
shown	O
diagrammatically	O
in	O
FIG	O
.	O
2	O
-	O
1	O
)	O
the	O
TR	O
model	O
202	O
is	O
trained	O
by	O
weighting	O
the	O
occupancy	O
counts	O
,	O
with	O
TR	O
(	O
w	O
)	O
,	O
in	O
the	O
Forward-Backward	O
(	O
FB	O
)	O
(	O
or	O
Viterbi	O
)	O
HMM	O
Maximum	O
Likelihood	O
training	O
process	O
implemented	O
by	O
a	O
training	O
component	O
216	O
,	O
using	O
relevancy	O
coefficients	O
ρ	O
(	O
w	O
)	O
226	O
for	O
each	O
word	O
in	O
the	O
TI	O
corpus	O
204	O
as	O
the	O
weighting	O
coefficient	O
.	O
This	O
embodiment	O
is	O
described	O
in	O
greater	O
detail	O
with	O
respect	O
to	O
FIGS	O
.	O
3	O
and	O
4	O
.	O

Alternatively	O
,	O
as	O
briefly	O
shown	O
diagrammatically	O
in	O
FIG	O
.	O
2	O
-	O
2	O
,	O
a	O
set	O
of	O
TI	O
acoustic	O
word	O
models	O
λ	O
(	O
w	O
)	O
304	O
can	O
first	O
be	O
trained	O
by	O
training	O
component	O
216	O
from	O
the	O
TI	O
training	O
corpus	O
204	O
.	O
Then	O
for	O
each	O
task	O
in	O
question	O
,	O
we	O
combine	O
(	O
with	O
combiner	O
318	O
)	O
the	O
word	O
models	O
,	O
λ	O
(	O
w	O
)	O
,	O
304	O
to	O
obtain	O
a	O
TR	O
model	O
Λ	O
320	O
by	O
weighting	O
these	O
model	O
parameters	O
according	O
to	O
relevancy	O
coefficients	O
ρ	O
(	O
w	O
)	O
,	O
226	O
.	O
Notice	O
different	O
words	O
,	O
w1	O
and	O
w2	O
,	O
may	O
contain	O
the	O
same	O
parameter	O
in	O
Λ	O
,	O
but	O
they	O
are	O
combined	O
by	O
different	O
weights	O
ρ	O
(	O
w1	O
)	O
and	O
ρ	O
(	O
w2	O
)	O
.	O
This	O
embodiment	O
is	O
described	O
in	O
greater	O
detail	O
with	O
respect	O
to	O
FIGS	O
.	O
5	O
and	O
6	O
.	O

There	O
are	O
advantages	O
and	O
disadvantages	O
associated	O
with	O
each	O
of	O
the	O
approaches	O
in	O
FIGS	O
.	O
2	O
-	O
1	O
and	O
2	O
-	O
2	O
.	O
For	O
example	O
,	O
one	O
advantage	O
of	O
the	O
approach	O
shown	O
in	O
FIG	O
.	O
2	O
-	O
2	O
is	O
that	O
the	O
system	O
need	O
not	O
keep	O
the	O
TI	O
data	O
available	O
anymore	O
once	O
the	O
statistics	O
from	O
each	O
word	O
w	O
are	O
computed	O
and	O
stored	O
as	O
model	O
λ	O
(	O
w	O
)	O
,	O
where	O
λ	O
represents	O
the	O
model	O
parameter	O
.	O
For	O
each	O
new	O
task	O
,	O
the	O
task	O
relevance	O
function	O
ρ	O
is	O
simply	O
defined	O
and	O
then	O
a	O
TR	O
model	O
Λ	O
can	O
be	O
generated	O
by	O
a	O
linear	O
combination	O
of	O
λ	O
(	O
wi	O
)	O
quickly	O
.	O
While	O
in	O
the	O
first	O
approach	O
,	O
the	O
system	O
retrains	O
the	O
TR	O
model	O
by	O
using	O
the	O
raw	O
training	O
data	O
,	O
which	O
can	O
be	O
more	O
time	O
consuming	O
.	O
The	O
system	O
also	O
maintains	O
the	O
training	O
waves	O
and	O
transcriptions	O
available	O
.	O

However	O
,	O
the	O
approach	O
shown	O
in	O
FIG	O
.	O
2	O
-	O
2	O
has	O
increased	O
disk	O
requirements	O
necessary	O
to	O
store	O
all	O
the	O
word	O
models	O
λ	O
(	O
wi	O
)	O
,	O
since	O
the	O
same	O
parameters	O
are	O
overlapped	O
by	O
different	O
λ	O
(	O
wi	O
)	O
models	O
.	O

The	O
two	O
exemplary	O
approaches	O
are	O
now	O
discussed	O
in	O
greater	O
detail	O
.	O
FIG	O
.	O
3	O
is	O
a	O
flow	O
diagram	O
better	O
illustrating	O
operation	O
of	O
the	O
embodiment	O
of	O
FIG	O
.	O
2	O
-	O
1	O
in	O
which	O
acoustic	O
model	O
202	O
is	O
generated	O
.	O
Similar	O
items	O
are	O
similarly	O
numbered	O
.	O
FIG	O
.	O
3	O
will	O
be	O
described	O
with	O
respect	O
to	O
FIG	O
.	O
4	O
as	O
well	O
which	O
is	O
a	O
more	O
detailed	O
block	O
diagram	O
of	O
a	O
system	O
209	O
used	O
to	O
generate	O
acoustic	O
model	O
202	O
.	O

FIG	O
.	O
4	O
shows	O
task-independent	O
training	O
corpus	O
204	O
in	O
greater	O
detail	O
.	O
It	O
can	O
be	O
seen	O
in	O
FIG	O
.	O
4	O
that	O
corpus	O
204	O
is	O
a	O
supervised	O
task-independent	O
training	O
corpus	O
and	O
thus	O
includes	O
a	O
plurality	O
of	O
WAV	O
files	O
210	O
,	O
each	O
of	O
which	O
represents	O
a	O
speech	O
utterance	O
(	O
such	O
as	O
a	O
sentence	O
)	O
.	O
Associated	O
with	O
each	O
WAV	O
file	O
is	O
a	O
manual	O
transcription	O
212	O
.	O
System	O
209	O
also	O
includes	O
a	O
relevance	O
generator	O
214	O
that	O
generates	O
a	O
relevance	O
or	O
a	O
task-relevance	O
TR	O
(	O
w	O
)	O
for	O
each	O
word	O
(	O
w	O
)	O
.	O
Further	O
,	O
FIG	O
.	O
4	O
includes	O
an	O
acoustic	O
model	O
training	O
component	O
(	O
such	O
as	O
a	O
Hidden	O
Markov	O
Model	O
training	O
component	O
)	O
216	O
which	O
trains	O
the	O
acoustic	O
model	O
202	O
.	O
FIG	O
.	O
4	O
also	O
shows	O
that	O
acoustic	O
model	O
202	O
can	O
be	O
provided	O
to	O
a	O
task	O
(	O
such	O
as	O
an	O
application	O
)	O
218	O
.	O

FIG	O
.	O
3	O
shows	O
that	O
system	O
209	O
first	O
receives	O
supervised	O
task-independent	O
training	O
corpus	O
(	O
or	O
data	O
)	O
204	O
.	O
This	O
is	O
indicated	O
by	O
block	O
220	O
in	O
FIG	O
.	O
3	O
.	O

Relevance	O
generator	O
214	O
then	O
selects	O
a	O
word	O
217	O
from	O
corpus	O
204	O
.	O
This	O
is	O
indicated	O
by	O
block	O
222	O
.	O
It	O
should	O
be	O
noted	O
that	O
,	O
in	O
the	O
embodiment	O
shown	O
in	O
FIG	O
.	O
4	O
,	O
relevance	O
generator	O
214	O
also	O
has	O
access	O
to	O
the	O
words	O
or	O
phonetic	O
units	O
(	O
such	O
as	O
phones	O
)	O
that	O
are	O
relevant	O
to	O
,	O
or	O
found	O
in	O
,	O
task	O
application	O
218	O
.	O
Such	O
words	O
or	O
phones	O
(	O
or	O
other	O
desired	O
speech	O
units	O
)	O
are	O
illustrated	O
by	O
block	O
224	O
in	O
FIG	O
.	O
4	O
.	O
It	O
should	O
also	O
be	O
noted	O
that	O
task	O
relevance	O
can	O
be	O
defined	O
at	O
different	O
levels	O
as	O
well	O
,	O
such	O
as	O
at	O
the	O
triphone	O
level	O
or	O
quinphone	O
level	O
.	O

The	O
input	O
224	O
to	O
relevance	O
generator	O
214	O
can	O
take	O
a	O
variety	O
of	O
different	O
forms	O
.	O
For	O
instance	O
,	O
given	O
an	O
application	O
task	O
218	O
,	O
system	O
209	O
thus	O
has	O
a	O
defined	O
lexicon	O
(	O
which	O
can	O
be	O
a	O
list	O
of	O
words	O
in	O
interest	O
)	O
.	O
If	O
an	O
n-gram	O
language	O
model	O
(	O
illustratively	O
one	O
which	O
is	O
trained	O
on	O
a	O
large	O
amount	O
of	O
task-dependent	O
text	O
)	O
or	O
a	O
context-free	O
grammar	O
is	O
available	O
for	O
the	O
application	O
task	O
218	O
,	O
these	O
models	O
can	O
be	O
used	O
as	O
the	O
input	O
224	O
to	O
the	O
relevance	O
generator	O
214	O
.	O

Relevance	O
generator	O
214	O
generates	O
a	O
relevance	O
measure	O
TR	O
(	O
w	O
)	O
for	O
each	O
word	O
(	O
w	O
)	O
in	O
task-independent	O
supervised	O
training	O
corpus	O
204	O
.	O
The	O
relevance	O
measure	O
is	O
indicated	O
by	O
block	O
226	O
in	O
FIG	O
.	O
4	O
and	O
the	O
method	O
of	O
defining	O
the	O
relevance	O
measure	O
for	O
the	O
selected	O
word	O
is	O
indicated	O
by	O
block	O
228	O
in	O
FIG	O
.	O
3	O
.	O
There	O
are	O
different	O
ways	O
for	O
generating	O
the	O
relevance	O
measure	O
226	O
and	O
two	O
examples	O
of	O
those	O
are	O
discussed	O
below	O
with	O
respect	O
to	O
FIGS	O
.	O
7	O
and	O
8	O
.	O

Each	O
relevance	O
measure	O
226	O
is	O
then	O
stored	O
for	O
the	O
associated	O
,	O
selected	O
word	O
as	O
indicated	O
by	O
block	O
230	O
in	O
FIG	O
.	O
3	O
.	O
Relevance	O
generator	O
214	O
does	O
this	O
for	O
each	O
word	O
in	O
the	O
TI	O
training	O
corpus	O
204	O
as	O
indicated	O
by	O
block	O
232	O
.	O

Once	O
the	O
relevance	O
measure	O
for	O
each	O
word	O
has	O
been	O
obtained	O
,	O
trainer	O
216	O
trains	O
the	O
task-dependent	O
acoustic	O
model	O
202	O
based	O
on	O
the	O
weighted	O
task-independent	O
training	O
data	O
in	O
corpus	O
204	O
.	O
This	O
is	O
indicated	O
by	O
block	O
234	O
in	O
FIG	O
.	O
3	O
.	O
It	O
can	O
thus	O
be	O
seen	O
that	O
system	O
209	O
has	O
effectively	O
extracted	O
task-dependent	O
data	O
from	O
corpus	O
204	O
and	O
used	O
it	O
(	O
by	O
weighting	O
it	O
more	O
heavily	O
)	O
in	O
generating	O
a	O
task-dependent	O
acoustic	O
model	O
.	O

FIG	O
.	O
5	O
is	O
a	O
flow	O
diagram	O
illustrating	O
another	O
embodiment	O
(	O
shown	O
in	O
FIG	O
.	O
2	O
-	O
2	O
)	O
for	O
generating	O
a	O
task-related	O
(	O
or	O
task-adapted	O
)	O
acoustic	O
model	O
from	O
a	O
task-independent	O
corpus	O
.	O
Similar	O
items	O
are	O
numbered	O
similar	O
to	O
those	O
shown	O
in	O
FIG	O
.	O
2	O
-	O
2	O
.	O
FIG	O
.	O
6	O
is	O
a	O
more	O
detailed	O
block	O
diagram	O
of	O
a	O
system	O
300	O
for	O
generating	O
the	O
acoustic	O
model	O
in	O
accordance	O
with	O
the	O
flow	O
diagram	O
shown	O
in	O
FIG	O
.	O
5	O
.	O
The	O
two	O
Figures	O
will	O
now	O
be	O
described	O
in	O
conjunction	O
with	O
one	O
another	O
.	O
System	O
300	O
is	O
similar	O
,	O
in	O
some	O
respects	O
,	O
to	O
system	O
209	O
shown	O
in	O
FIG	O
.	O
4	O
,	O
and	O
similar	O
items	O
are	O
similarly	O
numbered	O
.	O
However	O
,	O
a	O
number	O
of	O
differences	O
will	O
become	O
apparent	O
as	O
the	O
description	O
proceeds	O
.	O

System	O
300	O
first	O
receives	O
task-independent	O
,	O
supervised	O
training	O
corpus	O
204	O
.	O
This	O
is	O
indicated	O
by	O
block	O
302	O
in	O
FIG	O
.	O
5	O
.	O
Then	O
,	O
acoustic	O
model	O
training	O
component	O
216	O
trains	O
task-independent	O
acoustic	O
models	O
304	O
for	O
the	O
words	O
in	O
the	O
task-independent	O
corpus	O
204	O
.	O
This	O
is	O
indicated	O
by	O
block	O
306	O
in	O
FIG	O
.	O
5	O
.	O
In	O
training	O
the	O
task-independent	O
acoustic	O
models	O
304	O
,	O
trainer	O
216	O
generates	O
a	O
table	O
of	O
words	O
and	O
acoustic	O
model	O
parameters	O
associated	O
with	O
those	O
words	O
.	O
This	O
table	O
is	O
indicated	O
by	O
block	O
308	O
in	O
FIG	O
.	O
6	O
.	O
Thus	O
,	O
the	O
words	O
and	O
acoustic	O
model	O
parameters	O
from	O
task-independent	O
acoustic	O
models	O
304	O
are	O
saved	O
as	O
indicated	O
by	O
block	O
310	O
in	O
FIG	O
.	O
5	O
.	O

Relevance	O
generator	O
214	O
also	O
generates	O
a	O
relevance	O
measure	O
226	O
for	O
each	O
word	O
in	O
corpus	O
204	O
,	O
given	O
an	O
application	O
task	O
.	O
This	O
is	O
indicated	O
by	O
block	O
312	O
in	O
FIG	O
.	O
5	O
.	O

The	O
relevance	O
measures	O
226	O
are	O
then	O
applied	O
to	O
the	O
word	O
model	O
parameters	O
for	O
the	O
words	O
stored	O
in	O
table	O
308	O
.	O
In	O
other	O
words	O
,	O
each	O
word	O
model	O
parameter	O
is	O
weighted	O
by	O
its	O
relevance	O
measure	O
TR	O
(	O
w	O
)	O
by	O
weighting	O
component	O
314	O
.	O
This	O
is	O
indicated	O
by	O
block	O
316	O
in	O
FIG	O
.	O
5	O
.	O
Then	O
,	O
the	O
weighted	O
acoustic	O
model	O
parameters	O
are	O
combined	O
by	O
parameter	O
combination	O
component	O
318	O
in	O
order	O
to	O
obtain	O
task-adapted	O
(	O
or	O
task-related	O
)	O
acoustic	O
model	O
320	O
.	O
This	O
is	O
indicated	O
by	O
block	O
322	O
in	O
FIG	O
.	O
5	O
.	O

In	O
applying	O
the	O
weights	O
to	O
the	O
acoustic	O
model	O
parameters	O
,	O
assume	O
that	O
during	O
the	O
known	O
Baum-Welch	O
training	O
process	O
,	O
the	O
acoustic	O
model	O
parameters	O
in	O
table	O
308	O
are	O
generated	O
for	O
each	O
word	O
in	O
the	O
vocabulary	O
.	O
This	O
is	O
a	O
statistically	O
sufficient	O
representation	O
of	O
the	O
original	O
speech	O
corpus	O
204	O
,	O
divided	O
by	O
the	O
vocabulary	O
.	O
In	O
order	O
to	O
weight	O
the	O
parameters	O
to	O
a	O
specific	O
task	O
,	O
the	O
parameters	O
are	O
combined	O
putting	O
heavier	O
weights	O
on	O
those	O
counts	O
(	O
estimates	O
)	O
generated	O
by	O
task-related	O
words	O
.	O
Task-related	O
acoustic	O
model	O
320	O
is	O
thus	O
more	O
specifically	O
adapted	O
to	O
the	O
task	O
at	O
hand	O
.	O

The	O
relevance	O
measure	O
will	O
now	O
be	O
discussed	O
in	O
greater	O
detail	O
.	O
A	O
relevance	O
measure	O
may	O
best	O
be	O
defined	O
for	O
an	O
entire	O
utterance	O
,	O
or	O
even	O
a	O
whole	O
paragraph	O
.	O
However	O
,	O
it	O
is	O
often	O
difficult	O
to	O
define	O
the	O
concept	O
of	O
a	O
“	O
task	O
”	O
and	O
thus	O
difficult	O
to	O
classify	O
how	O
relevant	O
an	O
utterance	O
is	O
with	O
respect	O
to	O
the	O
task	O
in	O
question	O
.	O
Therefore	O
,	O
one	O
embodiment	O
of	O
the	O
present	O
invention	O
illustratively	O
defines	O
the	O
relevance	O
measure	O
on	O
a	O
word	O
basis	O
.	O
We	O
may	O
combine	O
the	O
task	O
relevance	O
of	O
each	O
word	O
in	O
a	O
sentence	O
to	O
define	O
the	O
task	O
relevance	O
of	O
the	O
sentence	O
.	O
But	O
to	O
simplify	O
discussion	O
here	O
,	O
we	O
assume	O
we	O
are	O
dealing	O
with	O
word	O
relevance	O
only	O
.	O
That	O
is	O
,	O
for	O
each	O
word	O
w	O
in	O
the	O
TI	O
training	O
corpus	O
,	O
define	O
0	O
<	O
=	O
ρ	O
(	O
w	O
)	O
<	O
=	O
1	O
to	O
be	O
the	O
task	O
relevance	O
coefficient	O
.	O
In	O
the	O
simplest	O
definition	O
:	O
[	O
in-line-formulae	O
]	O
ρ	O
(	O
w	O
)	O
=	O
1	O
if	O
w	O
is	O
in	O
the	O
TD	O
vocabulary	O
Eq	O
.	O
1	O
[	O
/in-line-formulae	O
]	O
[	O
in-line-formulae	O
]	O
ρ	O
(	O
w	O
)	O
=	O
0	O
otherwise	O
Eq	O
.	O
2	O
[	O
/in-line-formulae	O
]	O
However	O
,	O
if	O
the	O
TI	O
data	O
(	O
such	O
as	O
the	O
popular	O
LDC	O
Macrophone	O
corpus	O
)	O
does	O
not	O
contain	O
a	O
large	O
number	O
of	O
TD	O
words	O
(	O
such	O
as	O
a	O
name	O
directory	O
assistance	O
task	O
)	O
,	O
this	O
simple	O
definition	O
will	O
reject	O
most	O
TI	O
training	O
data	O
,	O
and	O
thus	O
render	O
an	O
under-trained	O
TD	O
acoustic	O
model	O
.	O
In	O
this	O
situation	O
,	O
a	O
more	O
sophisticated	O
ρ	O
function	O
such	O
as	O
the	O
following	O
should	O
be	O
defined	O
:	O
[	O
in-line-formulae	O
]	O
ρ	O
(	O
w	O
)	O
=	O
#	O
TD	O
triphones	O
in	O
this	O
word/#phones	O
in	O
this	O
word	O
Eq	O
.	O
3	O
[	O
/in-line-formulae	O
]	O
[	O
in-line-formulae	O
]	O
or	O
[	O
/in-line-formulae	O
]	O
[	O
in-line-formulae	O
]	O
ρ	O
(	O
w	O
)	O
=	O
#	O
TD	O
“	O
physical	O
”	O
triphones	O
in	O
this	O
word/#phones	O
in	O
this	O
word	O
.	O
Eq	O
.	O
4	O
[	O
/in-line-formulae	O
]	O
A	O
“	O
physical	O
”	O
triphone	O
,	O
in	O
a	O
senone-based	O
Hidden	O
Markov	O
Model	O
(	O
HMM	O
)	O
system	O
,	O
represents	O
a	O
cluster	O
of	O
triphones	O
which	O
share	O
the	O
same	O
senone	O
sequence	O
in	O
modeling	O
their	O
output	O
density	O
functions	O
.	O
For	O
purposes	O
of	O
this	O
application	O
senone	O
means	O
a	O
Markov	O
state	O
representation	O
of	O
a	O
cluster	O
of	O
similar	O
Markov	O
states	O
.	O
See	O
Mei-Yuh	B-Citation
Hwang	I-Citation
and	I-Citation
Xuedong	I-Citation
Huang	I-Citation
,	I-Citation
Subphonetic	I-Citation
Modeling	I-Citation
for	I-Citation
Speech	I-Citation
Recognition	I-Citation
,	I-Citation
Proc.	I-Citation
DARPA	I-Citation
workshop	I-Citation
on	I-Citation
speech	I-Citation
and	I-Citation
natural	I-Citation
language	I-Citation
,	I-Citation
pp	I-Citation
174	I-Citation
–	I-Citation
179	I-Citation
,	I-Citation
1992	I-Citation
.	O

The	O
task	O
relevance	O
measure	O
can	O
also	O
be	O
defined	O
at	O
the	O
triphone	O
level	O
,	O
t	O
.	O
In	O
that	O
embodiment	O
each	O
word	O
in	O
the	O
TI	O
training	O
corpus	O
is	O
expanded	O
into	O
a	O
sequence	O
of	O
triphones	O
per	O
utterance	O
.	O
The	O
task	O
relevance	O
function	O
ρ	O
(	O
t	O
)	O
is	O
then	O
defined	O
as	O
follows	O
:	O
[	O
in-line-formulae	O
]	O
ρ	O
(	O
t	O
)	O
=	O
1	O
if	O
t	O
or	O
physical	O
(	O
t	O
)	O
is	O
in	O
the	O
TD	O
vocabulary	O
;	O
Eq	O
.	O
5	O
[	O
/in-line-formulae	O
]	O
[	O
in-line-formulae	O
]	O
ρ	O
(	O
t	O
)	O
=	O
ε	O
otherwise	O
;	O
Eq	O
.	O
6	O
[	O
/in-line-formulae	O
]	O
where	O
ε	O
is	O
a	O
very	O
small	O
number	O
,	O
close	O
to	O
0	O
.	O

FIGS	O
.	O
7	O
and	O
8	O
illustrate	O
several	O
exemplary	O
embodiments	O
for	O
generating	O
a	O
relevance	O
measure	O
TR	O
(	O
w	O
)	O
(	O
or	O
ρ	O
(	O
w	O
)	O
)	O
.	O
However	O
,	O
the	O
present	O
invention	O
should	O
not	O
be	O
limited	O
to	O
these	O
relevance	O
measures	O
,	O
as	O
they	O
are	O
provided	O
for	O
exemplary	O
purposes	O
only	O
.	O

In	O
the	O
embodiment	O
shown	O
in	O
FIG	O
.	O
7	O
,	O
relevance	O
generator	O
214	O
first	O
receives	O
a	O
word	O
from	O
the	O
TI	O
training	O
corpus	O
204	O
as	O
indicated	O
by	O
block	O
330	O
.	O
Next	O
,	O
in	O
a	O
simple	O
embodiment	O
,	O
generator	O
214	O
simply	O
determines	O
whether	O
the	O
received	O
word	O
is	O
in	O
the	O
target	O
task	O
.	O
This	O
is	O
indicated	O
by	O
block	O
332	O
.	O
If	O
so	O
,	O
then	O
the	O
relevance	O
measure	O
is	O
set	O
to	O
a	O
1	O
for	O
this	O
word	O
as	O
indicated	O
by	O
block	O
334	O
.	O
If	O
not	O
,	O
the	O
relevance	O
measure	O
is	O
simply	O
set	O
to	O
a	O
0	O
as	O
indicated	O
by	O
block	O
336	O
.	O

Once	O
the	O
relevance	O
measure	O
for	O
this	O
word	O
has	O
been	O
obtained	O
,	O
generator	O
214	O
determines	O
whether	O
there	O
are	O
additional	O
words	O
that	O
need	O
a	O
relevance	O
measure	O
.	O
This	O
is	O
indicated	O
by	O
block	O
338	O
.	O
If	O
so	O
,	O
processing	O
continues	O
at	O
block	O
330	O
until	O
all	O
words	O
have	O
received	O
a	O
relevance	O
measure	O
.	O
However	O
,	O
if	O
no	O
words	O
remain	O
,	O
then	O
each	O
word	O
has	O
a	O
relevance	O
measure	O
,	O
and	O
the	O
process	O
concludes	O
at	O
block	O
340	O
.	O
In	O
this	O
embodiment	O
,	O
relevant	O
words	O
are	O
weighted	O
100	O
%	O
while	O
irrelevant	O
words	O
are	O
simply	O
discarded	O
in	O
the	O
acoustic	O
model	O
training	O
process	O
.	O
This	O
type	O
of	O
relevance	O
measure	O
may	O
be	O
used	O
where	O
there	O
is	O
a	O
sufficient	O
amount	O
of	O
task-dependent	O
data	O
to	O
adequately	O
train	O
an	O
acoustic	O
model	O
with	O
only	O
that	O
task-dependent	O
data	O
.	O
However	O
,	O
this	O
may	O
not	O
be	O
the	O
case	O
.	O

Therefore	O
,	O
FIG	O
.	O
8	O
shows	O
an	O
alternative	O
exemplary	O
embodiment	O
of	O
generating	O
a	O
relevance	O
measure	O
.	O
In	O
that	O
embodiment	O
,	O
relevance	O
generator	O
214	O
again	O
receives	O
a	O
word	O
as	O
indicated	O
by	O
block	O
350	O
.	O
Then	O
,	O
relevance	O
generator	O
214	O
determines	O
the	O
number	O
of	O
triphones	O
from	O
the	O
current	O
word	O
that	O
are	O
in	O
the	O
target	O
task	O
.	O
This	O
is	O
indicated	O
by	O
block	O
352	O
in	O
FIG	O
.	O
8	O
.	O
Relevance	O
generator	O
214	O
identifies	O
this	O
number	O
as	O
the	O
number	O
of	O
task-dependent	O
triphones	O
.	O

Next	O
,	O
relevance	O
generator	O
214	O
defines	O
the	O
relevance	O
for	O
the	O
present	O
word	O
TR	O
(	O
w	O
)	O
as	O
follows	O
:	O
Eq	O
.	O
7	O

TR	O
⁡	O

(	O
w	O
)	O

=	O

number	O
⁢	O

⁢	O
of	O
⁢	O

⁢	O
task	O
⁢	O

⁢	O
dependent	O
⁢	O

⁢	O
triphones	O

total	O
⁢	O

⁢	O
number	O
⁢	O

⁢	O
of	O
⁢	O

⁢	O
phones	O

Eq	O
.	O
7	O

In	O
other	O
words	O
,	O
the	O
relevance	O
measure	O
is	O
a	O
ratio	O
of	O
the	O
number	O
of	O
task-dependent	O
triphones	O
in	O
the	O
subject	O
word	O
divided	O
by	O
the	O
total	O
number	O
of	O
phones	O
in	O
the	O
word	O
.	O
An	O
example	O
may	O
be	O
helpful	O
.	O

Assume	O
that	O
the	O
present	O
word	O
is	O
the	O
word	O
“	O
book	O
”	O
.	O
That	O
word	O
may	O
illustratively	O
be	O
made	O
up	O
of	O
the	O
following	O
phones	O
(	O
or	O
phonemes	O
)	O
:	O
b	O
uk	O
k	O
.	O

Thus	O
,	O
the	O
total	O
triphones	O
for	O
the	O
word	O
book	O
are	O
shown	O
as	O
follows	O
:	O
#	O
b	O
uhb	O
uh	O
kuh	O
k	O
#	O

where	O
#	O
represents	O
a	O
word	O
boundary	O
.	O

Assume	O
also	O
that	O
the	O
first	O
and	O
third	O
triphones	O
listed	O
above	O
appear	O
in	O
the	O
task	O
,	O
but	O
that	O
the	O
second	O
triphone	O
does	O
not	O
.	O
In	O
that	O
case	O
,	O
the	O
word	O
“	O
book	O
”	O
in	O
the	O
task-independent	O
training	O
data	O
is	O
given	O
a	O
relevance	O
factor	O
TR	O
(	O
w	O
)	O
=	O
⅔	O
and	O
it	O
is	O
weighted	O
by	O
that	O
relevance	O
factor	O
.	O

Relevance	O
generator	O
214	O
then	O
determines	O
whether	O
any	O
additional	O
words	O
need	O
to	O
have	O
relevance	O
measures	O
generated	O
therefore	O
as	O
indicated	O
by	O
block	O
356	O
.	O
If	O
so	O
,	O
processing	O
continues	O
at	O
block	O
350	O
.	O
But	O
if	O
not	O
,	O
processing	O
is	O
concluded	O
at	O
block	O
358	O
.	O

The	O
method	O
set	O
out	O
in	O
FIG	O
.	O
8	O
is	O
extremely	O
efficient	O
in	O
addressing	O
the	O
sparse	O
data	O
problem	O
.	O
In	O
other	O
words	O
,	O
if	O
the	O
task-independent	O
corpus	O
does	O
not	O
have	O
sufficient	O
task-dependent	O
,	O
relevant	O
words	O
to	O
adequately	O
train	O
an	O
acoustic	O
model	O
,	O
the	O
data	O
associated	O
with	O
many	O
of	O
the	O
words	O
can	O
still	O
be	O
used	O
in	O
training	O
a	O
task-dependent	O
acoustic	O
model	O
.	O
A	O
word	O
having	O
a	O
large	O
number	O
of	O
relevant	O
portions	O
or	O
phones	O
is	O
weighted	O
higher	O
than	O
a	O
word	O
that	O
has	O
a	O
low	O
number	O
of	O
relevant	O
phones	O
.	O
Thus	O
,	O
a	O
vast	O
amount	O
of	O
data	O
can	O
still	O
be	O
used	O
to	O
train	O
the	O
task-dependent	O
data	O
model	O
,	O
even	O
if	O
the	O
number	O
of	O
task-dependent	O
,	O
relevant	O
words	O
in	O
the	O
task-independent	O
corpus	O
is	O
not	O
great	O
.	O

Particularly	O
in	O
our	O
internal	O
lab	O
experiments	O
,	O
we	O
were	O
able	O
to	O
reduce	O
the	O
word	O
error	O
rate	O
by	O
3	O
%	O
relatively	O
with	O
TR	O
(	O
w	O
)	O
training	O
compared	O
with	O
the	O
TI	O
acoustic	O
model	O
.	O
In	O
that	O
experiment	O
,	O
TR	O
(	O
w	O
)	O
was	O
defined	O
as	O
the	O
cube	O
of	O
the	O
number	O
of	O
TD	O
quinphones	O
divided	O
by	O
the	O
number	O
of	O
phones	O
in	O
this	O
word	O
.	O

Once	O
the	O
task-related	O
acoustic	O
model	O
has	O
been	O
generated	O
and	O
deployed	O
in	O
an	O
application	O
,	O
the	O
system	O
can	O
save	O
usage	O
data	O
to	O
obtain	O
additional	O
task-dependent	O
data	O
.	O
For	O
example	O
,	O
assume	O
that	O
the	O
task-independent	O
training	O
data	O
is	O
from	O
the	O
LDC	O
Macrophone	O
corpus	O
(	O
which	O
has	O
Wall	O
Street	O
Journal	O
data	O
,	O
digits	O
,	O
and	O
yes/no	O
types	O
of	O
sentences	O
recorded	O
over	O
the	O
telephone	O
)	O
and	O
the	O
application	O
is	O
a	O
weather	O
look-up	O
task	O
by	O
telephone	O
.	O
It	O
can	O
be	O
assumed	O
that	O
the	O
city	O
names	O
in	O
the	O
task-independent	O
Macrophone	O
corpus	O
will	O
be	O
more	O
heavily	O
weighted	O
than	O
other	O
words	O
during	O
the	O
creation	O
of	O
the	O
task-related	O
acoustic	O
model	O
.	O
In	O
accordance	O
with	O
one	O
embodiment	O
of	O
the	O
present	O
invention	O
,	O
once	O
the	O
application	O
which	O
deploys	O
the	O
task-dependent	O
acoustic	O
model	O
is	O
in	O
use	O
,	O
the	O
system	O
will	O
presumably	O
receive	O
phone	O
calls	O
.	O
Of	O
course	O
,	O
the	O
WAV	O
files	O
generated	O
during	O
these	O
phone	O
calls	O
can	O
be	O
stored	O
and	O
they	O
will	O
contain	O
precisely	O
task-dependent	O
data	O
.	O
This	O
data	O
can	O
be	O
manually	O
transcribed	O
and	O
used	O
to	O
retrain	O
,	O
or	O
modify	O
the	O
task-related	O
acoustic	O
model	O
to	O
even	O
further	O
improve	O
performance	O
.	O

However	O
,	O
as	O
discussed	O
in	O
the	O
background	O
portion	O
above	O
,	O
manual	O
transcription	O
of	O
this	O
data	O
is	O
tedious	O
,	O
error	O
prone	O
,	O
and	O
can	O
be	O
costly	O
.	O
Therefore	O
,	O
in	O
accordance	O
with	O
another	O
embodiment	O
of	O
the	O
present	O
invention	O
,	O
unsupervised	O
training	O
data	O
can	O
be	O
used	O
to	O
further	O
improve	O
the	O
task-related	O
acoustic	O
model	O
.	O

FIG	O
.	O
9	O
is	O
a	O
block	O
diagram	O
of	O
a	O
system	O
401	O
for	O
generating	O
a	O
task-dependent	O
acoustic	O
model	O
from	O
unsupervised	O
training	O
data	O
.	O
FIG	O
.	O
9	O
shows	O
unsupervised	O
speech	O
training	O
data	O
(	O
or	O
utterances	O
)	O
400	O
.	O
The	O
training	O
data	O
is	O
provided	O
to	O
a	O
speech	O
recognition	O
component	O
402	O
which	O
generates	O
a	O
plurality	O
of	O
outputs	O
that	O
are	O
provided	O
to	O
weighting	O
component	O
404	O
.	O
Weighting	O
component	O
404	O
is	O
coupled	O
to	O
an	O
acoustic	O
model	O
training	O
component	O
406	O
which	O
outputs	O
a	O
confidence	O
measure	O
weighted	O
task-dependent	O
acoustic	O
model	O
408	O
.	O
FIG	O
.	O
10	O
is	O
a	O
flow	O
diagram	O
better	O
illustrating	O
the	O
operation	O
of	O
the	O
system	O
shown	O
in	O
FIG	O
.	O
9	O
.	O

First	O
,	O
confidence	O
measure	O
training	O
system	O
401	O
(	O
and	O
specifically	O
speech	O
recognition	O
component	O
402	O
)	O
receives	O
the	O
unsupervised	O
task-dependent	O
speech	O
data	O
400	O
.	O
Of	O
course	O
,	O
the	O
unsupervised	O
task-dependent	O
speech	O
data	O
is	O
simply	O
task-dependent	O
,	O
recorded	O
speech	O
data	O
,	O
for	O
example	O
in	O
the	O
form	O
of	O
WAV	O
files	O
,	O
without	O
a	O
corresponding	O
transcription	O
.	O
That	O
data	O
is	O
fed	O
to	O
speech	O
recognition	O
component	O
402	O
.	O

It	O
is	O
known	O
that	O
many	O
state-of-the-art	O
speech	O
recognizers	O
have	O
a	O
component	O
which	O
computes	O
a	O
confidence	O
measure	O
for	O
words	O
that	O
the	O
system	O
recognizes	O
.	O
The	O
confidence	O
measure	O
involves	O
computing	O
the	O
acoustic	O
and/or	O
language	O
model	O
scores	O
for	O
the	O
recognized	O
words	O
,	O
compared	O
with	O
an	O
expected	O
score	O
.	O
The	O
confidence	O
measure	O
is	O
also	O
often	O
related	O
to	O
a	O
score	O
of	O
a	O
generic	O
phone	O
sequence	O
network	O
.	O
The	O
particular	O
manner	O
in	O
which	O
a	O
confidence	O
measure	O
is	O
computed	O
is	O
not	O
important	O
to	O
the	O
present	O
invention	O
.	O
The	O
present	O
invention	O
simply	O
assumes	O
that	O
such	O
a	O
confidence	O
measure	O
is	O
computed	O
,	O
and	O
is	O
made	O
available	O
,	O
for	O
each	O
word	O
recognized	O
by	O
the	O
speech	O
recognition	O
system	O
.	O

Further	O
more	O
it	O
is	O
assumed	O
that	O
the	O
value	O
of	O
the	O
confidence	O
measure	O
is	O
between	O
0	O
and	O
1	O
:	O
[	O
in-line-formulae	O
]	O
0	O
<	O
=	O
φ	O
(	O
w	O
)	O
<	O
=	O
1	O
Eq	O
.	O
8	O
[	O
/in-line-formulae	O
]	O
where	O
φ	O
(	O
w	O
)	O
and	O
CONF	O
(	O
w	O
)	O
will	O
be	O
used	O
to	O
represent	O
the	O
confidence	O
measure	O
,	O
interchangeably	O
.	O
Receiving	O
the	O
unsupervised	O
data	O
is	O
indicated	O
by	O
block	O
403	O
in	O
FIG	O
.	O
10	O
.	O

Once	O
speech	O
recognition	O
component	O
402	O
receives	O
the	O
unsupervised	O
,	O
task-dependent	O
speech	O
data	O
,	O
speech	O
recognition	O
component	O
402	O
performs	O
speech	O
recognition	O
on	O
the	O
utterances	O
in	O
data	O
400	O
.	O
Speech	O
recognition	O
component	O
402	O
illustratively	O
passes	O
through	O
the	O
acoustic	O
data	O
(	O
as	O
WAV	O
files	O
)	O
corresponding	O
to	O
the	O
utterances	O
as	O
designated	O
by	O
numeral	O
400	O
.	O
Speech	O
recognition	O
component	O
402	O
also	O
illustratively	O
outputs	O
both	O
a	O
transcription	O
of	O
a	O
sequence	O
of	O
recognized	O
words	O
412	O
,	O
for	O
the	O
utterances	O
input	O
to	O
speech	O
recognition	O
component	O
402	O
,	O
as	O
well	O
as	O
the	O
confidence	O
scores	O
(	O
or	O
confidence	O
measures	O
)	O
for	O
each	O
sub-utterance	O
unit	O
(	O
such	O
as	O
for	O
each	O
word	O
)	O
.	O
The	O
confidence	O
measures	O
are	O
designated	O
by	O
numeral	O
414	O
.	O
Generating	O
the	O
transcription	O
and	O
confidence	O
scores	O
is	O
indicated	O
by	O
block	O
405	O
in	O
FIG	O
.	O
10	O
.	O

The	O
acoustic	O
data	O
400	O
,	O
the	O
transcription	O
of	O
the	O
words	O
412	O
and	O
the	O
confidence	O
measures	O
414	O
are	O
all	O
input	O
to	O
weighting	O
component	O
404	O
.	O
Weighting	O
component	O
404	O
weights	O
each	O
segment	O
of	O
the	O
data	O
(	O
each	O
word	O
)	O
with	O
CONF	O
(	O
w	O
)	O
and	O
provides	O
it	O
to	O
acoustic	O
model	O
training	O
component	O
406	O
.	O
This	O
is	O
indicated	O
by	O
block	O
407	O
.	O

Thus	O
,	O
each	O
of	O
the	O
data	O
segments	O
is	O
weighted	O
by	O
the	O
confidence	O
score	O
during	O
the	O
maximum	O
likelihood	O
hidden	O
Markov	O
model	O
(	O
ML	O
HMM	O
)	O
training	O
process	O
.	O
Training	O
component	O
406	O
thus	O
outputs	O
a	O
confidence	O
measure	O
weighted	O
,	O
task-dependent	O
acoustic	O
model	O
(	O
the	O
TD	O
model	O
408	O
)	O
which	O
is	O
generated	O
from	O
data	O
in	O
which	O
highly	O
confident	O
speech	O
segments	O
are	O
weighted	O
most	O
highly	O
,	O
and	O
the	O
speech	O
segments	O
that	O
correspond	O
to	O
a	O
lower	O
confidence	O
are	O
weighted	O
lower	O
,	O
such	O
that	O
they	O
have	O
little	O
or	O
no	O
impact	O
on	O
TR	O
model	O
408	O
.	O
Assuming	O
that	O
the	O
confidence	O
measure	O
is	O
relatively	O
accurate	O
,	O
this	O
prevents	O
moving	O
the	O
model	O
parameters	O
in	O
the	O
wrong	O
direction	O
.	O
Generating	O
the	O
TR	O
model	O
is	O
indicated	O
by	O
block	O
409	O
.	O

This	O
is	O
in	O
contrast	O
to	O
other	O
systems	O
such	O
as	O
that	O
discussed	O
in	O
D.	B-Citation
Charlet	I-Citation
,	I-Citation
Confidence-Measure	I-Citation
Driven	I-Citation
Unsupervised	I-Citation
Incremental	I-Citation
Adaptation	I-Citation
for	I-Citation
HMM-Based	I-Citation
Speech	I-Citation
Recognition	I-Citation
,	I-Citation
ICASSP-2001	I-Citation
,	O
which	O
use	O
“	O
all-or-nothing	O
”	O
strategy	O
when	O
dealing	O
with	O
unsupervised	O
data	O
.	O
That	O
is	O
,	O
if	O
the	O
confidence	O
score	O
of	O
an	O
entire	O
utterance	O
(	O
which	O
consists	O
of	O
multiple	O
words	O
)	O
is	O
above	O
a	O
threshold	O
level	O
,	O
then	O
the	O
entire	O
utterance	O
is	O
used	O
,	O
and	O
all	O
components	O
of	O
the	O
entire	O
utterance	O
are	O
weighted	O
uniformly	O
to	O
update	O
the	O
model	O
.	O
Otherwise	O
,	O
if	O
the	O
confidence	O
score	O
for	O
the	O
entire	O
utterance	O
is	O
below	O
a	O
threshold	O
level	O
,	O
the	O
entire	O
utterance	O
is	O
simply	O
discarded	O
.	O
Of	O
course	O
,	O
the	O
present	O
invention	O
is	O
highly	O
advantageous	O
over	O
this	O
type	O
of	O
system	O
,	O
because	O
the	O
present	O
invention	O
uses	O
individual	O
words	O
that	O
have	O
a	O
high	O
confidence	O
associated	O
with	O
them	O
,	O
while	O
it	O
does	O
not	O
use	O
(	O
or	O
at	O
least	O
places	O
lower	O
emphasis	O
on	O
)	O
words	O
that	O
have	O
a	O
low	O
confidence	O
associated	O
with	O
them	O
.	O

The	O
present	O
invention	O
for	O
dealing	O
with	O
unsupervised	O
task-dependent	O
training	O
data	O
can	O
also	O
be	O
used	O
in	O
combination	O
with	O
the	O
task-relevance	O
training	O
systems	O
209	O
and	O
300	O
discussed	O
above	O
with	O
respect	O
to	O
FIGS	O
.	O
4	O
and	O
6	O
.	O
Such	O
a	O
system	O
is	O
illustrated	O
as	O
system	O
500	O
shown	O
in	O
FIG	O
.	O
11	O
.	O
System	O
500	O
contains	O
a	O
number	O
of	O
items	O
discussed	O
in	O
previous	O
Figures	O
,	O
and	O
those	O
items	O
are	O
similarly	O
numbered	O
.	O
FIG	O
.	O
12	O
is	O
a	O
flow	O
diagram	O
which	O
better	O
illustrates	O
the	O
operation	O
of	O
system	O
500	O
.	O

System	O
500	O
first	O
generates	O
the	O
task	O
relevance	O
weighted	O
task-dependent	O
acoustic	O
model	O
(	O
TR	O
model	O
202	O
or	O
320	O
)	O
,	O
depending	O
on	O
which	O
task	O
relevance	O
training	O
system	O
from	O
the	O
above	O
Figures	O
is	O
used	O
.	O
This	O
is	O
indicated	O
by	O
block	O
505	O
in	O
FIG	O
.	O
12	O
.	O
Next	O
,	O
system	O
500	O
generates	O
the	O
confidence	O
measure	O
weighted	O
task-dependent	O
acoustic	O
model	O
(	O
the	O
TD	O
model	O
)	O
408	O
as	O
discussed	O
with	O
respect	O
to	O
FIG	O
.	O
9	O
.	O
This	O
is	O
indicated	O
by	O
block	O
507	O
in	O
FIG	O
.	O
12	O
.	O
System	O
500	O
then	O
smoothes	O
TD	O
model	O
408	O
with	O
TR	O
model	O
202	O
or	O
320	O
to	O
obtain	O
a	O
composite	O
acoustic	O
model	O
,	O
which	O
is	O
a	O
task-adapted	O
(	O
TA	O
)	O
model	O
502	O
.	O
This	O
is	O
indicated	O
by	O
blocks	O
509	O
and	O
511	O
in	O
FIG	O
.	O
12	O
.	O

In	O
accordance	O
with	O
one	O
embodiment	O
of	O
the	O
present	O
invention	O
,	O
models	O
408	O
and	O
202	O
or	O
320	O
are	O
provided	O
to	O
data	O
volume	O
weighting	O
component	O
504	O
.	O
Component	O
504	O
weights	O
each	O
of	O
the	O
parameters	O
or	O
phonetic	O
units	O
in	O
each	O
acoustic	O
model	O
based	O
on	O
a	O
volume	O
of	O
data	O
used	O
to	O
generate	O
the	O
parameter	O
or	O
phonetic	O
unit	O
.	O
Thus	O
,	O
the	O
volume	O
of	O
supervised	O
data	O
used	O
to	O
generate	O
each	O
Gaussian	O
or	O
senone	O
(	O
designated	O
by	O
numeral	O
506	O
)	O
is	O
provided	O
to	O
component	O
504	O
,	O
as	O
is	O
the	O
volume	O
of	O
unsupervised	O
data	O
used	O
to	O
generate	O
each	O
Gaussian	O
or	O
senone	O
in	O
TD	O
model	O
408	O
.	O
This	O
is	O
designated	O
by	O
block	O
508	O
.	O
Once	O
the	O
components	O
are	O
weighted	O
,	O
they	O
are	O
fed	O
to	O
acoustic	O
model	O
generation	O
component	O
510	O
which	O
generates	O
the	O
TA	O
model	O
502	O
based	O
on	O
the	O
weighted	O
contributions	O
of	O
the	O
other	O
acoustic	O
models	O
408	O
and	O
202	O
or	O
320	O
.	O

FIG	O
.	O
11	O
-	O
1	O
illustrates	O
system	O
500	O
in	O
a	O
slightly	O
different	O
way	O
to	O
more	O
easily	O
describe	O
weighting	O
component	O
504	O
.	O
Similar	O
items	O
are	O
similarly	O
numbered	O
to	O
those	O
previously	O
discussed	O
.	O
The	O
diagram	O
has	O
been	O
simplified	O
,	O
however	O
,	O
by	O
simply	O
showing	O
ML	O
training	O
blocks	O
520	O
and	O
522	O
,	O
which	O
represent	O
the	O
particular	O
model	O
training	O
mechanism	O
being	O
used	O
.	O
However	O
,	O
as	O
shown	O
in	O
FIG	O
.	O
11	O
-	O
1	O
,	O
system	O
500	O
smoothes	O
TR	O
model	O
202	O
or	O
320	O
with	O
TD	O
model	O
408	O
to	O
obtain	O
final	O
TA	O
model	O
502	O
.	O
In	O
doing	O
so	O
,	O
system	O
500	O
combines	O
models	O
202	O
or	O
320	O
and	O
408	O
at	O
combination	O
component	O
524	O
.	O
Component	O
524	O
first	O
computes	O
a	O
weighting	O
coefficient	O
l	O
for	O
each	O
senone	O
(	O
for	O
each	O
shared	O
HMM	O
state	O
)	O
.	O
The	O
weighting	O
coefficient	O
l	O
is	O
piecewise	O
linearly	O
proportional	O
to	O
the	O
TD	O
occupancy	O
count	O
of	O
that	O
senone	O
.	O
For	O
example	O
,	O
FIG	O
.	O
11	O
-	O
2	O
shows	O
an	O
exemplary	O
graph	O
of	O
weighting	O
coefficient	O
l	O
against	O
senone	O
TD	O
occupancy	O
count	O
(	O
i	O
.	O
e	O
.	O
,	O
the	O
occupancy	O
count	O
from	O
the	O
TD	O
unsupervised	O
data	O
)	O
.	O
The	O
operation	O
of	O
system	O
500	O
shown	O
in	O
FIG	O
.	O
11	O
-	O
1	O
is	O
better	O
illustrated	O
by	O
the	O
flow	O
diagram	O
shown	O
in	O
FIG	O
.	O
12	O
-	O
1	O
.	O
Computing	O
weight	O
l	O
for	O
each	O
senone	O
is	O
illustrated	O
in	O
block	O
600	O
of	O
FIG	O
.	O
12	O
-	O
1	O
.	O
Then	O
,	O
the	O
Gaussian	O
means	O
for	O
the	O
acoustic	O
models	O
are	O
smoothed	O
at	O
the	O
mathematical	O
mean	O
as	O
follows	O
:	O
[	O
in-line-formulae	O
]	O
μ	O
=	O
lμTD	O
+	O
(	O
1	O
−	O
l	O
)	O
μTR	O
Eq	O
.	O
9	O
[	O
/in-line-formulae	O
]	O
where	O
μTD	O
is	O
the	O
mathematical	O
mean	O
for	O
TD	O
model	O
408	O
;	O
and	O
μTR	O
is	O
the	O
mathematical	O
mean	O
for	O
TR	O
model	O
202	O
or	O
320	O
.	O
This	O
is	O
indicated	O
by	O
block	O
602	O
in	O
FIG	O
.	O
12	O
-	O
1	O
.	O

It	O
can	O
be	O
seen	O
that	O
,	O
once	O
the	O
count	O
in	O
the	O
task	O
dependent	O
data	O
is	O
large	O
enough	O
,	O
the	O
final	O
mean	O
μ	O
will	O
be	O
determined	O
primarily	O
by	O
the	O
task-dependent	O
data	O
,	O
regardless	O
of	O
how	O
large	O
the	O
task-independent	O
corpus	O
is	O
.	O
In	O
this	O
way	O
,	O
the	O
mean	O
can	O
be	O
adjusted	O
faster	O
,	O
even	O
if	O
the	O
task-independent	O
model	O
is	O
trained	O
on	O
an	O
extremely	O
large	O
corpus	O
.	O

Of	O
course	O
,	O
other	O
variations	O
can	O
be	O
used	O
as	O
well	O
.	O
For	O
example	O
,	O
it	O
should	O
be	O
noted	O
that	O
if	O
the	O
recognition	O
accuracy	O
of	O
the	O
task	O
dependent	O
data	O
is	O
exceptionally	O
low	O
,	O
then	O
the	O
e	O
function	O
should	O
be	O
less	O
aggressive	O
.	O

Once	O
the	O
Gaussian	O
means	O
have	O
been	O
smoothed	O
,	O
the	O
Gaussian	O
variances	O
are	O
smoothed	O
based	O
on	O
occupancy	O
counts	O
.	O
This	O
is	O
indicated	O
by	O
block	O
604	O
in	O
FIG	O
.	O
12	O
-	O
1	O
.	O
In	O
this	O
way	O
,	O
the	O
variances	O
will	O
be	O
significantly	O
affected	O
only	O
if	O
a	O
significantly	O
large	O
amount	O
of	O
TD	O
training	O
data	O
are	O
observed	O
,	O
as	O
follows	O
:	O

σ	O
2	O

=	O

E	O
⁡	O

(	O

Z	O
-	O
μ	O

)	O

2	O

=	O

EZ	O
2	O

-	O

2	O
⁢	O
μ	O
⁢	O

⁢	O
EZ	O

+	O

μ	O
2	O

=	O

(	O

l	O
⁢	O

⁢	O
Σ	O
⁢	O

⁢	O

r	O
⁡	O

(	O
x	O
)	O

⁢	O

x	O
2	O

+	O

(	O

1	O
-	O
l	O

)	O

⁢	O
Σ	O
⁢	O

⁢	O

r	O
⁡	O

(	O
y	O
)	O

⁢	O

y	O
2	O

)	O

/	O

(	O

la	O
+	O

(	O

1	O
-	O
l	O

)	O

⁢	O
b	O

)	O

+	O

μ	O
2	O

-	O

2	O
⁢	O

μ	O
⁡	O

(	O

la	O
⁢	O

⁢	O

μ	O
TD	O

+	O

(	O

1	O
-	O
l	O

)	O

⁢	O
b	O
⁢	O

⁢	O

μ	O
TI	O

)	O

/	O

(	O

la	O
+	O

(	O

1	O
-	O
l	O

)	O

⁢	O
b	O

)	O

Eq	O
.	O

⁢	O
10	O

where	O
Z	O
is	O
the	O
combined	O
weighted	O
data	O
from	O
TD	O
model	O
408	O
and	O
TR	O
model	O
202	O
or	O
320	O
;	O

a	O
=	O
Σσ	O
(	O
x	O
)	O
for	O
all	O
task	O
dependent	O
data	O
aligned	O
with	O
the	O
present	O
senone	O
;	O
and	O
b	O
=	O
Σσ	O
(	O
y	O
)	O
for	O
all	O
TR	O
data	O
belonging	O
to	O
the	O
present	O
senone	O
.	O

Particulary	O
in	O
our	O
internal	O
lab	O
experiments	O
,	O
we	O
found	O
11	O
%	O
relative	O
error	O
rate	O
reduction	O
with	O
unsupervised	O
TD	O
training	O
described	O
above	O
compared	O
with	O
the	O
baseline	O
supervised	O
TI	O
acoustic	O
model	O
.	O

FIGS	O
.	O
13	O
and	O
14	O
illustate	O
yet	O
another	O
embodiment	O
of	O
the	O
present	O
invention	O
.	O
FIG	O
.	O
13	O
illustrates	O
a	O
system	O
700	O
for	O
generating	O
a	O
task-dependent	O
acoustic	O
model	O
based	O
on	O
unsupervised	O
,	O
task-independent	O
speech	O
data	O
only	O
.	O
FIG	O
.	O
14	O
is	O
a	O
flow	O
diagram	O
better	O
illustrating	O
the	O
operation	O
of	O
system	O
700	O
shown	O
in	O
FIG	O
.	O
13	O
.	O

While	O
the	O
prior	O
embodiments	O
have	O
either	O
dealt	O
with	O
supervised	O
,	O
task-independent	O
training	O
data	O
204	O
or	O
unsupervised	O
,	O
task-dependent	O
training	O
data	O
400	O
,	O
system	O
700	O
utilizes	O
unsupervised	O
,	O
task-independent	O
training	O
data	O
702	O
.	O
The	O
unsupervised	O
task-independent	O
training	O
data	O
702	O
illustratively	O
includes	O
acoustic	O
data	O
representative	O
of	O
speech	O
,	O
which	O
may	O
or	O
may	O
not	O
be	O
relevant	O
to	O
the	O
task	O
at	O
hand	O
,	O
and	O
for	O
which	O
no	O
known	O
transcriptions	O
are	O
available	O
.	O
This	O
information	O
is	O
illustratively	O
received	O
,	O
as	O
indicated	O
by	O
block	O
704	O
in	O
FIG	O
.	O
14	O
.	O
Then	O
we	O
run	O
SR	O
component	O
402	O
to	O
generate	O
hypothesized	O
transcriptions	O
708	O
and	O
confidence	O
measure	O
CONF	O
(	O
w	O
)	O
706	O
.	O
Then	O
each	O
word	O
w	O
in	O
the	O
hypothesized	O
transcriptions	O
is	O
assigned	O
a	O
task	O
relevance	O
TR	O
(	O
w	O
)	O
by	O
component	O
214	O
.	O
Finally	O
in	O
718	O
we	O
define	O
the	O
weight	O
of	O
each	O
word	O
w	O
to	O
be	O
the	O
product	O
of	O
TR	O
(	O
w	O
)	O
and	O
CONF	O
(	O
w	O
)	O
,	O
and	O
then	O
train	O
as	O
usual	O
to	O
obtain	O
the	O
final	O
model	O
,	O
which	O
is	O
now	O
weighted	O
both	O
by	O
task	O
relevance	O
and	O
confidence	O
score	O
.	O

It	O
should	O
be	O
noted	O
that	O
acoustic	O
model	O
714	O
is	O
based	O
on	O
unsupervised	O
data	O
,	O
but	O
it	O
is	O
weighted	O
with	O
the	O
confidence	O
measure	O
generated	O
by	O
the	O
speech	O
recognizer	O
in	O
block	O
401	O
and	O
it	O
is	O
also	O
weighted	O
by	O
the	O
relevance	O
measure	O
generated	O
by	O
the	O
task	O
relevance	O
training	O
component	O
209	O
or	O
300	O
.	O
It	O
should	O
further	O
be	O
noted	O
that	O
acoustic	O
model	O
714	O
can	O
be	O
input	O
to	O
the	O
combining	O
component	O
524	O
illustrated	O
in	O
FIG	O
.	O
11	O
-	O
1	O
,	O
and	O
the	O
acoustic	O
model	O
714	O
can	O
contribute	O
to	O
the	O
ultimate	O
acoustic	O
model	O
502	O
as	O
well	O
.	O
In	O
that	O
case	O
,	O
system	O
500	O
can	O
receive	O
,	O
as	O
training	O
data	O
,	O
task-dependent	O
,	O
unsupervised	O
training	O
data	O
;	O
task-independent	O
,	O
supervised	O
training	O
data	O
;	O
and	O
unsupervised	O
,	O
task-independent	O
training	O
data	O
.	O
Of	O
course	O
,	O
the	O
weighting	O
component	O
is	O
slightly	O
modified	O
to	O
accomodate	O
weighting	O
three	O
acoustic	O
models	O
,	O
instead	O
of	O
two	O
,	O
during	O
combination	O
to	O
obtain	O
the	O
ultimate	O
task-adapted	O
acoustic	O
model	O
502	O
.	O

Although	O
the	O
present	O
invention	O
has	O
been	O
described	O
with	O
reference	O
to	O
particular	O
embodiments	O
,	O
workers	O
skilled	O
in	O
the	O
art	O
will	O
recognize	O
that	O
changes	O
may	O
be	O
made	O
in	O
form	O
and	O
detail	O
without	O
departing	O
from	O
the	O
spirit	O
and	O
scope	O
of	O
the	O
invention	O
.	O

1	O
.	O
A	O
method	O
of	O
generating	O
an	O
acoustic	O
model	O
(	O
AM	O
)	O
for	O
use	O
in	O
a	O
speech	O
recognition	O
system	O
,	O
comprising	O
:	O
receiving	O
unsupervised	O
speech	O
data	O
as	O
utterances	O
formed	O
of	O
sub-utterances	O
units	O
,	O
the	O
utterances	O
being	O
represented	O
by	O
acoustic	O
data	O
and	O
including	O
words;generatinq	O
a	O
transcription	O
for	O
each	O
utterance	O
in	O
the	O
unsupervised	O
speech	O
data	O
and	O
a	O
confidence	O
measure	O
for	O
each	O
sub-utterance	O
unit	O
,	O
with	O
a	O
speech	O
recognizer;generating	O
a	O
confidence	O
measure	O
weighted	O
AM	O
based	O
on	O
the	O
acoustic	O
data	O
and	O
the	O
transcriptions	O
weighted	O
by	O
the	O
confidence	O
measures	O
on	O
a	O
sub-utterance	O
level	O
wherein	O
generates	O
the	O
confidence	O
measure	O
for	O
each	O
word	O
in	O
the	O
utterance	O
;	O
andcombining	O
the	O
confidence	O
measure	O
weight	O
AM	O
with	O
a	O
supervised	O
AM	O
generated	O
from	O
supervised	O
speech	O
data	O
to	O
obtain	O
a	O
composite	O
AM	O
.	O

2	O
.	O
The	O
method	O
of	O
claim	O
1	O
wherein	O
combining	O
comprises	O
:	O
weighting	O
a	O
contribution	O
of	O
the	O
confidence	O
measure	O
weighted	O
AM	O
and	O
the	O
supervised	O
AM	O
to	O
the	O
composite	O
AM	O
based	O
on	O
volume	O
weights	O
indicative	O
of	O
an	O
amount	O
of	O
supervised	O
and	O
unsupervised	O
speech	O
data	O
used	O
to	O
generate	O
the	O
AMs	O
.	O

3	O
.	O
The	O
method	O
of	O
claim	O
2	O
wherein	O
the	O
AMs	O
each	O
include	O
Gaussian	O
means	O
and	O
variances	O
and	O
wherein	O
weighting	O
comprises	O
:	O
computing	O
a	O
volume	O
weight	O
for	O
each	O
Gaussian	O
means	O
based	O
on	O
a	O
volume	O
of	O
data	O
used	O
to	O
generate	O
that	O
Gaussian	O
mean;applying	O
the	O
volume	O
weights	O
computed	O
to	O
each	O
Gaussian	O
mean	O
;	O
andcombining	O
the	O
weighted	O
Gaussian	O
means	O
for	O
the	O
confidence	O
measure	O
weighted	O
AM	O
and	O
the	O
supervised	O
AM	O
.	O

4	O
.	O
The	O
method	O
of	O
claim	O
3	O
wherein	O
the	O
weighted	O
Gaussian	O
means	O
comprises	O
:	O
averaging	O
the	O
weighted	O
Gaussian	O
means	O
.	O

5	O
.	O
The	O
method	O
of	O
claim	O
3	O
wherein	O
weighting	O
comprises	O
:	O
merging	O
the	O
Gaussian	O
variances	O
of	O
the	O
confidence	O
measure	O
weighed	O
AM	O
and	O
the	O
supervised	O
AM	O
at	O
a	O
count	O
level	O
.	O

6	O
.	O
The	O
method	O
of	O
claim	O
1	O
wherein	O
the	O
unsupervised	O
speech	O
data	O
comprises	O
task-dependent	O
data	O
,	O
relevant	O
to	O
a	O
desired	O
task	O
to	O
be	O
performed	O
by	O
the	O
speech	O
recognition	O
system	O
.	O

7	O
.	O
The	O
method	O
of	O
claim	O
6	O
wherein	O
the	O
supervised	O
speech	O
data	O
comprises	O
task-dependent	O
supervised	O
data	O
,	O
relevant	O
to	O
the	O
desired	O
task	O
.	O

8	O
.	O
The	O
method	O
of	O
claim	O
6	O
wherein	O
the	O
supervised	O
speech	O
data	O
comprises	O
task-independent	O
data	O
including	O
words	O
and	O
wherein	O
combining	O
the	O
confidence	O
measure	O
weighted	O
AM	O
with	O
the	O
supervised	O
AM	O
comprises	O
:	O
generating	O
the	O
supervised	O
AM	O
based	O
on	O
a	O
relevance	O
of	O
each	O
word	O
in	O
the	O
task-independent	O
data	O
to	O
the	O
desired	O
task	O
.	O

9	O
.	O
A	O
method	O
of	O
generating	O
an	O
acoustic	O
model	O
(	O
AM	O
)	O
for	O
use	O
in	O
a	O
speech	O
recognition	O
system	O
,	O
comprising	O
:	O
receiving	O
unsupervised	O
speech	O
data	O
as	O
utterances	O
formed	O
of	O
sub-utterances	O
units	O
,	O
the	O
utterances	O
being	O
represented	O
by	O
acoustic	O
data;generating	O
a	O
transcription	O
for	O
each	O
utterance	O
in	O
the	O
unsupervised	O
speech	O
data	O
and	O
a	O
confidence	O
measure	O
for	O
each	O
sub-utterance	O
unit	O
,	O
with	O
a	O
speech	O
recognizer;generating	O
a	O
confidence	O
measure	O
weighted	O
AM	O
based	O
on	O
the	O
acoustic	O
data	O
and	O
the	O
transcriptions	O
weighted	O
by	O
the	O
confidence	O
measure	O
on	O
a	O
sub-utterance	O
level	O
;	O
andwherein	O
the	O
unsupervised	O
speech	O
data	O
comprises	O
task-independent	O
speech	O
data	O
including	O
words	O
,	O
and	O
further	O
comprising:generating	O
a	O
relevance	O
measure	O
for	O
each	O
word	O
in	O
the	O
task-independent	O
data	O
,	O
the	O
relevance	O
measure	O
being	O
indicative	O
of	O
a	O
relevance	O
of	O
the	O
word	O
to	O
a	O
desired	O
task	O
to	O
be	O
performed	O
by	O
the	O
speech	O
recognition	O
system	O
.	O

10	O
.	O
The	O
method	O
of	O
claim	O
9	O
wherein	O
generating	O
the	O
confidence	O
measure	O
weighted	O
AM	O
comprises	O
:	O
generating	O
a	O
composite	O
AM	O
based	O
on	O
the	O
transcription	O
weighted	O
by	O
the	O
confidence	O
measures	O
and	O
based	O
on	O
the	O
relevance	O
measure	O
for	O
each	O
word	O
in	O
the	O
unsupervised	O
,	O
task-independent	O
speech	O
data	O
.	O

11	O
.	O
A	O
method	O
of	O
generating	O
an	O
acoustic	O
model	O
(	O
AM	O
)	O
for	O
a	O
speech	O
recognition	O
system	O
,	O
comprising	O
:	O
receiving	O
a	O
task-dependent	O
(	O
TD	O
)	O
AM	O
generated	O
from	O
task-dependent	O
speech	O
data	O
,	O
relevant	O
to	O
a	O
desired	O
task	O
to	O
be	O
performed	O
by	O
the	O
speech	O
recognition	O
system;receiving	O
a	O
task-independent	O
(	O
TI	O
)	O
AM	O
generated	O
from	O
task-independent	O
speech	O
data	O
,	O
the	O
TI	O
AM	O
and	O
the	O
TD	O
AM	O
each	O
including	O
Gaussian	O
means	O
and	O
variances	O
;	O
andcombining	O
the	O
Gaussian	O
means	O
and	O
variances	O
based	O
on	O
an	O
amount	O
of	O
data	O
used	O
to	O
generate	O
each	O
mean	O
and	O
each	O
variance	O
to	O
obtain	O
a	O
composite	O
AM	O
.	O

12	O
.	O
The	O
method	O
of	O
claim	O
11	O
wherein	O
combining	O
comprises	O
:	O
applying	O
data	O
volume	O
weights	O
to	O
each	O
Guassian	O
mean	O
to	O
obtain	O
weighted	O
Gaussian	O
means	O
,	O
the	O
data	O
volume	O
weight	O
being	O
indicative	O
of	O
the	O
amount	O
of	O
data	O
used	O
to	O
generate	O
a	O
corresponding	O
Gaussian	O
mean	O
.	O

13	O
.	O
The	O
method	O
of	O
claim	O
12	O
wherein	O
combining	O
further	O
comprises	O
:	O
averaging	O
the	O
weighted	O
Gaussian	O
means	O
.	O

14	O
.	O
The	O
method	O
of	O
claim	O
11	O
wherein	O
combining	O
comprises	O
:	O
merging	O
counts	O
of	O
the	O
Gaussian	O
variances	O
for	O
the	O
TI	O
AM	O
and	O
the	O
TD	O
AM	O
.	O

15	O
.	O
An	O
acoustic	O
model	O
(	O
AM	O
)	O
generation	O
system	O
,	O
comprising	O
:	O
a	O
speech	O
recognizer	O
receiving	O
unsupervised	O
speech	O
data	O
in	O
the	O
form	O
of	O
utterances	O
with	O
sub-utterance	O
units	O
and	O
words	O
and	O
generating	O
a	O
transcription	O
of	O
the	O
utterances	O
and	O
a	O
confidence	O
measure	O
associated	O
with	O
each	O
sub-utterance	O
unit	O
;	O
an	O
AM	O
generator	O
receiving	O
the	O
transcription	O
and	O
confidence	O
measures	O
and	O
generating	O
a	O
confidence	O
measure	O
AM	O
by	O
weiqhting	O
each	O
word	O
in	O
the	O
utterances	O
with	O
a	O
confidence	O
measure	O
;	O
anda	O
task	O
relevance	O
AM	O
generator	O
receiving	O
supervised	O
task-relevance	O
(	O
TI	O
)	O
speech	O
data	O
including	O
words	O
and	O
generating	O
a	O
task	O
relevance	O
AM	O
based	O
on	O
a	O
relevance	O
of	O
each	O
word	O
in	O
the	O
TI	O
speech	O
data	O
to	O
a	O
desired	O
task	O
for	O
the	O
task	O
relevance	O
AM	O
.	O

16	O
.	O
The	O
system	O
of	O
claim	O
15	O
and	O
further	O
comprising	O
:	O
a	O
composite	O
AM	O
generator	O
generating	O
a	O
composite	O
AM	O
based	O
on	O
the	O
task	O
relevance	O
AM	O
and	O
the	O
confidence	O
measure	O
AM	O
.	O

17	O
.	O
The	O
system	O
of	O
claim	O
15	O
wherein	O
the	O
composite	O
AM	O
generator	O
is	O
configured	O
to	O
weight	O
contributions	O
of	O
the	O
relevance	O
AM	O
and	O
the	O
confidence	O
measure	O
AM	O
based	O
on	O
amount	O
of	O
data	O
used	O
to	O
generate	O
the	O
relevance	O
AM	O
and	O
the	O
confidence	O
measure	O
AM	O
.	O

18	O
.	O
The	O
system	O
of	O
claim	O
16	O
wherein	O
the	O
relevance	O
AM	O
and	O
confidence	O
measure	O
AM	O
each	O
have	O
Gaussian	O
means	O
and	O
variances	O
.	O

19	O
.	O
The	O
system	O
of	O
claim	O
18	O
wherein	O
the	O
composite	O
AM	O
generator	O
generates	O
the	O
composite	O
AM	O
by	O
weighting	O
each	O
individual	O
Gaussian	O
means	O
based	O
on	O
the	O
amount	O
of	O
data	O
used	O
to	O
generate	O
the	O
mean	O
and	O
combining	O
the	O
weighted	O
means	O
from	O
the	O
relevance	O
AM	O
and	O
confidence	O
measure	O
AM	O
.	O

20	O
.	O
The	O
system	O
of	O
claim	O
19	O
wherein	O
the	O
composite	O
AM	O
generator	O
generates	O
the	O
composite	O
AM	O
by	O
combining	O
counts	O
associated	O
with	O
each	O
variance	O
in	O
the	O
relevance	O
AM	O
and	O
the	O
confidence	O
measure	O
AM	O
.	O

21	O
.	O
An	O
acoustic	O
model	O
(	O
AM	O
)	O
generation	O
system	O
,	O
comprising	O
:	O
a	O
speech	O
recognizer	O
receiving	O
unsupervised	O
speech	O
data	O
in	O
the	O
form	O
of	O
utterances	O
with	O
sub-utterance	O
units	O
and	O
words	O
generating	O
a	O
transcription	O
of	O
the	O
utterances	O
and	O
a	O
confidence	O
measure	O
associated	O
with	O
each	O
sub-utterance	O
unit	O
;	O
an	O
AM	O
generator	O
receiving	O
the	O
transcription	O
and	O
confidence	O
measures	O
and	O
generating	O
a	O
confidence	O
measure	O
AM	O
by	O
weighting	O
each	O
word	O
in	O
the	O
utterances	O
with	O
a	O
confidence	O
measure;wherein	O
the	O
unsupervised	O
speech	O
data	O
comprises	O
a	O
task-independent	O
(	O
TI	O
)	O
data	O
including	O
wordsa	O
relevance	O
generator	O
generating	O
a	O
relevance	O
measure	O
for	O
each	O
word	O
in	O
the	O
TI	O
data	O
,	O
the	O
relevance	O
for	O
each	O
word	O
in	O
the	O
TI	O
data	O
,	O
the	O
relevance	O
measure	O
being	O
indicative	O
of	O
a	O
relevance	O
of	O
the	O
word	O
in	O
the	O
TI	O
data	O
to	O
a	O
desired	O
task	O
for	O
the	O
AM	O
.	O

22	O
.	O
The	O
system	O
of	O
claim	O
21	O
wherein	O
the	O
AM	O
generator	O
generates	O
the	O
confidence	O
measure	O
AM	O
by	O
weighting	O
words	O
in	O
the	O
TI	O
data	O
according	O
to	O
relevance	O
measures	O
associated	O
with	O
the	O
words	O
in	O
the	O
TI	O
data	O
.	O

