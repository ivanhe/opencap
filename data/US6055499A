US	O
6055499	O
A	O
20000425	O

US	O
09071214	O
19980501	O

eng	O
eng	O

US19980071214	O

20000425	O

20000425	O

7G	O
10L	O
15/02	O
A	O
7	O
G	O
10	O
L	O
15	O
02	O
A	O

G10L	O
15/00	O
20060101C	O
I20051008RMEP	O

20060101	O

C	O
G	O
10	O
L	O
15	O
00	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/02	O
20060101A	O
I20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
02	O
I	O

20051008	O

EP	O

R	O
M	O

US	O

704/250	O
704	O
250	O

704/207	O
704	O
207	O

704/E15.004	O
704	O
E15	O
.	O
004	O

G10L	O
15/02	O
G	O
10	O
L	O
15	O
02	O

US	O

704/205	O
704	O
205	O

US	O

704/206	O
704	O
206	O

US	O

704/207	O
704	O
207	O

US	O

704/208	O
704	O
208	O

US	O

704/216	O
704	O
216	O

US	O

704/217	O
704	O
217	O

US	O

704/250	O
704	O
250	O

US	O

704/255	O
704	O
255	O

US	O

704/256	O
704	O
256	O

18	O
Use	O
of	O
periodicity	O
and	O
jitter	O
for	O
automatic	O
speech	O
recognition	O

US	O
5611019	O
A	O
Nakatoh	O
et_al.	O

19970311	O

19940519	O

US	O

704/233	O
704	O
233	O

US	O
5729694	O
A	O
Holzrichter	O
et_al.	O

19980317	O

19960206	O

US	O

704/208	O
704	O
208	O

J.	B-Citation
Schoentgen	I-Citation
et_al.	I-Citation
,	I-Citation
Predictable	I-Citation
and	I-Citation
Random	I-Citation
Components	I-Citation
of	I-Citation
Jitter	I-Citation
:	I-Citation
,	I-Citation
Speech	I-Citation
Communication	I-Citation
,	I-Citation
No.	I-Citation
21	I-Citation
,	I-Citation
1997	I-Citation
,	I-Citation
pp.	I-Citation
255	I-Citation
272	I-Citation
.	O

B	B-Citation
H.	I-Citation
Juang	I-Citation
et_al.	I-Citation
,	I-Citation
Minimum	I-Citation
Classification	I-Citation
Error	I-Citation
Rate	I-Citation
Methods	I-Citation
for	I-Citation
Speech	I-Citation
Recognition	I-Citation
,	I-Citation
IEEE	I-Citation
Transactions	I-Citation
on	I-Citation
Speech	I-Citation
and	I-Citation
Audio	I-Citation
Processing	I-Citation
,	I-Citation
vol.	I-Citation
5	I-Citation
,	I-Citation
No.	I-Citation
3	I-Citation
,	I-Citation
May	I-Citation
,	I-Citation
1997	I-Citation
,	I-Citation
pp.	I-Citation
257	I-Citation
265	I-Citation
.	O

E.	B-Citation
L.	I-Citation
Bocchieri	I-Citation
et_al.	I-Citation
,	I-Citation
Discriminative	I-Citation
Feature	I-Citation
Selection	I-Citation
for	I-Citation
Speech	I-Citation
Recognition	I-Citation
,	I-Citation
Computer	I-Citation
Speech	I-Citation
and	I-Citation
Language	I-Citation
,	I-Citation
(	I-Citation
1993	I-Citation
)	I-Citation
7	I-Citation
,	I-Citation
pp.	I-Citation
229	I-Citation
246	I-Citation
.	O

D.	B-Citation
P.	I-Citation
Prezas	I-Citation
et_al.	I-Citation
,	I-Citation
Fast	I-Citation
and	I-Citation
Accurate	I-Citation
Pitch	I-Citation
Detection	I-Citation
Using	I-Citation
Pattern	I-Citation
Recognition	I-Citation
and	I-Citation
Adaptive	I-Citation
Time	I-Citation
Domain	I-Citation
Analysis	I-Citation
,	I-Citation
ICASSP	I-Citation
86	I-Citation
,	I-Citation
Tokyo	I-Citation
,	I-Citation
pp.	I-Citation
109	I-Citation
112	I-Citation
.	O

W.	B-Citation
Chou	I-Citation
et_al.	I-Citation
,	I-Citation
Signal	I-Citation
Conditioned	I-Citation
Minimum	I-Citation
Error	I-Citation
Rate	I-Citation
Training	I-Citation
,	I-Citation
Eurospeech	I-Citation
95	I-Citation
pp.	I-Citation
495	I-Citation
498	I-Citation
.	O

B.	B-Citation
H.	I-Citation
Juang	I-Citation
et_al.	I-Citation
,	I-Citation
On	I-Citation
the	I-Citation
Use	I-Citation
of	I-Citation
Bandpass	I-Citation
Liftering	I-Citation
in	I-Citation
Speech	I-Citation
Recognition	I-Citation
,	I-Citation
ICASSP	I-Citation
86	I-Citation
,	I-Citation
Tokyo	I-Citation
,	I-Citation
pp.	I-Citation
765	I-Citation
768	I-Citation
.	O

B.	O
S.	B-Citation
Atal	I-Citation
et_al.	I-Citation
,	I-Citation
A	I-Citation
Pattern	I-Citation
Recognition	I-Citation
Approach	I-Citation
to	I-Citation
Voiced	I-Citation
Unvoiced	I-Citation
Silence	I-Citation
Classification	I-Citation
with	I-Citation
Applications	I-Citation
to	I-Citation
Speech	I-Citation
Recognition	I-Citation
,	I-Citation
IEEE	I-Citation
Transactions	I-Citation
On	I-Citation
Acoustics	I-Citation
,	I-Citation
Speech	I-Citation
,	I-Citation
And	I-Citation
Signal	I-Citation
Processing	I-Citation
,	I-Citation
vol.	I-Citation
ASSP	I-Citation
24	I-Citation
,	I-Citation
No.	I-Citation
3	I-Citation
,	I-Citation
Jun.	I-Citation
,	I-Citation
1976	I-Citation
,	I-Citation
pp.	I-Citation
201	I-Citation
212	I-Citation
.	O

US	O
7180892	O
B1	O
20070220	O

20000901	O

US	O
7653536	O
B2	O
20100126	O

20070220	O

US	O
7231350	O
B2	O
20070612	O

20051221	O

US	O
7016833	O
B2	O
20060321	O

20010612	O

US	O
6453283	O
B1	O
20020917	O

19990507	O

US	O
6285980	O
B1	O
20010904	O

19981102	O

JP	O
4182444	O
B9	O
20080912	O

20060609	O

US	O
7908137	O
B2	O
20110315	O

20070608	O

Lucent	O
Technologies	O
Inc.	O

Murray	O
Hill	O
US	O

Chengalvarayan	O
Rathinavelu	O

Lisle	O
US	O

Thomson	O
David	O
Lynn	O

Lisle	O
US	O

Penrod	O
Jack	O
R.	O

Dorvil	O
;	O
Richemond	O

US	O
6055499	O
A	O
20000425	O

19980501	O

US	O
6055499	O
A	O
20000425	O

19980501	O

A	O
class	O
of	O
features	O
related	O
to	O
voicing	O
parameters	O
that	O
indicate	O
whether	O
the	O
vocal	O
chords	O
are	O
vibrating	O
.	O
Features	O
describing	O
voicing	O
characteristics	O
of	O
speech	O
signals	O
are	O
integrated	O
with	O
an	O
existing	O
38-dimensional	O
feature	O
vector	O
consisting	O
of	O
first	O
and	O
second	O
order	O
time	O
derivatives	O
of	O
the	O
frame	O
energy	O
and	O
of	O
the	O
cepstral	O
coefficients	O
with	O
their	O
first	O
and	O
second	O
derivatives	O
.	O
Hidden	O
Markov	O
Model	O
(	O
HMM	O
)	O
-	O
based	O
connected	O
digit	O
recognition	O
experiments	O
comparing	O
the	O
traditional	O
and	O
extended	O
feature	O
sets	O
show	O
that	O
voicing	O
features	O
and	O
spectral	O
information	O
are	O
complementary	O
and	O
that	O
improved	O
speech	O
recognition	O
performance	O
is	O
obtained	O
by	O
combining	O
the	O
two	O
sources	O
of	O
information	O
.	O

19980501	O

AS	O
ASSIGNMENT	O
N	O
US	O
6055499A	O
LUCENT	O
TECHNOLOGIES	O
INC.	O
,	O
NEW	O
JERSEY	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNORS:CHENGALVARAYAN	O
,	O
RATHINAVELU;THOMSON	O
,	O
DAVID	O
LYNN;REEL/FRAME:009208/0889	O

19980501	O

20010405	O

AS	O
ASSIGNMENT	O
N	O
US	O
6055499A	O
THE	O
CHASE	O
MANHATTAN	O
BANK	O
,	O
AS	O
COLLATERAL	O
AGENT	O
,	O
TEX	O
CONDITIONAL	O
ASSIGNMENT	O
OF	O
AND	O
SECURITY	O
INTEREST	O
IN	O
PATENT	O
RIGHTS;ASSIGNOR:LUCENT	O
TECHNOLOGIES	O
INC.	O
(	O
DE	O
CORPORATION	O
)	O
;REEL/FRAME:011722/0048	O

20010222	O

20030919	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
6055499A	O
4	O

20061206	O

AS	O
ASSIGNMENT	O
C	O
US	O
6055499A	O
LUCENT	O
TECHNOLOGIES	O
INC.	O
,	O
NEW	O
JERSEY	O
TERMINATION	O
AND	O
RELEASE	O
OF	O
SECURITY	O
INTEREST	O
IN	O
PATENT	O
RIGHTS;ASSIGNOR:JPMORGAN	O
CHASE	O
BANK	O
,	O
N.	O
A.	O
(	O
FORMERLY	O
KNOWN	O
AS	O
THE	O
CHASE	O
MANHATTAN	O
BANK	O
)	O
,	O
AS	O
ADMINISTRATIVE	O
AGENT;REEL/FRAME:018590/0047	O

20061130	O

20071105	O

REMI	O
MAINTENANCE	O
FEE	O
REMINDER	O
MAILED	O
N	O
US	O
6055499A	O

20080425	O

LAPS	O
-	O
LAPSE	O
FOR	O
FAILURE	O
TO	O
PAY	O
MAINTENANCE	O
FEES	O
N	O
US	O
6055499A	O

20080617	O

FP	O
-	O
EXPIRED	O
DUE	O
TO	O
FAILURE	O
TO	O
PAY	O
MAINTENANCE	O
FEE	O
C	O
US	O
6055499A	O

20080425	O

TECHNICAL	O
FIELD	O
The	O
invention	O
relates	O
to	O
automatic	O
speech	O
recognition	O
and	O
more	O
particularly	O
to	O
a	O
method	O
and	O
apparatus	O
for	O
improved	O
recognition	O
using	O
features	O
heretofore	O
unused	O
for	O
speech	O
recognition	O
.	O

DESCRIPTION	O
OF	O
THE	O
PRIOR	O
ART	O
Historically	O
pitch	O
and	O
voicing	O
have	O
been	O
widely	O
used	O
in	O
speech	O
coding	O
,	O
but	O
not	O
in	O
speech	O
recognition	O
.	O
Speech	O
coding	O
methods	O
for	O
voiced-unvoiced	O
decision	O
making	O
often	O
work	O
in	O
conjunction	O
with	O
pitch	O
analysis	O
.	O
Voiced-unvoiced	O
decisions	O
can	O
be	O
determined	O
with	O
reasonable	O
accuracy	O
from	O
spectral	O
coefficients	O
,	O
since	O
unvoiced	O
`	O
speech	O
`	O
,	O
such	O
as	O
fricatives	O
,	O
consonants	O
and	O
background	O
noise	O
,	O
tends	O
to	O
contain	O
stronger	O
high	O
frequency	O
components	O
than	O
voiced	O
speech	O
.	O
However	O
,	O
previous	O
experimental	O
evidence	O
has	O
not	O
shown	O
that	O
voicing	O
features	O
derived	O
from	O
spectral	O
coefficients	O
improve	O
speech	O
recognition	O
error	O
rates	O
over	O
the	O
use	O
of	O
spectral	O
coefficients	O
alone	O
.	O
This	O
is	O
unfortunate	O
,	O
because	O
over	O
the	O
last	O
several	O
years	O
,	O
one	O
of	O
the	O
major	O
factors	O
in	O
reducing	O
the	O
error	O
rate	O
in	O
speech	O
recognition	O
systems	O
has	O
been	O
the	O
addition	O
of	O
new	O
feature	O
components	O
to	O
the	O
frame	O
vectors	O
.	O

Thus	O
,	O
there	O
is	O
a	O
need	O
in	O
the	O
art	O
for	O
an	O
automatic	O
speech	O
recognition	O
system	O
which	O
uses	O
pitch	O
and	O
voicing	O
information	O
in	O
a	O
way	O
that	O
reduces	O
automatic	O
speech	O
recognition	O
error	O
rates	O
.	O

SUMMARY	O
OF	O
THE	O
INVENTION	O
Briefly	O
stated	O
,	O
in	O
accordance	O
with	O
one	O
aspect	O
of	O
the	O
invention	O
,	O
the	O
aforementioned	O
problem	O
has	O
been	O
solved	O
and	O
a	O
technological	O
advance	O
achieved	O
by	O
providing	O
the	O
aforementioned	O
problem	O
has	O
been	O
solved	O
and	O
a	O
technological	O
advance	O
achieved	O
by	O
providing	O
a	O
method	O
for	O
speech	O
recognition	O
which	O
includes	O
the	O
steps	O
of	O
:	O
starting	O
with	O
a	O
standard	O
feature	O
vector	O
;	O
including	O
voicing	O
features	O
to	O
this	O
standard	O
feature	O
vector	O
;	O
and	O
using	O
this	O
standard	O
feature	O
vector	O
with	O
these	O
included	O
features	O
to	O
recognize	O
speech	O
.	O

In	O
accordance	O
with	O
another	O
aspect	O
of	O
the	O
invention	O
,	O
the	O
aforementioned	O
problem	O
has	O
been	O
solved	O
and	O
a	O
technological	O
advanced	O
made	O
by	O
providing	O
an	O
apparatus	O
for	O
speech	O
recognition	O
that	O
includes	O
a	O
device	O
for	O
determining	O
a	O
standard	O
feature	O
vector	O
;	O
a	O
device	O
for	O
including	O
voicing	O
features	O
to	O
this	O
standard	O
feature	O
vector	O
;	O
and	O
a	O
device	O
for	O
using	O
this	O
standard	O
feature	O
vector	O
and	O
this	O
included	O
voicing	O
features	O
to	O
recognize	O
speech	O
.	O
An	O
error	O
rate	O
of	O
this	O
apparatus	O
for	O
speech	O
recognition	O
is	O
reduced	O
over	O
comparable	O
speech	O
recognizers	O
because	O
of	O
the	O
robustness	O
of	O
the	O
speech	O
recognition	O
added	O
by	O
the	O
inclusion	O
of	O
voicing	O
features	O
.	O

BRIEF	O
DESCRIPTION	O
OF	O
THE	O
DRAWING	O
FIG	O
.	O
1	O
is	O
a	O
block	O
diagram	O
of	O
a	O
system	O
for	O
providing	O
automatic	O
speech	O
recognition	O
.	O

FIG	O
.	O
2	O
is	O
a	O
time	O
plot	O
of	O
a	O
voicing	O
sample	O
useful	O
for	O
understanding	O
the	O
invention	O
.	O

FIG	O
.	O
3	O
,	O
is	O
a	O
table	O
which	O
illustrates	O
training	O
and	O
testing	O
sets	O
of	O
a	O
database	O
used	O
in	O
one	O
embodiment	O
of	O
the	O
invention	O
.	O

FIG	O
.	O
4	O
is	O
a	O
flow	O
chart	O
illustrating	O
an	O
embodiment	O
of	O
the	O
invention	O
.	O

DETAILED	O
DESCRIPTION	O
Referring	O
now	O
to	O
FIG	O
.	O
1	O
,	O
a	O
system	O
100	O
for	O
implementing	O
the	O
invention	O
is	O
shown	O
.	O
The	O
system	O
100	O
has	O
a	O
processor	O
102	O
for	O
receiving	O
and	O
responding	O
to	O
instructions	O
and	O
processing	O
data	O
.	O
The	O
instructions	O
are	O
typically	O
stored	O
in	O
memory	O
104	O
,	O
which	O
may	O
contain	O
both	O
RAM	O
106	O
as	O
well	O
as	O
ROM	O
108	O
.	O
The	O
instructions	O
that	O
are	O
not	O
previously	O
in	O
ROM	O
108	O
are	O
typically	O
loaded	O
from	O
disk	O
114	O
into	O
RAM	O
106	O
with	O
the	O
assistance	O
of	O
disk	O
interface	O
112	O
.	O
RAM	O
106	O
,	O
disk	O
114	O
and	O
interface	O
112	O
are	O
also	O
used	O
for	O
the	O
storage	O
and	O
retrieval	O
of	O
speech	O
modeling	O
data	O
which	O
includes	O
training	O
data	O
and	O
connected	O
digit	O
database	O
115	O
,	O
as	O
will	O
be	O
explained	O
later	O
.	O

System	O
100	O
also	O
has	O
a	O
filter	O
116	O
and	O
a	O
digitizer	O
118	O
which	O
operate	O
on	O
incoming	O
speech	O
.	O
The	O
digitizer	O
118	O
is	O
connected	O
to	O
and	O
works	O
in	O
conjunction	O
with	O
filter	O
116	O
.	O
The	O
filter	O
116	O
is	O
an	O
anti-aliasing	O
type	O
which	O
removes	O
some	O
of	O
the	O
negative	O
effects	O
of	O
digitizing	O
the	O
input	O
speech	O
.	O
Filter	O
116	O
also	O
operates	O
as	O
a	O
buffer	O
memory	O
for	O
holding	O
one	O
or	O
more	O
units	O
,	O
such	O
as	O
a	O
frame	O
,	O
for	O
reading	O
or	O
retrieval	O
under	O
control	O
of	O
processor	O
102	O
.	O

Input	O
speech	O
is	O
segmented	O
into	O
overlapping	O
frames	O
30	O
msec	O
long	O
with	O
centers	O
10	O
msec	O
apart	O
.	O
Each	O
frame	O
is	O
processed	O
to	O
provide	O
12	O
LPC-derived	O
liftered	O
cepstral	O
coefficients	O
along	O
with	O
energy	O
and	O
voicing	O
features	O
.	O
Liftering	O
is	O
a	O
type	O
of	O
filtering	O
using	O
a	O
raised	O
cosine	O
weighting	O
function	O
.	O
Since	O
the	O
speech	O
input	O
signal	O
has	O
been	O
recorded	O
under	O
various	O
telephone	O
conditions	O
and	O
with	O
various	O
different	O
transducer	O
equipment	O
,	O
each	O
cepstral	O
feature	O
vector	O
is	O
further	O
processed	O
using	O
the	O
hierarchical	O
signal	O
bias	O
removal	O
(	O
HSBR	O
)	O
method	O
,	O
which	O
is	O
known	O
from	O
a	O
Chou	B-Citation
,	I-Citation
Rahim	I-Citation
and	I-Citation
Buhrke	I-Citation
article	I-Citation
entitled	I-Citation
"	I-Citation
Signal	I-Citation
conditioned	I-Citation
minimum	I-Citation
error	I-Citation
rate	I-Citation
training	I-Citation
"	I-Citation
,	O
in	O
order	O
to	O
reduce	O
the	O
effect	O
of	O
channel	O
distortion	O
.	O
The	O
combination	O
of	O
the	O
12	O
LPC-derived	O
liftered	O
cepstral	O
coefficients	O
and	O
voicing	O
features	O
are	O
united	O
into	O
a	O
feature	O
vector	O
that	O
is	O
subsequently	O
augmented	O
with	O
its	O
first	O
and	O
second	O
order	O
time	O
derivatives	O
resulting	O
in	O
two	O
different	O
feature	O
dimensions	O
as	O
explained	O
later	O
below	O
.	O

Discriminative	O
training	O
of	O
the	O
system	O
is	O
necessary	O
because	O
of	O
the	O
strong	O
correlation	O
between	O
voicing	O
and	O
the	O
first	O
spectral	O
coefficient	O
.	O
The	O
results	O
of	O
the	O
training	O
for	O
a	O
traditional	O
maximum	O
likelihood	O
(	O
ML	O
)	O
recognizer	O
and	O
for	O
a	O
minimum	O
string	O
error	O
(	O
MSE	O
)	O
recognizer	O
give	O
insights	O
to	O
the	O
effects	O
of	O
the	O
voicing	O
information	O
into	O
two	O
different	O
training	O
approaches	O
to	O
speech	O
recognizers	O
.	O
The	O
results	O
demonstrate	O
that	O
that	O
the	O
addition	O
of	O
two	O
voicing	O
parameters	O
called	O
periodicity	O
and	O
jitter	O
along	O
with	O
other	O
more	O
general	O
voicing	O
features	O
yield	O
a	O
speech	O
recognition	O
that	O
is	O
more	O
robust	O
.	O
This	O
is	O
due	O
in	O
part	O
to	O
the	O
fact	O
that	O
these	O
voicing	O
features	O
are	O
relatively	O
insensitive	O
to	O
differences	O
in	O
transmission	O
conditions	O
.	O

Periodicity	O
and	O
jitter	O
are	O
defined	O
as	O
follows	O
:	O
Periodicity	O
is	O
a	O
measure	O
of	O
the	O
periodic	O
structure	O
of	O
speech	O
.	O
Jitter	O
is	O
the	O
small	O
fluctuations	O
in	O
glottal	O
cycle	O
lengths	O
.	O
Jitter	O
has	O
been	O
recently	O
described	O
by	O
J.	B-Citation
Schoentgen	I-Citation
and	I-Citation
R.	I-Citation
Guchteneere	I-Citation
in	I-Citation
their	I-Citation
article	I-Citation
"	I-Citation
Predictable	I-Citation
and	I-Citation
random	I-Citation
components	I-Citation
of	I-Citation
Jitter	I-Citation
"	I-Citation
by	O
means	O
of	O
a	O
statistical	O
time	O
series	O
model	O
.	O
Both	O
periodicity	O
and	O
jitter	O
are	O
derived	O
from	O
pitch	O
analysis	O
.	O
There	O
are	O
a	O
number	O
of	O
known	O
methods	O
for	O
pitch	O
estimation	O
of	O
speech	O
signal	O
described	O
in	O
the	O
article	O
"	B-Citation
Fast	I-Citation
and	I-Citation
accurate	I-Citation
pitch	I-Citation
detection	I-Citation
using	I-Citation
pattern	I-Citation
recognition	I-Citation
and	I-Citation
adaptive	I-Citation
time	I-Citation
domain	I-Citation
analysis	I-Citation
"	I-Citation
by	I-Citation
Prezas	I-Citation
,	I-Citation
J.	I-Citation
Picone	I-Citation
and	I-Citation
D.	I-Citation
Thomson	I-Citation
.	O
The	O
pitch	O
estimation	O
technique	O
used	O
in	O
one	O
embodiment	O
of	O
the	O
present	O
invention	O
is	O
based	O
on	O
the	O
short-time	O
auto-correlation	O
function	O
given	O
by	O
:	O
#	O
#	O
EQU1	O
#	O
#	O
where	O
i	O
is	O
the	O
index	O
of	O
the	O
starting	O
sample	O
of	O
the	O
frame	O
and	O
N	O
(	O
corresponding	O
to	O
30	O
msec	O
)	O
is	O
the	O
frame	O
length	O
.	O
In	O
general	O
,	O
female	O
speech	O
has	O
higher	O
pitch	O
(	O
120	O
to	O
200	O
hz	O
)	O
than	O
male	O
speech	O
(	O
60	O
to	O
120	O
hz	O
)	O
.	O
The	O
range	O
of	O
delays	O
considered	O
spans	O
the	O
pitch	O
period	O
values	O
most	O
likely	O
to	O
occur	O
in	O
speech	O
(	O
20	O
to	O
120	O
samples	O
or	O
66	O
hz	O
to	O
400	O
hz	O
)	O
.	O
The	O
autocorrelation	O
function	O
is	O
normalized	O
with	O
the	O
peak	O
at	O
m	O
=	O
0	O
so	O
that	O
the	O
ratio	O
lies	O
between	O
0	O
and	O
1	O
.	O
The	O
largest	O
peak	O
in	O
the	O
normalized	O
function	O
is	O
chosen	O
as	O
the	O
estimate	O
of	O
the	O
pitch	O
period	O
and	O
the	O
value	O
of	O
the	O
pitch	O
becomes	O
the	O
periodicity	O
measure	O
.	O
#	O
#	O
EQU2	O
#	O
#	O
This	O
voicing	O
function	O
is	O
a	O
measure	O
of	O
how	O
strongly	O
periodic	O
the	O
speech	O
frame	O
is	O
.	O
It	O
is	O
often	O
used	O
to	O
make	O
a	O
voiced/unvoiced	O
decision	O
by	O
applying	O
a	O
threshold	O
.	O
For	O
speech	O
recognition	O
,	O
we	O
treat	O
it	O
as	O
an	O
indicator	O
of	O
the	O
probability	O
that	O
a	O
given	O
frame	O
is	O
voiced	O
.	O
Voicing	O
is	O
computed	O
every	O
10	O
msec	O
to	O
match	O
the	O
10	O
msec	O
frame	O
rate	O
of	O
the	O
speech	O
recognizer	O
.	O

Another	O
voicing	O
parameter	O
useful	O
in	O
speech	O
recognition	O
is	O
the	O
variation	O
in	O
estimated	O
pitch	O
between	O
frames	O
.	O
Whereas	O
the	O
pitch	O
in	O
voiced	O
speech	O
is	O
relatively	O
constant	O
,	O
the	O
measured	O
pitch	O
of	O
an	O
unvoiced	O
frame	O
is	O
essentially	O
random	O
,	O
since	O
most	O
unvoiced	O
speech	O
consists	O
of	O
noise	O
and	O
other	O
aperiodic	O
signals	O
.	O
The	O
change	O
in	O
pitch	O
between	O
frames	O
,	O
therefore	O
,	O
is	O
an	O
indicator	O
of	O
voicing	O
.	O
As	O
a	O
measure	O
of	O
change	O
of	O
pitch	O
,	O
a	O
variation	O
function	O
is	O
defined	O
as	O
:	O
V.	O
sub	O
.	O
n	O
=.vertline.P.sub.n	O
-	O
P.	O
sub	O
.	O
n-1	O
.	O
vertline	O
.	O
,	O
where	O
n	O
is	O
the	O
index	O
of	O
the	O
current	O
frame	O
and	O
P	O
is	O
the	O
measured	O
pitch	O
period	O
for	O
that	O
frame	O
.	O

One	O
complication	O
in	O
measuring	O
pitch	O
variation	O
is	O
pitch	O
multiplication	O
and	O
division	O
.	O
If	O
the	O
peak	O
at	O
the	O
n	O
th	O
sample	O
in	O
the	O
autocorrelation	O
function	O
corresponds	O
to	O
the	O
pitch	O
period	O
,	O
there	O
are	O
usually	O
also	O
peaks	O
at	O
k	O
x	O
n	O
,	O
where	O
k	O
is	O
an	O
integer	O
.	O
Peaks	O
at	O
k	O
x	O
n	O
are	O
sometimes	O
larger	O
than	O
the	O
peak	O
at	O
n	O
,	O
and	O
can	O
be	O
chosen	O
as	O
the	O
estimate	O
of	O
the	O
pitch	O
period	O
.	O
While	O
this	O
does	O
not	O
significantly	O
affect	O
the	O
periodicity	O
measure	O
,	O
it	O
must	O
be	O
taken	O
into	O
account	O
when	O
estimating	O
jitter	O
.	O
If	O
the	O
pitch	O
period	O
changes	O
from	O
n	O
to	O
2xn	O
,	O
for	O
example	O
,	O
the	O
pitch	O
variation	O
is	O
generally	O
considered	O
to	O
be	O
zero	O
.	O
So	O
,	O
to	O
allow	O
for	O
pitch	O
multiplication	O
and	O
division	O
a	O
variation	O
function	O
is	O
defined	O
as	O
follows	O
:	O
#	O
#	O
EQU3	O
#	O
#	O
where	O
j	O
and	O
k	O
are	O
integers	O
corresponding	O
to	O
the	O
pitch	O
multipliers	O
for	O
the	O
previous	O
and	O
current	O
frames	O
,	O
respectively	O
.	O
The	O
range	O
of	O
values	O
allowed	O
for	O
j	O
and	O
k	O
are	O
selected	O
to	O
minimize	O
the	O
expected	O
variation	O
function	O
for	O
voiced	O
speech	O
and	O
maximize	O
its	O
expected	O
value	O
for	O
unvoiced	O
speech	O
.	O
A	O
set	O
of	O
values	O
that	O
effectively	O
separate	O
voiced	O
from	O
unvoiced	O
speech	O
in	O
one	O
embodiment	O
of	O
the	O
invention	O
have	O
been	O
determined	O
experimentally	O
to	O
be	O
:	O
(	O
j	O
,	O
k	O
)	O
.	O
epsilon	O
.	O
[	O
(	O
1	O
,	O
1	O
)	O
,	O
(	O
1	O
,	O
2	O
)	O
,	O
(	O
2	O
,	O
1	O
)	O
,	O
(	O
3	O
,	O
1	O
)	O
,	O
(	O
1	O
,	O
3	O
)	O
]	O
.	O

These	O
values	O
provide	O
for	O
pitch	O
doubling	O
and	O
tripling	O
.	O
The	O
pitch	O
multiplier	O
is	O
also	O
allowed	O
to	O
change	O
from	O
double	O
to	O
triple	O
and	O
vice	O
versa	O
by	O
permitting	O
the	O
following	O
additional	O
values	O
:	O
#	O
#	O
EQU4	O
#	O
#	O
where	O
j	O
*	O
and	O
k	O
*	O
are	O
the	O
values	O
of	O
j	O
and	O
k	O
from	O
the	O
previous	O
frame	O
pair	O
n-1	O
and	O
n-2	O
.	O

More	O
combinations	O
are	O
possible	O
,	O
but	O
the	O
number	O
is	O
limited	O
because	O
if	O
too	O
many	O
are	O
permitted	O
,	O
unvoiced	O
speech	O
is	O
increasingly	O
likely	O
to	O
yield	O
a	O
small	O
value	O
for	O
the	O
variation	O
function	O
.	O
Once	O
the	O
variation	O
function	O
is	O
computed	O
between	O
frame	O
n	O
and	O
the	O
two	O
adjacent	O
frames	O
n-1	O
and	O
n	O
+	O
1	O
,	O
the	O
jitter	O
is	O
computed	O
as	O
an	O
average	O
of	O
the	O
two	O
variation	O
functions	O
,	O
normalized	O
by	O
the	O
average	O
pitch	O
for	O
the	O
three	O
frames	O
#	O
#	O
EQU5	O
#	O
#	O
FIG	O
.	O
2	O
illustrates	O
the	O
measured	O
of	O
jitter	O
and	O
periodicity	O
for	O
a	O
typical	O
digit	O
string	O
"	O
341898291659603	O
"	O
spoken	O
by	O
a	O
male	O
speaker	O
.	O
It	O
is	O
observed	O
that	O
the	O
periodicity	O
is	O
about	O
1	O
.	O
0	O
and	O
jitter	O
is	O
about	O
zero	O
for	O
voiced	O
speech	O
.	O
For	O
unvoiced	O
speech	O
,	O
periodicity	O
is	O
between	O
zero	O
and	O
0	O
.	O
5	O
and	O
jitter	O
is	O
a	O
random	O
variable	O
between	O
about	O
0	O
and	O
1	O
.	O
(	O
Silence	O
is	O
considered	O
unvoiced	O
.	O
)	O
FIG	O
.	O
2	O
suggests	O
speech	O
segments	O
can	O
be	O
reliably	O
classified	O
as	O
voiced	O
or	O
unvoiced	O
based	O
on	O
periodicity	O
and	O
jitter	O
measurements	O
.	O

Regarding	O
the	O
speech	O
models	O
,	O
there	O
are	O
two	O
methods	O
for	O
obtaining	O
estimates	O
of	O
the	O
Hidden	O
Markov	O
Model	O
(	O
HMM	O
)	O
parameters	O
namely	O
the	O
conventional	O
maximum	O
likelihood	O
(	O
ML	O
)	O
algorithm	O
,	O
and	O
a	O
more	O
effective	O
minimum	O
string	O
error	O
(	O
MSE	O
)	O
training	O
procedure	O
.	O
For	O
ML	O
training	O
,	O
the	O
segmental	O
k-means	O
training	O
procedure	O
was	O
used	O
.	O
This	O
training	O
procedure	O
is	O
similar	O
to	O
that	O
known	O
from	O
B.	O
Juang	O
and	O
L.	O
Rabiner	O
article	O
entitled	O
"	O
The	O
segmental	O
k-means	O
algorithm	O
for	O
estimating	O
parameters	O
of	O
hidden	O
Markov	O
models	O
"	O
.	O
The	O
MSE	O
training	O
directly	O
applies	O
discriminative	O
analysis	O
techniques	O
to	O
string	O
level	O
acoustic	O
model	O
matching	O
,	O
thereby	O
allowing	O
minimum	O
error	O
rate	O
training	O
to	O
be	O
implemented	O
at	O
the	O
string	O
level	O
.	O
A	O
similar	O
MSE	O
training	O
was	O
shown	O
by	O
B.	B-Citation
Juang	I-Citation
,	I-Citation
W.	I-Citation
Chou	I-Citation
and	I-Citation
C.	I-Citation
H.	I-Citation
Lee	I-Citation
in	I-Citation
their	I-Citation
article	I-Citation
"	I-Citation
Minimum	I-Citation
classification	I-Citation
error	I-Citation
rate	I-Citation
methods	I-Citation
for	I-Citation
speech	I-Citation
recognition	I-Citation
"	I-Citation
.	O
A	O
brief	O
formulation	O
of	O
the	O
MSE	O
algorithm	O
using	O
generalized	O
probabilistic	O
descent	O
(	O
GPD	O
)	O
method	O
is	O
as	O
follows	O
:	O
A	O
discriminant	O
function	O
in	O
MSE	O
training	O
is	O
defined	O
as	O
g	O
(	O
O,S.sub.k,.LAMBDA	O
.	O
)	O
=log.function	O
.	O
(	O
O,.THETA..sub.S.sbsb.k,S.sub.k	O
.vertline..LAMBDA	O
.	O
)	O
,	O
where	O
S.	O
sub	O
.	O
k	O
is	O
the	O
k-th	O
best	O
string	O
,	O
.	O
LAMBDA	O
.	O
is	O
the	O
HMM	O
set	O
used	O
in	O
the	O
N-best	O
decoding	O
,	O
.THETA..sub.k	O
is	O
the	O
optimal	O
state	O
sequence	O
of	O
the	O
k-th	O
string	O
given	O
the	O
model	O
set	O
.	O
LAMBDA	O
.	O
,	O
and	O
log	O
f	O
(	O
O,.THETA..sub.Sk	O
]	O
,	O
S.	O
sub	O
.	O
k	O
.vertline..LAMBDA	O
.	O
)	O
is	O
the	O
related	O
log-likelihood	O
score	O
on	O
the	O
optimal	O
path	O
of	O
the	O
k-th	O
string	O
.	O

The	O
misclassification	O
measure	O
is	O
determined	O
by	O
#	O
#	O
EQU6	O
#	O
#	O
which	O
provides	O
an	O
acoustic	O
confusability	O
measure	O
between	O
the	O
correct	O
and	O
competing	O
string	O
models	O
.	O

The	O
loss	O
function	O
is	O
defined	O
as	O
#	O
#	O
EQU7	O
#	O
#	O
where	O
.	O
gamma	O
.	O
is	O
a	O
positive	O
constant	O
,	O
which	O
controls	O
the	O
slope	O
of	O
the	O
sigmoid	O
function	O
.	O

The	O
model	O
parameters	O
are	O
updated	O
sequentially	O
according	O
to	O
the	O
GPD	O
algorith	O
m	O
.LAMBDA..sub.n+1	O
=.LAMBDA..sub.n	O
-	O
.epsilon..gradient.l	O
(	O
O	O
,	O
.	O
LAMBDA	O
.	O
)	O
.LAMBDA..sub.n	O
is	O
the	O
parameter	O
set	O
at	O
the	O
n-th	O
iteration	O
,	O
.gradient.l	O
(	O
O	O
,	O
.	O
LAMBDA	O
.	O
)	O
is	O
the	O
gradient	O
of	O
the	O
loss	O
function	O
for	O
the	O
training	O
sample	O
0	O
which	O
belongs	O
to	O
the	O
correct	O
class	O
C	O
,	O
and	O
.	O
epsilon	O
.	O
is	O
a	O
small	O
positive	O
learning	O
constant	O
.	O

For	O
the	O
present	O
invention	O
,	O
only	O
the	O
results	O
obtained	O
by	O
sequential	O
training	O
are	O
given	O
.	O
During	O
the	O
model	O
training	O
phase	O
one	O
complete	O
pass	O
through	O
the	O
training	O
data	O
set	O
is	O
referred	O
to	O
as	O
an	O
epoch	O
.	O
For	O
the	O
case	O
of	O
string-by-string	O
training	O
,	O
model	O
parameters	O
are	O
updated	O
several	O
times	O
over	O
an	O
epoch	O
.	O

The	O
database	O
used	O
in	O
for	O
training	O
and	O
testing	O
in	O
the	O
present	O
invention	O
was	O
a	O
connected	O
digit	O
database	O
115	O
that	O
has	O
speech	O
diversity	O
.	O
It	O
was	O
a	O
compilation	O
of	O
databases	O
collected	O
during	O
several	O
independent	O
data	O
collection	O
efforts	O
,	O
field	O
trials	O
,	O
and	O
live	O
service	O
deployments	O
.	O
These	O
independent	O
databases	O
are	O
denoted	O
as	O
DB1	O
through	O
DB6	O
.	O
The	O
connected	O
digit	O
database	O
115	O
contains	O
the	O
English	O
digits	O
one	O
through	O
nine	O
,	O
zero	O
and	O
oh	O
.	O
It	O
ranges	O
in	O
scope	O
from	O
one	O
where	O
the	O
talkers	O
read	O
prepared	O
lists	O
of	O
digit	O
strings	O
to	O
one	O
where	O
the	O
customers	O
actually	O
use	O
an	O
recognition	O
system	O
to	O
access	O
information	O
about	O
their	O
credit	O
card	O
accounts	O
.	O
The	O
data	O
were	O
collected	O
over	O
network	O
channels	O
using	O
a	O
variety	O
of	O
telephone	O
handsets	O
.	O
Digital	O
string	O
lengths	O
range	O
from	O
1	O
to	O
16	O
digits	O
.	O
For	O
one	O
embodiment	O
of	O
the	O
present	O
invention	O
,	O
the	O
connected	O
digit	O
database	O
115	O
was	O
divided	O
into	O
a	O
training	O
set	O
and	O
a	O
testing	O
set	O
.	O
The	O
training	O
set	O
,	O
DB1	O
through	O
DB3	O
,	O
includes	O
both	O
read	O
and	O
spontaneous	O
digit	O
input	O
from	O
a	O
variety	O
of	O
network	O
channels	O
,	O
microphones	O
and	O
dialect	O
regions	O
.	O
The	O
testing	O
set	O
is	O
designed	O
to	O
have	O
data	O
strings	O
from	O
both	O
matched	O
and	O
mismatched	O
environmental	O
conditions	O
and	O
includes	O
all	O
six	O
databases	O
.	O
All	O
recordings	O
in	O
the	O
training	O
and	O
testing	O
set	O
are	O
valid	O
digit	O
strings	O
,	O
totaling	O
7282	O
and	O
13114	O
strings	O
for	O
training	O
and	O
testing	O
,	O
respectively	O
.	O
The	O
data	O
distribution	O
of	O
the	O
training	O
and	O
testing	O
set	O
is	O
shown	O
in	O
FIG	O
.	O
3	O
,	O
which	O
shows	O
a	O
table	O
of	O
regional	O
distributions	O
of	O
spoken	O
digit	O
strings	O
and	O
the	O
speaker	O
population	O
among	O
the	O
training	O
and	O
testing	O
sets	O
of	O
the	O
connected	O
digit	O
database	O
115	O
.	O

The	O
feature	O
vector	O
used	O
in	O
one	O
embodiment	O
of	O
the	O
present	O
invention	O
starts	O
with	O
the	O
well-known	O
38	O
dimensional	O
frame	O
vector	O
as	O
a	O
baseline	O
system	O
and	O
augments	O
that	O
vector	O
.	O
Baseline	O
analysis	O
was	O
performed	O
on	O
the	O
38-dimensional	O
frame	O
vector	O
DDCEP+consisting	O
of	O
the	O
cepstrum	O
,	O
delta	O
cepstrum	O
,	O
delta-delta	O
cepstrum	O
,	O
delta	O
energy	O
and	O
delta-delta	O
energy	O
.	O
The	O
DDCEP	O
*	O
feature	O
set	O
has	O
44	O
components	O
which	O
includes	O
DDCEP+combined	O
with	O
the	O
voicing	O
set	O
and	O
the	O
delta	O
and	O
delta-delta	O
derivatives	O
of	O
the	O
voicing	O
set	O
.	O
The	O
voicing	O
set	O
includes	O
periodicity	O
and	O
jitter	O
,	O
computed	O
as	O
shown	O
in	O
the	O
periodicity	O
and	O
jitter	O
equations	O
.	O
#	O
#	O
EQU8	O
#	O
#	O
Following	O
feature	O
analysis	O
,	O
each	O
feature	O
vector	O
is	O
passed	O
to	O
the	O
recognizer	O
which	O
models	O
each	O
word	O
in	O
the	O
vocabulary	O
by	O
a	O
set	O
of	O
left-to-right	O
continuous	O
mixture	O
density	O
HMM	O
using	O
context-dependent	O
head-body-tail	O
models	O
known	O
from	O
Lee	O
,	O
Chou	O
,	O
Juang	O
,	O
Rabiner	O
and	O
Wilpon	O
article	O
"	O
Context-dependent	O
acoustic	O
modeling	O
for	O
connected	O
digit	O
recognition	O
"	O
.	O
Each	O
word	O
in	O
the	O
vocabulary	O
is	O
divided	O
into	O
a	O
head	O
,	O
a	O
body	O
,	O
and	O
a	O
tail	O
segment	O
.	O
To	O
model	O
inter-word	O
co-articulation	O
,	O
each	O
word	O
consists	O
of	O
one	O
body	O
with	O
multiple	O
heads	O
and	O
multiple	O
tails	O
depending	O
on	O
the	O
preceding	O
and	O
following	O
contexts	O
.	O
The	O
present	O
invention	O
models	O
all	O
possible	O
inter-word	O
co-articulation	O
,	O
resulting	O
in	O
a	O
total	O
of	O
276	O
context-dependent	O
sub-word	O
models	O
.	O
Both	O
the	O
head	O
and	O
tail	O
models	O
are	O
represented	O
with	O
3	O
states	O
,	O
while	O
the	O
body	O
models	O
are	O
represented	O
with	O
4	O
states	O
,	O
each	O
having	O
8	O
mixture	O
components	O
.	O
Silence	O
is	O
modeled	O
with	O
a	O
single	O
state	O
model	O
having	O
32	O
mixture	O
components	O
.	O
This	O
configuration	O
results	O
in	O
a	O
total	O
of	O
276	O
models	O
,	O
837	O
states	O
and	O
6720	O
mixture	O
components	O
.	O

Training	O
included	O
updating	O
all	O
the	O
parameters	O
of	O
the	O
model	O
,	O
namely	O
,	O
means	O
,	O
variances	O
and	O
mixture	O
gains	O
using	O
ML	O
estimation	O
followed	O
by	O
six	O
epochs	O
of	O
MSE	O
to	O
further	O
refine	O
the	O
estimate	O
of	O
the	O
parameters	O
.	O
The	O
number	O
of	O
competing	O
string	O
models	O
was	O
set	O
to	O
four	O
and	O
the	O
step	O
length	O
was	O
set	O
to	O
one	O
during	O
the	O
model	O
training	O
phase	O
.	O
A	O
HSBR	O
codebook	O
of	O
size	O
four	O
,	O
as	O
in	O
Chou	O
,	O
Rahim	O
and	O
Buhrke	O
,	O
is	O
extracted	O
from	O
the	O
mean	O
vectors	O
of	O
HMMs	O
,	O
and	O
each	O
training	O
utterance	O
is	O
signal	O
conditioned	O
by	O
applying	O
HSBR	O
prior	O
to	O
being	O
used	O
in	O
MSE	O
training	O
.	O
The	O
length	O
of	O
the	O
input	O
digit	O
strings	O
were	O
assumed	O
to	O
be	O
unknown	O
during	O
both	O
training	O
and	O
testing	O
portions	O
of	O
this	O
analysis	O
.	O

Table	O
2	O
shows	O
word	O
error	O
rate	O
(	O
Wd	O
.	O
sub	O
.	O
-	O
-	O
Er	O
)	O
and	O
string	O
error	O
rate	O
(	O
St	O
.	O
sub	O
.	O
-	O
-	O
Er	O
)	O
for	O
an	O
unknown-length	O
grammar-based	O
connected	O
digit	O
recognition	O
task	O
using	O
the	O
conventional	O
ML	O
and	O
MSE	O
training	O
methods	O
as	O
a	O
function	O
of	O
frame	O
vector	O
size	O
and	O
type	O
.	O
It	O
is	O
worth	O
noting	O
that	O
the	O
44-feature	O
vector	O
with	O
voicing	O
,	O
which	O
is	O
according	O
to	O
an	O
embodiment	O
of	O
the	O
present	O
invention	O
,	O
is	O
substantially	O
more	O
accurate	O
than	O
the	O
previous	O
38	O
dimensional	O
frame	O
vector	O
.	O

TABLE	O
2	O
______________________________________	O
Feature	O
Vector	O
ML	O
Training	O
MSE	O
Training	O
Size	O
and	O
Type	O
Wd	O
Err	O
St	O
.	O
Err	O
Wd	O
.	O
Err	O
St	O
.	O
Brr	O
______________________________________	O
38	O
DDCEP.sup.+	O
3	O
.	O
31	O
%	O
16	O
.	O
61	O
%	O
2	O
.	O
14	O
%	O
10	O
.	O
18	O
%	O
44	O
DDCEP	O
*	O
3	O
.	O
07	O
%	O
15	O
.	O
78	O
%	O
1	O
.	O
28	O
%	O
6	O
.	O
42	O
%	O
______________________________________	O

Other	O
sets	O
of	O
experiments	O
were	O
run	O
to	O
evaluate	O
the	O
connected	O
digit	O
recognizers	O
using	O
two	O
types	O
of	O
HMMs	O
(	O
DDCEP	O
+	O
and	O
DDCEP	O
)	O
and	O
two	O
types	O
of	O
training	O
(	O
ML	O
and	O
MSE	O
)	O
.	O
The	O
overall	O
performance	O
of	O
the	O
recognizers	O
,	O
organized	O
as	O
the	O
word	O
and	O
string	O
error	O
rate	O
as	O
a	O
function	O
of	O
the	O
feature	O
vector	O
size	O
is	O
summarized	O
in	O
Table	O
2	O
.	O

Table	O
2	O
illustrates	O
four	O
important	O
results	O
.	O
First	O
,	O
under	O
all	O
conditions	O
,	O
the	O
MSE	O
training	O
is	O
superior	O
to	O
the	O
ML	O
training	O
;	O
the	O
MSE-based	O
recognizer	O
achieves	O
an	O
average	O
of	O
50	O
%	O
string	O
and	O
word	O
error	O
rate	O
reduction	O
,	O
uniformly	O
across	O
all	O
types	O
of	O
speech	O
models	O
(	O
both	O
the	O
baseline	O
and	O
extended	O
feature	O
set	O
HMMs	O
)	O
,	O
over	O
the	O
ML-based	O
recognizer	O
.	O
Second	O
,	O
for	O
the	O
ML-based	O
recognizer	O
,	O
the	O
DDCEP	O
*	O
based	O
HMM	O
is	O
slightly	O
superior	O
to	O
the	O
baseline	O
HMM	O
.	O
Thirdly	O
,	O
for	O
the	O
MSE-based	O
recognizer	O
,	O
superiority	O
of	O
the	O
DDCEP	O
*	O
based	O
HMM	O
over	O
the	O
DDCEP+based	O
HMM	O
becomes	O
significantly	O
greater	O
than	O
the	O
ML	O
case	O
.	O
Finally	O
,	O
the	O
reduction	O
in	O
both	O
string	O
and	O
word	O
error	O
rate	O
in	O
going	O
from	O
the	O
ML	O
to	O
the	O
MSE	O
training	O
with	O
use	O
of	O
the	O
DDCEP	O
*	O
based	O
HMM	O
(	O
about	O
60	O
%	O
)	O
is	O
higher	O
than	O
with	O
the	O
baseline	O
HMM	O
(	O
about	O
40	O
%	O
)	O
.	O
This	O
difference	O
validates	O
the	O
notion	O
that	O
MSE	O
training	O
would	O
be	O
advantageous	O
with	O
the	O
extended	O
feature	O
set	O
of	O
the	O
present	O
invention	O
because	O
of	O
the	O
noted	O
strong	O
correlation	O
between	O
voicing	O
and	O
the	O
first	O
spectral	O
coefficient	O
.	O
A	O
method	O
400	O
according	O
to	O
one	O
embodiment	O
of	O
the	O
invention	O
is	O
shown	O
in	O
FIG	O
.	O
4	O
.	O
Thus	O
,	O
there	O
has	O
been	O
disclosed	O
a	O
method	O
and	O
apparatus	O
for	O
using	O
features	O
representing	O
the	O
periodicity	O
and	O
jitter	O
of	O
speech	O
signals	O
which	O
are	O
added	O
to	O
a	O
standard	O
38-dimensional	O
feature	O
vector	O
.	O
Connected	O
digit	O
recognition	O
results	O
comparing	O
the	O
traditional	O
maximum	O
likelihood	O
(	O
ML	O
)	O
method	O
and	O
the	O
minimum	O
string	O
error	O
(	O
MSE	O
)	O
training	O
methods	O
to	O
study	O
the	O
effects	O
of	O
including	O
voicing	O
features	O
were	O
improved	O
.	O
The	O
improvement	O
in	O
performance	O
with	O
voicing	O
is	O
more	O
significant	O
when	O
MSE	O
training	O
is	O
used	O
than	O
when	O
ML	O
training	O
is	O
used	O
,	O
but	O
both	O
are	O
improved	O
.	O
The	O
best	O
result	O
is	O
achieved	O
by	O
including	O
voicing	O
features	O
and	O
by	O
using	O
the	O
MSE	O
training	O
algorithm	O
,	O
yielding	O
a	O
string	O
error	O
rate	O
reduction	O
of	O
40	O
%	O
,	O
compared	O
to	O
the	O
MSE-trained	O
baseline	O
system	O
.	O

Thus	O
,	O
it	O
will	O
now	O
be	O
understood	O
that	O
there	O
has	O
been	O
disclosed	O
a	O
system	O
and	O
method	O
for	O
using	O
periodicity	O
and	O
jitter	O
information	O
in	O
automatic	O
speech	O
recognition	O
.	O
While	O
the	O
invention	O
has	O
been	O
particularly	O
illustrated	O
and	O
described	O
with	O
reference	O
to	O
preferred	O
embodiments	O
thereof	O
,	O
it	O
will	O
be	O
understood	O
by	O
those	O
skilled	O
in	O
the	O
art	O
that	O
various	O
changes	O
in	O
form	O
,	O
details	O
,	O
and	O
applications	O
may	O
be	O
made	O
therein	O
.	O
For	O
example	O
,	O
energy	O
per	O
frame	O
can	O
be	O
used	O
as	O
a	O
feature	O
as	O
an	O
absolute	O
quantity	O
or	O
normalized	O
.	O
There	O
are	O
alternative	O
methods	O
for	O
estimating	O
the	O
degree	O
to	O
which	O
a	O
speech	O
segment	O
appears	O
to	O
be	O
voiced	O
other	O
than	O
the	O
periodicity	O
function	O
shown	O
herein	O
.	O
Expressions	O
for	O
periodicity	O
,	O
other	O
than	O
the	O
periodicity	O
function	O
shown	O
herein	O
,	O
are	O
known	O
in	O
this	O
art	O
and	O
could	O
be	O
used	O
instead	O
.	O
There	O
are	O
also	O
other	O
known	O
methods	O
for	O
measuring	O
pitch	O
stability	O
besides	O
the	O
jitter	O
expression	O
shown	O
herein	O
.	O
It	O
is	O
accordingly	O
intended	O
that	O
the	O
appended	O
claims	O
shall	O
cover	O
all	O
such	O
changes	O
in	O
form	O
,	O
details	O
and	O
applications	O
which	O
do	O
not	O
depart	O
from	O
the	O
true	O
spirit	O
and	O
scope	O
of	O
the	O
invention	O
.	O

1	O
.	O
A	O
method	O
of	O
speech	O
recognition	O
comprising	O
the	O
step	O
of	O
:	O
a	O
.	O
starting	O
with	O
a	O
standard	O
feature	O
vector	O
;	O
b	O
.	O
including	O
at	O
least	O
one	O
voicing	O
feature	O
and	O
at	O
least	O
one	O
time	O
derivative	O
of	O
at	O
least	O
one	O
voicing	O
feature	O
with	O
said	O
standard	O
feature	O
vector	O
;	O
and	O
c	O
.	O
using	O
said	O
standard	O
feature	O
vector	O
with	O
said	O
included	O
features	O
to	O
recognize	O
speech	O
.	O

2	O
.	O
The	O
method	O
of	O
claim	O
1	O
,	O
wherein	O
said	O
standard	O
feature	O
vector	O
includes	O
time	O
derivatives	O
of	O
energy	O
.	O

3	O
.	O
The	O
method	O
of	O
claim	O
1	O
,	O
wherein	O
said	O
standard	O
feature	O
vector	O
includes	O
spectral	O
features	O
.	O

4	O
.	O
The	O
method	O
of	O
claim	O
3	O
,	O
wherein	O
said	O
spectral	O
features	O
include	O
cepstral	O
coefficients	O
.	O

5	O
.	O
The	O
method	O
of	O
claim	O
1	O
,	O
after	O
step	O
b	O
and	O
before	O
step	O
c	O
further	O
comprising	O
the	O
step	O
of	O
including	O
an	O
energy	O
feature	O
with	O
said	O
standard	O
feature	O
vector	O
and	O
said	O
voicing	O
features	O
.	O

6	O
.	O
The	O
method	O
of	O
claim	O
5	O
,	O
wherein	O
said	O
standard	O
feature	O
vector	O
has	O
38	O
features	O
,	O
said	O
energy	O
feature	O
has	O
one	O
feature	O
and	O
said	O
voicing	O
features	O
has	O
five	O
features	O
.	O

7	O
.	O
The	O
method	O
of	O
claim	O
1	O
,	O
wherein	O
a	O
sum	O
of	O
the	O
features	O
of	O
said	O
standard	O
feature	O
vector	O
and	O
said	O
voicing	O
features	O
is	O
44	O
.	O

8	O
.	O
The	O
method	O
of	O
claim	O
1	O
,	O
wherein	O
said	O
voicing	O
features	O
are	O
from	O
a	O
group	O
of	O
features	O
including	O
periodicity	O
features	O
and	O
jitter	O
features	O
.	O

9	O
.	O
The	O
method	O
of	O
claim	O
1	O
,	O
wherein	O
said	O
voicing	O
features	O
includes	O
periodicity	O
features	O
.	O

10	O
.	O
The	O
method	O
of	O
claim	O
1	O
,	O
wherein	O
said	O
voicing	O
features	O
includes	O
jitter	O
features	O
.	O

11	O
.	O
The	O
method	O
of	O
claim	O
1	O
,	O
wherein	O
said	O
voicing	O
features	O
includes	O
periodicity	O
features	O
and	O
jitter	O
features	O
.	O

12	O
.	O
The	O
method	O
of	O
claim	O
1	O
,	O
wherein	O
step	O
b	O
further	O
includes	O
the	O
steps	O
of	O
:	O
normalizing	O
an	O
autocorrelation	O
function	O
with	O
a	O
peak	O
at	O
m	O
=	O
0	O
so	O
that	O
a	O
ratio	O
lies	O
in	O
a	O
normalized	O
range	O
between	O
0	O
and	O
1	O
;	O
and	O
choosing	O
a	O
largest	O
peak	O
in	O
the	O
normalized	O
autocorrelation	O
function	O
as	O
the	O
estimate	O
of	O
a	O
pitch	O
period	O
and	O
a	O
corresponding	O
frequency	O
value	O
of	O
said	O
pitch	O
period	O
is	O
a	O
measure	O
of	O
a	O
periodicity	O
feature	O
.	O

13	O
.	O
An	O
apparatus	O
for	O
speech	O
recognition	O
,	O
comprising	O
:	O
means	O
for	O
determining	O
a	O
standard	O
feature	O
vector	O
;	O
means	O
for	O
storing	O
a	O
standard	O
feature	O
vector	O
after	O
it	O
has	O
been	O
determined	O
;	O
means	O
for	O
including	O
at	O
least	O
one	O
voicing	O
feature	O
and	O
at	O
least	O
one	O
time	O
derivative	O
of	O
at	O
least	O
one	O
voicing	O
feature	O
with	O
said	O
stored	O
standard	O
feature	O
vector	O
;	O
and	O
means	O
for	O
using	O
said	O
stored	O
standard	O
feature	O
vector	O
and	O
said	O
included	O
voicing	O
features	O
to	O
recognize	O
speech	O
;	O
wherein	O
an	O
error	O
rate	O
for	O
speech	O
recognition	O
is	O
reduced	O
because	O
of	O
a	O
robustness	O
resulting	O
from	O
including	O
the	O
at	O
least	O
one	O
voicing	O
feature	O
and	O
the	O
at	O
least	O
one	O
time	O
derivative	O
of	O
at	O
least	O
one	O
voicing	O
feature	O
.	O

14	O
.	O
The	O
apparatus	O
for	O
speech	O
recognition	O
according	O
to	O
claim	O
13	O
,	O
wherein	O
said	O
speech	O
recognition	O
is	O
less	O
sensitive	O
to	O
differences	O
in	O
transmission	O
conditions	O
because	O
of	O
including	O
said	O
at	O
least	O
one	O
voicing	O
feature	O
and	O
said	O
at	O
least	O
one	O
time	O
derivative	O
of	O
at	O
least	O
one	O
voicing	O
feature	O
.	O

15	O
.	O
The	O
apparatus	O
for	O
speech	O
recognition	O
according	O
to	O
claim	O
14	O
,	O
wherein	O
said	O
at	O
least	O
one	O
voicing	O
feature	O
includes	O
periodicity	O
features	O
and	O
jitter	O
features	O
.	O

16	O
.	O
The	O
apparatus	O
for	O
speech	O
recognition	O
according	O
to	O
claim	O
13	O
,	O
wherein	O
said	O
at	O
least	O
one	O
voicing	O
feature	O
includes	O
periodicity	O
features	O
and	O
jitter	O
features	O
.	O

17	O
.	O
The	O
apparatus	O
for	O
speech	O
recognition	O
according	O
to	O
claim	O
13	O
,	O
wherein	O
said	O
apparatus	O
has	O
been	O
trained	O
to	O
recognize	O
a	O
string	O
of	O
words	O
using	O
a	O
minimum	O
string	O
error	O
training	O
.	O

18	O
.	O
The	O
apparatus	O
for	O
speech	O
recognition	O
according	O
to	O
claim	O
13	O
,	O
wherein	O
said	O
apparatus	O
has	O
been	O
trained	O
to	O
recognize	O
at	O
least	O
one	O
word	O
using	O
maximum	O
likelihood	O
training	O
.	O

