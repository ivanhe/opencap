US	O
5142585	O
A	O
19920825	O

US	O
07812832	O
19911220	O

eng	O
eng	O

GB	O
8603756	O
A	O
19860215	O

358	O
,	O
050	O

11	O
,	O
088	O

8603756	O

GB19860003756	O

19920825	O

19920825	O

G	O
10L	O
7/08	O
A	O
G	O
10	O
L	O
7	O
08	O
A	O

G10L	O
15/00	O
20060101C	O
I20051008RMEP	O

20060101	O

C	O
G	O
10	O
L	O
15	O
00	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/06	O
20060101A	O
I20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
06	O
I	O

20051008	O

EP	O

R	O
M	O

US	O

704/233	O
704	O
233	O

381/94	O
.	O
2	O
381	O
94	O
.	O
2	O

704/244	O
704	O
244	O

704/E15.009	O
704	O
E15	O
.	O
009	O

G10L	O
15/06A	O
G	O
10	O
L	O
15	O
06	O

A	O

US	O

381/41	O
-	O
43	O
381	O
41	O
-	O
43	O

US	O

381/46	O
381	O
46	O

US	O

381/47	O
381	O
47	O

US	O

381/71	O
381	O
71	O

US	O

381/94	O
381	O
94	O

US	O

395/2	O
395	O
2	O

6	O
Speech	O
processing	O
apparatus	O
and	O
methods	O

US	O
4239936	O
A	O
Sakoe	O
19801216	O

19781228	O

US	O

381/43	O
381	O
43	O

US	O
4417098	O
A	O
Chaplin	O
et_al.	O

19831122	O

19810416	O

US	O

381/94	O
381	O
94	O

US	O
4624010	O
A	O
Takebayashi	O
19861118	O

19830124	O

US	O

381/43	O
381	O
43	O

US	O
4625083	O
A	O
Poikela	O
19861125	O

19850402	O

US	O

381/46	O
381	O
46	O

US	O
4720802	O
A	O
Damoulakis	O
et_al.	O

19880119	O

19830726	O

US	O

364/513	O
.	O
5	O
364	O
513	O
.	O
5	O

US	O
4937871	O
A	O
Hattori	O
19900626	O

19890524	O

US	O

381/46	O
381	O
46	O

North	B-Citation
et_al.	I-Citation
,	I-Citation
IEEE	I-Citation
1985	I-Citation
National	I-Citation
Aerospace	I-Citation
and	I-Citation
Electronics	I-Citation
Conference	I-Citation
NAECON	I-Citation
1985	I-Citation
,	I-Citation
Dynamic	I-Citation
Retraining	I-Citation
Approaches	I-Citation
for	I-Citation
an	I-Citation
Airborne	I-Citation
Speech	I-Citation
Recogniton	I-Citation
System	I-Citation
,	I-Citation
May	I-Citation
20	I-Citation
24	I-Citation
,	I-Citation
1985	I-Citation
,	I-Citation
pp.	I-Citation
970	I-Citation
974	I-Citation
.	O

Holmes	B-Citation
et_al.	I-Citation
,	I-Citation
IEEE	I-Citation
ICASSP	I-Citation
86	I-Citation
,	I-Citation
Noise	I-Citation
Compensation	I-Citation
for	I-Citation
Speech	I-Citation
Recognition	I-Citation
Using	I-Citation
Probabilistic	I-Citation
Models	I-Citation
,	I-Citation
Apr.	I-Citation
7	I-Citation
11	I-Citation
,	I-Citation
1986	I-Citation
,	I-Citation
pp.	I-Citation
741	I-Citation
744	I-Citation
.	O

US	O
5825898	O
A	O
19981020	O

19960627	O

US	O
5452397	O
A	O
19950919	O

19921211	O

WO	O
0075918	O
A1	O
20001214	O

20000531	O

US	O
6983245	O
B1	O
20060103	O

20000607	O

US	O
6594367	O
B1	O
20030715	O

19991025	O

US	O
6501833	O
B2	O
20021231	O

19971003	O

US	O
6363345	O
B1	O
20020326	O

19990218	O

US	O
6178248	O
B1	O
20010123	O

19970414	O

US	O
358050	O
19890530	O

ABANDONED	O

US	O
07812832	O
19911220	O

US	O
11088	O
19870205	O

ABANDONED	O

US	O
358050	O
19890530	O

Smiths	O
Industries	O
Public	O
Limited	O
Company	O

London	O
GB	O

Taylor	O
Michael	O
R.	O

Swindon	O
GB	O

Pollock	O
,	O
Vande	O
Sande	O
&	O
Priddy	O

Shaw	O
;	O
Dale	O
M.	O

Knepper	O
;	O
David	O
D.	O

US	O
5142585	O
A	O
19920825	O

19911220	O

DE	O
3766124	O
D1	O
19901220	O

19870130	O

EP	O
233718	O
B1	O
19901114	O

19870130	O

EP	O
233718	O
A1	O
19870826	O

19870130	O

GB	O
2186726	O
A	O
19870819	O

19870204	O

GB	O
2186726	O
B	O
19890125	O

19870204	O

GB	O
8702484	O
D0	O
19870311	O

19870204	O

EP	O
233718	O
B1	O
19901114	O

19870130	O

EP	O
233718	O
A1	O
19870826	O

19870130	O

GB	O
2186726	O
A	O
19870819	O

19870204	O

GB	O
2186726	O
B	O
19890125	O

19870204	O

GB	O
8702484	O
D0	O
19870311	O

19870204	O

US	O
5142585	O
A	O
19920825	O

19911220	O

DE	O
3766124	O
D1	O
19901220	O

19870130	O

Speech	O
processing	O
apparatus	O
has	O
a	O
store	O
containing	O
a	O
reference	O
vocabulary	O
of	O
words	O
and	O
carries	O
out	O
active	O
word	O
selection	O
in	O
accordance	O
with	O
mode	O
data	O
supplied	O
to	O
the	O
apparatus	O
.	O
After	O
this	O
,	O
the	O
apparatus	O
performs	O
dynamic	O
template	O
adaptation	O
in	O
dependence	O
on	O
the	O
output	O
of	O
a	O
sensor	O
that	O
monitors	O
environmental	O
influences	O
on	O
the	O
speaker	O
of	O
the	O
kind	O
that	O
modify	O
speech	O
sounds	O
made	O
by	O
the	O
speaker	O
.	O
The	O
sensor	O
may	O
be	O
responsive	O
to	O
vibration	O
or	O
acceleration	O
forces	O
on	O
the	O
speaker	O
,	O
the	O
apparatus	O
using	O
information	O
about	O
how	O
vowel	O
and	O
consonant	O
groupings	O
are	O
influenced	O
by	O
forces	O
on	O
the	O
speaker	O
to	O
perform	O
the	O
template	O
adaptation	O
.	O
Pattern	O
matching	O
is	O
used	O
to	O
compare	O
the	O
speech	O
sounds	O
of	O
the	O
speaker	O
with	O
the	O
modified	O
signals	O
after	O
template	O
adaptation	O
to	O
identify	O
the	O
most	O
likely	O
word	O
spoken	O
.	O

19920526	O

AS	O
ASSIGNMENT	O
N	O
US	O
5142585A	O
SMITHS	O
INDUSTRIES	O
PUBLIC	O
LIMITED	O
COMPANY	O
,	O
ENGLAND	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST.;ASSIGNOR:TAYLOR	O
,	O
MICHAEL	O
R.;REEL/FRAME:006142/0559	O

19920321	O

19960402	O

REMI	O
MAINTENANCE	O
FEE	O
REMINDER	O
MAILED	O
N	O
US	O
5142585A	O

19960825	O

LAPS	O
-	O
LAPSE	O
FOR	O
FAILURE	O
TO	O
PAY	O
MAINTENANCE	O
FEES	O
N	O
US	O
5142585A	O

19961105	O

FP	O
-	O
EXPIRED	O
DUE	O
TO	O
FAILURE	O
TO	O
PAY	O
MAINTENANCE	O
FEE	O
C	O
US	O
5142585A	O

19960828	O

This	O
application	O
is	O
a	O
,	O
continuation	O
of	O
Ser	O
.	O
No.	O
07/358	O
,	O
050	O
,	O
filed	O
on	O
May	O
30	O
,	O
1989	O
,	O
which	O
application	O
is	O
a	O
continuation-in-part	O
of	O
Ser	O
.	O
No.	O
07/011	O
,	O
088	O
filed	O
Feb.	O
5	O
,	O
1987	O
,	O
both	O
abandoned	O
.	O

BACKGROUND	O
OF	O
THE	O
INVENTION	O
This	O
invention	O
relates	O
to	O
speech	O
processing	O
apparatus	O
and	O
methods	O
.	O
In	O
complex	O
equipment	O
having	O
multiple	O
functions	O
it	O
can	O
be	O
useful	O
to	O
be	O
able	O
to	O
control	O
the	O
equipment	O
by	O
spoken	O
commands	O
.	O
This	O
is	O
also	O
useful	O
where	O
the	O
user	O
's	O
hands	O
are	O
occupied	O
with	O
other	O
tasks	O
or	O
where	O
the	O
user	O
is	O
disabled	O
and	O
is	O
unable	O
to	O
use	O
his	O
hands	O
to	O
operate	O
conventional	O
mechanical	O
switches	O
and	O
controls.The	O
problem	O
with	O
equipment	O
controlled	O
by	O
speech	O
is	O
that	O
speech	O
recognition	O
can	O
be	O
unreliable	O
,	O
especially	O
where	O
the	O
voice	O
of	O
the	O
speaker	O
is	O
altered	O
by	O
environmental	O
factors	O
,	O
such	O
as	O
vibration	O
.	O
This	O
can	O
lead	O
to	O
failure	O
to	O
operate	O
or	O
,	O
worse	O
still	O
,	O
to	O
incorrect	O
operation.Speech	O
signal	O
processing	O
can	O
also	O
be	O
used	O
in	O
communication	O
systems	O
,	O
to	O
improve	O
the	O
quality	O
of	O
speech	O
and	O
reduce	O
noise	O
.	O
Where	O
,	O
however	O
,	O
the	O
speech	O
is	O
severely	O
degraded	O
,	O
such	O
as	O
by	O
vibration	O
or	O
by	O
a	O
high	O
`	O
g	O
`	O
loading	O
,	O
the	O
processing	O
equipment	O
may	O
not	O
recognise	O
the	O
input	O
as	O
being	O
a	O
speech	O
signal	O
and	O
may	O
therefore	O
remove	O
vital	O
components	O
of	O
the	O
speech	O
signal	O
by	O
filtering	O
.	O
The	O
processing	O
equipment	O
can	O
,	O
therefore	O
,	O
in	O
some	O
circumstances	O
lead	O
to	O
a	O
reduction	O
in	O
the	O
speech	O
information	O
.	O

BRIEF	O
SUMMARY	O
OF	O
THE	O
INVENTION	O
It	O
is	O
an	O
object	O
of	O
the	O
present	O
invention	O
to	O
provide	O
improved	O
speech	O
processing	O
apparatus	O
and	O
methods.According	O
to	O
one	O
aspect	O
of	O
the	O
present	O
invention	O
there	O
is	O
provided	O
speech	O
processing	O
apparatus	O
including	O
microphone	O
means	O
arranged	O
to	O
sense	O
speech	O
sounds	O
made	O
by	O
a	O
speaker	O
and	O
derive	O
a	O
first	O
output	O
signal	O
in	O
accordance	O
therewith	O
,	O
the	O
apparatus	O
including	O
sensor	O
means	O
arranged	O
to	O
monitor	O
environmental	O
influences	O
on	O
the	O
speaker	O
of	O
the	O
kind	O
that	O
modify	O
the	O
speech	O
sounds	O
made	O
by	O
the	O
speaker	O
,	O
and	O
processing	O
means	O
arranged	O
to	O
receive	O
the	O
output	O
from	O
the	O
microphone	O
means	O
and	O
from	O
the	O
sensor	O
means	O
,	O
the	O
processing	O
means	O
being	O
arranged	O
to	O
produce	O
a	O
second	O
output	O
signal	O
in	O
accordance	O
with	O
the	O
speech	O
sounds	O
made	O
by	O
the	O
speaker	O
that	O
is	O
compensated	O
at	O
least	O
in	O
part	O
for	O
the	O
environmental	O
influences.The	O
sensor	O
means	O
may	O
be	O
responsive	O
to	O
vibration	O
or	O
acceleration	O
forces	O
on	O
the	O
speaker	O
.	O
Alternatively	O
,	O
the	O
sensor	O
means	O
may	O
be	O
responsive	O
to	O
the	O
level	O
of	O
audible	O
noise	O
heard	O
by	O
the	O
speaker.The	O
speech	O
processing	O
apparatus	O
preferably	O
includes	O
storage	O
means	O
containing	O
information	O
of	O
a	O
reference	O
vocabulary	O
of	O
words	O
,	O
the	O
storage	O
means	O
being	O
arranged	O
to	O
derive	O
a	O
signal	O
by	O
modifying	O
the	O
information	O
in	O
accordance	O
with	O
the	O
output	O
of	O
the	O
sensor	O
means	O
,	O
and	O
the	O
apparatus	O
including	O
comparator	O
means	O
that	O
is	O
arranged	O
to	O
compare	O
the	O
first	O
output	O
signal	O
with	O
the	O
signal	O
derived	O
by	O
the	O
storage	O
means	O
.	O
The	O
apparatus	O
may	O
be	O
arranged	O
to	O
perform	O
dynamic	O
template	O
adaptation	O
the	O
reference	O
vocabulary	O
in	O
accordance	O
with	O
the	O
output	O
of	O
the	O
sensor	O
means	O
.	O
The	O
processing	O
means	O
may	O
contain	O
information	O
about	O
how	O
vowel	O
and	O
consonant	O
groupings	O
are	O
influenced	O
by	O
forces	O
on	O
the	O
speaker	O
,	O
the	O
processing	O
means	O
using	O
this	O
information	O
in	O
producing	O
the	O
second	O
output	O
signal	O
.	O
The	O
comparator	O
may	O
be	O
arranged	O
to	O
perform	O
comparison	O
by	O
pattern	O
matching	O
techniques	O
.	O
The	O
storage	O
means	O
may	O
be	O
arranged	O
to	O
carry	O
out	O
active	O
word	O
selection	O
from	O
a	O
vocabulary	O
in	O
accordance	O
with	O
mode	O
data	O
.	O
The	O
storage	O
means	O
may	O
be	O
arranged	O
to	O
perform	O
sub-set	O
selection	O
from	O
the	O
vocabulary	O
in	O
accordance	O
with	O
words	O
previously	O
spoken	O
and	O
recognised	O
.	O
The	O
apparatus	O
may	O
include	O
feedback	O
means	O
arranged	O
to	O
provide	O
information	O
to	O
the	O
speaker.According	O
to	O
another	O
aspect	O
of	O
the	O
present	O
invention	O
,	O
there	O
is	O
provided	O
a	O
method	O
of	O
providing	O
speech	O
processing	O
comprising	O
the	O
steps	O
of	O
deriving	O
speech	O
signals	O
from	O
a	O
speaker	O
's	O
speech	O
sounds	O
,	O
deriving	O
environmental	O
signals	O
in	O
accordance	O
with	O
environmental	O
influences	O
on	O
the	O
speaker	O
affecting	O
the	O
nature	O
of	O
the	O
speech	O
sounds	O
,	O
deriving	O
signals	O
from	O
a	O
reference	O
vocabulary	O
of	O
words	O
,	O
modifying	O
the	O
signals	O
derived	O
from	O
the	O
reference	O
vocabulary	O
in	O
accordance	O
with	O
the	O
environmental	O
signals	O
,	O
and	O
comparing	O
the	O
speech	O
signals	O
with	O
the	O
modified	O
signals.According	O
to	O
a	O
further	O
aspect	O
of	O
the	O
present	O
invention	O
there	O
is	O
provided	O
apparatus	O
for	O
performing	O
a	O
method	O
according	O
to	O
the	O
said	O
other	O
aspect	O
of	O
the	O
present	O
invention.A	O
speech	O
recognition	O
system	O
in	O
accordance	O
with	O
the	O
present	O
invention	O
,	O
will	O
now	O
be	O
described	O
,	O
by	O
way	O
of	O
example	O
,	O
with	O
reference	O
to	O
the	O
accompanying	O
drawings	O
.	O

BRIEF	O
DESCRIPTION	O
OF	O
THE	O
DRAWINGS	O
FIG	O
.	O
1	O
shows	O
the	O
system	O
schematically;FIGS	O
.	O
2A	O
to	O
2D	O
illustrate	O
probability	O
functions	O
for	O
different	O
sounds	O
;	O
andFIG	O
.	O
3	O
is	O
a	O
flowchart	O
showing	O
operation	O
of	O
the	O
system	O
.	O

DETAILED	O
DESCRIPTION	O
With	O
reference	O
first	O
to	O
FIG	O
.	O
1	O
,	O
the	O
speech	O
recognition	O
system	O
includes	O
a	O
processing	O
unit	O
10	O
that	O
receives	O
input	O
signals	O
from	O
a	O
microphone	O
1	O
and	O
contextual	O
inputs	O
from	O
environmental	O
sensors	O
2	O
and	O
a	O
databus	O
170	O
.	O
The	O
microphone	O
1	O
is	O
located	O
close	O
to	O
the	O
speaker	O
's	O
mouth	O
so	O
as	O
to	O
detect	O
his	O
speech	O
sounds	O
,	O
whilst	O
the	O
sensors	O
2	O
are	O
located	O
where	O
they	O
will	O
respond	O
to	O
the	O
same	O
environmental	O
influences	O
to	O
which	O
the	O
speaker	O
is	O
subjected	O
.	O
The	O
sensors	O
2	O
may	O
be	O
different	O
for	O
different	O
applications	O
,	O
but	O
typically	O
they	O
will	O
be	O
responsive	O
to	O
vibration	O
,	O
which	O
can	O
cause	O
frequency	O
modulation	O
of	O
speech	O
sounds	O
,	O
and	O
to	O
acceleration	O
,	O
which	O
can	O
cause	O
severe	O
timing	O
problems	O
for	O
the	O
speech	O
articulators	O
.	O
Alternatively	O
,	O
the	O
sensors	O
2	O
may	O
include	O
an	O
audible	O
noise	O
detector	O
that	O
provides	O
an	O
output	O
indicative	O
of	O
environmental	O
audible	O
noise	O
heard	O
by	O
the	O
user	O
and	O
,	O
in	O
this	O
respect	O
,	O
the	O
noise	O
detector	O
would	O
be	O
located	O
in	O
the	O
region	O
of	O
the	O
user	O
's	O
ears	O
.	O
High	O
noise	O
levels	O
have	O
been	O
found	O
to	O
alter	O
speech	O
and	O
the	O
present	O
apparatus	O
can	O
be	O
used	O
to	O
compensate	O
for	O
this	O
.	O
The	O
signal	O
from	O
the	O
microphone	O
1	O
is	O
first	O
supplied	O
to	O
a	O
spectral	O
analysis	O
unit	O
11	O
which	O
produces	O
output	O
signals	O
in	O
accordance	O
with	O
the	O
frequency	O
bands	O
within	O
which	O
the	O
sound	O
falls	O
.	O
These	O
signals	O
are	O
supplied	O
to	O
an	O
optional	O
spectral	O
correction	O
and	O
noise	O
adaptation	O
unit	O
12	O
which	O
improves	O
the	O
signal-to-noise	O
ratio	O
or	O
eliminates	O
,	O
or	O
marks	O
,	O
those	O
signals	O
that	O
can	O
only	O
have	O
arisen	O
from	O
noise	O
rather	O
than	O
speech	O
.	O
Output	O
signals	O
from	O
the	O
unit	O
12	O
are	O
supplied	O
to	O
one	O
input	O
of	O
a	O
comparator	O
or	O
pattern	O
matching	O
unit	O
13	O
.	O
The	O
other	O
input	O
to	O
the	O
pattern	O
matching	O
unit	O
13	O
is	O
taken	O
from	O
an	O
active	O
vocabulary	O
storage	O
30	O
which	O
is	O
described	O
in	O
greater	O
detail	O
below	O
.	O
The	O
pattern	O
matching	O
unit	O
13	O
compares	O
the	O
frequency/time	O
patterns	O
derived	O
from	O
the	O
microphone	O
1	O
with	O
the	O
stored	O
vocabulary	O
and	O
produces	O
an	O
output	O
on	O
line	O
15	O
in	O
accordance	O
with	O
the	O
word	O
which	O
is	O
the	O
best	O
fit	O
or	O
has	O
the	O
highest	O
probability	O
of	O
being	O
the	O
sound	O
received	O
by	O
the	O
microphone	O
1	O
.	O
The	O
output	O
on	O
line	O
15	O
is	O
supplied	O
to	O
one	O
input	O
of	O
a	O
post	O
recognition	O
processing	O
unit	O
16	O
,	O
the	O
other	O
input	O
receiving	O
mode	O
and	O
dynamic	O
data	O
from	O
a	O
remote	O
terminal	O
17	O
.	O
The	O
unit	O
16	O
performs	O
various	O
tasks	O
on	O
the	O
string	O
of	O
word	O
outputs	O
from	O
the	O
pattern	O
matching	O
unit	O
13	O
as	O
discussed	O
in	O
greater	O
detail	O
later	O
.	O
The	O
post	O
recognition	O
processing	O
unit	O
16	O
has	O
three	O
outputs	O
.	O
One	O
output	O
is	O
provided	O
on	O
line	O
18	O
as	O
a	O
feedback	O
channel	O
to	O
an	O
indicator	O
21	O
.	O
This	O
may	O
be	O
an	O
audible	O
or	O
visual	O
indicator	O
perceivable	O
by	O
the	O
speaker	O
which	O
either	O
confirms	O
his	O
spoken	O
commands	O
,	O
as	O
recognised	O
by	O
the	O
units	O
13	O
and	O
16	O
,	O
or	O
requests	O
repetition	O
of	O
all	O
or	O
part	O
of	O
the	O
command	O
,	O
where	O
an	O
unsatisfactory	O
recognition	O
is	O
achieved	O
.	O
The	O
second	O
output	O
is	O
provided	O
on	O
line	O
19	O
to	O
a	O
word	O
sub-set	O
selection	O
unit	O
32	O
forming	O
a	O
part	O
of	O
the	O
vocabulary	O
store	O
30	O
,	O
the	O
operation	O
of	O
which	O
is	O
described	O
in	O
detail	O
below	O
.	O
The	O
third	O
output	O
is	O
provided	O
on	O
line	O
20	O
as	O
the	O
system	O
command	O
signal	O
to	O
the	O
remote	O
terminal	O
17	O
.	O
The	O
system	O
command	O
signal	O
is	O
produced	O
when	O
the	O
unit	O
10	O
identifies	O
a	O
spoken	O
command	O
with	O
sufficient	O
probability	O
and	O
may	O
,	O
for	O
example	O
,	O
be	O
used	O
to	O
effect	O
operation	O
of	O
external	O
equipment	O
via	O
the	O
databus	O
170	O
.	O
The	O
operation	O
of	O
the	O
vocabulary	O
storage	O
30	O
will	O
now	O
be	O
described	O
in	O
greater	O
detail	O
.	O
The	O
store	O
30	O
includes	O
a	O
reference	O
vocabulary	O
31	O
in	O
the	O
form	O
of	O
pattern	O
templates	O
or	O
word	O
models	O
of	O
the	O
frequency/time	O
pattern	O
or	O
state	O
descriptions	O
of	O
different	O
words	O
.	O
This	O
vocabulary	O
is	O
established	O
by	O
the	O
speaker	O
speaking	O
a	O
list	O
of	O
words	O
,	O
the	O
sounds	O
made	O
being	O
entered	O
in	O
the	O
vocabulary	O
31	O
and	O
labelled	O
with	O
the	O
associated	O
word	O
.	O
The	O
total	O
vocabulary	O
31	O
may	O
be	O
further	O
reduced	O
by	O
an	O
optional	O
sub-set	O
selection	O
at	O
32	O
under	O
control	O
of	O
signals	O
on	O
line	O
19	O
in	O
accordance	O
with	O
words	O
previously	O
spoken	O
and	O
recognised.Following	O
sub-set	O
selection	O
,	O
the	O
vocabulary	O
is	O
further	O
subjected	O
to	O
active	O
word	O
selection	O
at	O
33	O
in	O
response	O
to	O
mode	O
data	O
on	O
line	O
34	O
from	O
the	O
remote	O
terminal	O
17	O
which	O
is	O
derived	O
from	O
information	O
supplied	O
to	O
the	O
remote	O
terminal	O
on	O
the	O
databus	O
170	O
.	O
For	O
example	O
,	O
in	O
an	O
aircraft	O
,	O
the	O
mode	O
data	O
may	O
indicate	O
whether	O
the	O
aircraft	O
is	O
landing	O
or	O
taking	O
off	O
,	O
or	O
is	O
in	O
mid	O
flight	O
.	O
Alternatively	O
,	O
for	O
example	O
,	O
if	O
a	O
radio	O
channel	O
had	O
already	O
been	O
selected	O
via	O
a	O
spoken	O
command	O
,	O
the	O
probability	O
of	O
reselection	O
will	O
be	O
small	O
so	O
the	O
words	O
associated	O
with	O
selection	O
of	O
that	O
radio	O
channel	O
can	O
be	O
excluded	O
from	O
the	O
vocabulary	O
at	O
33	O
.	O
Poor	O
correlation	O
with	O
selected	O
,	O
active	O
templates	O
could	O
be	O
used	O
to	O
invoke	O
re-processing	O
of	O
the	O
speech	O
on	O
a	O
wider	O
syntax.After	O
active	O
selection	O
at	O
33	O
,	O
the	O
vocabulary	O
is	O
subjected	O
to	O
template	O
adaptation	O
at	O
35	O
.	O
The	O
function	O
performed	O
at	O
35	O
is	O
a	O
dynamic	O
adaptation	O
of	O
the	O
active	O
word	O
templates	O
by	O
anticipating	O
the	O
behaviour	O
of	O
the	O
human	O
speech	O
production	O
mechanism	O
during	O
unusual	O
environmental	O
conditions	O
as	O
detected	O
by	O
the	O
sensors	O
2	O
.	O
Knowledge	O
about	O
how	O
speech	O
components	O
such	O
as	O
vowel/Consonant/vowel	O
,	O
vowel/consonant	O
,	O
consonant/vowel	O
or	O
consonant/vowel/consonant	O
groupings	O
are	O
influenced	O
by	O
forces	O
on	O
the	O
speaker	O
are	O
also	O
taken	O
into	O
account	O
.	O
More	O
particularly	O
,	O
the	O
reference	O
speech	O
patterns	O
are	O
stochastic	O
models	O
in	O
the	O
form	O
of	O
,	O
for	O
example	O
,	O
Hidden	O
Markov	O
Models	O
of	O
the	O
kind	O
well	O
reported	O
in	O
speech	O
recognition	O
applications	O
where	O
states	O
can	O
be	O
used	O
either	O
implicitly	O
or	O
explicitly	O
to	O
describe	O
the	O
acoustic-phonetic	O
structure	O
of	O
each	O
word	O
in	O
terms	O
of	O
the	O
probability	O
of	O
any	O
test	O
vector	O
or	O
frame	O
from	O
an	O
unknown	O
input	O
belonging	O
to	O
a	O
particular	O
state	O
of	O
the	O
reference	O
model	O
.	O
FIG	O
.	O
2A	O
shows	O
an	O
example	O
of	O
a	O
probability	O
density	O
function	O
in	O
terms	O
of	O
the	O
temporal	O
duration	O
of	O
a	O
consonant	O
.	O
FIGS	O
.	O
2B	O
to	O
2D	O
show	O
similar	O
probability	O
density	O
functions	O
in	O
terms	O
of	O
frequency	O
(	O
or	O
some	O
other	O
spectral	O
measure	O
)	O
for	O
the	O
first	O
formant	O
of	O
most	O
vowels	O
(	O
FIG	O
.	O
2B	O
)	O
,	O
for	O
the	O
second	O
formant	O
of	O
front	O
vowels	O
such	O
as	O
/i/	O
and	O
/e/	O
(	O
FIG	O
.	O
2C	O
)	O
and	O
for	O
the	O
second	O
formant	O
of	O
the	O
back	O
vowel	O
/u/	O
(	O
FIG	O
.	O
2D	O
)	O
.	O
For	O
speech	O
not	O
influenced	O
by	O
environmental	O
factors	O
,	O
these	O
probability	O
density	O
functions	O
are	O
represented	O
by	O
the	O
solid	O
lines	O
in	O
FIGS	O
.	O
2A	O
to	O
2D	O
.	O
It	O
is	O
known	O
that	O
the	O
probability	O
density	O
functions	O
are	O
influenced	O
by	O
acceleration	O
forces	O
on	O
the	O
speaker	O
in	O
the	O
following	O
ways	O
.	O
For	O
a	O
consonant	O
,	O
the	O
probability	O
increases	O
that	O
it	O
will	O
have	O
a	O
duration	O
which	O
decreases	O
with	O
increasing	O
acceleration	O
,	O
as	O
shown	O
by	O
the	O
broken	O
line	O
in	O
FIG	O
.	O
2A	O
.	O
For	O
the	O
first	O
formant	O
of	O
most	O
vowels	O
,	O
the	O
probability	O
increases	O
that	O
it	O
will	O
have	O
a	O
higher	O
frequency	O
with	O
increasing	O
acceleration	O
,	O
as	O
shown	O
by	O
the	O
broken	O
line	O
in	O
FIG	O
.	O
2B	O
.	O
For	O
the	O
second	O
formant	O
of	O
front	O
vowels	O
,	O
the	O
probability	O
increases	O
that	O
the	O
formant	O
frequency	O
will	O
be	O
lower	O
with	O
increasing	O
acceleration	O
as	O
shown	O
by	O
the	O
broken	O
line	O
in	O
FIG	O
.	O
2C	O
.	O
For	O
the	O
second	O
formant	O
of	O
a	O
back	O
vowel	O
,	O
the	O
probability	O
is	O
increased	O
that	O
the	O
formant	O
will	O
be	O
higher	O
with	O
increasing	O
acceleration	O
as	O
shown	O
by	O
the	O
broken	O
line	O
in	O
FIG	O
.	O
2D	O
.	O
The	O
extent	O
of	O
modification	O
of	O
these	O
probability	O
density	O
functions	O
is	O
dependent	O
on	O
the	O
level	O
and	O
direction	O
of	O
acceleration	O
.	O
Other	O
environmental	O
influences	O
on	O
the	O
speaker	O
such	O
as	O
vibration	O
,	O
noise	O
level	O
or	O
stress	O
have	O
analogous	O
effects	O
on	O
these	O
probability	O
density	O
functions.With	O
this	O
knowledge	O
of	O
the	O
way	O
in	O
which	O
the	O
speech	O
signal	O
is	O
modified	O
,	O
the	O
apparatus	O
modifies	O
the	O
probability	O
density	O
functions	O
of	O
the	O
stored	O
reference	O
vocabulary	O
templates	O
or	O
models	O
in	O
the	O
manner	O
illustrated	O
in	O
the	O
flow	O
chart	O
in	O
FIG	O
.	O
3	O
.	O
FIG	O
.	O
3	O
,	O
for	O
simplicity	O
,	O
illustrates	O
only	O
the	O
way	O
in	O
which	O
acceleration	O
influences	O
dynamic	O
adaptation	O
of	O
the	O
templates	O
.	O
In	O
some	O
circumstances	O
,	O
acceleration	O
may	O
be	O
the	O
only	O
factor	O
measured	O
and	O
used	O
for	O
this	O
dynamic	O
adaptation	O
.	O
Where	O
other	O
environmental	O
factors	O
are	O
used	O
,	O
it	O
will	O
be	O
appreciated	O
that	O
these	O
can	O
be	O
employed	O
in	O
a	O
similar	O
manner	O
.	O
Briefly	O
,	O
FIG	O
.	O
3	O
shows	O
that	O
the	O
states	O
in	O
the	O
reference	O
vocabulary	O
representing	O
the	O
consonants	O
and	O
different	O
vowel	O
classes	O
are	O
identified	O
and	O
labelled	O
and	O
that	O
the	O
probability	O
density	O
functions	O
of	O
these	O
are	O
increased	O
or	O
decreased	O
appropriately	O
by	O
an	O
amount	O
dependent	O
on	O
the	O
level	O
of	O
acceleration	O
detected	O
.	O
These	O
modified	O
probability	O
density	O
functions	O
,	O
which	O
now	O
bear	O
a	O
closer	O
resemblance	O
to	O
the	O
speech	O
produced	O
by	O
the	O
speaker	O
under	O
high	O
g-forces	O
are	O
then	O
supplied	O
to	O
the	O
pattern	O
matching	O
unit	O
13	O
.	O
As	O
mentioned	O
previously	O
,	O
mode	O
data	O
and	O
dynamic	O
environmental	O
data	O
are	O
also	O
supplied	O
to	O
the	O
post	O
recognition	O
processing	O
unit	O
16	O
,	O
via	O
line	O
36	O
.	O
The	O
tasks	O
performed	O
by	O
the	O
unit	O
16	O
are	O
as	O
follows	O
:	O
1	O
.	O
Grammar	O
parsing	O
and	O
word	O
spotting	O
techniques	O
are	O
used	O
to	O
detect	O
errors	O
and	O
recover	O
words	O
that	O
have	O
not	O
been	O
identified;2	O
.	O
Identification	O
of	O
the	O
template	O
string	O
of	O
words	O
which	O
best	O
fits	O
the	O
contextual	O
information	O
at	O
that	O
time	O
.	O
Since	O
particular	O
strings	O
of	O
words	O
are	O
more	O
likely	O
than	O
others	O
to	O
be	O
spoken	O
during	O
particular	O
environmental	O
circumstances	O
,	O
this	O
can	O
be	O
used	O
to	O
improve	O
the	O
identification	O
of	O
the	O
particular	O
command	O
spoken	O
by	O
the	O
user	O
;	O
and3	O
.	O
Following	O
the	O
final	O
identification	O
,	O
the	O
processing	O
unit	O
16	O
may	O
generate	O
signals	O
for	O
use	O
in	O
bringing	O
up	O
to	O
date	O
the	O
vocabulary	O
sub-set	O
selection	O
performed	O
by	O
32	O
.	O
These	O
signals	O
are	O
supplied	O
to	O
the	O
vocabulary	O
storage	O
30	O
via	O
line	O
19.Although	O
the	O
above	O
system	O
has	O
been	O
described	O
for	O
producing	O
command	O
signals	O
,	O
such	O
as	O
for	O
controlling	O
equipment	O
,	O
a	O
similar	O
system	O
may	O
be	O
used	O
in	O
a	O
speech	O
communication	O
system	O
.	O
In	O
such	O
an	O
alternative	O
arrangement	O
,	O
the	O
line	O
20	O
,	O
instead	O
of	O
carrying	O
command	O
signals	O
would	O
carry	O
speech	O
signals	O
in	O
respect	O
of	O
the	O
identified	O
words	O
and	O
phrases	O
.	O

1	O
.	O
Speech	O
processing	O
apparatus	O
comprising	O
:	O
first	O
means	O
for	O
deriving	O
a	O
first	O
signal	O
in	O
accordance	O
with	O
the	O
utterances	O
made	O
by	O
a	O
speaker	O
,	O
second	O
means	O
responsive	O
to	O
the	O
physical	O
forces	O
,	O
including	O
at	O
least	O
one	O
of	O
the	O
forces	O
of	O
physical	O
vibration	O
and	O
acceleration	O
acting	O
on	O
the	O
speaker	O
,	O
which	O
have	O
the	O
effect	O
of	O
altering	O
the	O
characteristics	O
of	O
the	O
voice	O
sounds	O
uttered	O
by	O
the	O
speaker	O
;	O
storage	O
means	O
containing	O
a	O
reference	O
vocabulary	O
in	O
the	O
form	O
of	O
patterned	O
templates	O
of	O
words	O
;	O
and	O
a	O
processing	O
unit	O
receiving	O
the	O
output	O
from	O
the	O
first	O
means	O
and	O
from	O
the	O
second	O
means	O
and	O
performing	O
dynamic	O
adaptation	O
on	O
said	O
templates	O
in	O
accordance	O
with	O
the	O
output	O
of	O
said	O
second	O
means	O
to	O
thereby	O
modify	O
said	O
templates	O
to	O
anticipate	O
the	O
behavior	O
of	O
the	O
speaker	O
's	O
speech	O
production	O
mechanism	O
as	O
affected	O
by	O
the	O
physical	O
forces	O
detected	O
by	O
said	O
second	O
means	O
;	O
said	O
processing	O
unit	O
comparing	O
the	O
first	O
output	O
signal	O
with	O
the	O
modified	O
templates	O
to	O
produce	O
a	O
second	O
output	O
signal	O
in	O
accordance	O
with	O
the	O
speech	O
sounds	O
uttered	O
by	O
the	O
speaker	O
which	O
are	O
thus	O
compensated	O
,	O
at	O
least	O
in	O
part	O
,	O
by	O
the	O
physical	O
forces	O
acting	O
on	O
the	O
speaker	O
.	O

2	O
.	O
Speech	O
processing	O
apparatus	O
according	O
to	O
claim	O
1	O
,	O
wherein	O
said	O
processing	O
unit	O
contains	O
information	O
about	O
how	O
vowel	O
and	O
consonant	O
groupings	O
are	O
influenced	O
by	O
forces	O
on	O
the	O
speaker	O
said	O
processing	O
unit	O
in	O
response	O
to	O
said	O
information	O
modifies	O
said	O
templates	O
.	O

3	O
.	O
Speech	O
processing	O
apparatus	O
according	O
to	O
claim	O
2	O
,	O
wherein	O
said	O
processing	O
unit	O
compares	O
the	O
first	O
output	O
signal	O
with	O
the	O
modified	O
templates	O
by	O
pattern	O
matching	O
techniques	O
.	O

4	O
.	O
Speech	O
processing	O
apparatus	O
according	O
to	O
claim	O
1	O
,	O
wherein	O
said	O
apparatus	O
includes	O
means	O
supplying	O
mode	O
data	O
to	O
said	O
storage	O
means	O
,	O
and	O
wherein	O
said	O
storage	O
means	O
includes	O
means	O
for	O
carrying	O
out	O
active	O
word	O
selection	O
from	O
said	O
reference	O
vocabulary	O
in	O
accordance	O
with	O
said	O
mode	O
data	O
.	O

5	O
.	O
Speech	O
processing	O
apparatus	O
according	O
to	O
claim	O
1	O
,	O
wherein	O
said	O
apparatus	O
includes	O
feedback	O
indicator	O
means	O
for	O
providing	O
information	O
to	O
the	O
speaker	O
.	O

6	O
.	O
A	O
method	O
of	O
providing	O
speech	O
processing	O
comprising	O
the	O
steps	O
of	O
:	O
deriving	O
first	O
signals	O
from	O
speech	O
sounds	O
uttered	O
by	O
a	O
speaker	O
;	O
deriving	O
second	O
signals	O
having	O
a	O
manifestation	O
representative	O
of	O
at	O
least	O
one	O
of	O
the	O
environmental	O
physical	O
forces	O
corresponding	O
respectively	O
to	O
the	O
acceleration	O
and	O
vibration	O
forces	O
acting	O
on	O
the	O
speaker	O
of	O
the	O
kind	O
which	O
tend	O
to	O
affect	O
the	O
nature	O
of	O
the	O
speech	O
utterances	O
made	O
by	O
the	O
speaker	O
;	O
performing	O
dynamic	O
adaptation	O
on	O
stored	O
pattern	O
templates	O
of	O
words	O
in	O
a	O
vocabulary	O
store	O
in	O
accordance	O
with	O
said	O
force	O
signals	O
to	O
modify	O
the	O
templates	O
to	O
anticipate	O
the	O
behavior	O
of	O
the	O
speaker	O
's	O
speech	O
production	O
mechanism	O
during	O
the	O
force	O
;	O
and	O
comparing	O
the	O
said	O
speech	O
signals	O
with	O
the	O
modified	O
templates	O
to	O
provide	O
thereby	O
an	O
indication	O
of	O
the	O
similarity	O
between	O
speech	O
sounds	O
and	O
stored	O
words	O
that	O
is	O
compensated	O
at	O
least	O
in	O
part	O
for	O
the	O
forces	O
.	O

