US	O
6463412	O
B1	O
20021008	O

US	O
09464847	O
19991216	O

09	O
eng	O
eng	O

US	O
09464847	O
19991216	O

20021008	O

20021008	O

7G	O
10L	O
13/00	O
A	O
7	O
G	O
10	O
L	O
13	O
00	O
A	O

7G	O
10L	O
15/00	O
B	O
7	O
G	O
10	O
L	O
15	O
00	O
B	O

G10L	O
15/00	O
20060101C	O
I20051008RMEP	O

20060101	O

C	O
G	O
10	O
L	O
15	O
00	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
15/26	O
20060101A	O
I20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
15	O
26	O
I	O

20051008	O

EP	O

R	O
M	O

G10L	O
21/00	O
20060101C	O
N20051008RMEP	O

20060101	O

C	O
G	O
10	O
L	O
21	O
00	O
N	O

20051008	O

EP	O

R	O
M	O

G10L	O
21/00	O
20060101A	O
N20051008RMEP	O

20060101	O

A	O
G	O
10	O
L	O
21	O
00	O
N	O

20051008	O

EP	O

R	O
M	O

US	O

704/246	O
704	O
246	O

704/261	O
704	O
261	O

704/269	O
704	O
269	O

704/E15.045	O
704	O
E15	O
.	O
045	O

G10L	O
15/26A	O
G	O
10	O
L	O
15	O
26	O

A	O

S10L	O
21	O
:	O
00V	O
S	O
10	O
L	O
21	O
00	O

V	O

US	O

704/235	O
704	O
235	O

US	O

704/246	O
704	O
246	O

US	O

704/258	O
704	O
258	O

US	O

704/260	O
704	O
260	O

US	O

704/261	O
704	O
261	O

US	O

704/266	O
704	O
266	O

US	O

704/267	O
704	O
267	O

US	O

704/268	O
704	O
268	O

US	O

704/269	O
704	O
269	O

US	O

704/270	O
704	O
270	O

US	O

704/270	O
.	O
1	O
704	O
270	O
.	O
1	O

32	O
High	O
performance	O
voice	O
transformation	O
apparatus	O
and	O
method	O

US	O
5915237	O
A	O
Boss	O
et_al.	O

19990622	O

19961213	O

US	O
5933805	O
A	O
Boss	O
et_al.	O

19990803	O

19961213	O

US	O
6151576	O
A	O
Warnock	O
et_al.	O

20001121	O

19980811	O

US	O
6173250	O
B1	O
Jong	O
20010109	O

19980603	O

US	O
6247015	O
B1	O
Baumgartner	O
et_al.	O

20010612	O

19980908	O

US	O
6336092	O
B1	O
Gibson	O
et_al.	O

20020101	O

19970428	O

US	O
7177801	O
B2	O
20070213	O

20011221	O

EP	O
1899955	O
A1	O
20080319	O

20060320	O

DE	O
102004048707	O
B3	O
20051229	O

20041006	O

EP	O
1899955	O
B1	O
20090902	O

20060320	O

US	O
7610200	O
B2	O
20091027	O

20040830	O

US	O
7676372	O
B1	O
20100309	O

20000216	O

US	O
7464034	O
B2	O
20081209	O

20040927	O

US	O
7412377	O
B2	O
20080812	O

20031219	O

US	O
7379872	O
B2	O
20080527	O

20030117	O

US	O
7299177	O
B2	O
20071120	O

20030530	O

US	O
7228273	O
B2	O
20070605	O

20021112	O

US	O
6987514	O
B1	O
20060117	O

20001109	O

US	O
6836761	O
B1	O
20041228	O

20001020	O

US	O
7702503	O
B2	O
20100420	O

20080731	O

US	O
7778832	O
B2	O
20100817	O

20070926	O

US	O
7825321	O
B2	O
20101102	O

20060126	O

US	O
7899742	O
B2	O
20110301	O

20030619	O

US	O
7940897	O
B2	O
20110510	O

20050624	O

CN	O
101253547	O
B	O
20110706	O

20060320	O

US	O
7987092	O
B2	O
20110726	O

20080408	O

International	O
Business	O
Machines	O
Corporation	O
02	O

Armonk	O
NY	O

Baumgartner	O
,	O
Jason	O
Raymond	O

Austin	O
TX	O

Roberts	O
,	O
Steven	O
Leonard	O

Austin	O
TX	O

Malik	O
,	O
Nadeem	O

Austin	O
TX	O

Andersen	O
,	O
Flemming	O

Austin	O
TX	O

Yee	O
,	O
Duke	O
W.	O

McBurney	O
,	O
Mark	O
E.	O

Walder	O
,	O
Jr	O
.	O
,	O
Stephen	O
J.	O

Banks-Harold	O
Marsha	O
D.	O

2654	O

Lerner	O
Martin	O

US	O
6463412	O
B1	O
20021008	O

19991216	O

US	O
6463412	O
B1	O
20021008	O

19991216	O

A	O
high	O
performance	O
voice	O
transformation	O
apparatus	O
and	O
method	O
is	O
provided	O
in	O
which	O
voice	O
input	O
is	O
transformed	O
into	O
a	O
symbolic	O
representation	O
of	O
phonemes	O
in	O
the	O
voice	O
input	O
.	O
The	O
symbolic	O
representation	O
is	O
used	O
to	O
retrieve	O
output	O
voice	O
segments	O
of	O
a	O
selected	O
target	O
speaker	O
for	O
use	O
in	O
outputting	O
the	O
voice	O
input	O
in	O
a	O
different	O
voice	O
.	O
In	O
addition	O
,	O
voice	O
input	O
characteristics	O
are	O
extracted	O
from	O
the	O
voice	O
input	O
and	O
are	O
then	O
applied	O
to	O
the	O
output	O
voice	O
segments	O
to	O
thereby	O
provide	O
a	O
more	O
realistic	O
human	O
sounding	O
voice	O
output	O
.	O

19991229	O

AS	O
ASSIGNMENT	O
N	O
US	O
6463412B1	O
INTERNATIONAL	O
BUSINESS	O
MACHINES	O
CORPORATION	O
,	O
NEW	O
Y	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNORS:BAUMGARTNER	O
,	O
JASON	O
RAYMOND;ROBERTS	O
,	O
STEVEN	O
LEONARD;MALIK	O
,	O
NADEEM	O
;	O
AND	O
OTHERS;REEL/FRAME:010486/0005	O

19991216	O

20060109	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
6463412B1	O
4	O

20090306	O

AS	O
ASSIGNMENT	O
N	O
US	O
6463412B1	O
NUANCE	O
COMMUNICATIONS	O
,	O
INC.	O
,	O
MASSACHUSETTS	O
ASSIGNMENT	O
OF	O
ASSIGNORS	O
INTEREST;ASSIGNOR:INTERNATIONAL	O
BUSINESS	O
MACHINES	O
CORPORATION;REEL/FRAME:022354/0566	O

20081231	O

20100408	O

FPAY	O
+	O
FEE	O
PAYMENT	O
N	O
US	O
6463412B1	O
8	O

BACKGROUND	O
OF	O
THE	O
INVENTION	O
1	O
.	O
Technical	O
Field	O
The	O
present	O
invention	O
is	O
directed	O
to	O
an	O
apparatus	O
and	O
method	O
for	O
high	O
performance	O
voice	O
transformation	O
.	O
In	O
particular	O
,	O
the	O
present	O
invention	O
is	O
directed	O
to	O
an	O
apparatus	O
and	O
method	O
for	O
transforming	O
an	O
input	O
voice	O
into	O
an	O
output	O
voice	O
different	O
from	O
the	O
input	O
voice	O
while	O
maintaining	O
some	O
voice	O
characteristics	O
between	O
the	O
input	O
voice	O
and	O
the	O
output	O
voice	O
.	O

2	O
.	O
Description	O
of	O
Related	O
Art	O
Voice	O
recognition	O
devices	O
are	O
generally	O
known	O
in	O
the	O
art	O
of	O
voice	O
technologies	O
.	O
With	O
voice	O
recognition	O
devices	O
,	O
a	O
user	O
speaks	O
into	O
a	O
microphone	O
and	O
the	O
voice	O
recognition	O
device	O
recognizes	O
words	O
and	O
phrases	O
from	O
the	O
user	O
's	O
speech	O
.	O
These	O
recognized	O
words	O
and	O
phrases	O
may	O
then	O
be	O
used	O
,	O
for	O
example	O
,	O
to	O
generate	O
textual	O
messages	O
on	O
a	O
computer	O
display	O
.	O

Voice	O
synthesis	O
is	O
also	O
generally	O
known	O
in	O
the	O
art	O
.	O
With	O
voice	O
synthesis	O
,	O
textual	O
messages	O
are	O
input	O
to	O
a	O
voice	O
synthesis	O
device	O
which	O
then	O
synthesizes	O
the	O
text	O
into	O
a	O
speech	O
output	O
.	O
Voice	O
synthesis	O
devices	O
are	O
limited	O
in	O
the	O
quality	O
of	O
the	O
output	O
speech	O
due	O
to	O
their	O
objective	O
manner	O
of	O
analyzing	O
the	O
textual	O
messages	O
.	O
Thus	O
,	O
the	O
speech	O
that	O
is	O
output	O
by	O
the	O
voice	O
synthesis	O
device	O
typically	O
has	O
a	O
mechanical	O
quality	O
to	O
it	O
and	O
does	O
not	O
accurately	O
reflect	O
human	O
speech	O
patterns	O
.	O

Moreover	O
,	O
with	O
the	O
increased	O
use	O
of	O
computer	O
games	O
and	O
,	O
in	O
particular	O
,	O
modem	O
or	O
networked	O
video	O
games	O
,	O
the	O
ability	O
to	O
speak	O
with	O
other	O
players	O
during	O
play	O
has	O
been	O
emphasized	O
.	O
The	O
current	O
video	O
game	O
technology	O
is	O
limited	O
to	O
conversing	O
with	O
other	O
players	O
through	O
typed	O
messages	O
or	O
by	O
way	O
of	O
using	O
ones	O
own	O
digitized	O
speech	O
.	O

With	O
this	O
latter	O
manner	O
of	O
communicating	O
,	O
if	O
a	O
player	O
has	O
a	O
speech	O
impediment	O
or	O
a	O
thick	O
accent	O
,	O
other	O
players	O
may	O
find	O
it	O
difficult	O
to	O
communicate	O
with	O
him/her	O
.	O
Furthermore	O
,	O
players	O
may	O
find	O
it	O
more	O
enjoyable	O
to	O
speak	O
in	O
a	O
voice	O
other	O
than	O
their	O
own	O
,	O
such	O
as	O
a	O
character	O
in	O
the	O
video	O
game	O
which	O
they	O
are	O
playing	O
.	O

Thus	O
,	O
it	O
would	O
be	O
advantageous	O
to	O
have	O
an	O
apparatus	O
and	O
method	O
that	O
may	O
transform	O
an	O
input	O
voice	O
into	O
a	O
different	O
output	O
voice	O
while	O
maintaining	O
some	O
of	O
the	O
characteristics	O
of	O
the	O
input	O
voice	O
to	O
more	O
closely	O
resemble	O
actual	O
human	O
speech	O
.	O

SUMMARY	O
OF	O
THE	O
INVENTION	O
The	O
present	O
invention	O
provides	O
a	O
high	O
performance	O
voice	O
transformation	O
apparatus	O
and	O
method	O
.	O
The	O
voice	O
transformation	O
apparatus	O
includes	O
a	O
controller	O
,	O
an	O
input	O
device	O
interface	O
,	O
an	O
input	O
voice	O
characteristic	O
extraction	O
device	O
,	O
a	O
voice	O
recognition	O
device	O
,	O
a	O
voice	O
dictionary	O
interface	O
,	O
and	O
a	O
speech	O
output	O
generator	O
.	O

The	O
input	O
device	O
interface	O
provides	O
a	O
communication	O
pathway	O
to	O
a	O
voice	O
input	O
device	O
.	O
The	O
voice	O
input	O
from	O
the	O
voice	O
input	O
device	O
is	O
provided	O
to	O
the	O
voice	O
transformation	O
apparatus	O
,	O
which	O
responds	O
with	O
the	O
controller	O
instructing	O
the	O
input	O
voice	O
characteristic	O
extraction	O
device	O
to	O
extract	O
voice	O
characteristics	O
from	O
the	O
voice	O
input	O
.	O

At	O
the	O
same	O
time	O
as	O
the	O
input	O
voice	O
characteristic	O
extraction	O
is	O
being	O
performed	O
,	O
or	O
before	O
or	O
after	O
the	O
input	O
voice	O
characteristic	O
extraction	O
is	O
performed	O
,	O
the	O
controller	O
instructs	O
the	O
voice	O
recognition	O
device	O
to	O
perform	O
voice	O
recognition	O
functions	O
on	O
the	O
voice	O
input	O
.	O
The	O
voice	O
recognition	O
functions	O
include	O
breaking	O
down	O
the	O
voice	O
input	O
into	O
symbolic	O
representations	O
of	O
the	O
phonemes	O
that	O
make	O
up	O
the	O
voice	O
input	O
,	O
which	O
are	O
then	O
forwarded	O
to	O
the	O
voice	O
dictionary	O
interface	O
.	O

The	O
voice	O
dictionary	O
interface	O
provides	O
a	O
communication	O
pathway	O
to	O
one	O
or	O
more	O
voice	O
dictionaries	O
.	O
The	O
voice	O
dictionaries	O
consist	O
of	O
an	O
array	O
of	O
symbolic	O
representations	O
for	O
phonemes	O
associated	O
with	O
a	O
target	O
speaker	O
output	O
speech	O
pattern	O
segment	O
.	O
The	O
voice	O
dictionary	O
interface	O
“	O
looks-up	O
”	O
target	O
speaker	O
output	O
speech	O
pattern	O
segments	O
based	O
on	O
the	O
symbolic	O
representations	O
of	O
the	O
phonemes	O
from	O
the	O
input	O
voice	O
pattern	O
.	O

The	O
target	O
speaker	O
output	O
speech	O
pattern	O
segments	O
are	O
forwarded	O
to	O
the	O
speech	O
output	O
generator	O
which	O
generates	O
the	O
output	O
speech	O
signals	O
that	O
are	O
then	O
transformed	O
into	O
output	O
by	O
the	O
output	O
device	O
.	O
The	O
speech	O
output	O
generator	O
generates	O
the	O
output	O
speech	O
signals	O
by	O
using	O
the	O
target	O
speaker	O
output	O
speech	O
pattern	O
segments	O
forwarded	O
by	O
the	O
voice	O
dictionary	O
interface	O
and	O
applying	O
the	O
voice	O
input	O
characteristics	O
extracted	O
from	O
the	O
voice	O
input	O
by	O
the	O
input	O
voice	O
characteristic	O
extraction	O
device	O
.	O

In	O
the	O
case	O
that	O
some	O
sounds	O
in	O
the	O
voice	O
input	O
may	O
not	O
be	O
recognized	O
,	O
the	O
voice	O
recognition	O
device	O
may	O
forward	O
the	O
unrecognized	O
segment	O
of	O
the	O
voice	O
input	O
to	O
the	O
speech	O
output	O
generator	O
without	O
performing	O
a	O
voice	O
dictionary	O
look-up	O
function	O
.	O
In	O
this	O
way	O
,	O
the	O
voice	O
input	O
segment	O
that	O
is	O
not	O
recognized	O
may	O
be	O
output	O
by	O
the	O
output	O
device	O
rather	O
than	O
performing	O
an	O
erroneous	O
look-up	O
of	O
an	O
output	O
voice	O
pattern	O
segment	O
.	O

In	O
addition	O
,	O
to	O
provide	O
a	O
more	O
graceful	O
transition	O
between	O
the	O
output	O
voice	O
pattern	O
segments	O
and	O
the	O
voice	O
input	O
segments	O
which	O
could	O
not	O
be	O
recognized	O
,	O
in	O
the	O
output	O
of	O
the	O
output	O
device	O
,	O
the	O
voice	O
input	O
segment	O
that	O
was	O
not	O
recognized	O
may	O
have	O
voice	O
pattern	O
characteristics	O
of	O
the	O
selected	O
voice	O
dictionary	O
speaker	O
applied	O
to	O
it	O
.	O
These	O
voice	O
pattern	O
characteristics	O
of	O
the	O
selected	O
voice	O
dictionary	O
speaker	O
may	O
be	O
obtained	O
from	O
the	O
voice	O
dictionary	O
as	O
a	O
default	O
setting	O
.	O

Thus	O
,	O
with	O
the	O
present	O
invention	O
,	O
a	O
user	O
may	O
input	O
his/her	O
voice	O
and	O
designate	O
a	O
different	O
output	O
voice	O
from	O
his/her	O
own	O
to	O
be	O
used	O
for	O
outputting	O
transformed	O
speech	O
.	O
Furthermore	O
,	O
the	O
output	O
voice	O
may	O
more	O
closely	O
resemble	O
actual	O
human	O
speech	O
because	O
the	O
characteristics	O
of	O
the	O
user	O
's	O
input	O
voice	O
pattern	O
are	O
applied	O
to	O
the	O
output	O
voice	O
.	O
Thus	O
,	O
the	O
output	O
voice	O
will	O
use	O
the	O
same	O
voice	O
fluctuations	O
,	O
same	O
pitch	O
,	O
volume	O
,	O
etc	O
.	O
as	O
that	O
of	O
the	O
user	O
.	O

BRIEF	O
DESCRIPTION	O
OF	O
THE	O
DRAWINGS	O
The	O
novel	O
features	O
believed	O
characteristic	O
of	O
the	O
invention	O
are	O
set	O
forth	O
in	O
the	O
appended	O
claims	O
.	O
The	O
invention	O
itself	O
,	O
however	O
,	O
as	O
well	O
as	O
a	O
preferred	O
mode	O
of	O
use	O
,	O
further	O
objectives	O
and	O
advantages	O
thereof	O
,	O
will	O
best	O
be	O
understood	O
by	O
reference	O
to	O
the	O
following	O
detailed	O
description	O
of	O
an	O
illustrative	O
embodiment	O
when	O
read	O
in	O
conjunction	O
with	O
the	O
accompanying	O
drawings	O
,	O
wherein	O
like	O
numerals	O
designate	O
like	O
elements	O
,	O
and	O
wherein	O
:	O

FIG	O
.	O
1is	O
an	O
exemplary	O
block	O
diagram	O
illustrating	O
a	O
voice	O
transformation	O
apparatus	O
according	O
the	O
present	O
invention	O
;	O

FIG	O
.	O
2is	O
an	O
exemplary	O
data	O
flow	O
diagram	O
according	O
to	O
the	O
present	O
invention	O
;	O

FIG	O
.	O
3is	O
an	O
exemplary	O
diagram	O
of	O
a	O
voice	O
dictionary	O
according	O
to	O
the	O
present	O
invention	O
;	O

FIG	O
.	O
4is	O
a	O
flowchart	O
outlining	O
an	O
exemplary	O
operation	O
of	O
the	O
voice	O
transformation	O
apparatus	O
according	O
to	O
the	O
present	O
invention	O
;	O
and	O

FIG	O
.	O
5is	O
an	O
exemplary	O
block	O
diagram	O
of	O
a	O
network	O
system	O
in	O
which	O
the	O
present	O
invention	O
may	O
be	O
implemented	O
.	O

DETAILED	O
DESCRIPTION	O
OF	O
THE	O
PREFERRED	O
EMBODIMENT	O

FIG	O
.	O
1is	O
an	O
exemplary	O
block	O
diagram	O
of	O
a	O
voice	O
transformation	O
apparatus100according	O
to	O
the	O
present	O
invention	O
.	O
As	O
shown	O
inFIG	O
.	O
1	O
,	O
the	O
voice	O
transformation	O
apparatus100includes	O
a	O
controller110	O
,	O
an	O
input	O
device	O
interface120	O
,	O
a	O
input	O
voice	O
characteristic	O
extraction	O
device130	O
,	O
a	O
voice	O
recognition	O
device140	O
,	O
a	O
voice	O
dictionary	O
interface150	O
,	O
and	O
a	O
speech	O
output	O
generator160	O
.	O

The	O
above	O
elements110-160may	O
be	O
,	O
for	O
example	O
,	O
hardware	O
components	O
of	O
a	O
data	O
processing	O
system	O
that	O
are	O
dedicated	O
to	O
performing	O
the	O
voice	O
transformation	O
apparatus100functions	O
hereafter	O
described	O
.	O
Alternatively	O
,	O
the	O
above	O
elements110-160may	O
be	O
embodied	O
in	O
software	O
executed	O
by	O
a	O
programmable	O
processing	O
device	O
.	O

The	O
elements110-160are	O
in	O
communication	O
with	O
one	O
another	O
over	O
control/signal	O
bus195	O
.	O
Although	O
a	O
bus	O
architecture	O
is	O
shown	O
inFIG	O
.	O
1	O
,	O
the	O
invention	O
is	O
not	O
limited	O
to	O
such	O
an	O
embodiment	O
.	O
Rather	O
,	O
any	O
type	O
of	O
architecture	O
capable	O
of	O
facilitating	O
communication	O
among	O
the	O
elements110-160may	O
be	O
used	O
without	O
departing	O
from	O
the	O
spirit	O
and	O
scope	O
of	O
the	O
invention	O
.	O

The	O
input	O
device	O
interface120provides	O
a	O
communication	O
pathway	O
to	O
a	O
voice	O
input	O
device170	O
.	O
The	O
voice	O
input	O
device170may	O
be	O
,	O
for	O
example	O
,	O
a	O
microphone	O
or	O
other	O
audio	O
pick-up	O
device	O
that	O
is	O
capable	O
of	O
converting	O
speech	O
into	O
analog	O
or	O
digital	O
signals	O
.	O
If	O
the	O
voice	O
input	O
device170transforms	O
speech	O
into	O
analog	O
signals	O
,	O
an	O
A/D	O
converter	O
(	O
not	O
shown	O
)	O
may	O
be	O
used	O
to	O
convert	O
the	O
analog	O
signals	O
into	O
digital	O
signals	O
for	O
processing	O
by	O
the	O
voice	O
transformation	O
apparatus100	O
.	O

Alternatively	O
,	O
the	O
voice	O
input	O
device170may	O
be	O
a	O
storage	O
medium	O
upon	O
which	O
a	O
voice	O
print	O
is	O
stored	O
.	O
For	O
example	O
,	O
the	O
voice	O
input	O
device170may	O
be	O
a	O
CD-ROM	O
,	O
a	O
hard	O
disk	O
,	O
a	O
floppy	O
disk	O
,	O
a	O
magnetic	O
tape	O
,	O
or	O
the	O
like	O
,	O
on	O
which	O
a	O
voice	O
is	O
stored	O
as	O
an	O
audio	O
file	O
,	O
such	O
as	O
a	O
.	O
WAV	O
file	O
.	O
In	O
short	O
,	O
any	O
type	O
of	O
device	O
capable	O
of	O
inputting	O
voice	O
data	O
into	O
the	O
voice	O
transformation	O
apparatus100is	O
intended	O
to	O
be	O
within	O
the	O
spirit	O
and	O
scope	O
of	O
the	O
present	O
invention	O
.	O

The	O
voice	O
input	O
from	O
the	O
voice	O
input	O
device170is	O
input	O
to	O
the	O
voice	O
transformation	O
apparatus100via	O
the	O
input	O
interface120	O
.	O
In	O
response	O
to	O
receiving	O
the	O
voice	O
input	O
from	O
the	O
voice	O
input	O
device170	O
,	O
the	O
controller110instructs	O
the	O
input	O
voice	O
characteristic	O
extraction	O
device130to	O
extract	O
voice	O
characteristics	O
from	O
the	O
voice	O
input	O
.	O
Such	O
characteristics	O
include	O
,	O
for	O
example	O
,	O
speech	O
volume	O
,	O
pitch	O
,	O
pause	O
lengths	O
,	O
and	O
the	O
like	O
.	O
These	O
characteristics	O
are	O
preferably	O
normalized	O
to	O
account	O
for	O
variations	O
amongst	O
a	O
plurality	O
of	O
speakers	O
and	O
thereby	O
capture	O
correct	O
voice	O
characteristics	O
.	O

The	O
extraction	O
of	O
voice	O
characteristics	O
by	O
the	O
input	O
voice	O
characteristic	O
extraction	O
device130may	O
be	O
performed	O
,	O
for	O
example	O
,	O
through	O
digital	O
filtering	O
techniques	O
.	O
For	O
example	O
,	O
filtering	O
may	O
obtain	O
speech	O
volume	O
characteristic	O
information	O
by	O
determining	O
the	O
normalized	O
amplitudes	O
of	O
the	O
input	O
voice	O
pattern	O
for	O
a	O
plurality	O
of	O
samplings	O
.	O
Methods	O
other	O
than	O
filtering	O
to	O
extracting	O
voice	O
characteristics	O
from	O
an	O
input	O
voice	O
pattern	O
may	O
be	O
utilized	O
without	O
departing	O
from	O
the	O
spirit	O
and	O
scope	O
of	O
the	O
present	O
invention	O
.	O

At	O
the	O
same	O
time	O
as	O
the	O
input	O
voice	O
characteristic	O
extraction	O
is	O
being	O
performed	O
,	O
or	O
before	O
or	O
after	O
the	O
input	O
voice	O
characteristic	O
extraction	O
is	O
performed	O
,	O
the	O
controller110instructs	O
the	O
voice	O
recognition	O
device140to	O
perform	O
voice	O
recognition	O
functions	O
on	O
the	O
input	O
voice	O
pattern	O
.	O
The	O
voice	O
recognition	O
device140may	O
make	O
use	O
of	O
a	O
trained	O
neural	O
network	O
for	O
recognizing	O
speech	O
segments	O
from	O
a	O
voice	O
input	O
.	O
The	O
voice	O
recognition	O
device140may	O
be	O
trained	O
through	O
repeated	O
use	O
of	O
training	O
phrases	O
such	O
that	O
the	O
neural	O
network	O
learns	O
the	O
manner	O
in	O
which	O
a	O
user	O
speaks	O
.	O
Thus	O
,	O
even	O
if	O
the	O
user	O
has	O
a	O
speech	O
impediment	O
,	O
the	O
neural	O
network	O
of	O
the	O
voice	O
recognition	O
device140may	O
accommodate	O
him/her	O
.	O

The	O
voice	O
recognition	O
functions	O
performed	O
by	O
the	O
voice	O
recognition	O
device140include	O
extracting	O
the	O
symbolic	O
phoneme-based	O
content	O
of	O
the	O
input	O
voice	O
pattern	O
.	O
Phonemes	O
are	O
a	O
speech	O
utterance	O
,	O
such	O
as	O
“	O
k	O
,	O
”	O
“	O
ch	O
,	O
”	O
and	O
“	O
sh	O
,	O
”	O
that	O
is	O
used	O
in	O
synthetic	O
speech	O
systems	O
to	O
compose	O
words	O
for	O
audio	O
output	O
.	O
The	O
voice	O
recognition	O
device140breaks	O
down	O
the	O
input	O
voice	O
pattern	O
into	O
symbolic	O
representations	O
of	O
the	O
phonemes	O
that	O
make	O
up	O
the	O
input	O
voice	O
pattern	O
which	O
are	O
then	O
forwarded	O
to	O
the	O
voice	O
dictionary	O
interface150	O
.	O

The	O
voice	O
dictionary	O
interface150provides	O
a	O
communication	O
pathway	O
to	O
one	O
or	O
more	O
voice	O
dictionaries180	O
.	O
The	O
voice	O
dictionaries180consist	O
of	O
an	O
array	O
of	O
symbolic	O
representations	O
for	O
phonemes	O
associated	O
with	O
a	O
target	O
speaker	O
output	O
speech	O
pattern	O
segment	O
.	O
The	O
voice	O
dictionary	O
interface150	O
“	O
looks-up	O
”	O
target	O
speaker	O
output	O
speech	O
pattern	O
segments	O
based	O
on	O
the	O
symbolic	O
representations	O
of	O
the	O
phonemes	O
from	O
the	O
input	O
voice	O
pattern	O
.	O
Thus	O
,	O
for	O
each	O
symbolic	O
representation	O
,	O
a	O
target	O
speaker	O
output	O
speech	O
pattern	O
segment	O
is	O
retrieved	O
from	O
one	O
of	O
the	O
voice	O
dictionaries180	O
.	O

The	O
target	O
speaker	O
output	O
speech	O
pattern	O
segments	O
may	O
be	O
representative	O
of	O
a	O
speaker	O
other	O
than	O
the	O
speaker	O
of	O
the	O
input	O
voice	O
pattern	O
.	O
Thus	O
,	O
by	O
using	O
the	O
present	O
voice	O
transformation	O
apparatus100	O
,	O
a	O
speaker	O
of	O
the	O
input	O
voice	O
pattern	O
may	O
change	O
his/her	O
voice	O
using	O
the	O
voice	O
dictionary	O
such	O
that	O
the	O
speaker	O
maintains	O
anonymity	O
.	O
Additionally	O
,	O
if	O
the	O
user	O
of	O
the	O
voice	O
transformation	O
apparatus100has	O
a	O
speech	O
impediment	O
,	O
the	O
target	O
speaker	O
output	O
speech	O
pattern	O
segments	O
may	O
provide	O
a	O
speech	O
output	O
that	O
is	O
free	O
of	O
the	O
speech	O
impediment	O
.	O
Furthermore	O
,	O
the	O
speaker	O
may	O
utilize	O
the	O
voice	O
transformation	O
apparatus100for	O
entertainment	O
purposes	O
to	O
provide	O
a	O
voice	O
output	O
different	O
from	O
his/her	O
own	O
.	O

The	O
target	O
speaker	O
output	O
speech	O
pattern	O
segments	O
are	O
forwarded	O
to	O
the	O
speech	O
output	O
generator160which	O
generates	O
the	O
output	O
speech	O
signals	O
that	O
are	O
then	O
transformed	O
into	O
output	O
by	O
the	O
output	O
device190	O
.	O
The	O
speech	O
output	O
generator160generates	O
the	O
output	O
speech	O
signals	O
by	O
using	O
the	O
target	O
speaker	O
output	O
speech	O
pattern	O
segments	O
forwarded	O
by	O
the	O
voice	O
dictionary	O
interface150and	O
applying	O
the	O
input	O
voice	O
characteristics	O
extracted	O
from	O
the	O
input	O
voice	O
pattern	O
.	O
In	O
this	O
way	O
,	O
the	O
characteristics	O
,	O
such	O
as	O
the	O
pitch	O
,	O
volume	O
,	O
pause	O
lengths	O
,	O
and	O
the	O
like	O
,	O
of	O
the	O
input	O
voice	O
pattern	O
may	O
be	O
utilized	O
to	O
provide	O
a	O
more	O
realistic	O
speech	O
output	O
.	O
The	O
application	O
of	O
the	O
input	O
voice	O
characteristics	O
extracted	O
from	O
the	O
input	O
voice	O
pattern	O
may	O
be	O
performed	O
using	O
digital	O
filtering	O
techniques	O
,	O
for	O
example	O
.	O

As	O
noted	O
above	O
,	O
the	O
voice	O
dictionaries180may	O
include	O
one	O
or	O
more	O
voice	O
dictionaries	O
.	O
These	O
voice	O
dictionaries180may	O
be	O
incorporated	O
with	O
the	O
voice	O
transformation	O
apparatus100	O
,	O
for	O
example	O
,	O
in	O
a	O
plug-and-play	O
manner	O
.	O
Thus	O
,	O
the	O
voice	O
dictionaries180may	O
be	O
provided	O
as	O
data	O
on	O
an	O
integrated	O
circuit	O
(	O
IC	O
)	O
card	O
,	O
a	O
floppy	O
disk	O
,	O
a	O
CD-ROM	O
,	O
a	O
game	O
cartridge	O
,	O
or	O
any	O
other	O
type	O
of	O
storage	O
medium	O
which	O
may	O
be	O
“	O
plugged	O
into	O
”	O
the	O
voice	O
transformation	O
apparatus100or	O
otherwise	O
loaded	O
into	O
the	O
voice	O
transformation	O
apparatus100	O
.	O
In	O
addition	O
,	O
the	O
voice	O
dictionaries180may	O
be	O
made	O
available	O
for	O
download	O
from	O
one	O
or	O
more	O
servers	O
on	O
a	O
network	O
,	O
such	O
as	O
the	O
Internet	O
.	O

When	O
a	O
plurality	O
of	O
voice	O
dictionaries180are	O
being	O
used	O
,	O
the	O
user	O
may	O
select	O
which	O
dictionary	O
is	O
to	O
be	O
used	O
for	O
outputting	O
speech	O
.	O
The	O
selection	O
may	O
be	O
performed	O
,	O
for	O
example	O
,	O
through	O
a	O
user	O
interface	O
(	O
not	O
shown	O
)	O
associated	O
with	O
the	O
voice	O
transformation	O
apparatus100	O
.	O
Based	O
on	O
the	O
user	O
's	O
selection	O
,	O
the	O
appropriate	O
voice	O
dictionary	O
may	O
be	O
activated	O
.	O
Alternatively	O
,	O
based	O
on	O
the	O
user	O
's	O
selection	O
,	O
the	O
symbolic	O
representations	O
forwarded	O
from	O
the	O
voice	O
recognition	O
device140to	O
the	O
voice	O
dictionary	O
interface150may	O
include	O
an	O
identifier	O
indicating	O
which	O
voice	O
directory180to	O
utilize	O
.	O

Thus	O
,	O
if	O
a	O
user	O
wishes	O
his/her	O
output	O
speech	O
to	O
resemble	O
,	O
for	O
example	O
,	O
a	O
celebrity	O
such	O
as	O
Arnold	O
Schwartzenegger	O
,	O
the	O
user	O
may	O
select	O
a	O
first	O
voice	O
dictionary	O
.	O
If	O
the	O
user	O
wishes	O
to	O
have	O
output	O
speech	O
resemble	O
a	O
cartoon	O
character	O
such	O
as	O
Bugs	O
Bunny	O
,	O
for	O
example	O
,	O
a	O
second	O
voice	O
dictionary	O
may	O
be	O
selected	O
.	O

The	O
above	O
description	O
of	O
the	O
voice	O
transformation	O
apparatus100assumes	O
that	O
the	O
voice	O
recognition	O
device140may	O
accurately	O
identify	O
all	O
of	O
the	O
sounds	O
in	O
the	O
input	O
voice	O
pattern	O
.	O
However	O
,	O
this	O
may	O
not	O
always	O
be	O
the	O
case	O
.	O
In	O
the	O
case	O
that	O
some	O
sounds	O
in	O
the	O
input	O
voice	O
pattern	O
may	O
not	O
be	O
recognized	O
,	O
the	O
voice	O
recognition	O
device140may	O
forward	O
the	O
unrecognized	O
segment	O
of	O
the	O
input	O
voice	O
pattern	O
to	O
the	O
speech	O
output	O
generator160without	O
performing	O
a	O
voice	O
dictionary	O
look-up	O
function	O
.	O
In	O
this	O
way	O
,	O
the	O
input	O
voice	O
pattern	O
segment	O
that	O
is	O
not	O
recognized	O
may	O
be	O
output	O
by	O
the	O
output	O
device190rather	O
than	O
performing	O
an	O
erroneous	O
look-up	O
of	O
an	O
output	O
voice	O
pattern	O
segment	O
.	O

In	O
addition	O
,	O
to	O
provide	O
a	O
more	O
graceful	O
transition	O
between	O
the	O
output	O
voice	O
pattern	O
segments	O
and	O
the	O
input	O
voice	O
pattern	O
segments	O
which	O
could	O
not	O
be	O
recognized	O
,	O
the	O
input	O
voice	O
pattern	O
segment	O
that	O
was	O
not	O
recognized	O
may	O
have	O
voice	O
pattern	O
characteristics	O
of	O
the	O
selected	O
voice	O
dictionary	O
speaker	O
applied	O
to	O
it	O
.	O
These	O
voice	O
pattern	O
characteristics	O
of	O
the	O
selected	O
voice	O
dictionary	O
speaker	O
may	O
be	O
obtained	O
from	O
the	O
voice	O
dictionary	O
as	O
a	O
default	O
setting	O
.	O
In	O
this	O
way	O
,	O
if	O
,	O
for	O
example	O
,	O
the	O
voice	O
dictionary	O
speaker	O
has	O
a	O
low	O
tone	O
voice	O
,	O
the	O
unrecognized	O
input	O
voice	O
pattern	O
segment	O
may	O
be	O
modified	O
to	O
more	O
closely	O
resemble	O
the	O
output	O
voice	O
pattern	O
segments	O
.	O

Additionally	O
,	O
some	O
sounds	O
that	O
are	O
input	O
via	O
the	O
voice	O
input	O
device170may	O
not	O
be	O
speech	O
at	O
all	O
,	O
i	O
.	O
e	O
.	O
a	O
horn	O
being	O
blown	O
.	O
In	O
the	O
event	O
that	O
a	O
non-speech	O
sound	O
is	O
received	O
,	O
the	O
voice	O
transformation	O
apparatus100may	O
forward	O
the	O
non-speech	O
sound	O
to	O
the	O
output	O
device190without	O
performing	O
voice	O
transformation	O
processing	O
on	O
the	O
non-speech	O
sound	O
.	O
The	O
voice	O
recognition	O
device140may	O
be	O
trained	O
such	O
that	O
it	O
may	O
recognize	O
non-speech	O
sounds	O
and	O
will	O
not	O
attempt	O
to	O
convert	O
these	O
non-speech	O
sounds	O
into	O
symbolic	O
representations	O
.	O

The	O
above	O
processing	O
of	O
input	O
voice	O
patterns	O
into	O
output	O
speech	O
patterns	O
is	O
preferably	O
performed	O
in	O
a	O
streaming	O
manner	O
.	O
Thus	O
,	O
as	O
voice	O
input	O
is	O
received	O
,	O
the	O
voice	O
transformation	O
apparatus100performs	O
the	O
voice	O
transformation	O
processing	O
on	O
those	O
portions	O
of	O
voice	O
input	O
received	O
and	O
outputs	O
the	O
speech	O
output	O
as	O
the	O
processing	O
is	O
completed	O
.	O
In	O
this	O
way	O
,	O
a	O
more	O
real-time	O
transformation	O
of	O
voice	O
input	O
into	O
speech	O
output	O
may	O
be	O
performed	O
.	O

With	O
the	O
present	O
invention	O
,	O
a	O
user	O
may	O
input	O
his/her	O
voice	O
and	O
designate	O
a	O
different	O
output	O
voice	O
from	O
his/her	O
own	O
to	O
be	O
used	O
for	O
outputting	O
transformed	O
speech	O
.	O
Furthermore	O
,	O
the	O
output	O
voice	O
may	O
more	O
closely	O
resemble	O
actual	O
human	O
speech	O
because	O
the	O
characteristics	O
of	O
the	O
user	O
's	O
input	O
voice	O
pattern	O
are	O
applied	O
to	O
the	O
output	O
voice	O
.	O
Thus	O
,	O
the	O
output	O
voice	O
will	O
use	O
the	O
same	O
voice	O
fluctuations	O
,	O
same	O
pitch	O
,	O
volume	O
,	O
etc	O
.	O
as	O
that	O
of	O
the	O
user	O
.	O

FIG	O
.	O
2is	O
an	O
exemplary	O
data	O
flow	O
describing	O
the	O
interaction	O
among	O
the	O
elements	O
of	O
FIG	O
.	O
1	O
.	O
As	O
shown	O
inFIG	O
.	O
2	O
,	O
the	O
voice	O
input	O
device170receives	O
voice	O
input	O
from	O
a	O
user	O
,	O
converts	O
the	O
voice	O
input	O
to	O
an	O
input	O
voice	O
pattern	O
and	O
forwards	O
the	O
input	O
voice	O
pattern	O
to	O
the	O
input	O
interface120	O
.	O
The	O
voice	O
recognition	O
device140and	O
the	O
input	O
voice	O
characteristic	O
extraction	O
device130then	O
process	O
the	O
input	O
voice	O
pattern	O
.	O
As	O
noted	O
above	O
,	O
althoughFIG	O
.	O
2depicts	O
this	O
processing	O
being	O
done	O
simultaneously	O
,	O
the	O
voice	O
recognition	O
device140may	O
perform	O
its	O
functions	O
before	O
or	O
after	O
the	O
input	O
voice	O
characteristic	O
extraction	O
device130performs	O
its	O
functions	O
.	O

The	O
voice	O
recognition	O
device140then	O
transforms	O
segments	O
of	O
the	O
input	O
voice	O
pattern	O
into	O
symbolic	O
representations	O
of	O
the	O
phonemes	O
that	O
make	O
up	O
the	O
input	O
voice	O
pattern	O
segments	O
.	O
Unrecognized	O
input	O
voice	O
pattern	O
segments	O
are	O
passed	O
to	O
the	O
speech	O
output	O
generator160without	O
further	O
voice	O
recognition	O
processing	O
.	O

The	O
symbolic	O
representation	O
of	O
the	O
phonemes	O
is	O
forwarded	O
to	O
the	O
voice	O
dictionary	O
interface150which	O
performs	O
a	O
look-up	O
in	O
the	O
voice	O
dictionary180to	O
find	O
target	O
speaker	O
voice	O
output	O
segments	O
corresponding	O
to	O
the	O
phonemes	O
.	O
The	O
target	O
speaker	O
voice	O
output	O
segments	O
are	O
then	O
forwarded	O
to	O
the	O
speech	O
output	O
generator160along	O
with	O
the	O
voice	O
characteristics	O
extracted	O
by	O
the	O
input	O
voice	O
characteristic	O
extraction	O
device130	O
.	O

The	O
speech	O
output	O
generator160applies	O
the	O
voice	O
characteristics	O
to	O
the	O
target	O
speaker	O
voice	O
output	O
segments	O
and	O
outputs	O
speech	O
output	O
signals	O
to	O
the	O
output	O
device190	O
.	O
The	O
output	O
device190then	O
produces	O
an	O
output	O
corresponding	O
to	O
the	O
speech	O
output	O
signals	O
.	O
The	O
output	O
may	O
take	O
the	O
form	O
of	O
audio	O
output	O
signals	O
or	O
may	O
be	O
,	O
for	O
example	O
,	O
data	O
signals	O
that	O
are	O
to	O
be	O
used	O
by	O
a	O
remote	O
device	O
for	O
outputting	O
audio	O
output	O
signals	O
.	O

Thus	O
,	O
with	O
the	O
present	O
invention	O
,	O
voice	O
input	O
may	O
be	O
transformed	O
into	O
a	O
different	O
voice	O
output	O
.	O
During	O
the	O
transformation	O
,	O
the	O
user	O
's	O
voice	O
characteristics	O
are	O
maintained	O
and	O
used	O
to	O
provide	O
a	O
more	O
realistic	O
synthesized	O
human	O
speech	O
output	O
.	O

FIG	O
.	O
3is	O
an	O
exemplary	O
diagram	O
of	O
a	O
voice	O
dictionary180according	O
to	O
the	O
present	O
invention	O
.	O
As	O
shown	O
inFIG	O
.	O
3	O
,	O
the	O
voice	O
dictionary180may	O
include	O
two	O
or	O
more	O
fields	O
.	O
Field310includes	O
symbolic	O
representations	O
of	O
input	O
voice	O
pattern	O
phonemes	O
.	O
Field320includes	O
target	O
speaker	O
voice	O
output	O
segments	O
.	O
Thus	O
,	O
by	O
performing	O
a	O
search	O
of	O
field310using	O
the	O
symbolic	O
representation	O
received	O
from	O
the	O
voice	O
recognition	O
device140	O
,	O
the	O
voice	O
dictionary	O
interface150may	O
retrieve	O
associated	O
target	O
speaker	O
voice	O
output	O
segments	O
from	O
field320	O
.	O
Additional	O
fields	O
may	O
be	O
included	O
in	O
the	O
voice	O
dictionary180in	O
accordance	O
with	O
the	O
present	O
invention	O
.	O

Although	O
a	O
simple	O
database	O
structure	O
is	O
shown	O
to	O
represent	O
the	O
voice	O
dictionary180	O
,	O
more	O
complex	O
and	O
more	O
efficient	O
manners	O
of	O
representing	O
the	O
correlation	O
between	O
the	O
symbolic	O
representations	O
in	O
field310and	O
the	O
target	O
speaker	O
voice	O
output	O
segments	O
may	O
be	O
employed	O
with	O
this	O
invention	O
.	O
For	O
example	O
,	O
a	O
two-level	O
hierarchical	O
dictionary	O
similar	O
to	O
that	O
described	O
in	O
co-pending	O
and	O
commonly	O
assigned	O
U.S.	O
patent	O
application	O
Ser	O
.	O
No.	O
09/148	O
,	O
828	O
,	O
which	O
is	O
hereby	O
incorporated	O
by	O
reference	O
,	O
may	O
be	O
used	O
with	O
the	O
present	O
invention	O
.	O
In	O
such	O
a	O
two-level	O
dictionary	O
,	O
a	O
top-level	O
key	O
into	O
the	O
dictionary	O
may	O
be	O
a	O
symbolic	O
representation	O
of	O
a	O
phoneme	O
or	O
other	O
phonic	O
symbol	O
from	O
the	O
input	O
voice	O
pattern	O
.	O
The	O
second	O
level	O
keys	O
may	O
be	O
a	O
series	O
of	O
keys	O
for	O
instantaneous	O
audio	O
samples	O
of	O
a	O
target	O
speaker	O
's	O
voice	O
.	O

The	O
two-level	O
dictionary	O
may	O
be	O
created	O
by	O
having	O
a	O
target	O
speaker	O
speak	O
a	O
predefined	O
set	O
of	O
sentences	O
comprising	O
an	O
entire	O
set	O
of	O
phonemes	O
in	O
the	O
target	O
speaker	O
language	O
.	O
The	O
target	O
speaker	O
's	O
speech	O
is	O
then	O
dissected	O
into	O
phonemes	O
,	O
each	O
phoneme	O
being	O
comprised	O
of	O
one	O
or	O
more	O
instantaneous	O
samples	O
from	O
the	O
target	O
speaker	O
's	O
speech	O
.	O
Preferably	O
,	O
the	O
instantaneous	O
samples	O
are	O
normalized	O
so	O
that	O
the	O
sample	O
is	O
monotonic	O
.	O
This	O
helps	O
to	O
eliminate	O
unwanted	O
voice	O
characteristics	O
,	O
such	O
as	O
any	O
pitch	O
rising	O
due	O
to	O
a	O
question	O
being	O
asked	O
.	O

Each	O
instantaneous	O
sample	O
is	O
given	O
a	O
second	O
level	O
key	O
and	O
its	O
value	O
(	O
the	O
sample	O
itself	O
)	O
is	O
stored	O
in	O
a	O
second-level	O
dictionary	O
.	O
Each	O
phoneme	O
is	O
stored	O
in	O
a	O
top-level	O
dictionary	O
with	O
corresponding	O
second-level	O
keys	O
of	O
the	O
instantaneous	O
samples	O
of	O
the	O
target	O
speaker	O
's	O
speech	O
that	O
make	O
up	O
the	O
phoneme	O
.	O

A	O
look-up	O
of	O
a	O
top	O
level	O
phoneme	O
in	O
the	O
top	O
level	O
dictionary	O
thereby	O
returns	O
an	O
appropriate	O
sequence	O
of	O
second	O
level	O
keys	O
,	O
each	O
second	O
level	O
key	O
being	O
translated	O
via	O
the	O
second	O
level	O
dictionary	O
into	O
its	O
corresponding	O
sample	O
.	O
The	O
natural	O
redundancy	O
of	O
a	O
person	O
's	O
voice	O
will	O
likely	O
result	O
in	O
much	O
reuse/repetition	O
of	O
instantaneous	O
samples	O
.	O
Thus	O
,	O
the	O
size	O
of	O
the	O
second-level	O
dictionary	O
may	O
be	O
minimized	O
by	O
making	O
use	O
of	O
the	O
same	O
instantaneous	O
sample	O
for	O
reproduction	O
of	O
different	O
phonemes	O
and	O
the	O
like	O
.	O
Other	O
methods	O
and	O
devices	O
for	O
storing	O
information	O
representing	O
a	O
correlation	O
between	O
symbolic	O
representations	O
of	O
phonemes	O
and	O
target	O
speaker	O
voice	O
output	O
segments	O
may	O
be	O
used	O
without	O
departing	O
from	O
the	O
spirit	O
and	O
scope	O
of	O
the	O
present	O
invention	O
.	O

FIG	O
.	O
4is	O
a	O
flowchart	O
outlining	O
an	O
exemplary	O
operation	O
of	O
the	O
voice	O
transformation	O
apparatus100	O
.	O
As	O
shown	O
inFIG	O
.	O
4	O
,	O
the	O
operation	O
starts	O
with	O
the	O
controller110receiving	O
an	O
input	O
voice	O
segment	O
from	O
a	O
voice	O
input	O
device170	O
(	O
step410	O
)	O
.	O
The	O
controller110instructs	O
the	O
input	O
voice	O
characteristic	O
extraction	O
device130to	O
extract	O
voice	O
characteristics	O
from	O
the	O
input	O
voice	O
segment	O
(	O
step420	O
)	O
.	O

Either	O
before	O
,	O
after	O
,	O
or	O
at	O
the	O
same	O
time	O
as	O
the	O
voice	O
characteristic	O
extraction	O
process	O
is	O
performed	O
,	O
the	O
controller110instructs	O
the	O
voice	O
recognition	O
device140to	O
perform	O
voice	O
recognition	O
and	O
convert	O
the	O
input	O
voice	O
segment	O
to	O
a	O
symbolic	O
representation	O
of	O
phonemes	O
(	O
step430	O
)	O
.	O
If	O
the	O
voice	O
recognition	O
device140is	O
able	O
to	O
convert	O
the	O
input	O
voice	O
segment	O
into	O
corresponding	O
symbolic	O
representations	O
of	O
phonemes	O
(	O
step440:YES	O
)	O
,	O
the	O
controller110instructs	O
the	O
voice	O
dictionary	O
interface150to	O
retrieve	O
target	O
speaker	O
voice	O
output	O
segments	O
(	O
step450	O
)	O
.	O
Otherwise	O
,	O
if	O
the	O
voice	O
recognition	O
device140is	O
unable	O
to	O
convert	O
the	O
input	O
voice	O
segment	O
into	O
corresponding	O
symbolic	O
representations	O
of	O
phonemes	O
(	O
step440	O
:	O
NO	O
)	O
,	O
the	O
controller110instructs	O
the	O
voice	O
dictionary	O
interface150to	O
retrieve	O
target	O
speaker	O
voice	O
characteristics	O
from	O
the	O
voice	O
dictionary180	O
(	O
step460	O
)	O
.	O

The	O
controller110then	O
instructs	O
the	O
speech	O
output	O
generator160to	O
apply	O
the	O
voice	O
characteristics	O
to	O
the	O
target	O
speaker	O
voice	O
output	O
segments	O
(	O
step470	O
)	O
.	O
The	O
voice	O
characteristics	O
may	O
be	O
,	O
for	O
example	O
,	O
the	O
voice	O
characteristics	O
extracted	O
by	O
the	O
input	O
voice	O
characteristic	O
extraction	O
device130and/or	O
the	O
target	O
speaker	O
voice	O
characteristics	O
retrieved	O
by	O
the	O
voice	O
dictionary	O
interface150	O
.	O
Which	O
is	O
used	O
depends	O
on	O
whether	O
or	O
not	O
the	O
voice	O
recognition	O
device140was	O
able	O
to	O
convert	O
the	O
input	O
voice	O
segment	O
into	O
a	O
symbolic	O
representation	O
.	O

The	O
speech	O
output	O
generator160then	O
generates	O
speech	O
output	O
signals	O
which	O
are	O
output	O
to	O
the	O
output	O
device190	O
(	O
step480	O
)	O
.	O
WhileFIG	O
.	O
4shows	O
the	O
operation	O
for	O
a	O
single	O
input	O
voice	O
segment	O
,	O
it	O
should	O
be	O
appreciated	O
that	O
this	O
process	O
may	O
be	O
repeated	O
for	O
each	O
input	O
voice	O
segment	O
.	O

The	O
above	O
embodiments	O
of	O
the	O
present	O
invention	O
have	O
assumed	O
that	O
the	O
voice	O
transformation	O
apparatus100is	O
an	O
integrated	O
apparatus	O
,	O
however	O
,	O
the	O
invention	O
is	O
not	O
limited	O
to	O
such	O
embodiments.FIG	O
.	O
5shows	O
an	O
exemplary	O
system500in	O
which	O
the	O
present	O
invention	O
may	O
be	O
employed	O
.	O
As	O
shown	O
inFIG	O
.	O
5	O
,	O
the	O
system500includes	O
two	O
user	O
devices510and520	O
,	O
at	O
least	O
one	O
network530	O
,	O
and	O
a	O
server540	O
.	O
The	O
user	O
devices510and520may	O
be	O
any	O
computerized	O
device	O
capable	O
of	O
performing	O
the	O
voice	O
transformation	O
functions	O
.	O
The	O
network	O
may	O
be	O
any	O
type	O
of	O
network	O
that	O
facilitates	O
the	O
transmission	O
of	O
data	O
from	O
one	O
device	O
to	O
another	O
.	O
In	O
a	O
preferred	O
embodiment	O
,	O
the	O
network530is	O
the	O
Internet	O
.	O

Portions	O
of	O
the	O
voice	O
transformation	O
apparatus100may	O
be	O
distributed	O
among	O
each	O
of	O
the	O
devices510,520and540	O
.	O
Each	O
device510,520and540may	O
have	O
one	O
or	O
more	O
of	O
the	O
elements	O
of	O
the	O
voice	O
transformation	O
apparatus100and	O
may	O
perform	O
the	O
corresponding	O
functions	O
.	O

For	O
example	O
,	O
the	O
user	O
devices510and520may	O
each	O
make	O
use	O
of	O
a	O
corresponding	O
voice	O
transformation	O
apparatus100	O
.	O
When	O
a	O
first	O
party	O
,	O
using	O
user	O
device510	O
,	O
wishes	O
to	O
communicate	O
with	O
a	O
second	O
party	O
,	O
using	O
user	O
device520	O
,	O
in	O
a	O
different	O
voice	O
from	O
his/her	O
own	O
voice	O
,	O
the	O
first	O
party	O
may	O
select	O
the	O
desired	O
voice	O
using	O
the	O
user	O
device510	O
(	O
thereby	O
selecting	O
a	O
corresponding	O
voice	O
dictionary	O
)	O
.	O
The	O
first	O
party	O
may	O
then	O
speak	O
into	O
a	O
voice	O
input	O
device	O
associated	O
with	O
the	O
user	O
device510and	O
a	O
voice	O
transformation	O
apparatus	O
associated	O
with	O
the	O
user	O
device510may	O
extract	O
voice	O
characteristics	O
and	O
perform	O
voice	O
recognition	O
on	O
the	O
first	O
party	O
's	O
speech	O
input	O
.	O

The	O
resulting	O
symbolic	O
representation	O
of	O
phonemes	O
,	O
the	O
voice	O
characteristic	O
data	O
,	O
and	O
an	O
identifier	O
of	O
the	O
selected	O
voice	O
dictionary	O
may	O
then	O
be	O
transmitted	O
to	O
the	O
user	O
device520	O
.	O
Upon	O
receipt	O
,	O
this	O
data	O
is	O
used	O
by	O
a	O
voice	O
dictionary	O
interface	O
to	O
retrieve	O
corresponding	O
voice	O
output	O
segments	O
.	O
The	O
voice	O
output	O
segments	O
along	O
with	O
the	O
voice	O
characteristic	O
data	O
is	O
then	O
provided	O
to	O
a	O
speech	O
output	O
generator	O
associated	O
with	O
user	O
device520and	O
speech	O
output	O
signals	O
are	O
thereby	O
generated	O
.	O
The	O
second	O
party	O
may	O
respond	O
,	O
in	O
kind	O
,	O
making	O
use	O
of	O
his/her	O
voice	O
transformation	O
apparatus	O
in	O
a	O
similar	O
manner	O
.	O

Alternatively	O
,	O
the	O
user	O
devices510and520may	O
not	O
include	O
any	O
of	O
the	O
elements	O
of	O
the	O
voice	O
transformation	O
apparatus100or	O
only	O
certain	O
portions	O
of	O
the	O
voice	O
transformation	O
apparatus100	O
.	O
Rather	O
,	O
the	O
server540may	O
perform	O
all	O
or	O
some	O
of	O
the	O
voice	O
transformation	O
apparatus100functions	O
.	O
With	O
such	O
an	O
embodiment	O
,	O
the	O
first	O
party	O
may	O
speak	O
into	O
a	O
voice	O
input	O
device	O
associated	O
with	O
the	O
user	O
device510and	O
have	O
the	O
voice	O
input	O
digitized	O
and	O
transmitted	O
to	O
the	O
server540	O
.	O
The	O
server540may	O
then	O
perform	O
the	O
appropriate	O
voice	O
transformation	O
apparatus100functions	O
on	O
the	O
received	O
voice	O
input	O
data	O
and	O
send	O
the	O
speech	O
output	O
signals	O
to	O
the	O
user	O
device520	O
.	O
Any	O
manner	O
of	O
distributing	O
the	O
elements	O
of	O
the	O
voice	O
transformation	O
apparatus100across	O
a	O
plurality	O
of	O
devices	O
coupled	O
via	O
a	O
network	O
may	O
be	O
used	O
without	O
departing	O
from	O
the	O
spirit	O
and	O
scope	O
of	O
the	O
present	O
invention	O
.	O

Thus	O
,	O
the	O
present	O
invention	O
provides	O
an	O
improved	O
method	O
,	O
apparatus	O
and	O
computer	O
implemented	O
instructions	O
for	O
transforming	O
one	O
voice	O
into	O
another	O
voice	O
in	O
real	O
time	O
.	O
The	O
mechanism	O
of	O
the	O
present	O
invention	O
avoids	O
mechanical	O
and	O
monotonic	O
qualities	O
of	O
presently	O
available	O
systems	O
.	O
This	O
advantage	O
is	O
provided	O
through	O
identifying	O
input	O
voice	O
characteristics	O
and	O
using	O
these	O
characteristics	O
to	O
modify	O
target	O
speaker	O
output	O
voice	O
segments	O
to	O
more	O
closely	O
resemble	O
human	O
speech	O
fluctuations	O
.	O
Thus	O
,	O
with	O
the	O
real	O
time	O
transformation	O
of	O
one	O
voice	O
into	O
another	O
voice	O
,	O
the	O
present	O
invention	O
may	O
be	O
used	O
in	O
various	O
applications	O
,	O
such	O
as	O
,	O
for	O
example	O
,	O
networked	O
video	O
games	O
,	O
speech	O
translation	O
from	O
one	O
language	O
to	O
another	O
language	O
,	O
reducing	O
accents	O
,	O
and	O
educational	O
tools	O
.	O

It	O
is	O
important	O
to	O
note	O
that	O
while	O
the	O
present	O
invention	O
has	O
been	O
described	O
in	O
the	O
context	O
of	O
a	O
fully	O
functioning	O
data	O
processing	O
system	O
,	O
those	O
of	O
ordinary	O
skill	O
in	O
the	O
art	O
will	O
appreciate	O
that	O
the	O
processes	O
of	O
the	O
present	O
invention	O
are	O
capable	O
of	O
being	O
distributed	O
in	O
the	O
form	O
of	O
a	O
computer	O
readable	O
medium	O
of	O
instructions	O
and	O
a	O
variety	O
of	O
forms	O
and	O
that	O
the	O
present	O
invention	O
applies	O
equally	O
regardless	O
of	O
the	O
particular	O
type	O
of	O
signal	O
bearing	O
media	O
actually	O
used	O
to	O
carry	O
out	O
the	O
distribution	O
.	O
Examples	O
of	O
computer	O
readable	O
media	O
include	O
recordable-type	O
media	O
such	O
a	O
floppy	O
disc	O
,	O
a	O
hard	O
disk	O
drive	O
,	O
a	O
RAM	O
,	O
and	O
CD-ROMs	O
and	O
transmission-type	O
media	O
such	O
as	O
digital	O
and	O
analog	O
communications	O
links	O
.	O

The	O
description	O
of	O
the	O
present	O
invention	O
has	O
been	O
presented	O
for	O
purposes	O
of	O
illustration	O
and	O
description	O
,	O
but	O
is	O
not	O
intended	O
to	O
be	O
exhaustive	O
or	O
limited	O
to	O
the	O
invention	O
in	O
the	O
form	O
disclosed	O
.	O
Many	O
modifications	O
and	O
variations	O
will	O
be	O
apparent	O
to	O
those	O
of	O
ordinary	O
skill	O
in	O
the	O
art	O
.	O
The	O
embodiment	O
was	O
chosen	O
and	O
described	O
in	O
order	O
to	O
best	O
explain	O
the	O
principles	O
of	O
the	O
invention	O
,	O
the	O
practical	O
application	O
,	O
and	O
to	O
enable	O
others	O
of	O
ordinary	O
skill	O
in	O
the	O
art	O
to	O
understand	O
the	O
invention	O
for	O
various	O
embodiments	O
with	O
various	O
modifications	O
as	O
are	O
suited	O
to	O
the	O
particular	O
use	O
contemplated	O
.	O

1	O
.	O
A	O
method	O
of	O
transforming	O
a	O
voice	O
input	O
into	O
a	O
voice	O
output	O
of	O
a	O
target	O
speaker	O
,	O
comprising:receiving	O
the	O
voice	O
input;extracting	O
voice	O
input	O
characteristics	O
from	O
the	O
voice	O
input;identifying	O
voice	O
input	O
segments	O
in	O
the	O
voice	O
input;identifying	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
corresponding	O
to	O
the	O
voice	O
input	O
segments;applying	O
the	O
voice	O
input	O
characteristics	O
to	O
the	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
to	O
obtain	O
output	O
segments;outputting	O
the	O
output	O
segments;identifying	O
portions	O
of	O
the	O
voice	O
input	O
that	O
cannot	O
be	O
identified	O
as	O
voice	O
input	O
segments	O
;	O
andoutputting	O
the	O
portions	O
of	O
the	O
voice	O
input	O
that	O
cannot	O
be	O
identified	O
as	O
voice	O
input	O
segments;wherein	O
outputting	O
the	O
portions	O
of	O
the	O
voice	O
input	O
that	O
cannot	O
be	O
identified	O
as	O
voice	O
input	O
segments	O
includes	O
identifying	O
target	O
speaker	O
output	O
voice	O
characteristics	O
;	O
andapplying	O
the	O
target	O
speaker	O
output	O
voice	O
characteristics	O
to	O
the	O
portions	O
of	O
the	O
voice	O
input	O
that	O
cannot	O
be	O
identified	O
as	O
voice	O
input	O
segments	O
.	O

2	O
.	O
The	O
method	O
ofclaim	O
1	O
,	O
wherein	O
extracting	O
voice	O
input	O
characteristics	O
from	O
the	O
voice	O
input	O
includes	O
filtering	O
the	O
voice	O
input	O
for	O
the	O
voice	O
input	O
characteristics	O
.	O

3	O
.	O
The	O
method	O
ofclaim	O
1	O
,	O
wherein	O
the	O
voice	O
input	O
characteristics	O
include	O
at	O
least	O
one	O
of	O
volume	O
,	O
pitch	O
and	O
pause	O
length	O
.	O

4	O
.	O
The	O
method	O
ofclaim	O
1	O
,	O
wherein	O
extracting	O
voice	O
input	O
characteristics	O
from	O
the	O
voice	O
input	O
and	O
identifying	O
voice	O
input	O
segments	O
in	O
the	O
voice	O
input	O
are	O
performed	O
at	O
a	O
same	O
time	O
.	O

5	O
.	O
The	O
method	O
ofclaim	O
1	O
,	O
wherein	O
the	O
voice	O
input	O
segments	O
are	O
at	O
least	O
one	O
of	O
words	O
and	O
phonemes	O
.	O

6	O
.	O
The	O
method	O
ofclaim	O
1	O
,	O
wherein	O
identifying	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
corresponding	O
to	O
the	O
voice	O
input	O
segments	O
includes	O
retrieving	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
from	O
a	O
voice	O
dictionary	O
.	O

7	O
.	O
The	O
method	O
ofclaim	O
1	O
,	O
wherein	O
identifying	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
corresponding	O
to	O
the	O
voice	O
input	O
segments	O
includes	O
retrieving	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
from	O
one	O
of	O
a	O
plurality	O
of	O
voice	O
dictionaries	O
.	O

8	O
.	O
The	O
method	O
ofclaim	O
7	O
,	O
wherein	O
identifying	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
corresponding	O
to	O
the	O
voice	O
input	O
segments	O
further	O
includes	O
identifying	O
one	O
of	O
the	O
plurality	O
of	O
voice	O
dictionaries	O
from	O
which	O
the	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
are	O
retrieved	O
based	O
on	O
a	O
user	O
selection	O
.	O

9	O
.	O
A	O
voice	O
transformation	O
apparatus	O
for	O
transforming	O
a	O
voice	O
input	O
to	O
a	O
voice	O
output	O
of	O
a	O
target	O
speaker	O
,	O
comprising:a	O
voice	O
input	O
device	O
interface	O
that	O
receives	O
the	O
voice	O
input	O
;	O
a	O
voice	O
input	O
characteristic	O
extraction	O
device	O
that	O
extracts	O
voice	O
input	O
characteristics	O
from	O
the	O
voice	O
input	O
;	O
a	O
speech	O
recognition	O
device	O
that	O
identifies	O
voice	O
input	O
segments	O
in	O
the	O
voice	O
input	O
;	O
a	O
voice	O
dictionary	O
interface	O
that	O
identifies	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
corresponding	O
to	O
the	O
voice	O
input	O
segments	O
;	O
andan	O
output	O
device	O
that	O
applies	O
the	O
voice	O
input	O
characteristics	O
to	O
the	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
to	O
obtain	O
output	O
segments	O
and	O
outputs	O
the	O
output	O
segments;wherein	O
the	O
speech	O
recognition	O
device	O
identifies	O
portions	O
of	O
the	O
voice	O
input	O
that	O
cannot	O
be	O
identified	O
as	O
voice	O
input	O
segments	O
,	O
and	O
wherein	O
the	O
output	O
device	O
outputs	O
the	O
portions	O
of	O
the	O
voice	O
input	O
that	O
cannot	O
be	O
identified	O
as	O
voice	O
input	O
segments;wherein	O
the	O
output	O
device	O
identifies	O
target	O
speaker	O
output	O
voice	O
characteristics	O
and	O
applies	O
the	O
target	O
speaker	O
output	O
voice	O
characteristics	O
to	O
the	O
portions	O
of	O
the	O
voice	O
input	O
that	O
cannot	O
be	O
identified	O
as	O
voice	O
input	O
segments	O
prior	O
to	O
outputting	O
the	O
portions	O
of	O
the	O
voice	O
input	O
that	O
cannot	O
be	O
identified	O
as	O
voice	O
input	O
segments	O
.	O

10	O
.	O
The	O
apparatus	O
ofclaim	O
9	O
,	O
wherein	O
the	O
voice	O
input	O
characteristic	O
extracting	O
device	O
extracts	O
voice	O
input	O
characteristics	O
from	O
the	O
voice	O
input	O
by	O
filtering	O
the	O
voice	O
input	O
for	O
the	O
voice	O
input	O
characteristics	O
.	O

11	O
.	O
The	O
apparatus	O
ofclaim	O
9	O
,	O
wherein	O
the	O
voice	O
input	O
characteristics	O
include	O
at	O
least	O
one	O
of	O
volume	O
,	O
pitch	O
and	O
pause	O
length	O
.	O

12	O
.	O
The	O
apparatus	O
ofclaim	O
9	O
,	O
wherein	O
the	O
voice	O
input	O
characteristic	O
extraction	O
device	O
extracts	O
voice	O
input	O
characteristics	O
from	O
the	O
voice	O
input	O
and	O
the	O
speech	O
recognition	O
device	O
identifies	O
voice	O
input	O
segments	O
in	O
the	O
voice	O
input	O
at	O
a	O
same	O
time	O
.	O

13	O
.	O
The	O
apparatus	O
ofclaim	O
9	O
,	O
wherein	O
the	O
voice	O
input	O
segments	O
are	O
at	O
least	O
one	O
of	O
words	O
and	O
phonemes	O
.	O

14	O
.	O
The	O
apparatus	O
ofclaim	O
9	O
,	O
wherein	O
the	O
voice	O
dictionary	O
interface	O
identifies	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
corresponding	O
to	O
the	O
voice	O
input	O
segments	O
by	O
retrieving	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
from	O
a	O
voice	O
dictionary	O
.	O

15	O
.	O
The	O
apparatus	O
ofclaim	O
9	O
,	O
wherein	O
the	O
voice	O
dictionary	O
interface	O
identifies	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
corresponding	O
to	O
the	O
voice	O
input	O
segments	O
by	O
retrieving	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
from	O
one	O
of	O
a	O
plurality	O
of	O
voice	O
dictionaries	O
.	O

16	O
.	O
The	O
apparatus	O
ofclaim	O
15	O
,	O
wherein	O
the	O
voice	O
dictionary	O
interface	O
identifies	O
one	O
of	O
the	O
plurality	O
of	O
voice	O
dictionaries	O
from	O
which	O
the	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
are	O
retrieved	O
based	O
on	O
a	O
user	O
selection	O
.	O

17	O
.	O
A	O
computer	O
program	O
product	O
in	O
a	O
computer	O
readable	O
medium	O
for	O
transforming	O
a	O
voice	O
input	O
into	O
a	O
voice	O
output	O
of	O
a	O
target	O
speaker	O
,	O
comprising:first	O
instructions	O
for	O
extracting	O
voice	O
input	O
characteristics	O
from	O
the	O
voice	O
input;second	O
instructions	O
for	O
identifying	O
voice	O
input	O
segments	O
in	O
the	O
voice	O
input;third	O
instructions	O
for	O
identifying	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
corresponding	O
to	O
the	O
voice	O
input	O
segments;fourth	O
instructions	O
for	O
applying	O
the	O
voice	O
input	O
characteristics	O
to	O
the	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
to	O
obtain	O
output	O
segments	O
;	O
andfifth	O
instructions	O
for	O
outputting	O
the	O
output	O
segments;sixth	O
instructions	O
for	O
identifying	O
portions	O
of	O
the	O
voice	O
input	O
that	O
cannot	O
be	O
identified	O
as	O
voice	O
input	O
segments	O
;	O
andseventh	O
instructions	O
for	O
outputting	O
the	O
portions	O
of	O
the	O
voice	O
input	O
that	O
cannot	O
be	O
identified	O
as	O
voice	O
input	O
segments;wherein	O
the	O
seventh	O
instructions	O
include	O
instructions	O
for	O
identifying	O
target	O
speaker	O
output	O
voice	O
characteristics	O
and	O
instructions	O
for	O
applying	O
the	O
target	O
speaker	O
output	O
voice	O
characteristics	O
to	O
the	O
portions	O
of	O
the	O
voice	O
input	O
that	O
cannot	O
be	O
identified	O
as	O
voice	O
input	O
segments	O
.	O

18	O
.	O
The	O
computer	O
program	O
product	O
ofclaim	O
17	O
,	O
wherein	O
the	O
first	O
instructions	O
include	O
instructions	O
for	O
filtering	O
the	O
voice	O
input	O
for	O
the	O
voice	O
input	O
characteristics	O
.	O

19	O
.	O
The	O
computer	O
program	O
product	O
ofclaim	O
17	O
,	O
wherein	O
the	O
voice	O
input	O
characteristics	O
include	O
at	O
least	O
one	O
of	O
volume	O
,	O
pitch	O
and	O
pause	O
length	O
.	O

20	O
.	O
The	O
computer	O
program	O
product	O
ofclaim	O
17	O
,	O
wherein	O
the	O
first	O
instructions	O
and	O
the	O
second	O
instructions	O
are	O
executed	O
at	O
a	O
same	O
time	O
.	O

21	O
.	O
The	O
computer	O
program	O
product	O
ofclaim	O
17	O
,	O
wherein	O
the	O
voice	O
input	O
segments	O
are	O
at	O
least	O
one	O
of	O
words	O
and	O
phonemes	O
.	O

22	O
.	O
The	O
computer	O
program	O
product	O
ofclaim	O
17	O
,	O
wherein	O
the	O
third	O
instructions	O
include	O
instructions	O
for	O
retrieving	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
from	O
a	O
voice	O
dictionary	O
.	O

23	O
.	O
The	O
computer	O
program	O
product	O
ofclaim	O
17	O
,	O
wherein	O
the	O
third	O
instructions	O
include	O
instructions	O
for	O
retrieving	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
from	O
one	O
of	O
a	O
plurality	O
of	O
voice	O
dictionaries	O
.	O

24	O
.	O
The	O
computer	O
program	O
product	O
ofclaim	O
23	O
,	O
wherein	O
the	O
third	O
instructions	O
further	O
include	O
instructions	O
for	O
identifying	O
one	O
of	O
the	O
plurality	O
of	O
voice	O
dictionaries	O
from	O
which	O
the	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
are	O
retrieved	O
based	O
on	O
a	O
user	O
selection	O
.	O

25	O
.	O
A	O
system	O
for	O
transforming	O
a	O
voice	O
input	O
into	O
a	O
voice	O
output	O
of	O
a	O
target	O
speaker	O
,	O
comprising:a	O
first	O
device	O
;	O
a	O
second	O
device	O
;	O
anda	O
network	O
coupled	O
to	O
both	O
the	O
first	O
device	O
and	O
the	O
second	O
device	O
,	O
wherein	O
the	O
first	O
device	O
receives	O
the	O
voice	O
input	O
,	O
converts	O
the	O
voice	O
input	O
into	O
a	O
symbolic	O
representation	O
of	O
voice	O
input	O
segments	O
,	O
extracts	O
voice	O
input	O
characteristics	O
from	O
the	O
voice	O
input	O
,	O
and	O
transmits	O
the	O
symbolic	O
representation	O
of	O
voice	O
input	O
segments	O
and	O
the	O
voice	O
input	O
characteristics	O
across	O
the	O
network	O
to	O
the	O
second	O
device	O
,	O
and	O
wherein	O
the	O
second	O
device	O
identifies	O
target	O
speaker	O
output	O
segments	O
corresponding	O
to	O
the	O
symbolic	O
representation	O
of	O
voice	O
input	O
segments	O
,	O
applies	O
the	O
voice	O
input	O
characteristics	O
to	O
the	O
target	O
speaker	O
output	O
segments	O
to	O
produce	O
the	O
voice	O
output	O
,	O
and	O
outputs	O
the	O
voice	O
output;wherein	O
the	O
first	O
device	O
identifies	O
portions	O
of	O
the	O
voice	O
input	O
that	O
cannot	O
be	O
identified	O
as	O
voice	O
input	O
segments	O
,	O
and	O
wherein	O
the	O
first	O
device	O
transmits	O
the	O
portions	O
of	O
the	O
voice	O
input	O
that	O
cannot	O
be	O
identified	O
as	O
voice	O
input	O
segments	O
to	O
the	O
second	O
device	O
via	O
the	O
network;wherein	O
the	O
second	O
device	O
identifies	O
target	O
speaker	O
output	O
voice	O
characteristics	O
,	O
applies	O
the	O
target	O
speaker	O
output	O
voice	O
characteristics	O
to	O
the	O
portions	O
of	O
the	O
voice	O
input	O
that	O
cannot	O
be	O
identified	O
as	O
voice	O
input	O
segments	O
,	O
and	O
outputs	O
the	O
portions	O
of	O
the	O
voice	O
input	O
that	O
cannot	O
be	O
identified	O
as	O
voice	O
input	O
segments	O
.	O

26	O
.	O
The	O
system	O
ofclaim	O
25	O
,	O
wherein	O
the	O
first	O
device	O
extracts	O
voice	O
input	O
characteristics	O
from	O
the	O
voice	O
input	O
by	O
filtering	O
the	O
voice	O
input	O
for	O
the	O
voice	O
input	O
characteristics	O
.	O

27	O
.	O
The	O
system	O
ofclaim	O
25	O
,	O
wherein	O
the	O
voice	O
input	O
characteristics	O
include	O
at	O
least	O
one	O
of	O
volume	O
,	O
pitch	O
and	O
pause	O
length	O
.	O

28	O
.	O
The	O
system	O
ofclaim	O
25	O
,	O
wherein	O
the	O
first	O
device	O
extracts	O
voice	O
input	O
characteristics	O
from	O
the	O
voice	O
input	O
and	O
converts	O
the	O
voice	O
input	O
into	O
a	O
symbolic	O
representation	O
of	O
voice	O
input	O
segments	O
at	O
a	O
same	O
time	O
.	O

29	O
.	O
The	O
system	O
ofclaim	O
25	O
,	O
wherein	O
the	O
voice	O
input	O
segments	O
are	O
at	O
least	O
one	O
of	O
words	O
and	O
phonemes	O
.	O

30	O
.	O
The	O
system	O
ofclaim	O
25	O
,	O
wherein	O
the	O
second	O
device	O
identifies	O
target	O
speaker	O
output	O
segments	O
corresponding	O
to	O
the	O
symbolic	O
representation	O
of	O
voice	O
input	O
segments	O
by	O
retrieving	O
target	O
speaker	O
output	O
segments	O
from	O
a	O
voice	O
dictionary	O
.	O

31	O
.	O
The	O
system	O
ofclaim	O
25	O
,	O
wherein	O
the	O
second	O
device	O
identifies	O
target	O
speaker	O
output	O
segments	O
corresponding	O
to	O
the	O
symbolic	O
representation	O
of	O
voice	O
input	O
segments	O
by	O
retrieving	O
target	O
speaker	O
output	O
segments	O
from	O
one	O
of	O
a	O
plurality	O
of	O
voice	O
dictionaries	O
.	O

32	O
.	O
The	O
system	O
ofclaim	O
31	O
,	O
wherein	O
the	O
first	O
device	O
identifies	O
one	O
of	O
the	O
plurality	O
of	O
voice	O
dictionaries	O
from	O
which	O
the	O
voice	O
output	O
segments	O
of	O
the	O
target	O
speaker	O
are	O
retrieved	O
based	O
on	O
a	O
user	O
selection	O
and	O
transmits	O
an	O
identifier	O
of	O
the	O
one	O
of	O
the	O
plurality	O
of	O
voice	O
dictionaries	O
to	O
the	O
second	O
device	O
via	O
the	O
network	O
.	O

